Alphachain: a self-potentiating, decentralised crypto-spool for independent journalism July 24, 2017 Kadhim Shubber; Paul Murphy; Matthew Klein; Alexandra Scaggs; Cardiff Garcia; Bryce Elder; Joseph Cotterill; Dan McCrum; David Keohane; and, involuntarily, Izabella Kaminska. ”Never stay up on the barren heights of cleverness, but come down into the green valleys of silliness.” – Wittgenstein  Publishing is under attack from all sides. New technologies have been both a boon and a bone for journalism. The spread of cryptocurrencies offers a new paradigm for independent reporting, which we are appetent to engage. Alphachain empowers smart-contract bubble journalism, decentralising hack finance for a trustless news protocol with a deep commitment to verified insecurity. Microtransactions enable macro-story publication, which readers access through a token-mediated crypto-portal. The platform reverses the content to consumption value transfer, thanks to its upfront multi-party funding protocol. This scalable and transparent capital reversal model ensures maximum foundation security, without compromising best-in-class trustless payment collection. Our supply constrained approach to journalism ensures a self-correcting market mechanism for news dissemination, incentivised by the coin distribution dynamics of Alphachains smart contracting. Alphachain believes funding its principals, while adhering to a self-referential governance framework, is the foundation of a party-to-party hack network. 1  For this reason, we are launching an initial coin offering on August 1st, the day of our crypt-dependence. Each coin, or token, offers its owner a perpetual subscription to the Alphachain publishing network. Tokens are fully fungible in a Turing-complete context, enabling subsequent resale or lease transactions with Zero-Day settlement invulnerability. The initial offering will be un-capped, meaning the more tokens sold, the more readers will be able to access our stories, and hence the more value ascribed to our content. This will increase the desirability of the tokens, allowing readers to share in the value uplift built into the Alphachain system. Alphachain represents a quantum leap upward in platform-rigorous cryptopublishing. Smile with us on this exciting and profitable endeavour into the techno-future.  Disclaimer 1. Alphaville may or may not be inclined to build the Alphachain network. This paper does not represent an obligation, promise, duty, suggestion or implied contract to do so. 2. The Alphachain tokens do not have any rights, uses, purpose, attributes, functionalities or features, express or implied, including, without limitation, any uses, purpose, attributes, functionalities or features on the Alphachain Platform. 3. The proceeds of the Alphachain initial coin offering may be used for any purpose, stated or otherwise. 4. Alphachain tokens are not securities, commodities, swaps on either securities or commodities, or similar financial instruments. They are not designed for investment or speculative purposes and should not be considered as a type of investment. The tokens are not available for purchase by the residents of any country. 5. The value of tokens may go up, up a lot, up a great deal, or up a tremendous amount. It is also possible they may not, or be subject to diminished prices. No prediction or forecast should be inferred, tendered or predicated on any of the aforementioned statements. 6. Alphachain tokens are not emoluments.  2  Binance Exchange www.binance.com  Whitepaper V1.2  Intro  3  Problems  3  Binance Exchange Matching Engine Feature Rollout Coins Device Coverage  4 4 4 5 5  Multilingual Support UI Preview  5 6  Revenue Model  7  Binance Coin (BNB) Allocation  7 8  ICO ICO Schedule BNB Value & The Burn BNB Vesting Plan for the Team Funds Usage Team Changpeng Zhao - CEO  8 8 9 9 10 10 10  Roger Wang - CTO James Hofbauer - Chief Architect Paul Jankunas - VP of Engineering  11 12 12  Allan Yan - Product Director Sunny Li - Operations Director  13 13  Investors & Advisors  14  Risks Security is Paramount Market Competition  17 17 17  Intro In our view, there are fundamentally two different types of exchanges: the ones that deal with fiat currency; and the ones that deal purely in crypto. It is the latter one that we will focus on. Even though they are small now, we strongly believe that pure crypto exchanges will be bigger, many times bigger, than fiat based exchanges in the near future. They will play an ever more important role in world finance and we call this new paradigm ​Binance​; Binary Finance. With your help, ​Binance will buildf a world-class crypto exchange, powering the future of crypto finance.  Problems Some of the current crypto exchanges suffer from a number of problems:   Poor technical architecture Many exchanges are “put together quickly”, by good tech people, but who have little or no experience in finance or in operating an exchange. They often choose the simplest approach to get the system up and running. While this may work well in the beginning, as traffic grows, the system will not able to handle the increased load. Exchange systems need to be engineered from the ground up with security, efficiency, speed, and scalability in mind. This often slows down the initial development, but is critical for long-term success. Our team has decades of combined experience building and maintaining world class financial systems that shape the economy. We understand how these systems are built from the ground up.    Insecure platform There are hundreds of exchanges that went down due to being hacked1. Binance is built to high standards, audited, and penetration tested. We have experience building financial systems to the highest security standards and strive to ensure security first.    1  Poor market liquidity Professional traders and normal users are significantly affected by this. Having a shallow orderbook means high slippage when trading, which is very expensive for traders. Getting miners, institutional investors and large traders into a new exchange is a chicken and egg problem, and requires a team with deep industry resources.  ​https://bitcointalk.org/index.php?topic=576337  Binance’s team have been in both the finance and crypto industry for many years. The team has worked on and operated a number of exchanges, and have accumulated a large network of partners in this space. These partners will be key in bootstrapping the exchange.   Poor customer service Traders are a different breed when it comes to users. Understanding the trader mentality is vital for running a successful exchange. Money is literally on-the-line. Many exchanges service traders as if they were running a social media site. A 3-second delay in seeing your friends’ status update would hardly be noticed, but on an exchange, the same would be unacceptable, resulting in a torrent of user complaints. In additional to the technology stack, Binance is built with service in mind. Binance shares support responsibilities across the entire staff and company. When a trader has a problem, they get an answer directly from someone who knows the system and not someone reading from a script.    Poor internationalization and language support Blockchains have no borders. Most exchanges focus only on one language or one country. Our international multi-lingual team has extensive working experience in North America, Europe and Asia, and we are able to smoothly support the global market.  Binance Exchange Matching Engine Our matching engine is capable of sustaining 1,400,000 orders / second, making Binance one of the fastest exchanges in the market today. You can be certain, on our exchange, that your orders will never be stuck due to the matching engine being overwhelmed.  Feature Rollout We will roll out the platform in roughly the following order:  Spot trading  Margin trading  Futures  Anonymous instant exchange  Decentralized (on-chain) exchange  and more…  Coins Binance will support trading pairs in the following coins:  BTC  ETH  LTC  NEO (ANS)  BNB (Binance Coin) More coins will be added over time. We generally will only add coins that have strong credibility, user base, and liquidity. If you have a coin that you wish to be listed on Binance later, participating in our ICO will help. We have no plans to support any fiat currencies such as USD, RMB, JPY, or KRX.  Device Coverage We will provide cross-platform trading clients for:  Web-based trading client  Android native client  iOS native client (pending App Store review)  Mobile HTML5 client (including WeChat H5 client)  PC (Windows) native client  REST API  Multilingual Support We will support English, Chinese，Japanese and Korean on all of our user interfaces. (The very initial release will be in English and Chinese only.) More languages will be added over time.  UI Preview  Professional Web Trading Interface  Mobile HTML5 Market Data  iOS Trading Page  Windows PC Native Client - Multi-Interval View  Revenue Model Binance’s revenue will come from the following sources: Source  Description  Exchange Fee  Binance initially will charge a 0.2% fixed fee per trade. Other variations will be subsequently introduced, including maker-taker, volumed based tiering and 0 fee promotions. We have no plan to charge above 0.2%.  Withdrawal Fee  Binance may charge a small fee for withdrawals.  Listing Fee  Binance will select innovative coins and other assets to be listed on the exchange, there may be a fee associated with those listings.  Margin Fee  If you trade on margin, there may be a fee or interest on the borrowed amount.  Other Fees  There may be other fees the platform may collect for various services such as automated algorithmic order etc.  Binance Coin (BNB) We will issue our token coin, called the Binance Coin. A strict limit of 200MM BNB will be created, never to be increased. BNB will run natively on the Ethereum blockchain with ERC 20.  Allocation %  Amount (BNB)  Participant  50%  100,000,000  ICO  40%  80,000,000  Founding Team  10%  20,000,000  Angel investors  ICO The ICO will be done in BTC and ETH, on multiple platforms around the world.  ICO Schedule All times below are China Standard Time (CST), UTC+8 hours. Date  Task  2017/06/14  Confirmed start of the Binance project  2017/06/16  Initial draft white paper completed, circulated to potential angel investors  2017/06/22  Announce Binance ICO plan, and release whitepaper to general public  2017/07/01  ICO starts (platforms will be announced soon)  2017/07/15  Binance.com release v0.1 go live, active trading begins  2017/07/21  ICO finishes, or whenever the coins are sold out  ICO will start from 3PM July 1st, investors can purchase BNB tokens in 3 phases on a first-come, first-served basis until 100,000,000 tokens are sold. As each new phase starts, the price will increase. Investors will receive BNB tokens within 5 working days after the ICO finishes. The detailed schedule is as below: ICO Phase  1st week  2nd week  3rd week  CST/GMT+8 15:00 July 1th 15:00 July 7th  15:00 July 7th 15:00 July 14th  15:00 July 14th 15:00 July 21th  1ETH  2700 BNB  2500 BNB  2300 BNB  1BTC  Based on market price  BNB Value & The Burn You can use BNB to pay for any fees on our platform, including but not limited to:  Exchange fees  Withdraw fees  Listing fees  Any other fee When you use BNB to pay for fees, you will receive a significant discount:  Discount Rate  1st year  2nd year  3rd year  4th year  5th year  50%  25%  12.5%  6.75%  no discount  The Burn Every quarter, we will destroy BNB based on the trading volume on our crypto-to-crypto platform until we destroy 50% of all the BNB. All transactions will be on the blockchain. We eventually will destroy 100MM BNB, leaving 100MM BNB remaining. Decentralized Exchange In the future, Binance will build a decentralized exchange, where BNB will be used as one of the key base assets as well as gas to be spent.  BNB Vesting Plan for the Team Initial release: After 1 year: After 2 year: After 3 year: After 4 year:  20% (16MM) 20% (16MM) 20% (16MM) 20% (16MM) 20% (16MM)  Funds Usage       35% of the funds will be used to build the Binance platform and perform upgrades to the system, which includes team recruiting, training, and the development budget. 50% will be used for Binance branding and marketing, including continuous promotion and education of Binance and blockchain innovations in industry mediums. A sufficient budget for various advertisement activities, to help Binance become popular among investors, and to attract active users to the platform. 15% will be kept in reserve to cope with any emergency or unexpected situation that might come up.  Team We have a solid team led by Changpeng Zhao, with both traditional wall street finance and cryptocurrency experience. We have a track record of successful startups under our belt.  Changpeng Zhao - CEO (aka CZ in the crypto community) ​LinkedIn Profile CZ is the founder and CEO of BijieTech, a company that provides cloud-based exchange systems to exchange operators. Since founding in Sept 2015, BijieTech now powers 30+ exchanges in Asia. In the first 12 months since founding, BijieTech closed 36.1 million RMB ($5.3MM USD) in revenue, and will double that in its second year. BijieTech has never accepted any outside investments, being cash flow positive from day one. As soon as the Binance ICO finishes, CZ will remain a shareholder of BijieTech, but will relinquish all of his management duties to a new CEO. CZ will focus exclusively on Binance. This applies to all BijieTech members listed in this whitepaper. Prior to BijieTech, CZ was the co-founder and CTO of OKCoin. During his stay there, OKCoin launched their international site, and their futures trading platform. Co-ordinating with Stefan Thomas, CZ also lead the first proof-of-reserves in any China crypto exchange. Most other major exchanges in China followed soon after. In addition to managing the tech team there, he also lead the international marketing team. He is still mentor to and good friends with Zane Tackett.  Before OKCoin, CZ was the Head of Technology and the 3rd person to join the Blockchain.info team. He worked closely with Ben Reeves, Roger Ver, Anthony Antonopoulos and Nicolas Cary to grow the Blockchain.info service. Before Blockchain.info, CZ co-founded Fusion Systems Ltd in 2005, a company that specializes in ultra-low-latency trading systems for brokers. Fusion Systems was started in Shanghai, and currently has offices in Tokyo, Hong Kong, and Los Angeles. Among other tasks, CZ was responsible closing and deploying trading systems at Credit Suisse, Goldman Sachs, Deutsche Bank, and more. CZ left Fusion System to work full time in the blockchain industry in 2013. Before Fusion Systems, CZ was the Head of Development at Bloomberg Tradebook Futures for 4 years, in New York. There CZ managed a team that was responsible for the entire futures trading platform in Bloomberg, with annual revenues exceeding $300 million USD. Prior to Bloomberg, CZ’s college internship and first job out of college was in Tokyo, working for a tech outsource company that was involved in developing trading systems for the Tokyo Stock Exchange. This is where his exchange experience began. CZ was born in China and went to high school and college in Canada. CZ is fully bilingual in English and Chinese, and can speak basic Japanese.  Roger Wang - CTO LinkedIn Profile Roger is a co-founder and the CTO. He has been working in the financial industry for 10+ years, responsible for building up technical teams, designing the high level architecture of the exchange and clearing systems, and running ops teams to ensure the security and stability of exchange systems. Prior to BijieTech, Roger worked at Nomura Securities, the largest investment bank in Japan. He was responsible for a global credit booking, analytics, and marking system, which supported thousands of global traders and analysts. He has also successfully implemented a smart bond matching engine, which consumes firm wide trading/order/position data, client enquiry info, 3rd party data as well as public bond data, to discover business opportunities for the firm using sophisticated custom built algorithms. It now generates over 100MM USD revenue annually for Nomura. Before Nomura, he was a tech leader in Morgan Stanley, where he designed and built a financial TB level data warehouse, which supported a large number of users to  do real time data analysis and financial modeling. He was also a core developer for low latency algorithmic trading systems, which served the firm’s largest clients like Blackstone and Wellington Fund, that delivered significant commission income for Morgan Stanley. Roger is fully trilingual in English, Chinese and Japanese.  James Hofbauer - Chief Architect LinkedIn Profile James is a co-founder and the Chief Architect of BijieTech. He architects and implements the core matching engine and its middleware. He also oversees client exchanges' public endpoints to ensure security and high performance. Before BijieTech, James worked at Palantir, a Silicon Valley company that focuses on big data analysis. Palantir's large-scale high-performance systems are used for cyber-security, anti-money laundering, fraud detection, counter-terrorism, and many other data relationship analysis purposes by both private and government entities. Before Palantir, James worked at Fusion Systems. A notable project James worked on was a global investment bank's systems architecture redesign, focusing on reducing the number of systems, encrypting and securely handling sensitive data, and introduced a new user security system which provides authentication, authorization, and action audit logging. James was raised and educated in America, earning a Bachelor’s of Science, ​Cum Laude,​ in Computer Science. He is bilingual, native in English and fluent in Japanese (JLPT N2 certified), and has lived in Japan for over 10 years. James has known CZ for 7 years and they have worked in two startups together.  Paul Jankunas - VP of Engineering LinkedIn Profile Paul is the VP of Engineering at BijieTech, responsible for the C++ implementation of the core machine engine. He has over 15 years of experience in developing exchange systems and financial trading applications. He is constantly looking for new ways to improve the performance and scalability of the system.  Prior to BijieTech, Paul worked at SBI BITS, part of SBI Group, in Tokyo. SBI Group is a listed financial services company with interests in a wide assortment of businesses. Paul was responsible for both client and server side development for trading applications. Before that, Paul worked at Fusions Systems in Tokyo as the Head of Development on Raptor, a market gateway with latencies under 2 microseconds, and before that for Bloomberg in New York. Paul has known CZ for 9 years and they have worked together in 3 companies.  Allan Yan - Product Director LinkedIn Profile Allan is a co-founder and the Product Director of BijieTech. Allan has over 10 years of experience in product design, user experience and trading. He drives the innovations in the exchange systems built by BijieTech, and pushes the product far ahead of the the competition in this ultra competitive space. Before BijieTech, Allan worked in Orient International Holding, which is one of the biggest import & export firms in Shanghai. He was responsible for the implementation of several informationization products, including ERP and e-Fax. Meanwhile, he led a variety of game and VOD content platforms.  Sunny Li - Operations Director LinkedIn Profile Sunny is a co-founder and the Operations Director of BijieTech. He has many years of management and technology consulting experience, has led 20+ exchange systems projects, and provided comprehensive consulting for strategy, operations, risk control and system development. Prior to BijieTech, Sunny worked at Accenture as the senior consultant. He provided many Top 500 companies for strategic and IT consulting, and l​ed a number of IOT, big data, ERP information integration systems projects.  Investors & Advisors In no particular order.  Matthew Roszak  Roger Ver  Ron Cao  Bloq co-founder. Tally Capital Founding Partner.  Angel investors in many blockchain businesses. CEO of Bitcoin.com.  MD of Sky9 Capital Institutional investor in BTCChina.  Chandler Guo  He Yi  Yang Linke  Angel Investor in blockchain businesses in China.  CMO of Yixia Technology. Previously Co-founder at OKCoin.  Co-founder of BTCChina. ICOCoin Founder.  Zhao Dong  Da Hongfei  Jun Du  One of the largest crypto OTC brokers in China.  AntShares Founder. Onchain CEO.  Co-founder of Huobi. Angel investor.  Vincent Zhou  Lu Bin  Liu Sutong  Founder of FinTech Blockchain Group. Active angel blockchain investor.  CEO of Andui.com, a blockchain financial service company in China  Finance Channel TV Anchor. CEO of Heng Pool.  Eric Zhang  Leah Zhang  Wang Qijun  AntShares Core Member. Lead match- engine developer at Huobi.com  CMO of F2Pool. Previously Investment Manager of AngelCrunch.  Co-founder of Andui. Formerly Marketing at Blockchain.info  Roy Zou  Jackie Wang  Li Da  CEO of Bitkio, Secretarl at Ethereum Classic Consortium (ECC)  Founding team member of Bitbank.com and BW.com and CHBTC.  Co-founder of JiulianTech.  Xiaoning Nan Founder of BitOcean.  Jeff Cui  Guicheng Xiong  Founder and CEO of TKing.cn. Tech lead at Morgan Stanley.  Co-founder of 91 Wireless, acquired by Baidu at $1.9 billion USD.  Some investors choose to remain private.  Xin Chen  William Liu  Previous Product Director of OKCoin. Analyst at Guotai Junan Securities.  Senior Partner at AllBright Law Offices, the biggest Law Firm in Shanghai.  Risks There are many risks involved in running an exchange. We understand this and have the skills, experience, and leadership to overcome them.  Security is Paramount Many crypto exchanges have failed due to poor security procedures. Most security breaches could have been prevented by taking simple precautions to protect critical resources. Our team has developed Binance with security as the foremost concern in their minds. We strive to ensure that we have followed all the industry best practices when it comes to securing infrastructure and data including ​ISO/IEC 27001:20132 and the CryptoCurrency Security Standard (CCSS)3.  Market Competition We know this will be an ultra competitive space. There are probably hundreds, if not thousands of teams wanting, planning or doing exchanges. Competition will be fierce. But in this age, this is a common risk in any decent concept/startup or mature company. The question is: given our team, track record, experience, industry resources, and product, do you believe we stand a better chance than the rest of the pack? If yes, then please join our ICO.  2 3  https://en.wikipedia.org/wiki/ISO/IEC_27001:2013 https://cryptoconsortium.org/standards/CCSS  䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀 圀栀椀 琀 攀瀀愀瀀攀爀  ㈀㄀ 㠀⸀㜀  吀䄀䈀䰀䔀伀䘀䌀伀一吀䔀一吀匀 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀㨀唀渀攀愀爀 琀 栀椀 渀最伀瀀瀀漀爀 琀 甀渀椀 琀 礀昀 漀爀䄀氀 氀  ㌀  䄀戀猀 琀 爀 愀挀 琀  㐀  䤀 渀琀 爀 漀搀甀挀 琀 椀 漀渀  㐀  伀爀 椀 最椀 渀猀  㐀  䠀愀爀 搀䘀漀爀 欀  㔀  吀爀 愀渀猀 愀挀 琀 椀 漀渀猀  㜀  倀爀 漀漀昀 ⴀ 漀昀 ⴀ 圀漀爀 欀䄀氀 最漀爀 椀 琀 栀洀  㤀  䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀  ㄀  圀愀氀 氀 攀琀䘀攀愀琀 甀爀 攀猀  ㄀ ㄀  䠀漀眀琀 漀䄀挀 焀甀椀 爀 攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀  ㄀ ㈀  刀漀愀搀洀愀瀀  ㄀ ㈀  䘀椀 渀愀渀挀 椀 愀氀 匀琀 爀 愀琀 攀最礀  ㄀ ㌀  䌀漀渀挀 氀 甀猀 椀 漀渀  ㄀ 㔀  刀攀昀 攀爀 攀渀挀 攀猀  㘀 ㄀  䈀椀 琀 挀漀椀 渀䐀椀 愀洀漀渀搀 唀渀攀愀爀 琀 栀椀 渀最伀瀀瀀漀爀 琀 甀渀椀 琀 礀昀 漀爀䄀氀 氀  䄀戀猀 琀 爀 愀挀 琀 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀⠀ 䈀䌀䐀⤀椀 猀愀昀 漀爀 欀漀昀䈀椀 琀 挀 漀椀 渀㬀 愀琀瀀爀 攀搀攀琀 攀爀 洀椀 渀攀搀戀氀 漀挀 欀栀攀椀 最栀琀㐀㤀㔀㠀㘀㘀Ⰰ 琀 栀攀渀攀眀挀 栀愀椀 渀 眀愀猀挀 爀 攀愀琀 攀搀⸀ 䄀猀琀 栀攀漀爀 椀 最椀 渀愀氀 䈀椀 琀 挀 漀椀 渀⠀ 䈀吀䌀⤀戀氀 漀挀 欀挀 栀愀椀 渀挀 漀渀琀 椀 渀甀攀猀漀渀甀渀愀氀 琀 攀爀 攀搀Ⰰ 琀 栀椀 猀渀攀眀 挀 爀 礀 瀀琀 漀挀 甀爀 爀 攀渀挀 礀渀漀眀漀瀀攀爀 愀琀 攀猀漀渀椀 琀 猀漀眀渀挀 栀愀椀 渀挀 愀氀 氀 攀搀ᰠ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀ᴠ ⸀ 圀椀 琀 栀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀Ⰰ 洀椀 渀攀爀 猀眀椀 氀 氀 戀攀最椀 渀挀 爀 攀愀琀 椀 渀最戀氀 漀挀 欀猀甀猀 椀 渀最愀渀攀眀瀀爀 漀漀昀 ⴀ 漀昀 ⴀ 眀漀爀 欀愀氀 最漀爀 椀 琀 栀洀 眀栀椀 挀 栀戀攀琀 琀 攀爀猀 攀爀 瘀 攀猀 匀愀琀 漀猀 栀椀 ᤠ 猀漀爀 椀 最椀 渀愀氀 最漀愀氀 漀昀欀攀攀瀀椀 渀最䈀椀 琀 挀 漀椀 渀搀攀挀 攀渀琀 爀 愀氀 椀 稀 攀搀⸀ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀漀û攀爀 猀猀 攀瘀 攀爀 愀氀 琀 攀挀 栀渀椀 挀 愀氀 愀搀瘀 愀渀挀 攀洀攀渀琀 猀椀 渀猀 挀 愀氀 愀戀椀 氀 椀 琀 礀愀氀 漀渀最眀椀 琀 栀愀渀琀 椀 ⴀ 爀 攀瀀氀 愀礀瀀爀 漀琀 攀挀 琀 椀 漀渀愀渀搀眀愀氀 氀 攀琀攀渀栀愀渀挀 攀洀攀渀琀 猀 ⸀ 圀椀 琀 栀琀 栀攀猀 攀 挀 栀愀渀最攀猀琀 漀琀 栀攀䈀椀 琀 挀 漀椀 渀瀀爀 漀琀 漀挀 漀氀 Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀猀 攀攀欀猀琀 漀愀挀 栀椀 攀瘀 攀匀愀琀 漀猀 栀椀 一愀欀愀洀漀琀 漀✀ 猀瘀 椀 猀 椀 漀渀昀 漀爀愀 瀀攀攀爀 ⴀ 琀 漀ⴀ 瀀攀攀爀攀氀 攀挀 琀 爀 漀渀椀 挀挀 愀猀 栀猀 礀 猀 琀 攀洀 琀 栀愀琀 ᤠ 猀愀挀 挀 攀猀 猀 椀 戀氀 攀愀渀搀甀猀 愀戀氀 攀琀 漀攀瘀 攀爀 礀 漀渀攀Ⰰ 爀 攀最愀爀 搀氀 攀猀 猀漀昀 攀挀 漀渀漀洀椀 挀猀 琀 愀琀 甀猀漀爀挀 漀甀渀琀 爀 礀漀昀漀爀 椀 最椀 渀⸀  䤀 渀琀 爀 漀搀甀挀 琀 椀 漀渀 䄀昀 琀 攀爀渀椀 渀攀礀 攀愀爀 猀漀昀爀 愀瀀椀 搀搀攀瘀 攀氀 漀瀀洀攀渀琀 Ⰰ 䈀椀 琀 挀 漀椀 渀挀 愀渀渀漀氀 漀渀最攀爀洀攀攀琀琀 栀攀搀攀洀愀渀搀猀漀昀椀 琀 猀爀 椀 猀 椀 渀最渀甀洀戀攀爀 猀漀昀 挀 氀 椀 攀渀琀 猀 ⸀ 䈀椀 琀 挀 漀椀 渀栀愀猀栀椀 最栀琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 攀攀猀 Ⰰ 猀 氀 漀眀琀 爀 愀渀猀 愀挀 琀 椀 漀渀挀 漀渀ǻ爀 洀愀琀 椀 漀渀猀 Ⰰ 愀渀搀栀椀 最栀琀 栀爀 攀猀 栀漀氀 搀猀昀 漀爀渀攀眀洀椀 渀攀爀 猀 ⸀ 䘀甀爀 琀 栀攀爀 洀漀爀 攀Ⰰ 䈀椀 琀 挀 漀椀 渀栀愀猀猀 琀 椀 氀 氀 昀 愀椀 氀 攀搀琀 漀愀搀漀瀀琀猀 挀 愀氀 椀 渀最猀 漀氀 甀琀 椀 漀渀猀猀 甀挀 栀愀猀匀攀最圀椀 琀 Ⰰ 眀椀 琀 栀眀攀氀 氀 漀瘀 攀爀栀愀氀 昀漀昀挀 甀爀 爀 攀渀琀 䈀椀 琀 挀 漀椀 渀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀猀 琀 椀 氀 氀 甀猀 椀 渀最琀 栀攀漀氀 搀攀爀 Ⰰ 氀 攀猀 猀攀ϻ挀 椀 攀渀琀瀀爀 漀琀 漀挀 漀氀 ⸀ 䄀猀愀搀漀瀀琀 椀 漀渀昀 漀爀搀椀 最椀 琀 愀氀 挀 甀爀 爀 攀渀挀 礀挀 漀渀琀 椀 渀甀攀猀琀 漀 漀甀琀 瀀愀挀 攀䈀椀 琀 挀 漀椀 渀ᤠ 猀愀戀椀 氀 椀 琀 礀琀 漀猀 挀 愀氀 攀Ⰰ 琀 栀攀爀 攀椀 猀愀搀愀渀最攀爀琀 栀愀琀匀愀琀 漀猀 栀椀 ᤠ 猀搀椀 最椀 琀 愀氀 挀 愀猀 栀眀椀 氀 氀 攀瘀 攀渀琀 甀愀氀 氀 礀昀 愀氀 氀 戀攀栀椀 渀搀漀琀 栀攀爀 瀀愀礀 洀攀渀琀瀀氀 愀琀 昀 漀爀 洀猀 ⸀ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀搀搀爀 攀猀 猀 攀猀琀 栀攀猀 攀˻愀眀猀戀礀椀 洀瀀氀 攀洀攀渀琀 椀 渀最渀攀眀琀 攀挀 栀渀椀 挀 愀氀 椀 洀瀀爀 漀瘀 攀洀攀渀琀 猀琀 栀愀琀爀 攀猀 漀氀 瘀 攀椀 猀 猀 甀攀猀 挀 漀渀挀 攀爀 渀椀 渀最䈀吀䌀ᤠ 猀栀椀 最栀琀 爀 愀渀猀 愀挀 琀 椀 漀渀挀 漀猀 琀愀渀搀猀 氀 漀眀挀 漀渀ǻ爀 洀愀琀 椀 漀渀猀 ⸀ 䈀礀挀 漀洀戀椀 渀椀 渀最匀攀最爀 攀最愀琀 攀搀圀椀 琀 渀攀猀 猀眀椀 琀 栀愀渀㠀 䴀䈀戀氀 漀挀 欀猀 椀 稀 攀Ⰰ 䈀䌀䐀椀 猀挀 愀瀀愀戀氀 攀漀昀瀀攀爀 昀 漀爀 洀椀 渀最漀瘀 攀爀㄀ 琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀瀀攀爀猀 攀挀 漀渀搀漀爀㐀⸀ 㠀洀椀 氀 氀 椀 漀渀愀搀愀礀 Ⰰ 漀瘀 攀爀㄀ 琀 椀 洀攀猀琀 栀攀挀 甀爀 爀 攀渀琀猀 瀀攀攀搀漀昀䈀吀䌀⸀ 䈀䌀䐀瀀爀 攀瘀 攀渀琀 猀琀 栀攀挀 攀渀琀 爀 愀氀 椀 稀 愀琀 椀 漀渀漀昀洀椀 渀椀 渀最瀀漀眀攀爀戀礀甀猀 椀 渀最堀㄀ ㌀倀爀 漀漀昀漀昀圀漀爀 欀Ⰰ 愀洀椀 渀椀 渀最愀氀 最漀爀 椀 琀 栀洀 琀 栀愀琀椀 猀爀 攀猀 椀 猀 琀 愀渀琀琀 漀䄀匀䤀 䌀猀愀渀搀昀 爀 椀 攀渀搀氀 礀琀 漀䜀倀唀猀 ⸀ 䤀 渀愀搀搀椀 琀 椀 漀渀琀 漀琀 栀攀猀 攀猀 琀 愀爀 琀 椀 渀最攀渀栀愀渀挀 攀洀攀渀琀 猀 Ⰰ 琀 栀攀䈀䌀䐀搀攀瘀 攀氀 漀瀀洀攀渀琀琀 攀愀洀 椀 猀眀漀爀 欀椀 渀最漀渀琀 栀攀椀 洀瀀氀 攀洀攀渀琀 愀琀 椀 漀渀漀昀琀 栀攀䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀愀猀眀攀氀 氀 愀猀眀愀氀 氀 攀琀 挀 氀 椀 攀渀琀 猀愀挀 爀 漀猀 猀洀甀氀 琀 椀 瀀氀 攀瀀氀 愀琀 昀 漀爀 洀猀 ⸀  伀爀 椀 最椀 渀猀 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀眀愀猀昀 漀爀 欀攀搀漀û琀 栀攀䈀椀 琀 挀 漀椀 渀⠀ 䈀吀䌀⤀戀氀 漀挀 欀挀 栀愀椀 渀漀渀一漀瘀 攀洀戀攀爀㈀㐀Ⰰ ㈀㄀ 㜀愀琀戀氀 漀挀 欀栀攀椀 最栀琀㐀㤀㔀㠀㘀㘀 愀昀 琀 攀爀吀 攀愀洀 䔀嘀䔀夀愀渀搀吀 攀愀洀  㜀瀀愀爀 琀 渀攀爀 攀搀琀 漀搀攀瘀 攀氀 漀瀀琀 栀攀渀攀挀 攀猀 猀 愀爀 礀甀瀀最爀 愀搀攀猀琀 漀椀 洀瀀爀 漀瘀 攀甀瀀漀渀䈀椀 琀 挀 漀椀 渀ᤠ 猀  漀爀 椀 最椀 渀愀氀 昀 爀 愀洀攀眀漀爀 欀⸀ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀搀攀瘀 攀氀 漀瀀攀爀 猀栀愀瘀 攀椀 渀挀 漀爀 瀀漀爀 愀琀 攀搀愀渀攀眀瀀爀 漀漀昀 ⴀ 漀昀 ⴀ 眀漀爀 欀愀氀 最漀爀 椀 琀 栀洀 愀渀搀眀椀 氀 氀 挀 漀渀琀 椀 渀甀攀琀 漀攀渀栀愀渀挀 攀漀爀 椀 最椀 渀愀氀 䈀椀 琀 挀 漀椀 渀昀 攀愀琀 甀爀 攀猀眀椀 琀 栀最爀 攀愀琀 攀爀猀 瀀攀攀搀Ⰰ 瀀爀 漀琀 攀挀 琀 椀 漀渀Ⰰ 愀渀搀猀 挀 愀氀 愀戀椀 氀 椀 琀 礀 ⸀  䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀爀 愀椀 猀 攀搀琀 栀攀戀氀 漀挀 欀猀 椀 稀 攀氀 椀 洀椀 琀昀 爀 漀洀 ㈀ⴀ 㐀䴀䈀琀 漀㠀ⴀ ㌀㈀䴀䈀愀猀瀀愀爀 琀漀昀愀洀愀猀 猀 椀 瘀 攀漀渀ⴀ 挀 栀愀椀 渀 猀 挀 愀氀 椀 渀最愀瀀瀀爀 漀愀挀 栀琀 漀挀 爀 攀愀琀 攀愀洀瀀氀 攀挀 愀瀀愀挀 椀 琀 礀昀 漀爀栀椀 最栀攀爀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 琀 漀爀 愀最攀⸀ 吀栀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀挀 愀瀀愀挀 椀 琀 礀 漀昀戀氀 漀挀 欀猀眀椀 氀 氀 戀攀椀 渀挀 爀 攀愀猀 攀搀ǻ瘀 攀ⴀ 昀 漀氀 搀愀渀搀琀 栀攀甀氀 琀 椀 洀愀琀 攀最漀愀氀 椀 猀琀 漀椀 洀瀀爀 漀瘀 攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀挀 漀渀ǻ爀 洀愀琀 椀 漀渀 猀 瀀攀攀搀昀 漀爀琀 栀攀攀渀琀 椀 爀 攀戀氀 漀挀 欀挀 栀愀椀 渀⸀ 圀椀 琀 栀琀 栀攀愀搀搀椀 琀 椀 漀渀漀昀匀攀最圀椀 琀 Ⰰ 琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀挀 愀渀渀漀眀猀 挀 愀氀 攀愀琀愀昀 愀爀 最爀 攀愀琀 攀爀瀀愀挀 攀琀 栀愀渀愀渀礀䈀椀 琀 挀 漀椀 渀挀 栀愀椀 渀戀攀昀 漀爀 攀椀 琀 ⸀ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀氀 猀 漀漀û攀爀 猀爀 攀瀀氀 愀礀瀀爀 漀琀 攀挀 琀 椀 漀渀愀猀琀 栀攀 昀 漀爀 洀愀琀昀 漀爀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀栀愀猀戀攀攀渀挀 栀愀渀最攀搀猀 椀 渀挀 攀琀 栀攀䈀䌀䐀昀 漀爀 欀⸀ 吀栀椀 猀洀攀愀渀猀琀 栀愀琀䈀吀䌀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 挀 愀渀渀漀琀戀攀爀 攀瀀氀 愀礀 攀搀椀 渀琀 栀攀䈀䌀䐀渀攀琀 眀漀爀 欀愀猀愀眀愀礀琀 漀猀 琀 攀愀氀 甀猀 攀爀昀 甀渀搀猀 ⸀ 䈀䌀䐀ᤠ 猀漀戀樀 攀挀 琀 椀 瘀 攀猀愀爀 攀琀 漀昀 漀猀 琀 攀爀琀 栀攀眀椀 搀攀猀 瀀爀 攀愀搀甀猀 攀漀昀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀爀 漀甀渀搀琀 栀攀眀漀爀 氀 搀Ⰰ 琀 漀 攀洀瀀漀眀攀爀甀渀戀愀渀欀攀搀瀀攀漀瀀氀 攀琀 漀甀猀 攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀琀 漀戀甀椀 氀 搀眀攀愀氀 琀 栀昀 漀爀琀 栀攀洀猀 攀氀 瘀 攀猀愀渀搀琀 栀攀椀 爀 昀 愀洀椀 氀 椀 攀猀 Ⰰ 琀 漀攀猀 琀 愀戀氀 椀 猀 栀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀猀琀 栀攀渀甀洀戀攀爀漀渀攀挀 爀 礀 瀀琀 漀挀 甀爀 爀 攀渀挀 礀甀猀 攀搀椀 渀攀洀攀爀 最椀 渀最 攀挀 漀渀漀洀椀 攀猀 Ⰰ 愀渀搀琀 漀洀愀欀攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀挀 挀 攀猀 猀 椀 戀氀 攀愀渀搀甀猀 愀戀氀 攀昀 漀爀攀瘀 攀爀 礀 搀愀礀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 ⸀ 吀栀攀 琀 漀琀 愀氀 愀洀漀甀渀琀漀昀䈀椀 琀 挀 漀椀 渀搀椀 愀洀漀渀搀椀 猀琀 攀渀琀 椀 洀攀猀琀 栀愀琀漀昀䈀椀 琀 挀 漀椀 渀眀栀椀 挀 栀琀 爀 愀渀猀 氀 愀琀 攀猀椀 渀琀 漀愀挀 漀猀 琀爀 攀搀甀挀 琀 椀 漀渀 昀 漀爀渀攀眀瀀愀爀 琀 椀 挀 椀 瀀愀琀 椀 漀渀愀渀搀愀爀 攀搀甀挀 琀 椀 漀渀漀昀渀攀挀 攀猀 猀 愀爀 礀琀 栀爀 攀猀 栀漀氀 搀猀 ⸀ 刀攀最愀爀 搀氀 攀猀 猀漀昀猀 挀 愀氀 愀戀椀 氀 椀 琀 礀 Ⰰ 愀挀 栀愀椀 渀椀 猀 漀渀氀 礀愀猀猀 琀 爀 漀渀最愀猀椀 琀 猀挀 漀渀猀 攀渀猀 甀猀 ⸀ 吀 漀爀 攀搀甀挀 攀琀 栀攀搀愀渀最攀爀漀昀洀椀 渀椀 渀最挀 攀渀琀 爀 愀氀 椀 稀 愀琀 椀 漀渀Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀 甀猀 攀猀愀渀愀氀 最漀爀 椀 琀 栀洀 琀 栀愀琀洀愀欀攀猀椀 琀椀 渀挀 爀 攀搀椀 戀氀 礀搀椀 ϻ挀 甀氀 琀昀 漀爀猀 椀 渀最氀 攀攀渀琀 椀 琀 椀 攀猀琀 漀挀 漀洀洀愀渀搀氀 愀爀 最攀猀 琀 愀欀攀猀漀昀 琀 栀攀瀀爀 漀挀 攀猀 猀 椀 渀最瀀漀眀攀爀昀 漀爀戀氀 漀挀 欀瘀 愀氀 椀 搀愀琀 椀 漀渀⸀  䠀愀爀 搀䘀漀爀 欀 䤀 渀戀氀 漀挀 欀挀 栀愀椀 渀Ⰰ 愀栀愀爀 搀昀 漀爀 欀椀 猀愀挀 栀愀渀最攀琀 漀愀挀 爀 礀 瀀琀 漀最爀 愀瀀栀椀 挀瀀爀 漀琀 漀挀 漀氀 琀 栀愀琀挀 愀甀猀 攀猀愀瀀攀爀 洀愀渀攀渀琀搀椀 瘀 攀爀 最攀渀挀 攀昀 爀 漀洀 琀 栀攀瀀爀 攀瘀 椀 漀甀猀瘀 攀爀 猀 椀 漀渀⸀ 圀栀攀渀琀 栀椀 猀挀 栀愀渀最攀漀挀 挀 甀爀 猀 Ⰰ 愀氀 氀 甀猀 攀爀 猀洀甀猀 琀搀攀挀 椀 搀攀眀栀攀琀 栀攀爀琀 漀愀搀漀瀀琀琀 栀攀渀攀眀瀀爀 漀琀 漀挀 漀氀 ⠀ 昀 漀爀 欀 琀 漀琀 栀攀渀攀眀挀 栀愀椀 渀⤀漀爀挀 漀渀琀 椀 渀甀攀琀 漀猀 甀瀀瀀漀爀 琀琀 栀攀漀氀 搀瀀爀 漀琀 漀挀 漀氀 ⸀ 䤀 昀攀渀漀甀最栀甀猀 攀爀 猀爀 攀洀愀椀 渀漀渀琀 栀攀漀氀 搀挀 栀愀椀 渀Ⰰ 琀 眀漀 戀氀 漀挀 欀挀 栀愀椀 渀猀眀椀 氀 氀 琀 栀攀渀攀砀 椀 猀 琀眀栀椀 挀 栀瀀漀猀 猀 攀猀 猀椀 搀攀渀琀 椀 挀 愀氀 琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀昀 爀 漀洀 戀攀昀 漀爀 攀琀 栀攀昀 漀爀 欀Ⰰ 戀甀琀渀漀眀爀 甀渀漀渀猀 攀瀀愀爀 愀琀 攀 挀 栀愀椀 渀猀眀椀 琀 栀琀 栀攀椀 爀漀眀渀甀渀椀 焀甀攀栀椀 猀 琀 漀爀 礀 Ⰰ 渀漀搀攀猀愀渀搀瀀爀 漀琀 漀挀 漀氀 猀 ⸀ 䤀 琀眀愀猀琀 栀爀 漀甀最栀琀 栀椀 猀欀椀 渀搀漀昀栀愀爀 搀昀 漀爀 欀琀 栀愀琀䈀椀 琀 挀 漀椀 渀 䌀愀猀 栀Ⰰ 䈀椀 琀 挀 漀椀 渀䜀漀氀 搀Ⰰ 匀攀最眀椀 琀㈀砀 Ⰰ 愀渀搀渀漀眀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀眀攀爀 攀挀 爀 攀愀琀 攀搀昀 爀 漀洀 琀 栀攀漀爀 椀 最椀 渀愀氀 䈀椀 琀 挀 漀椀 渀挀 栀愀椀 渀⸀ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀⠀ 䈀䌀䐀⤀  伀爀 椀 最椀 渀愀氀䈀椀 琀 挀 漀椀 渀䌀栀愀椀 渀  㠀洀戀䈀氀 漀挀 欀猀 堀㄀ ㌀⠀ 䜀倀唀⤀䴀椀 渀椀 渀最 ㈀㄀ 洀椀 氀 氀 椀 漀渀猀 甀瀀瀀氀 礀  ⠀ 䈀攀昀 漀爀 攀一漀瘀㈀ ㄀ 㜀⤀  䘀漀爀 欀愀琀戀氀 漀挀 欀ⴀ 栀攀椀 最栀琀㐀㤀㔀㠀㘀㘀Ⰰ 一漀瘀㈀㐀Ⰰ㈀ ㄀ 㜀  伀爀 椀 最椀 渀愀氀䈀椀 琀 挀 漀椀 渀⠀ 䈀吀䌀⤀ ㄀ 䴀䈀䈀氀 漀挀 欀猀⠀ ㈀ⴀ 㐀䴀䈀眀⼀匀攀最眀椀 琀 ⤀ 匀䠀䄀ⴀ ㈀㔀㘀⠀ 䄀匀䤀 䌀⤀䴀椀 渀椀 渀最 ㈀㄀洀椀 氀 氀 椀 漀渀猀 甀瀀瀀氀 礀  䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀眀愀猀昀 漀爀 欀攀搀漀渀一漀瘀 攀洀戀攀爀㈀㐀Ⰰ ㈀㄀ 㜀眀栀攀渀䈀䌀䐀渀漀搀攀猀戀攀最愀渀琀 漀猀 甀瀀瀀漀爀 琀琀 栀攀渀攀眀 瀀爀 漀琀 漀挀 漀氀 愀昀 琀 攀爀戀氀 漀挀 欀㐀㤀㔀㠀㘀㘀愀渀搀戀爀 愀渀挀 栀攀搀愀眀愀礀昀 爀 漀洀 琀 栀攀䈀吀䌀挀 栀愀椀 渀⸀ 䘀爀 漀洀 琀 栀椀 猀戀氀 漀挀 欀漀渀眀愀爀 搀猀 Ⰰ 䈀吀䌀洀椀 渀攀爀 猀愀爀 攀渀漀氀 漀渀最攀爀愀戀氀 攀琀 漀洀椀 渀攀戀氀 漀挀 欀猀漀渀琀 栀攀䈀䌀䐀挀 栀愀椀 渀愀渀搀瘀 椀 挀 攀瘀 攀爀 猀 愀⸀ 匀椀 渀挀 攀戀漀琀 栀挀 栀愀椀 渀猀 猀 栀愀爀 攀琀 栀攀猀 愀洀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀栀椀 猀 琀 漀爀 礀昀 爀 漀洀 戀攀昀 漀爀 攀琀 栀攀猀 瀀氀 椀 琀 Ⰰ 愀渀礀 漀渀攀琀 栀愀琀漀眀渀攀搀䈀吀䌀愀琀琀 栀攀琀 椀 洀攀漀昀琀 栀攀 昀 漀爀 欀眀漀甀氀 搀渀漀眀漀眀渀琀 攀渀琀 椀 洀攀猀琀 栀攀愀洀漀甀渀琀椀 渀䈀䌀䐀⸀  䌀漀洀瀀愀爀 椀 猀 漀渀䌀栀愀爀 琀 一愀洀攀  䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀⠀ 䈀䌀䐀⤀  䈀椀 琀 挀 漀椀 渀⠀ 䈀吀䌀⤀  䈀椀 琀 挀 漀椀 渀䌀愀猀 栀⠀ 䈀䌀䠀⤀  䴀愀砀匀甀瀀瀀氀 礀⠀ 洀椀 氀 氀 椀 漀渀猀 ⤀  ㈀㄀  ㈀㄀  ㈀㄀  䐀椀 猀 琀 爀 椀 戀甀琀 椀 漀渀  䴀椀 渀椀 渀最Ⰰ 䌀氀 愀椀 洀椀 渀最  䴀椀 渀椀 渀最  䴀椀 渀椀 渀最Ⰰ 䌀氀 愀椀 洀椀 渀最  䴀漀瘀 椀 渀最䄀氀 最漀爀 椀 琀 栀洀  伀瀀琀 椀 洀椀 稀 攀搀堀㄀ ㌀⠀ 䜀倀唀⤀  匀䠀䄀㈀㔀㘀⠀ 䄀匀䤀 䌀⤀  匀䠀䄀㈀㔀㘀⠀ 䄀匀䤀 䌀⤀  䈀氀 漀挀 欀琀 椀 洀攀⠀ 洀椀 渀甀琀 攀猀 ⤀  ㄀  ㄀  ㄀  䴀愀砀䈀氀 漀挀 欀猀 椀 稀 攀⠀ 匀䔀䜀圀䤀 吀⤀  㠀ⴀ ㌀㈀䴀䈀  ㄀ 䴀䈀⠀ ㈀ⴀ 㐀䴀䈀⤀  㠀䴀䈀  䈀氀 漀挀 欀挀 栀愀椀 渀匀椀 稀 攀  縀㄀ ㌀㔀䜀䈀  縀㄀ 㐀㔀䜀䈀  縀㄀ ㌀㔀䜀䈀  䐀椀 ϻ挀 甀氀 琀 礀䄀搀樀 甀猀 琀 洀攀渀琀  ㄀ ㈀䠀漀甀爀 猀  ㈀圀攀攀欀猀  ㈀圀攀攀欀猀⬀䔀䐀䄀  䴀愀砀 琀 砀⼀䐀愀礀  縀㐀⸀ 㠀洀椀 氀 氀 椀 漀渀  縀㄀ ⸀ ㈀洀椀 氀 氀 椀 漀渀  縀㐀⸀ 㠀洀椀 氀 氀 椀 漀渀  匀䔀䜀圀䤀 吀  夀 攀猀  夀 攀猀  一漀  刀攀瀀氀 愀礀倀爀 漀琀 攀挀 琀 椀 漀渀  夀 攀猀  一漀琀一攀挀 攀猀 猀 愀爀 礀  夀 攀猀  吀椀 洀攀漀昀䔀猀 琀 愀戀氀 椀 猀 栀洀攀渀琀  一漀瘀 攀洀戀攀爀㈀ ㄀ 㜀  ㈀ 㤀  䄀甀最甀猀 琀㈀ ㄀ 㜀  䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀  夀 攀猀  夀 攀猀  一漀  吀爀 愀渀猀 愀挀 琀 椀 漀渀猀 䰀愀爀 最攀爀䈀氀 漀挀 欀猀昀 漀爀䘀愀猀 琀 攀爀吀爀 愀渀猀 愀挀 琀 椀 漀渀䌀漀渀ǻ爀 洀愀琀 椀 漀渀猀 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀爀 愀椀 猀 攀搀琀 栀攀戀氀 漀挀 欀猀 椀 稀 攀氀 椀 洀椀 琀琀 漀㠀䴀䈀愀猀瀀愀爀 琀漀昀愀洀愀猀 猀 椀 瘀 攀漀渀ⴀ 挀 栀愀椀 渀猀 挀 愀氀 椀 渀最愀瀀瀀爀 漀愀挀 栀⸀ 吀栀攀爀 攀椀 猀渀漀眀愀洀瀀氀 攀挀 愀瀀愀挀 椀 琀 礀昀 漀爀攀瘀 攀爀 礀 漀渀攀✀ 猀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀琀 漀戀攀瀀爀 漀挀 攀猀 猀 攀搀⸀ 吀栀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀 挀 愀瀀愀挀 椀 琀 礀漀昀戀氀 漀挀 欀猀眀椀 氀 氀 戀攀椀 渀挀 爀 攀愀猀 攀搀ǻ瘀 攀ⴀ 昀 漀氀 搀愀渀搀琀 栀攀甀氀 琀 椀 洀愀琀 攀最漀愀氀 椀 猀琀 漀椀 洀瀀爀 漀瘀 攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀 挀 漀渀ǻ爀 洀愀琀 椀 漀渀猀 瀀攀攀搀昀 漀爀琀 栀攀攀渀琀 椀 爀 攀戀氀 漀挀 欀挀 栀愀椀 渀⸀ 圀椀 琀 栀氀 椀 最栀琀 渀椀 渀最昀 愀猀 琀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 Ⰰ 栀椀 最栀氀 礀搀椀 氀 甀琀 攀搀 琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 攀攀猀 Ⰰ 愀渀搀琀 攀渀琀 椀 洀攀猀愀猀洀甀挀 栀猀 甀瀀瀀氀 礀愀猀漀琀 栀攀爀氀 攀愀搀椀 渀最䈀椀 琀 挀 漀椀 渀昀 漀爀 欀猀 Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀ᤠ 猀 戀氀 漀挀 欀挀 栀愀椀 渀瀀爀 椀 漀爀 椀 琀 椀 稀 攀猀琀 爀 甀猀 琀 Ⰰ 愀挀 挀 攀猀 猀 椀 戀椀 氀 椀 琀 礀 Ⰰ 愀渀搀愀û漀爀 搀愀戀椀 氀 椀 琀 礀 ⸀ 圀栀椀 氀 攀琀 栀攀爀 攀愀爀 攀挀 漀渀挀 攀爀 渀猀琀 栀愀琀氀 愀爀 最攀戀氀 漀挀 欀猀洀愀礀爀 愀瀀椀 搀氀 礀椀 渀挀 爀 攀愀猀 攀琀 栀攀戀氀 漀挀 欀挀 栀愀椀 渀ᤠ 猀琀 漀琀 愀氀 猀 椀 稀 攀Ⰰ 琀 栀攀 瀀爀 攀猀 攀渀琀渀甀洀戀攀爀漀昀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀椀 渀挀 氀 甀搀攀搀椀 渀攀愀挀 栀戀氀 漀挀 欀椀 猀猀 琀 椀 氀 氀 昀 愀爀昀 爀 漀洀 栀椀 琀 琀 椀 渀最琀 栀攀甀瀀瀀攀爀氀 椀 洀椀 琀漀昀琀 栀攀 戀氀 漀挀 欀猀 椀 稀 攀⸀ 䤀 渀挀 愀猀 攀漀昀愀昀 甀琀 甀爀 攀瘀 漀氀 甀洀攀椀 渀挀 爀 攀愀猀 攀Ⰰ 愀搀搀椀 琀 椀 漀渀愀氀 洀攀挀 栀愀渀椀 猀 洀猀猀 甀挀 栀愀猀猀 栀愀爀 搀椀 渀最愀爀 攀 愀氀 爀 攀愀搀礀戀攀椀 渀最挀 漀渀猀 椀 搀攀爀 攀搀琀 漀爀 攀搀甀挀 攀琀 栀攀瀀爀 漀戀氀 攀洀 漀昀猀 琀 漀爀 椀 渀最愀挀 漀氀 漀猀 猀 愀氀 戀氀 漀挀 欀挀 栀愀椀 渀猀 椀 稀 攀⸀  䰀漀眀攀爀 椀 渀最吀爀 愀渀猀 愀挀 琀 椀 漀渀䌀漀猀 琀 猀眀椀 琀 栀愀䰀愀爀 最攀爀匀甀瀀瀀氀 礀 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀爀 攀搀甀挀 攀猀琀 栀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 攀攀猀愀渀搀琀 栀攀挀 漀猀 琀漀昀瀀愀爀 琀 椀 挀 椀 瀀愀琀 椀 漀渀㨀 吀栀攀琀 漀琀 愀氀 愀洀漀甀渀琀漀昀 䈀䌀䐀椀 猀㄀ 琀 椀 洀攀猀琀 栀愀琀漀昀䈀吀䌀猀 漀琀 栀愀琀椀 琀爀 攀搀甀挀 攀猀琀 栀攀挀 漀猀 琀漀昀瀀愀爀 琀 椀 挀 椀 瀀愀琀 椀 漀渀⸀ 䈀䌀䐀椀 洀瀀爀 漀瘀 攀猀琀 栀攀猀 椀 琀 甀愀琀 椀 漀渀 漀昀漀瘀 攀爀 瀀爀 椀 挀 攀搀䈀椀 琀 挀 漀椀 渀Ⰰ 椀 渀挀 爀 攀愀猀 椀 渀最琀 栀攀琀 漀琀 愀氀 猀 甀瀀瀀氀 礀漀昀䈀䌀䐀愀渀搀氀 漀眀攀爀 椀 渀最琀 栀攀瀀爀 椀 挀 攀⸀ 吀栀椀 猀猀 甀瀀瀀氀 礀 挀 栀愀渀最攀椀 渀挀 爀 攀愀猀 攀猀挀 椀 爀 挀 甀氀 愀琀 椀 漀渀愀渀搀栀攀氀 瀀猀攀洀瀀栀愀猀 椀 稀 攀猀琀 栀攀甀猀 攀漀昀䈀䌀䐀昀 漀爀猀 洀愀氀 氀 戀甀猀 椀 渀攀猀 猀 攀猀愀渀搀 洀椀 挀 爀 漀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 ⸀ 圀椀 琀 栀爀 攀氀 愀琀 椀 瘀 攀氀 礀氀 漀眀琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 攀攀猀 Ⰰ 愀猀 攀挀 甀爀 攀愀渀搀瀀爀 椀 瘀 愀琀 攀戀氀 漀挀 欀挀 栀愀椀 渀Ⰰ 愀渀搀 愀û漀爀 搀愀戀氀 攀挀 漀椀 渀瀀爀 椀 挀 攀猀 Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀椀 猀眀攀氀 氀 猀 甀椀 琀 攀搀昀 漀爀洀愀欀椀 渀最攀瘀 攀爀 礀 搀愀礀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 ⸀  匀攀最爀 攀最愀琀 攀搀圀椀 琀 渀攀猀 猀琀 漀伀瀀琀 椀 洀椀 稀 攀匀琀 漀爀 愀最攀 匀攀最爀 攀最愀琀 攀搀圀椀 琀 渀攀猀 猀⠀ 匀攀最圀椀 琀 ⤀椀 猀琀 栀攀瀀爀 漀挀 攀猀 猀戀礀眀栀椀 挀 栀猀 椀 最渀愀琀 甀爀 攀猀椀 渀愀䈀椀 琀 挀 漀椀 渀琀 爀 愀渀猀 愀挀 琀 椀 漀渀愀爀 攀 ᰠ 猀 攀最爀 攀最愀琀 攀搀ᴠ昀 爀 漀洀 琀 栀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀搀愀琀 愀⸀ 匀攀最圀椀 琀搀攀ǻ渀攀猀愀渀攀眀猀 琀 爀 甀挀 琀 甀爀 攀挀 愀氀 氀 攀搀愀ᰠ 眀椀 琀 渀攀猀 猀 ᴠ琀 栀愀琀椀 猀 挀 漀洀洀椀 琀 琀 攀搀琀 漀戀氀 漀挀 欀猀猀 攀瀀愀爀 愀琀 攀氀 礀昀 爀 漀洀 琀 栀攀洀攀爀 欀氀 攀琀 爀 攀攀琀 栀愀琀栀漀氀 搀猀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 ⸀ 䈀礀爀 攀洀漀瘀 椀 渀最 猀 椀 最渀愀琀 甀爀 攀搀愀琀 愀椀 渀琀 栀椀 猀洀愀渀渀攀爀 Ⰰ 㘀㔀─漀昀猀 琀 漀爀 愀最攀猀 瀀愀挀 攀椀 猀昀 爀 攀攀搀甀瀀猀 漀琀 栀愀琀戀氀 漀挀 欀挀 愀瀀愀挀 椀 琀 礀昀 漀爀 琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀椀 猀椀 渀挀 爀 攀愀猀 攀搀⸀  一漀渀ⴀ 匀攀最眀椀 琀䈀氀 漀挀 欀猀 䈀氀 漀挀 欀䠀攀愀搀攀爀  䈀氀 漀挀 欀䠀攀愀搀攀爀  匀攀最眀椀 琀䈀氀 漀挀 欀猀  䈀氀 漀挀 欀䠀攀愀搀攀爀  䈀氀 漀挀 欀䠀攀愀搀攀爀 圀椀 琀 渀攀猀 猀  䈀氀 漀挀 欀䠀攀愀搀攀爀 圀椀 琀 渀攀猀 猀  䈀氀 漀挀 欀䠀攀愀搀攀爀 圀椀 琀 渀攀猀 猀  吀爀 愀渀猀 愀挀 琀 椀 漀渀  吀爀 愀渀猀 愀挀 琀 椀 漀渀  吀爀 愀渀猀 愀挀 琀 椀 漀渀  吀爀 愀渀猀 愀挀 琀 椀 漀渀  吀爀 愀渀猀 愀挀 琀 椀 漀渀  吀爀 愀渀猀 愀挀 琀 椀 漀渀  䄀氀 椀 挀 攀ᤠ 猀 倀甀戀氀 椀 挀䬀攀礀  䈀漀戀ᤠ 猀 倀甀戀氀 椀 挀䬀攀礀  䘀爀 愀渀欀ᤠ 猀 倀甀戀氀 椀 挀䬀攀礀  䄀氀 椀 挀 攀ᤠ 猀 倀甀戀氀 椀 挀䬀攀礀  䈀漀戀ᤠ 猀 倀甀戀氀 椀 挀䬀攀礀  䘀爀 愀渀欀ᤠ 猀 倀甀戀氀 椀 挀䬀攀礀  䠀愀猀 栀  䠀愀猀 栀  䠀愀猀 栀  䠀愀猀 栀  䠀愀猀 栀  䠀愀猀 栀  䘀爀 愀渀欀ᤠ 猀匀椀 最渀愀琀 甀爀 攀  䄀氀 椀 挀 攀ᤠ 猀匀椀 最渀愀琀 甀爀 攀  䈀漀戀ᤠ 猀匀椀 最渀愀琀 甀爀 攀 䘀爀 愀渀欀ᤠ 猀匀椀 最渀愀琀 甀爀 攀  䄀氀 椀 挀 攀ᤠ 猀匀椀 最渀愀琀 甀爀 攀  䈀漀戀ᤠ 猀匀椀 最渀愀琀 甀爀 攀  吀 爀 愀渀猀 愀挀 琀 椀 漀渀栀愀猀 栀椀 渀挀 氀 甀搀攀猀猀 攀渀搀攀爀 ᤠ 猀瀀甀戀氀 椀 挀欀攀礀愀渀搀猀 椀 最渀愀琀 甀爀 攀  匀椀 最渀愀琀 甀爀 攀猀愀爀 攀爀 攀氀 漀挀 愀琀 攀搀愀眀愀礀昀 爀 漀洀 琀 栀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀栀愀猀 栀椀 渀琀 栀攀ᰠ 圀椀 琀 渀攀猀 猀 ᴠ 琀 漀椀 渀挀 爀 攀愀猀 攀挀 愀瀀愀挀 椀 琀 礀愀渀搀挀 甀爀 戀洀愀氀 氀 氀 攀愀戀椀 氀 椀 琀 礀愀琀 琀 愀挀 欀猀  䤀 渀愀搀搀椀 琀 椀 漀渀琀 漀漀瀀琀 椀 洀椀 稀 椀 渀最猀 琀 漀爀 愀最攀Ⰰ 匀攀最圀椀 琀愀氀 猀 漀瀀爀 攀瘀 攀渀琀 猀洀愀氀 氀 攀愀戀椀 氀 椀 琀 礀愀琀 琀 愀挀 欀猀戀礀眀栀椀 挀 栀愀爀 攀挀 攀椀 瘀 攀爀 洀漀搀椀 ǻ攀猀愀猀 攀渀搀攀爀 ᤠ 猀琀 爀 愀渀猀 愀挀 琀 椀 漀渀䤀 䐀椀 渀漀爀 搀攀爀琀 漀最攀琀洀漀爀 攀挀 漀椀 渀猀 ⸀ 匀椀 渀挀 攀搀椀 最椀 琀 愀氀 猀 椀 最渀愀琀 甀爀 攀猀愀爀 攀渀漀眀 猀 攀瀀愀爀 愀琀 攀甀渀搀攀爀匀攀最圀椀 琀 Ⰰ 琀 栀攀愀琀 琀 愀挀 欀攀爀挀 愀渀渀漀琀挀 栀愀渀最攀愀琀 爀 愀渀猀 愀挀 琀 椀 漀渀䤀 䐀眀椀 琀 栀漀甀琀愀氀 猀 漀渀甀氀 氀 椀 昀 礀 椀 渀最琀 栀攀 搀椀 最椀 琀 愀氀 猀 椀 最渀愀琀 甀爀 攀⸀ 䄀渀琀 椀 ⴀ 刀攀瀀氀 愀礀倀爀 漀琀 攀挀 琀 椀 漀渀 䄀猀 漀ⴀ 挀 愀氀 氀 攀搀爀 攀瀀氀 愀礀愀琀 琀 愀挀 欀挀 愀渀漀挀 挀 甀爀眀栀攀渀瘀 愀氀 椀 搀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀漀渀琀 栀攀䈀吀䌀挀 栀愀椀 渀愀爀 攀ᰠ 爀 攀瀀氀 愀礀 攀搀ᴠ漀渀 琀 栀攀䈀䌀䐀挀 栀愀椀 渀⸀ 唀猀 椀 渀最琀 栀椀 猀猀 琀 爀 愀琀 攀最礀 Ⰰ 愀琀 琀 愀挀 欀攀爀 猀挀 漀甀氀 搀甀猀 攀瘀 愀氀 椀 搀䈀吀䌀琀 爀 愀渀猀 愀挀 琀 椀 漀渀琀 漀爀 漀戀甀猀 攀爀 猀漀昀䈀䌀䐀 攀瘀 攀渀琀 栀漀甀最栀戀漀琀 栀挀 栀愀椀 渀猀栀愀瘀 攀昀 漀爀 欀攀搀⸀ 吀 漀瀀爀 攀瘀 攀渀琀琀 栀椀 猀 Ⰰ 琀 栀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 漀爀 洀愀琀漀昀䈀䌀䐀栀愀猀戀攀攀渀 挀 栀愀渀最攀搀猀 椀 渀挀 攀琀 栀攀昀 漀爀 欀猀 漀琀 栀愀琀䈀吀䌀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀挀 愀渀渀漀琀戀攀洀椀 猀 琀 愀欀攀渀愀猀瘀 愀氀 椀 搀⸀ 吀栀攀猀 攀挀 栀愀渀最攀猀琀 漀琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 漀爀 洀愀琀椀 渀挀 氀 甀搀攀㨀 䄀渀攀眀琀 爀 愀渀猀 愀挀 琀 椀 漀渀瘀 攀爀 猀 椀 漀渀渀甀洀戀攀爀漀昀㄀ ㈀Ⰰ 爀 愀琀 栀攀爀琀 栀愀渀䈀吀䌀ᤠ 猀㄀ ⴀ ㌀⸀ 䄀渀攀眀ǻ攀氀 搀挀 愀氀 氀 攀搀ᰠ 倀爀 攀猀 攀渀琀䈀氀 漀挀 欀䠀愀猀 栀ᴠ眀栀椀 挀 栀挀 漀渀琀 愀椀 渀猀琀 栀攀栀愀猀 栀瘀 愀氀 甀攀漀昀愀戀氀 漀挀 欀ᤠ 猀 栀攀愀搀攀爀 ☠䘀漀爀攀砀 愀洀瀀氀 攀Ⰰ 眀栀攀渀琀 栀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 攀渀琀愀琀琀 栀攀栀攀椀 最栀琀㔀 ㈀ Ⰰ 琀 栀攀ǻ攀氀 搀琀 愀欀攀猀琀 栀攀 栀愀猀 栀瘀 愀氀 甀攀漀昀戀氀 漀挀 欀㔀 ㈀ Ⰰ 㔀 ㄀ 㤀漀爀㔀 ㄀ 㠀⸀ 吀栀攀瘀 愀氀 甀攀椀 猀渀漀琀猀 琀 爀 椀 挀 琀 氀 礀挀 栀攀挀 欀攀搀 挀 甀爀 爀 攀渀琀 氀 礀 Ⰰ 猀 漀愀氀 氀 琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀洀愀琀 挀 栀椀 渀最琀 栀攀昀 漀爀 洀愀琀挀 愀渀戀攀瘀 攀爀 椀 ǻ攀搀 䄀搀搀椀 琀 椀 漀渀愀氀 挀 栀愀渀最攀猀琀 漀䈀䌀䐀ᤠ 猀琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 漀爀 洀愀琀愀爀 攀瀀氀 愀渀渀攀搀昀 漀爀琀 栀攀昀 甀琀 甀爀 攀⸀ 吀栀椀 猀眀椀 氀 氀 椀 渀挀 氀 甀搀攀 昀 攀愀琀 甀爀 攀猀氀 椀 欀攀琀 爀 愀渀猀 愀挀 琀 椀 漀渀瀀爀 漀漀昀 猀愀渀搀瀀攀爀 椀 漀搀猀漀昀琀 爀 愀渀猀 愀挀 琀 椀 漀渀瘀 愀氀 椀 搀椀 琀 礀 ⸀ 夀 漀甀挀 愀渀挀 漀洀瀀愀爀 攀琀 栀攀 琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 漀爀 洀愀琀搀椀 û攀爀 攀渀挀 攀猀戀攀琀 眀攀攀渀䈀吀䌀愀渀搀䈀䌀䐀椀 渀琀 栀攀搀椀 愀最爀 愀洀 戀攀氀 漀眀㨀  吀爀 愀渀猀 愀挀 琀 椀 漀渀䘀漀爀 洀愀琀䌀漀洀瀀愀爀 椀 猀 漀渀 䈀吀䌀ᤠ 猀䌀甀爀 爀 攀渀琀䘀漀爀 洀愀琀 䘀椀 攀氀 搀  䐀攀猀 挀 爀 椀 瀀琀 椀 漀渀  瘀 攀爀 猀 椀 漀渀  䈀䌀䐀ᤠ 猀一攀眀䘀漀爀 洀愀琀 䘀椀 攀氀 搀  䐀攀猀 挀 爀 椀 瀀琀 椀 漀渀  瘀 攀爀 猀 椀 漀渀  琀 砀 开椀 渀挀 漀甀渀琀  䌀漀甀渀琀 攀爀漀昀椀 渀瀀甀琀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀  琀 砀 开椀 渀挀 漀甀渀琀  䌀漀甀渀琀 攀爀漀昀椀 渀瀀甀琀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀  琀 砀 开椀 渀  䄀爀 爀 愀礀漀昀椀 渀瀀甀琀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀  琀 砀 开椀 渀  䄀爀 爀 愀礀漀昀椀 渀瀀甀琀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀  琀 砀 开漀甀琀  䄀爀 爀 愀礀漀昀漀甀琀 瀀甀琀愀搀搀爀 攀猀 猀 攀猀  琀 砀 开漀甀琀  䄀爀 爀 愀礀漀昀漀甀琀 瀀甀琀愀搀搀爀 攀猀 猀 攀猀  琀 砀 开漀甀琀挀 漀甀渀琀  䌀漀甀渀琀 攀爀漀昀漀甀琀 瀀甀琀愀搀搀爀 攀猀 猀 攀猀  琀 砀 开漀甀琀挀 漀甀渀琀  䌀漀甀渀琀 攀爀漀昀漀甀琀 瀀甀琀愀搀搀爀 攀猀 猀 攀猀  氀 漀挀 欀开琀 椀 洀攀  䈀氀 漀挀 欀栀攀椀 最栀琀渀攀攀搀攀搀琀 漀愀挀 挀 攀瀀琀  氀 漀挀 欀开琀 椀 洀攀  䈀氀 漀挀 欀栀攀椀 最栀琀渀攀攀搀攀搀琀 漀愀挀 挀 攀瀀琀  倀爀 漀漀昀 ⴀ 漀昀 ⴀ 圀漀爀 欀䄀氀 最漀爀 椀 琀 栀洀 匀愀琀 漀猀 栀椀 一愀欀愀洀漀琀 漀搀攀猀 椀 最渀攀搀䈀椀 琀 挀 漀椀 渀ᤠ 猀洀椀 渀椀 渀最猀 礀 猀 琀 攀洀 愀猀愀眀愀礀昀 漀爀洀愀樀 漀爀 椀 琀 礀搀攀挀 椀 猀 椀 漀渀猀琀 漀戀攀洀愀搀攀漀渀愀 瀀攀攀爀 ⴀ 琀 漀ⴀ 瀀攀攀爀戀愀猀 椀 猀 ⸀ 吀栀椀 猀倀爀 漀漀昀 ⴀ 漀昀 ⴀ 圀漀爀 欀甀猀 攀搀䌀倀唀瀀漀眀攀爀琀 漀最甀愀爀 愀渀琀 攀攀琀 栀愀琀渀漀搀攀猀眀攀爀 攀昀 愀椀 爀 氀 礀爀 攀瀀爀 攀猀 攀渀琀 攀搀眀椀 琀 栀 ᰠ 漀渀攀ⴀ 䌀倀唀ⴀ 漀渀攀ⴀ 瘀 漀琀 攀ᴠ ⸀ 匀愀琀 漀猀 栀椀 搀攀挀 椀 搀攀搀琀 漀甀猀 攀匀䠀䄀ⴀ ㈀㔀㘀愀猀琀 栀攀愀氀 最漀爀 椀 琀 栀洀 昀 漀爀琀 栀椀 猀倀爀 漀漀昀 ⴀ 漀昀 ⴀ 眀漀爀 欀Ⰰ 眀栀椀 挀 栀眀漀爀 欀攀搀眀攀氀 氀 昀 漀爀猀 攀瘀 攀爀 愀氀 礀 攀愀爀 猀 ⸀ 䠀漀眀攀瘀 攀爀 Ⰰ 愀猀䈀椀 琀 挀 漀椀 渀最愀椀 渀攀搀瀀漀瀀甀氀 愀爀 椀 琀 礀 Ⰰ 琀 栀攀洀椀 渀椀 渀最猀 攀挀 琀 漀爀栀愀猀戀攀挀 漀洀攀洀漀爀 攀愀渀搀洀漀爀 攀挀 漀洀瀀攀琀 椀 琀 椀 瘀 攀⸀ 吀栀攀 搀攀瘀 攀氀 漀瀀洀攀渀琀漀昀䄀瀀瀀氀 椀 挀 愀琀 椀 漀渀匀瀀攀挀 椀 ǻ挀䤀 渀琀 攀最爀 愀琀 攀搀䌀椀 爀 挀 甀椀 琀 猀⠀ 䄀匀䤀 䌀猀 ⤀渀漀眀洀攀愀渀猀琀 栀愀琀愀渀礀 漀渀攀眀椀 琀 栀愀挀 挀 攀猀 猀琀 漀琀 栀攀 氀 愀琀 攀猀 琀洀椀 渀椀 渀最栀愀爀 搀眀愀爀 攀挀 愀渀漀甀琀 瀀愀挀 攀琀 爀 愀搀椀 琀 椀 漀渀愀氀 䌀倀唀洀椀 渀攀爀 猀眀椀 琀 栀攀愀猀 攀⸀ 䘀漀爀䈀椀 琀 挀 漀椀 渀琀 漀爀 攀洀愀椀 渀搀攀挀 攀渀琀 爀 愀氀 椀 稀 攀搀Ⰰ 愀渀攀眀洀椀 渀椀 渀最愀氀 最漀爀 椀 琀 栀洀 洀甀猀 琀戀攀椀 洀瀀氀 攀洀攀渀琀 攀搀琀 栀愀琀挀 愀渀爀 攀猀 椀 猀 琀愀琀 琀 攀洀瀀琀 猀戀礀 栀愀爀 搀眀愀爀 攀洀愀渀甀昀 愀挀 琀 甀爀 攀爀 猀琀 漀漀甀琀 瀀愀挀 攀琀 爀 愀搀椀 琀 椀 漀渀愀氀 洀椀 渀攀爀 猀 ⸀ 漀爀 攀猀 琀 漀爀 攀昀 愀椀 爀洀椀 渀椀 渀最瀀爀 愀挀 琀 椀 挀 攀猀 Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀甀琀 椀 氀 椀 稀 攀猀愀渀搀栀愀猀椀 洀瀀爀 漀瘀 攀搀甀瀀漀渀琀 栀攀堀㄀ ㌀倀爀 漀漀昀 ⴀ 漀昀 ⴀ 圀漀爀 欀 吀 愀氀 最漀爀 椀 琀 栀洀⸀ 吀栀椀 猀洀攀愀渀猀琀 栀愀琀愀氀 氀 䄀匀䤀 䌀猀搀攀猀 椀 最渀攀搀昀 漀爀䈀椀 琀 挀 漀椀 渀匀䠀䄀ⴀ ㈀㔀㘀愀爀 攀渀漀眀攀渀琀 椀 爀 攀氀 礀椀 渀攀û攀挀 琀 椀 瘀 攀漀渀䈀䌀䐀⸀ 圀椀 琀 栀 堀㄀ ㌀Ⰰ 挀 爀 攀愀琀 椀 渀最渀攀眀䄀匀䤀 䌀栀愀爀 搀眀愀爀 攀椀 猀洀愀搀攀椀 渀挀 爀 攀搀椀 戀氀 礀搀椀 ϻ挀 甀氀 琀戀礀愀栀椀 最栀氀 攀瘀 攀氀 漀昀挀 漀洀瀀氀 攀砀 椀 琀 礀 Ⰰ 爀 攀搀甀挀 椀 渀最琀 栀攀琀 栀爀 攀愀琀 漀昀洀椀 渀椀 渀最挀 攀渀琀 爀 愀氀 椀 稀 愀琀 椀 漀渀⸀ 堀㄀ ㌀眀愀猀猀 瀀攀挀 椀 ǻ挀 愀氀 氀 礀挀 爀 攀愀琀 攀搀昀 漀爀最爀 愀瀀栀椀 挀 猀挀 愀爀 搀洀椀 渀椀 渀最Ⰰ 愀猀 琀 愀渀搀愀爀 搀漀昀栀愀爀 搀眀愀爀 攀琀 栀愀琀椀 猀  眀椀 搀攀氀 礀愀挀 挀 攀猀 猀 椀 戀氀 攀猀 漀琀 栀愀琀挀 漀洀瀀攀琀 椀 琀 椀 漀渀昀 漀爀洀椀 渀椀 渀最椀 猀昀 愀椀 爀昀 漀爀琀 栀攀愀瘀 攀爀 愀最攀甀猀 攀爀 ⸀ 圀栀椀 氀 攀琀 栀攀堀㄀ ㌀愀氀 最漀爀 椀 琀 栀洀 攀砀 椀 猀 琀 攀搀戀攀昀 漀爀 攀䈀䌀䐀ᤠ 猀氀 愀甀渀挀 栀Ⰰ 琀 栀攀堀㄀ ㌀搀攀瘀 攀氀 漀瀀洀攀渀琀琀 攀愀洀 栀愀猀愀搀搀攀搀愀搀搀椀 琀 椀 漀渀愀氀 椀 洀瀀爀 漀瘀 攀洀攀渀琀 猀琀 漀䈀䌀䐀ᤠ 猀瘀 攀爀 猀 椀 漀渀琀 漀戀漀氀 猀 琀 攀爀愀渀搀攀渀猀 甀爀 攀猀 攀挀 甀爀 椀 琀 礀 ⸀ 吀栀椀 猀椀 渀挀 氀 甀搀攀猀琀 栀攀匀䴀㌀栀愀猀 栀愀氀 最漀爀 椀 琀 栀洀Ⰰ 椀 猀 猀 甀攀搀 戀礀琀 栀攀䌀栀椀 渀攀猀 攀䌀爀 礀 瀀琀 漀最爀 愀瀀栀礀䄀搀洀椀 渀椀 猀 琀 爀 愀琀 椀 漀渀椀 渀㈀ ㄀ 㘀愀猀琀 栀攀渀愀琀 椀 漀渀愀氀 猀 琀 愀渀搀愀爀 搀昀 漀爀挀 爀 礀 瀀琀 漀最爀 愀瀀栀椀 挀愀瀀瀀氀 椀 挀 愀琀 椀 漀渀猀 ⸀ 䌀甀爀 爀 攀渀琀 氀 礀 Ⰰ 䈀䌀䐀椀 猀琀 栀攀漀渀氀 礀挀 爀 礀 瀀琀 漀挀 甀爀 爀 攀渀挀 礀琀 漀甀琀 椀 氀 椀 稀 攀琀 栀攀堀㄀ ㌀愀氀 最漀爀 椀 琀 栀洀 椀 渀琀 栀椀 猀洀愀渀渀攀爀 ⸀ 䄀猀琀 椀 洀攀最漀攀猀漀渀Ⰰ 䈀䌀䐀ᤠ 猀 搀攀瘀 攀氀 漀瀀洀攀渀琀琀 攀愀洀 眀椀 氀 氀 挀 漀渀琀 椀 渀甀攀琀 漀椀 洀瀀爀 漀瘀 攀椀 琀 猀洀椀 渀椀 渀最愀氀 最漀爀 椀 琀 栀洀 琀 漀攀渀猀 甀爀 攀琀 栀愀琀匀愀琀 漀猀 栀椀 ᤠ 猀漀爀 椀 最椀 渀愀氀 瘀 椀 猀 椀 漀渀昀 漀爀 搀椀 猀 琀 爀 椀 戀甀琀 攀搀挀 漀渀猀 攀渀猀 甀猀爀 攀洀愀椀 渀猀椀 渀琀 愀挀 琀 ⸀  䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀 圀栀椀 氀 攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀瀀爀 漀琀 漀挀 漀氀 瀀爀 漀瘀 椀 搀攀猀昀 愀猀 琀 攀爀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 Ⰰ 愀搀搀椀 琀 椀 漀渀愀氀 猀 挀 愀氀 椀 渀最猀 漀氀 甀琀 椀 漀渀猀愀爀 攀猀 琀 椀 氀 氀 爀 攀焀甀椀 爀 攀搀昀 漀爀 琀 栀攀瀀爀 漀琀 漀挀 漀氀 琀 漀挀 漀洀瀀攀琀 攀眀椀 琀 栀琀 爀 愀搀椀 琀 椀 漀渀愀氀 瀀愀礀 洀攀渀琀洀攀琀 栀漀搀猀 ⸀ 吀 漀愀挀 挀 漀洀瀀氀 椀 猀 栀琀 栀椀 猀 Ⰰ 琀 栀攀䈀䌀䐀搀攀瘀 攀氀 漀瀀洀攀渀琀琀 攀愀洀 椀 猀 眀漀爀 欀椀 渀最琀 漀椀 洀瀀氀 攀洀攀渀琀䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀Ⰰ 愀∀ 猀 攀挀 漀渀搀氀 愀礀 攀爀 ∀瀀愀礀 洀攀渀琀瀀爀 漀琀 漀挀 漀氀 ǻ爀 猀 琀瀀爀 漀瀀漀猀 攀搀戀礀䨀 漀猀 攀瀀栀倀漀漀渀 愀渀搀吀栀愀搀搀攀甀猀䐀爀 礀 樀 愀椀 渀㈀ ㄀ 㘀⸀ 吀栀椀 猀猀 漀氀 甀琀 椀 漀渀愀氀 氀 漀眀猀甀猀 攀爀 猀琀 漀漀瀀攀渀ᰠ 瀀愀礀 洀攀渀琀挀 栀愀渀渀攀氀 猀 ᴠ戀礀挀 漀洀洀椀 琀 琀 椀 渀最愀渀 愀洀漀甀渀琀漀昀䈀椀 琀 挀 漀椀 渀琀 栀愀琀挀 愀渀琀 栀攀渀戀攀猀 攀渀琀琀 漀漀琀 栀攀爀瀀愀爀 琀 椀 挀 椀 瀀愀渀琀 猀漀渀琀 栀攀挀 栀愀渀渀攀氀 眀椀 琀 栀漀甀琀戀攀椀 渀最挀 漀渀ǻ爀 洀攀搀漀渀琀 栀攀 洀愀椀 渀挀 栀愀椀 渀⸀ 唀猀 椀 渀最䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀挀 愀渀攀渀愀戀氀 攀甀猀 攀爀 猀琀 漀洀愀欀攀椀 渀猀 琀 愀渀琀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀愀琀愀 渀攀愀爀 氀 礀氀 椀 洀椀 琀 氀 攀猀 猀瀀愀挀 攀昀 漀爀愀瘀 攀爀 礀氀 漀眀挀 漀猀 琀 ⸀  倀愀礀洀攀渀琀䌀栀愀渀渀攀氀 猀漀渀䰀椀 最栀琀 渀椀 渀最 伀瀀攀渀䌀栀愀渀渀攀氀 ⠀ 漀渀ⴀ 挀 栀愀椀 渀⤀  䄀氀 椀 挀 攀愀渀搀䈀漀戀挀 愀渀洀愀欀攀愀栀椀 最栀瘀 漀氀 甀洀攀漀昀 椀 渀猀 琀 愀渀琀 Ⰰ 琀 爀 甀猀 琀 氀 攀猀 猀洀椀 挀 爀 漀瀀愀礀 洀攀渀琀 猀漀ûⴀ 挀 栀愀椀 渀  䄀氀 椀 挀 攀  䈀漀戀  䌀氀 漀猀 攀䌀栀愀渀渀攀氀 ⠀ 漀渀ⴀ 挀 栀愀椀 渀⤀  圀栀椀 氀 攀搀攀瘀 攀氀 漀瀀洀攀渀琀漀渀愀䈀吀䌀ⴀ 瘀 攀爀 猀 椀 漀渀漀昀䰀椀 最栀琀 渀椀 渀最椀 猀甀渀搀攀爀 眀愀礀甀猀 椀 渀最䜀漀氀 愀渀最Ⰰ 琀 栀攀䈀䌀䐀琀 攀愀洀 栀愀猀椀 渀猀 琀 攀愀搀 漀瀀琀 攀搀琀 漀眀爀 椀 琀 攀琀 栀攀椀 爀椀 洀瀀氀 攀洀攀渀琀 愀琀 椀 漀渀椀 渀琀 栀攀䌀瀀爀 漀最爀 愀洀洀椀 渀最氀 愀渀最甀愀最攀⸀ 䌀眀愀猀挀 栀漀猀 攀渀猀 瀀攀挀 椀 ǻ挀 愀氀 氀 礀昀 漀爀椀 琀 猀 瀀漀爀 琀 愀戀椀 氀 椀 琀 礀愀渀搀攀ϻ挀 椀 攀渀挀 礀 Ⰰ 愀氀 氀 漀眀椀 渀最椀 琀琀 漀攀愀猀 椀 氀 礀爀 甀渀漀渀愀爀 愀渀最攀漀昀搀椀 û攀爀 攀渀琀搀攀瘀 椀 挀 攀猀愀渀搀挀 漀渀猀 甀洀攀氀 攀猀 猀爀 攀猀 漀甀爀 挀 攀猀 琀 栀愀渀椀 琀 猀䈀吀䌀挀 漀甀渀琀 攀爀 瀀愀爀 琀 ⸀ 吀栀椀 猀瘀 攀爀 猀 椀 漀渀漀昀䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀栀愀猀愀氀 爀 攀愀搀礀瀀愀猀 猀 攀搀昀 甀渀挀 琀 椀 漀渀愀氀 椀 琀 礀琀 攀猀 琀 猀猀 甀挀 栀愀猀 挀 爀 攀愀琀 椀 渀最渀漀搀攀猀愀渀搀瀀愀礀 洀攀渀琀挀 栀愀渀渀攀氀 猀 Ⰰ 搀攀挀 氀 愀爀 椀 渀最愀渀搀瀀愀礀 椀 渀最琀 爀 愀渀猀 愀挀 琀 椀 漀渀爀 攀焀甀攀猀 琀 猀 Ⰰ 挀 漀渀ǻ爀 洀椀 渀最爀 攀挀 攀椀 瀀琀 猀愀挀 爀 漀猀 猀 渀漀搀攀猀 Ⰰ 愀渀搀琀 攀猀 琀 椀 渀最瀀愀礀 洀攀渀琀 猀戀攀琀 眀攀攀渀猀 攀渀搀攀爀 猀愀渀搀爀 攀挀 攀椀 瘀 攀爀 猀 ⸀ 䄀猀 琀 愀戀氀 攀瘀 攀爀 猀 椀 漀渀椀 猀攀砀 瀀攀挀 琀 攀搀琀 漀戀攀搀攀瀀氀 漀礀 攀搀 漀渀䨀 甀氀 礀㌀㄀ Ⰰ ㈀㄀ 㠀Ⰰ 愀昀 琀 攀爀愀挀 漀洀瀀氀 攀琀 攀愀猀 猀 攀猀 猀 洀攀渀琀戀礀搀攀瘀 攀氀 漀瀀攀爀 猀愀渀搀琀 攀猀 琀 攀爀 猀 ⸀  圀愀氀 氀 攀琀䘀攀愀琀 甀爀 攀猀 䠀䐀圀愀氀 氀 攀琀䜀攀渀攀爀 愀琀 椀 漀渀 䠀椀 攀爀 愀爀 挀 栀椀 挀 愀氀 搀攀琀 攀爀 洀椀 渀椀 猀 琀 椀 挀眀愀氀 氀 攀琀 猀 Ⰰ 漀琀 栀攀爀 眀椀 猀 攀欀渀漀眀渀愀猀䠀䐀眀愀氀 氀 攀琀 猀 Ⰰ 椀 猀愀昀 攀愀琀 甀爀 攀椀 洀瀀氀 攀洀攀渀琀 攀搀漀渀 䈀䌀䐀琀 漀挀 爀 攀愀琀 攀洀甀氀 琀 椀 瀀氀 攀愀挀 挀 漀甀渀琀 猀昀 爀 漀洀 愀猀 椀 渀最氀 攀爀 漀漀琀欀攀礀 ⸀ 圀椀 琀 栀琀 栀椀 猀爀 甀氀 攀Ⰰ 挀 氀 椀 攀渀琀 猀漀渀氀 礀渀攀攀搀琀 漀猀 愀瘀 攀愀 洀愀猀 琀 攀爀瀀爀 椀 瘀 愀琀 攀欀攀礀 Ⰰ 眀栀椀 挀 栀挀 愀渀最攀渀攀爀 愀琀 攀洀甀氀 琀 椀 瀀氀 攀猀 甀戀ⴀ 瀀爀 椀 瘀 愀琀 攀欀攀礀 猀愀渀搀猀 甀戀ⴀ 愀搀搀爀 攀猀 猀 攀猀 ⸀ 䌀氀 椀 攀渀琀 猀挀 愀渀 渀漀眀攀愀猀 椀 氀 礀洀愀渀愀最攀琀 栀攀戀愀氀 愀渀挀 攀猀漀昀愀氀 氀 愀挀 挀 漀甀渀琀 猀甀渀搀攀爀琀 栀椀 猀漀渀攀洀愀猀 琀 攀爀瀀爀 椀 瘀 愀琀 攀欀攀礀愀渀搀猀 攀氀 攀挀 琀 椀 瘀 攀氀 礀 椀 猀 猀 甀攀挀 栀椀 氀 搀欀攀礀 猀眀椀 琀 栀氀 椀 洀椀 琀 攀搀愀挀 挀 攀猀 猀 ⸀ 吀栀椀 猀栀攀氀 瀀猀爀 攀搀甀挀 攀琀 栀攀瀀漀猀 猀 椀 戀椀 氀 椀 琀 礀漀昀愀洀愀猀 琀 攀爀瀀爀 椀 瘀 愀琀 攀欀攀礀 攀砀 瀀漀猀 甀爀 攀Ⰰ 攀渀猀 甀爀 椀 渀最琀 栀攀猀 愀昀 攀琀 礀漀昀昀 甀渀搀猀 ⸀ 䔀氀 攀挀 琀 爀 甀洀 䤀 渀琀 攀最爀 愀琀 椀 漀渀 䔀氀 攀挀 琀 爀 甀洀 椀 猀愀氀 椀 最栀琀 眀攀椀 最栀琀䈀椀 琀 挀 漀椀 渀眀愀氀 氀 攀琀琀 栀愀琀栀愀猀戀攀攀渀搀攀瘀 攀氀 漀瀀攀搀愀渀搀猀 甀瀀瀀漀爀 琀 攀搀猀 椀 渀挀 攀㈀ ㄀ ㄀ ⸀ 䈀礀 漀瀀攀爀 愀琀 椀 渀最椀 渀挀 漀渀樀 甀渀挀 琀 椀 漀渀眀椀 琀 栀猀 攀爀 瘀 攀爀 猀琀 栀愀琀椀 渀搀攀砀琀 栀攀戀氀 漀挀 欀挀 栀愀椀 渀Ⰰ 䔀氀 攀挀 琀 爀 甀洀 挀 氀 椀 攀渀琀 猀挀 愀渀爀 甀渀眀椀 琀 栀 椀 渀猀 琀 愀渀琀猀 琀 愀爀 琀 甀瀀琀 椀 洀攀猀愀渀搀氀 漀眀爀 攀猀 漀甀爀 挀 攀甀猀 愀最攀⸀ 䔀氀 攀挀 琀 爀 甀洀 愀氀 猀 漀漀û攀爀 猀愀眀椀 搀攀爀 愀渀最攀漀昀昀 甀渀挀 琀 椀 漀渀愀氀 椀 琀 礀 椀 渀挀 氀 甀搀椀 渀最挀 漀氀 搀猀 琀 漀爀 愀最攀Ⰰ 洀甀氀 琀 椀 猀 椀 最猀 攀挀 甀爀 椀 琀 礀愀渀搀椀 渀琀 攀最爀 愀琀 椀 漀渀眀椀 琀 栀栀愀爀 搀眀愀爀 攀眀愀氀 氀 攀琀 猀 ⸀ 吀 漀挀 愀瀀椀 琀 愀氀 椀 稀 攀漀渀 䔀氀 攀挀 琀 爀 甀洀ᤠ 猀昀 攀愀琀 甀爀 攀ⴀ 猀 攀琀 Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀栀愀猀爀 攀氀 攀愀猀 攀搀椀 琀 猀漀眀渀瘀 攀爀 猀 椀 漀渀漀昀琀 栀攀䔀氀 攀挀 琀 爀 甀洀 眀愀氀 氀 攀琀 ⸀ 䄀昀 琀 攀爀 搀漀眀渀氀 漀愀搀椀 渀最琀 栀攀眀愀氀 氀 攀琀 Ⰰ 甀猀 攀爀 猀挀 愀渀焀甀椀 挀 欀氀 礀挀 漀渀渀攀挀 琀琀 漀琀 栀攀䈀䌀䐀渀攀琀 眀漀爀 欀愀渀搀挀 氀 愀椀 洀 愀渀礀挀 漀椀 渀猀琀 栀愀琀 琀 栀攀礀洀愀礀栀愀瘀 攀攀愀爀 渀攀搀昀 爀 漀洀 琀 栀攀一漀瘀 攀洀戀攀爀昀 漀爀 欀⸀ 䈀䌀䐀倀愀礀䴀漀戀椀 氀 攀 吀 漀攀渀猀 甀爀 攀琀 栀愀琀愀氀 氀 甀猀 攀爀 猀挀 愀渀攀愀猀 椀 氀 礀瀀攀爀 昀 漀爀 洀 瀀愀礀 洀攀渀琀 猀 Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀眀椀 氀 氀 戀攀琀 栀攀ǻ爀 猀 琀䈀椀 琀 挀 漀椀 渀 瀀爀 漀樀 攀挀 琀琀 漀猀 甀瀀瀀漀爀 琀愀渀漀ϻ挀 椀 愀氀 洀漀戀椀 氀 攀眀愀氀 氀 攀琀 ⸀ 䄀渀䄀渀搀爀 漀椀 搀眀愀氀 氀 攀琀戀愀猀 攀搀漀渀䔀氀 攀挀 琀 爀 甀洀 栀愀猀愀氀 爀 攀愀搀礀戀攀攀渀 爀 攀氀 攀愀猀 攀搀眀椀 琀 栀愀渀椀 伀匀瘀 攀爀 猀 椀 漀渀挀 甀爀 爀 攀渀琀 氀 礀甀渀搀攀爀搀攀瘀 攀氀 漀瀀洀攀渀琀 ⸀ 䌀甀爀 爀 攀渀琀眀愀氀 氀 攀琀昀 攀愀琀 甀爀 攀猀椀 渀挀 氀 甀搀攀眀愀氀 氀 攀琀 渀愀洀攀洀漀搀椀 ǻ挀 愀琀 椀 漀渀Ⰰ 愀挀 挀 漀甀渀琀猀 眀椀 琀 挀 栀椀 渀最Ⰰ 愀渀搀儀刀猀 挀 愀渀渀椀 渀最⸀ 圀椀 琀 栀䈀䌀䐀倀愀礀 Ⰰ 搀椀 最椀 琀 愀氀 挀 甀爀 爀 攀渀挀 礀眀椀 氀 氀 戀攀 洀愀搀攀愀挀 挀 攀猀 猀 椀 戀氀 攀琀 漀洀椀 氀 氀 椀 漀渀猀漀昀甀渀戀愀渀欀攀搀椀 渀搀椀 瘀 椀 搀甀愀氀 猀椀 渀攀洀攀爀 最椀 渀最洀愀爀 欀攀琀 猀 Ⰰ 戀礀 瀀愀猀 猀 椀 渀最琀 栀攀ǻ渀愀渀挀 椀 愀氀 戀愀爀 爀 椀 攀爀 猀琀 栀愀琀栀愀搀瀀爀 攀瘀 椀 漀甀猀 氀 礀攀砀 挀 氀 甀搀攀搀琀 栀攀洀 昀 爀 漀洀 琀 栀攀最氀 漀戀愀氀 攀挀 漀渀漀洀礀 ⸀  䠀漀眀琀 漀䄀挀 焀甀椀 爀 攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀 唀猀 攀爀 猀眀栀漀栀攀氀 搀䈀吀䌀愀琀琀 栀攀琀 椀 洀攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀眀愀猀挀 爀 攀愀琀 攀搀栀愀瘀 攀愀甀琀 漀洀愀琀 椀 挀 愀氀 氀 礀戀攀挀 漀洀攀漀眀渀攀爀 猀 漀昀䈀䌀䐀⸀ 倀氀 攀愀猀 攀渀漀琀 攀琀 栀愀琀愀眀愀氀 氀 攀琀眀椀 琀 栀㄀䈀吀䌀眀椀 氀 氀 栀漀氀 搀㄀ 䈀䌀䐀戀愀猀 攀搀漀渀琀 栀攀猀 甀瀀瀀氀 礀挀 栀愀渀最攀⸀ 唀猀 攀爀 猀 洀愀礀愀氀 猀 漀攀愀爀 渀䈀䌀䐀戀礀洀椀 渀椀 渀最眀椀 琀 栀最爀 愀瀀栀椀 挀 猀挀 愀爀 搀猀漀爀戀甀礀 椀 渀最挀 漀椀 渀猀昀 爀 漀洀 愀渀攀砀 挀 栀愀渀最攀漀爀猀 攀挀 漀渀搀愀爀 礀 洀愀爀 欀攀琀 ⸀ 吀 漀攀渀挀 漀甀爀 愀最攀琀 栀攀挀 漀洀洀甀渀椀 琀 礀琀 漀愀猀 猀 椀 猀 琀椀 渀琀 栀攀挀 漀渀猀 琀 爀 甀挀 琀 椀 漀渀漀昀䈀䌀䐀ᤠ 猀攀挀 漀猀 礀 猀 琀 攀洀Ⰰ 挀 漀渀琀 爀 椀 戀甀琀 漀爀 猀眀椀 氀 氀 爀 攀挀 攀椀 瘀 攀猀 瀀攀挀 椀 ǻ挀愀洀漀甀渀琀 猀漀昀䈀䌀䐀愀猀愀爀 攀眀愀爀 搀愀猀眀攀氀 氀 ⸀  刀漀愀搀洀愀瀀 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀ᤠ 猀瀀甀爀 瀀漀猀 攀椀 猀琀 漀栀攀氀 瀀甀渀戀愀渀欀攀搀愀渀搀ǻ渀愀渀挀 椀 愀氀 氀 礀甀渀搀攀爀 猀 攀爀 瘀 攀搀瀀攀漀瀀氀 攀椀 渀眀愀礀 猀琀 栀愀琀 戀攀渀攀ǻ琀琀 栀攀洀⸀ 吀栀攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀椀 渀椀 琀 椀 愀琀 椀 瘀 攀眀椀 氀 氀 攀猀 瀀攀挀 椀 愀氀 氀 礀昀 漀挀 甀猀漀渀瀀攀漀瀀氀 攀氀 椀 瘀 椀 渀最椀 渀愀爀 攀愀猀眀栀攀爀 攀 漀琀 栀攀爀挀 甀爀 爀 攀渀挀 椀 攀猀愀渀搀ǻ渀愀渀挀 椀 愀氀 椀 渀猀 琀 椀 琀 甀琀 椀 漀渀猀栀愀瘀 攀昀 愀椀 氀 攀搀琀 栀攀洀Ⰰ 猀 甀挀 栀愀猀椀 渀琀 攀爀 渀愀琀 椀 漀渀愀氀 琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 Ⰰ 猀 攀挀 甀爀 椀 琀 礀 Ⰰ 愀渀搀眀攀愀氀 琀 栀猀 琀 漀爀 愀最攀⸀ 唀氀 琀 椀 洀愀琀 攀氀 礀 Ⰰ 琀 栀椀 猀眀椀 氀 氀 椀 渀挀 爀 攀愀猀 攀愀搀漀瀀琀 椀 漀渀愀渀搀甀猀 愀最攀漀昀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀 愀渀搀瀀爀 漀瘀 椀 搀攀洀愀爀 最椀 渀愀氀 椀 稀 攀搀瀀攀漀瀀氀 攀眀椀 琀 栀愀挀 甀爀 爀 攀渀挀 礀琀 栀愀琀眀漀爀 欀猀椀 渀琀 栀攀椀 爀椀 渀琀 攀爀 攀猀 琀 ⸀ 伀甀爀漀戀樀 攀挀 琀 椀 瘀 攀猀 椀 渀挀 氀 甀搀攀昀 漀猀 琀 攀爀 椀 渀最琀 栀攀眀椀 搀攀猀 瀀爀 攀愀搀甀猀 攀漀昀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀爀 漀甀渀搀琀 栀攀眀漀爀 氀 搀Ⰰ 攀洀瀀漀眀攀爀 椀 渀最 甀渀戀愀渀欀攀搀瀀攀漀瀀氀 攀琀 漀甀猀 攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀琀 漀戀甀椀 氀 搀眀攀愀氀 琀 栀昀 漀爀琀 栀攀洀猀 攀氀 瘀 攀猀 Ⰰ 攀猀 琀 愀戀氀 椀 猀 栀椀 渀最䈀椀 琀 挀 漀椀 渀 䐀椀 愀洀漀渀搀愀猀琀 栀攀渀甀洀戀攀爀漀渀攀挀 爀 礀 瀀琀 漀挀 甀爀 爀 攀渀挀 礀椀 渀攀洀攀爀 最椀 渀最攀挀 漀渀漀洀椀 攀猀 Ⰰ 愀渀搀洀愀欀椀 渀最䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀 愀挀 挀 攀猀 猀 椀 戀氀 攀愀渀搀甀猀 愀戀氀 攀昀 漀爀攀瘀 攀爀 礀 搀愀礀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 ⸀ 圀攀猀 攀攀欀琀 漀愀挀 栀椀 攀瘀 攀漀甀爀甀氀 琀 椀 洀愀琀 攀最漀愀氀 漀昀䈀椀 琀 挀 漀椀 渀 䐀椀 愀洀漀渀搀戀攀挀 漀洀椀 渀最琀 栀攀ᰠ 䈀椀 琀 挀 漀椀 渀ᴠ琀 栀愀琀愀挀 栀椀 攀瘀 攀猀匀愀琀 漀猀 栀椀 ᤠ 猀瘀 椀 猀 椀 漀渀漀昀戀攀挀 漀洀椀 渀最愀最氀 漀戀愀氀 氀 礀愀挀 挀 攀瀀琀 攀搀 搀椀 最椀 琀 愀氀 挀 愀猀 栀⸀ 圀椀 琀 栀琀 栀椀 猀椀 渀洀椀 渀搀Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀眀椀 氀 氀 猀 琀 爀 椀 瘀 攀琀 漀瀀爀 漀瘀 椀 搀攀戀攀琀 琀 攀爀猀 漀氀 甀琀 椀 漀渀猀昀 漀爀ǻ渀愀渀挀 椀 愀氀 猀 攀爀 瘀 椀 挀 攀猀眀漀爀 氀 搀眀椀 搀攀⸀ 吀栀椀 猀瀀氀 愀渀⠀ 䐀攀挀㈀ ㄀ 㜀縀㈀ ㄀ 㠀儀㈀⤀椀 猀最攀渀攀爀 愀琀 攀搀戀礀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀䌀漀洀洀甀渀椀 琀 礀 Ⰰ 戀愀猀 攀搀漀渀琀 栀攀 挀 漀洀洀甀渀椀 挀 愀琀 椀 漀渀眀椀 琀 栀琀 栀攀䈀䌀䐀搀攀瘀 攀氀 漀瀀洀攀渀琀琀 攀愀洀猀䔀嘀䔀夀愀渀搀  㜀 ⸀  䄀猀漀昀䨀 甀氀 礀㈀ ㄀ 㠀Ⰰ 琀 栀攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀吀 攀猀 琀 一攀琀椀 猀爀 甀渀渀椀 渀最猀 洀漀漀琀 栀氀 礀 Ⰰ 眀椀 琀 栀渀攀眀瘀 攀爀 猀 椀 漀渀猀琀 漀戀攀 爀 攀氀 攀愀猀 攀搀猀 漀漀渀琀 栀愀琀眀椀 氀 氀 椀 渀琀 攀最爀 愀琀 攀猀 攀瘀 攀爀 愀氀 甀瀀搀愀琀 攀猀 ⸀ 唀瀀搀愀琀 攀猀琀 栀愀琀愀爀 攀攀砀 瀀攀挀 琀 攀搀琀 漀戀攀椀 渀琀 栀攀渀攀砀 琀 瘀 攀爀 猀 椀 漀渀椀 渀挀 氀 甀搀攀戀甀琀愀爀 攀渀漀琀氀 椀 洀椀 琀 攀搀琀 漀㨀 渀攀眀戀漀漀猀 琀瘀 攀爀 猀 椀 漀渀爀 攀焀甀椀 爀 攀洀攀渀琀 猀眀椀 琀 栀愀瘀 ㄀ ⸀ 㐀㜀 ⸀ 洀椀 渀椀 洀甀洀Ⰰ 甀瀀最爀 愀搀攀搀娀䴀儀琀 漀倀礀 琀 栀漀渀㌀猀 甀瀀瀀漀爀 琀 Ⰰ 唀渀椀 昀 礀挀 漀搀攀猀 琀 礀 氀 攀Ⰰ 愀戀椀 氀 椀 琀 礀琀 漀搀攀氀 攀琀 攀爀 攀搀甀渀搀愀渀琀瀀爀 椀 漀爀 椀 琀 礀 樀 甀搀最攀洀攀渀琀挀 漀搀攀Ⰰ 爀 攀琀 甀爀 渀攀爀 爀 漀爀挀 漀搀攀猀 Ⰰ 洀漀搀椀 ǻ挀 愀琀 椀 漀渀漀昀猀 漀洀攀洀椀 猀 氀 攀愀搀椀 渀最栀椀 渀琀 猀 Ⰰ 爀 攀挀 漀渀猀 琀 爀 甀挀 琀 椀 渀最 娀愀瀀圀愀氀 氀 攀琀 吀砀 攀猀琀 漀椀 渀挀 爀 攀愀猀 攀猀 琀 愀戀椀 氀 椀 琀 礀 Ⰰ 愀搀搀椀 琀 椀 漀渀漀昀昀 甀渀挀 琀 椀 漀渀猀琀 漀挀 爀 攀愀琀 攀瀀甀爀 猀 攀猀琀 栀爀 漀甀最栀䨀 匀伀一刀倀䌀 爀 攀焀甀攀猀 琀 猀 Ⰰ 爀 攀瀀氀 愀挀 攀洀攀渀琀漀昀漀氀 搀猀 礀 渀琀 愀砀眀椀 琀 栀渀攀眀䌀⬀ ⬀昀 攀愀琀 甀爀 攀猀 Ⰰ 漀瀀琀 椀 洀椀 稀 愀琀 椀 漀渀漀昀渀愀洀攀猀 瀀愀挀 攀猀 Ⰰ 甀瀀搀愀琀 攀搀 愀渀渀漀琀 愀琀 椀 漀渀猀 Ⰰ 攀砀 瀀愀渀猀 椀 漀渀漀昀搀漀挀 甀洀攀渀琀 猀 Ⰰ 愀渀搀椀 渀挀 爀 攀愀猀 攀搀搀漀挀 甀洀攀渀琀爀 攀愀搀愀戀椀 氀 椀 琀 礀 ⸀  ㈀㄀ 㜀 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀椀 猀戀漀爀 渀愀琀琀 栀攀栀愀爀 搀昀 漀爀 欀愀琀戀椀 琀 挀 漀椀 渀戀氀 漀挀 欀栀攀椀 最栀琀 㨀 㐀㤀㔀㠀㘀㘀⸀ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀洀愀椀 渀渀攀琀 Ⰰ 眀愀氀 氀 攀琀 Ⰰ 渀漀搀攀猀挀 漀搀攀Ⰰ 愀渀搀䄀倀䤀 愀爀 攀爀 攀氀 攀愀猀 攀搀 唀瀀搀愀琀 攀洀愀椀 渀渀攀琀  ㈀㄀ 㠀 䐀攀瀀氀 漀礀䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀愀渀搀䈀䌀䐀眀愀氀 氀 攀琀 䈀甀椀 氀 搀椀 渀最䈀䌀䐀愀瀀瀀氀 椀 挀 愀琀 椀 漀渀攀挀 漀猀 礀 猀 琀 攀洀Ⰰ 椀 渀挀 氀 甀搀椀 渀最䈀䌀䐀洀漀戀椀 氀 攀愀瀀瀀 䈀䌀䐀倀愀礀氀 愀甀渀挀 栀愀渀搀椀 洀瀀氀 攀洀攀渀琀 愀琀 椀 漀渀  ㈀㄀ 㤀 䘀甀爀 琀 栀攀爀椀 洀瀀爀 漀瘀 攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀渀搀椀 琀 猀昀 甀渀挀 琀 椀 漀渀愀氀 椀 琀 礀  䘀椀 渀愀渀挀 椀 愀氀匀琀 爀 愀琀 攀最礀 匀琀 愀爀 琀 甀瀀☀伀瀀攀爀 愀琀 椀 漀渀愀氀䔀砀瀀攀渀猀 攀猀 䔀愀爀 氀 礀䐀攀瘀攀氀 漀瀀洀攀渀琀  ⸀ 㘀─  䜀氀 漀戀愀氀 䴀愀爀 欀攀琀䔀砀 瀀愀渀搀椀 渀最  ㄀ ─  䌀漀洀洀甀渀椀 琀 礀䌀漀渀猀 琀 爀 甀挀 琀 椀 漀渀  ⸀ ㈀─  䰀攀最愀氀 ☀䌀漀洀瀀氀 椀 愀渀挀 攀  ⸀ ㈀─  吀 漀琀 愀氀  ㈀─  䐀攀瘀攀氀 漀瀀洀攀渀琀☀䔀挀 漀氀 漀最椀 挀 愀氀⠀ 吀椀 洀攀ⴀ 氀 漀挀 欀攀搀昀 甀渀搀猀 㬀㈀ ─爀 攀氀 攀愀猀 攀搀瀀攀爀礀攀愀爀 ⤀ 䐀攀瘀攀氀 漀瀀洀攀渀琀  ㄀ ─  䔀挀 漀氀 漀最椀 挀 愀氀  ㌀⸀ 㘀─  吀 漀琀 愀氀  ㈀─  匀琀 愀爀 琀 甀瀀☀伀瀀攀爀 愀琀 椀 漀渀愀氀䔀砀瀀攀渀猀 攀猀 䔀愀爀 氀 礀䐀攀瘀 攀氀 漀瀀洀攀渀琀 䴀愀椀 渀渀攀琀☀圀愀氀 氀 攀琀䐀攀瘀 攀氀 漀瀀洀攀渀琀 䴀椀 渀椀 渀最☀倀漀漀氀 猀倀爀 漀最爀 愀洀 䐀攀瘀 攀氀 漀瀀洀攀渀琀 一漀搀攀猀☀匀攀爀 瘀 攀爀 猀䌀漀渀猀 琀 爀 甀挀 琀 椀 漀渀 匀礀 猀 琀 攀洀 ☀匀攀挀 甀爀 椀 琀 礀䴀愀椀 渀琀 攀渀愀渀挀 攀 䔀愀爀 氀 礀䐀攀瘀 攀氀 漀瀀攀爀 猀刀攀眀愀爀 搀猀 䜀氀 漀戀愀氀 䴀愀爀 欀攀琀䔀砀 瀀愀渀搀椀 渀最 䴀攀攀琀 甀瀀猀 ⼀ 䐀攀瘀 攀氀 漀瀀攀爀䌀漀渀昀 攀爀 攀渀挀 攀猀 䜀氀 漀戀愀氀 䄀搀瘀 椀 猀 漀爀 礀刀攀挀 爀 甀椀 琀 洀攀渀琀 䨀 漀椀 渀琀䔀砀 挀 栀愀渀最攀䔀瘀 攀渀琀 猀 匀漀挀 椀 愀氀 䴀攀搀椀 愀☀倀爀 攀猀 猀刀攀氀 攀愀猀 攀猀 䄀搀瘀 攀爀 琀 椀 猀 椀 渀最 䌀漀洀洀甀渀椀 琀 礀䌀漀渀猀 琀 爀 甀挀 琀 椀 漀渀 䄀挀 琀 椀 瘀 椀 琀 椀 攀猀爀 攀最甀氀 愀爀 氀 礀栀攀氀 搀戀礀昀 甀氀 氀 ⴀ 琀 椀 洀攀攀洀瀀氀 漀礀 攀攀猀琀 漀椀 洀瀀氀 攀洀攀渀琀琀 栀攀䜀氀 漀戀愀氀 䌀漀洀洀甀渀椀 琀 礀 刀攀眀愀爀 搀猀倀爀 漀最爀 愀洀 䰀 攀最愀氀 ☀䌀漀洀瀀氀 椀 愀渀挀 攀 䘀甀渀搀猀爀 攀猀 攀爀 瘀 攀搀昀 漀爀昀 甀琀 甀爀 攀最氀 漀戀愀氀 挀 漀洀瀀氀 椀 愀渀挀 攀椀 猀 猀 甀攀猀  䐀攀瘀攀氀 漀瀀洀攀渀琀☀䔀挀 漀猀 礀猀 琀 攀洀猀 䐀攀瘀 攀氀 漀瀀洀攀渀琀 䌀漀爀 攀䐀攀瘀 攀氀 漀瀀洀攀渀琀吀 攀愀洀 䰀椀 最栀琀 渀椀 渀最一攀琀 眀漀爀 欀 䈀愀猀 椀 挀 猀漀昀䔀挀 漀氀 漀最椀 挀 愀氀 䌀漀渀猀 琀 爀 甀挀 琀 椀 漀渀 吀 攀挀 栀渀漀氀 漀最礀唀瀀最爀 愀搀攀猀 䐀攀瘀 攀氀 漀瀀攀爀刀攀眀愀爀 搀倀氀 愀渀 䤀 渀挀 攀渀琀 椀 瘀 攀猀瀀愀椀 搀椀 渀䈀䌀䐀昀 漀爀搀攀瘀 攀氀 漀瀀攀爀 猀 ᤠ 漀瀀攀渀猀 漀甀爀 挀 攀挀 漀搀攀琀 栀愀琀洀攀攀琀 猀挀 攀爀 琀 愀椀 渀 爀 攀焀甀椀 爀 攀洀攀渀琀 猀  䔀挀 漀猀 礀 猀 琀 攀洀 倀愀礀 洀攀渀琀 猀䔀挀 漀猀 礀 猀 琀 攀洀 䘀椀 渀愀渀挀 椀 愀氀 倀爀 漀最爀 愀洀猀琀 栀愀琀ǻ渀愀渀挀 攀瀀椀 氀 漀琀瀀爀 漀最爀 愀洀猀猀 甀挀 栀愀猀搀攀戀椀 琀挀 愀爀 搀猀 Ⰰ 䄀吀䴀 洀愀挀 栀椀 渀攀猀 Ⰰ 愀渀搀 漀琀 栀攀爀昀 漀爀 洀猀漀昀瀀愀礀 洀攀渀琀 䌀爀 漀猀 猀 ⴀ 䈀漀爀 搀攀爀攀ⴀ 挀 漀洀洀攀爀 挀 攀 倀愀礀 洀攀渀琀甀猀 攀搀琀 漀猀 琀 爀 攀愀洀氀 椀 渀攀琀 爀 愀搀椀 琀 椀 漀渀愀氀 氀 礀猀 氀 漀眀挀 爀 漀猀 猀 ⴀ 戀漀爀 搀攀爀瀀愀礀 洀攀渀琀愀渀搀琀 漀猀 漀氀 瘀 攀 昀 漀爀 攀椀 最渀挀 甀爀 爀 攀渀挀 礀瀀愀礀 洀攀渀琀椀 猀 猀 甀攀猀 倀栀礀 猀 椀 挀 愀氀 䐀椀 猀 琀 爀 椀 戀甀琀 椀 漀渀 䴀甀氀 琀 椀 渀愀琀 椀 漀渀愀氀 瀀栀礀 猀 椀 挀 愀氀 搀椀 猀 琀 爀 椀 戀甀琀 椀 漀渀Ⰰ 眀栀椀 氀 攀猀 漀氀 瘀 椀 渀最琀 栀攀椀 猀 猀 甀攀漀昀琀 爀 甀猀 琀愀渀搀猀 眀愀瀀 昀 漀爀 攀椀 最渀挀 甀爀 爀 攀渀挀 椀 攀猀 伀渀ⴀ 䌀栀愀椀 渀䄀瀀瀀氀 椀 挀 愀琀 椀 漀渀 䤀 渀挀 氀 甀搀椀 渀最戀甀琀渀漀琀氀 椀 洀椀 琀 攀搀琀 漀愀瀀瀀氀 椀 挀 愀琀 椀 漀渀漀昀挀 栀愀椀 渀漀眀渀攀爀 猀 栀椀 瀀挀 攀爀 琀 椀 ǻ挀 愀琀 攀猀 Ⰰ 椀 渀猀 甀爀 愀渀挀 攀 瀀漀氀 椀 挀 椀 攀猀愀渀搀漀琀 栀攀爀挀 漀渀猀 琀 爀 甀挀 琀 椀 漀渀ⴀ 戀愀猀 攀搀䈀䌀䐀戀愀挀 欀戀漀渀攀 䄀挀 栀椀 攀瘀 攀挀 漀漀瀀攀爀 愀琀 椀 漀渀瀀爀 漀樀 攀挀 琀 猀眀椀 琀 栀漀琀 栀攀爀戀氀 漀挀 欀挀 栀愀椀 渀琀 攀挀 栀渀漀氀 漀最礀愀渀搀爀 攀猀 漀甀爀 挀 攀猀昀 漀爀 椀 渀琀 攀爀 漀瀀攀爀 愀戀椀 氀 椀 琀 礀 ⸀  䌀漀渀挀 氀 甀猀 椀 漀渀 圀攀栀愀瘀 攀瀀爀 漀瀀漀猀 攀搀愀渀攀眀猀 礀 猀 琀 攀洀 昀 漀爀攀氀 攀挀 琀 爀 漀渀椀 挀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀眀椀 琀 栀琀 栀攀洀椀 猀 猀 椀 漀渀琀 漀洀愀欀攀搀椀 最椀 琀 愀氀 挀 甀爀 爀 攀渀挀 礀愀挀 挀 攀猀 猀 椀 戀氀 攀愀渀搀甀猀 愀戀氀 攀昀 漀爀攀瘀 攀爀 礀 漀渀攀Ⰰ 爀 攀最愀爀 搀氀 攀猀 猀漀昀琀 栀攀椀 爀攀挀 漀渀漀洀椀 挀猀 琀 愀琀 甀猀 Ⰰ 挀 漀甀渀琀 爀 礀漀昀 漀爀 椀 最椀 渀Ⰰ 漀爀氀 攀瘀 攀氀 漀昀愀戀椀 氀 椀 琀 礀 ⸀ 圀椀 琀 栀氀 椀 最栀琀 渀椀 渀最昀 愀猀 琀琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 Ⰰ 栀椀 最栀氀 礀搀椀 氀 甀琀 攀搀琀 爀 愀渀猀 愀挀 琀 椀 漀渀昀 攀攀猀 Ⰰ 愀渀搀琀 攀渀 琀 椀 洀攀猀愀猀洀甀挀 栀猀 甀瀀瀀氀 礀愀猀漀琀 栀攀爀氀 攀愀搀椀 渀最䈀椀 琀 挀 漀椀 渀昀 漀爀 欀猀 Ⰰ 漀甀爀戀氀 漀挀 欀挀 栀愀椀 渀瀀爀 椀 漀爀 椀 琀 椀 稀 攀猀琀 爀 甀猀 琀 Ⰰ 愀挀 挀 攀猀 猀 椀 戀椀 氀 椀 琀 礀 Ⰰ 愀渀搀愀û漀爀 搀愀戀椀 氀 椀 琀 礀 ⸀ 䤀 渀愀渀愀最攀眀栀攀爀 攀洀愀渀礀瀀攀漀瀀氀 攀愀爀 攀昀 漀爀 挀 攀搀琀 漀猀 攀爀 瘀 攀洀漀渀攀礀 Ⰰ 䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀椀 猀愀 挀 甀爀 爀 攀渀挀 礀琀 栀愀琀猀 攀爀 瘀 攀猀琀 栀攀瀀攀漀瀀氀 攀⸀ 圀椀 琀 栀琀 栀攀䈀䌀䐀倀愀礀椀 渀椀 琀 椀 愀琀 椀 瘀 攀Ⰰ 眀攀愀爀 攀愀戀氀 攀琀 漀栀攀氀 瀀琀 栀攀甀渀戀愀渀欀攀搀 愀渀搀ǻ渀愀渀挀 椀 愀氀 氀 礀甀渀搀攀爀 猀 攀爀 瘀 攀搀瀀攀漀瀀氀 攀椀 渀眀愀礀 猀琀 栀愀琀戀攀渀攀ǻ琀琀 栀攀洀⸀ 吀栀攀椀 渀椀 琀 椀 愀琀 椀 瘀 攀眀椀 氀 氀 攀猀 瀀攀挀 椀 愀氀 氀 礀昀 漀挀 甀猀漀渀 瀀攀漀瀀氀 攀氀 椀 瘀 椀 渀最椀 渀愀爀 攀愀猀眀栀攀爀 攀漀琀 栀攀爀挀 甀爀 爀 攀渀挀 椀 攀猀愀渀搀ǻ渀愀渀挀 椀 愀氀 椀 渀猀 琀 椀 琀 甀琀 椀 漀渀猀栀愀瘀 攀昀 愀椀 氀 攀搀琀 栀攀洀Ⰰ 猀 甀挀 栀愀猀 椀 渀琀 攀爀 渀愀琀 椀 漀渀愀氀 琀 爀 愀渀猀 愀挀 琀 椀 漀渀猀 Ⰰ 猀 攀挀 甀爀 椀 琀 礀 Ⰰ 愀渀搀眀攀愀氀 琀 栀猀 琀 漀爀 愀最攀⸀ 唀氀 琀 椀 洀愀琀 攀氀 礀 Ⰰ 琀 栀椀 猀眀椀 氀 氀 椀 渀挀 爀 攀愀猀 攀愀搀漀瀀琀 椀 漀渀愀渀搀 甀猀 愀最攀漀昀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀愀渀搀瀀爀 漀瘀 椀 搀攀洀愀爀 最椀 渀愀氀 椀 稀 攀搀瀀攀漀瀀氀 攀眀椀 琀 栀愀挀 甀爀 爀 攀渀挀 礀琀 栀愀琀眀漀爀 欀猀椀 渀琀 栀攀椀 爀 椀 渀琀 攀爀 攀猀 琀 ⸀ 䤀 渀漀爀 搀攀爀琀 漀洀愀砀 椀 洀椀 稀 攀琀 栀攀愀搀漀瀀琀 椀 漀渀漀昀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀椀 渀攀洀攀爀 最椀 渀最洀愀爀 欀攀琀 猀 Ⰰ 眀攀栀愀瘀 攀 椀 搀攀渀琀 椀 ǻ攀搀猀 攀瘀 攀爀 愀氀 愀挀 琀 椀 漀渀猀 琀 攀瀀猀琀 栀攀䈀椀 琀 挀 漀椀 渀䐀椀 愀洀漀渀搀䘀漀甀渀搀愀琀 椀 漀渀猀 栀漀甀氀 搀琀 愀欀攀⸀ 吀栀攀䈀䌀䐀倀愀礀椀 渀椀 琀 椀 愀琀 椀 瘀 攀 眀椀 氀 氀 椀 渀挀 氀 甀搀攀愀爀 愀渀最攀漀昀漀û攀爀 椀 渀最猀 Ⰰ 椀 渀挀 氀 甀搀椀 渀最挀 漀洀洀甀渀椀 琀 礀攀渀最愀最攀洀攀渀琀 Ⰰ 攀搀甀挀 愀琀 椀 漀渀愀氀 椀 渀椀 琀 椀 愀琀 椀 瘀 攀猀 Ⰰ 琀 栀攀 䈀䌀䐀倀愀礀䤀 渀琀 攀爀 渀愀琀 椀 漀渀愀氀 䴀愀爀 欀攀琀 瀀氀 愀挀 攀Ⰰ 愀渀搀漀渀氀 椀 渀攀椀 渀昀 爀 愀猀 琀 爀 甀挀 琀 甀爀 攀⸀ 䈀䌀䐀眀椀 氀 氀 挀 漀渀琀 椀 渀甀攀琀 漀眀漀爀 欀琀 漀眀愀爀 搀猀 洀愀欀椀 渀最琀 栀攀挀 漀椀 渀椀 渀挀 爀 攀愀猀 椀 渀最氀 礀愀挀 挀 攀猀 猀 椀 戀氀 攀愀渀搀甀猀 愀戀氀 攀⸀  Bitcoin Gold (BTG) www.btcgpu.org press@btcgpu.org support@btcgpu.org Abstract. Bitcoin Gold is a community-led project to create an experimental hard fork of Bitcoin to a new proof-of-work algorithm. The purpose for doing this is to make Bitcoin mining decentralized again. Satoshi Nakamoto’s idealistic vision of “one CPU one vote” has been superseded by a reality where the manufacture and distribution of mining equipment has become dominated by a very small number of entities, some of whom have engaged in abusive practices against individual miners and the Bitcoin network as a whole. Bitcoin Gold will provide an opportunity for countless new people around the world to participate in the mining process with widely-available consumer hardware that is manufactured and distributed by reputable mainstream corporations. A more decentralized, democratic mining infrastructure is more resilient and more in line with Satoshi’s original vision. Perhaps, if the Bitcoin Gold experiment is judged by the community to be a success, it may one day help build consensus for a proof-of-work hard fork on Bitcoin itself.  Introduction Bitcoin was created for many different reasons and every day, people find new reasons to adopt Bitcoin. One of the historical reason is that people do not trust states or banks or any such intermediaries to control their money. One of the central component of the Bitcoin architecture is mining. Simply put miners verify every transaction and compete with each other to get rewards. To get the reward, a miner has to solve a math problem before anyone else in the network. Back in the days, a miner would be any geek with a computer, willing to trade electricity for Bitcoins. Today, a miner is usually a huge warehouse full of very advanced computers, constantly running to solve the math problems as fast as possible.  As it becomes more and more difficult to mine Bitcoin, more capital is required to operate profitable mining operations. They often are located in a country where the electricity is very cheap. Today, a great majority of the miners are located in China because they have access to cheap electricity. In Satoshi Nakamoto’s white paper, one of the main idea was that every CPU was going to be an equally important part of the network. We want Bitcoin to be a shared and independent currency. We don’t want any fat cat to drive our monetary architecture. The importance of miners in the network is constantly growing. To preserve the independence of the Bitcoin ecosystem from miners’ influence, some people thought that it would be a good idea to change the bitcoin protocol in such a way that more people can have access to Bitcoin mining. That’s why Bitcoin Gold was born, in order to bring Bitcoin mining back to the “people”.  Origins of Bitcoin Gold In July 2017, Jack Liao, CEO of LightingAsic and BitExchange, made an announcement that he was working on a hard fork of Bitcoin to change the proof-of-work algorithm from the SHA256 algorithm originally selected by Satoshi Nakamoto to Equihash. The effect of this change will be to enable a whole new class of individuals and businesses to participate in mining this new branch of the Bitcoin blockchain without being required to purchase specialized equipment that is primarily manufactured by one firm that competes against its own customers with newer, more efficient versions of the old equipment that it sells at a high markup. Given the dysfunctional current reality of the Bitcoin mining sector, it is no wonder that there is a tremendous appetite for a proof-of-work change hard fork. Since the Bitcoin Gold project was announced, it has grown rapidly, attracting developers, miners, and supporters from across the globe.  Mechanics of a Hard Fork Bitcoin is a distributed consensus system. All Bitcoin full nodes are running software that enforces the same consensus rules; full nodes that enforce different consensus rules are not part of the Bitcoin network, by definition. If a miner finds a new block that follows the network consensus rules and broadcasts it to the network, all full nodes in the network will accept that block and all of the transactions in it as valid, and miners will build the next block on top of that one. A blockchain hard fork occurs when a block is mined that does not comply with the network consensus rules. Prior to BTC block 478558, Bitcoin nodes and Bitcoin Cash nodes were still enforcing the same consensus rules and accepting the same blockchain as valid. But from that block onward, Bitcoin Cash’s new consensus rules came into effect, which caused Bitcoin nodes to reject blocks that were mined by miners using Bitcoin Cash software, and Bitcoin Cash nodes to reject blocks that were mined by miners who continued to mine with Bitcoin software. Thus, the network bifurcated. The Bitcoin blockchain continued to add a new block every 10 minutes on average, but Bitcoin Cash began building a new blockchain that branched away from Bitcoin. This had the effect of creating a new cryptocurrency that shares the same transaction history and ownership distribution up until the fork block, but then diverges from it. Bitcoin Gold changes different consensus rules than Bitcoin Cash did, but it will fork from Bitcoin in the same manner - by enforcing new consensus rules as of a predetermined BTC block height. The new rules will come into effect at block 491407. From this block onward, Bitcoin Gold miners will begin building a new branch of the Bitcoin blockchain. This new branch is a cryptocurrency with same transaction history and ownership distribution as Bitcoin at the fork block; if you hold BTC, you will automatically receive an equal amount of BTG.  Here are some of the differences between Bitcoin Gold and other forks of Bitcoin:  -  Proof-of-Work Algorithm Bitcoin mining is a proof-of-work system that implements “a distributed timestamp server on a peer-topeer basis.” This is how the Bitcoin manages to maintain consensus across a vast, globally-distributed, permissionless network of nodes. Satoshi Nakamoto chose SHA256 as the algorithm to use in the original design of Bitcoin’s PoW system. SHA256 served Bitcoin well during the early years of its existence, but as Bitcoin became more popular and more valuable, competition in mining became more fierce. Skilled engineers from a small number of companies developed Application Specific Integrated Circuits (ASICs) that could perform SHA256 calculations millions of times faster and more efficiently than any other computer. This made non-specialized computer hardware obsolete for mining Bitcoin. Satoshi’s vision of “one-CPU-one-vote” was replaced by one-ASIC-one-vote.  Now, the only way to participate in Bitcoin mining is to buy hardware from one of those manufactures the biggest of which is believed to manufacture over 70% of the global supply of SHA256 ASICs. This has led to a situation where one entity can hold the entire network hostage, and this is exactly what happened when the backwards compatible Segregated Witness upgrade was blocked by a faction of miners, despite there being universal consensus from Bitcoin experts that it should be activated. In order to counteract this concentration of power in the mining sector, Bitcoin Gold will implement a new proof-of-work algorithm - Equihash. Replacing the SHA256 algorithm means that all of the ASICs designed for Bitcoin will be useless for mining Bitcoin Gold. Equihash is a memory-hard algorithm that can be most efficiently solved by GPUs - a standard type of computer and smartphone hardware that is manufactured by mainstream companies and available around the world. With ASIC manufacturers out of the picture, Bitcoin Gold will provide an opportunity for a whole new class entrepreneurs and investors to get involved with mining. Bitcoin Gold mining will be decentralized again, closer to Satoshi’s original vision. ASIC-resistance is a permanent attribute of Bitcoin Gold. It is much more difficult to create ASICs for a memory hard algorithm like Equihash than SHA256, however it is not impossible. If the day ever comes when Equihash ASICs begin to proliferate and mining begins to centralize again, Bitcoin Gold will have another hard fork to implement a new PoW algorithm.  Difficulty Adjustment Algorithm In Bitcoin, the difficulty of mining adjusts every 2016 blocks (approximately two weeks) in order to maintain an average interval of 10 minutes between blocks. If the average time between blocks was less than 10 minutes, the difficulty will increase; if the average time was more than 10 minutes, the difficulty will decrease. Bitcoin Gold will adopt a difficulty adjustment algorithm called DigiShield V3. The idea behind it is to look at how much time has elapsed between the most recent block and the median of a set number of preceding blocks, and to adjust the difficulty every block to target a 10 minute block interval. This more responsive difficulty adjustment algorithm is extremely useful in protecting against big swings in the total amount of hash power. Such swings can result in extreme deviation from the normal 10 minute target block interval. Bitcoin Cash attempted to protect against this risk by implementing an “emergency difficulty adjustment” algorithm, but that had the catastrophic effect of causing sometimes 50 blocks to be mined in one hour, and other times more than 12 hours between two blocks.  Replay Protection The risk of a replay attack is inherent to every cryptocurrency hard fork and has to be taken into consideration to protect users from losing their funds. A hard fork is an exact duplicate of the blockchain, and as such, a transaction that is broadcast publicly to the network can be replayed on both sides of a fork, unless replay protection is implemented. Bitcoin Gold will implement a solution called SIGHASH_FORK_ID replay protection. It is an effective two-way replay protection mechanism that enforces a new algorithm to calculate the hash of a transaction so that all the new Bitcoin transactions will be invalid in Bitcoin Gold blockchain and vice versa. Bitcoin Gold will implement replay protection BEFORE THE LAUNCH.  Unique Address Format By default, both sides of a cryptocurrency hard fork will continue to use the same address format. That means it’s possible to send coins to an address on the other blockchain unintentionally, which can cause users to lose funds by mistake. Bitcoin Cash, for example, is a hard fork that did not change the address format; its addresses are indistinguishable from Bitcoin addresses. There have been many reports of people accidently sending their BTC to a BCC address and vice versa. In some cases these coins could be permanently lost. In order to ensure that this potential confusion does not exist in Bitcoin Gold, a unique address format will be implemented. The prefix of PUBKEY_ADDRESS and SCRIPT_ADDRESS will be changed to a new prefix (yet to be determined) that can easily be distinguished from Bitcoin addresses.  How to Acquire Bitcoin Gold The hardfork will occur on block 491407. To acquire free Bitcoin Gold you simply have to hold Bitcoin at the time of the fork. If you hold BTC at that time, you will automatically receive an equal amount of BTG at the same address (new and old address format are convertible), spendable with the same private keys, when the Bitcoin Gold network launches in November. It is also very important to make a backup of your private key and/or keep the mnemonic phrase required to recover your wallet.  However, if you have your BTC on an exchange or custodial service without access to the private key, then you have to make sure that the service will support Bitcoin Gold after the fork. If you have any doubts about that, then you would be advised to transfer your BTC to one of the many reputable services that will support it.  Timeline Step 1: The hard fork occurs: a ‘snapshot’ of the blockchain is taken Usually a hard fork will happen at the same time when Bitcoin reaches the fork block. However, Bitcoin Gold uses a different way to launch the hard fork: by “taking a snapshot” of the Bitcoin blockchain before the fork block height 491407. Instead of forking immediately, the Bitcoin Gold p2p network will launch a few days later from that snapshot. When Bitcoin reaches the block 491407, nothing special will happen. Bitcoin block 491407 will be mined with SHA256 as normal. No block will be mined in the Bitcoin Gold p2p network because it is not launched yet. However, when the full node client of Bitcoin Gold is ready a few days later, instead of mining from the latest Bitcoin block, Bitcoin Gold will start to mine its own 491407th block on top of block 491406. Bitcoin Gold full nodes will only accept a block 491407 that is mined with Equihash, so they will not recognize BTC block 491407 as a valid BTG block. At the same time, Bitcoin already have a longer blockchain. That’s why it’s called a “snapshot hard fork”. We didn’t follow the common realtime hard fork pattern because a PoW change means there will always be a gap between the fork block. The first Equihash block will be block 491407 of the Bitcoin Gold blockchain, and from that point on GPU miners participating in the Bitcoin Gold network will begin mining more Equihash blocks on top of it. In this way, the Bitcoin blockchain will bifurcate and a new coin - Bitcoin Gold (BTG) - will be created. Everyone who holds BTC at block 491406 will then control an equal amount of coins on the BTG blockchain branch, which can be spent at any time in the future with the corresponding private keys.  Step 2: The BTG blockchain is activated If you have BTC in a paper wallet, hardware wallet, multi-signature address, or any other form of secure private key storage, you will be able to spend your corresponding BTG at any time in the future. There is no expiration date for your BTG. If you have BTC in cold storage that you did not plan to touch for many years, do not change your plans because of this fork. Your BTG will still be there decades from now. In 491407 hard fork is the one and only opportunity to get initial BTG. After that time, your options to acquire it will be to buy it on an exchange like any other cryptocurrency, to mine it with your own computer hardware (GPUs), or to earn it by trading your goods and services for it. Cryptocurrency exchanges are custodial businesses, which means they control your private keys, not you. When the Bitcoin Gold fork occurs on block height 491407, any exchange that is holding BTC on your behalf will also receive the corresponding BTG. While they should credit your account with the equal amount of BTG, there is no legal authority that can force them to do so. The Bitcoin Gold home page will display the names and logos of exchanges that have promised to credit their users with BTG at the 1:1 ratio. If your exchange is not shown, please consider transferring your BTC to a supporting exchange or withdraw to a personal wallet where you control the private keys.  Financial Strategy In order to support the current and future development of Bitcoin Gold, the first blocks after the fork will have a reduced difficulty level that will allow the development team to mine these blocks very rapidly, and then the new difficulty adjustment algorithm will kick in and everyone will have the opportunity to mine on equal footing. As a result, the Bitcoin Gold development team will manage 0.476% of the total coin supply, which will be the main source of funding for all future development of this project, including valuable research and testing that may one day help bring about consensus for a proof-of-work change on Bitcoin itself.   The initial BTG mined by the Bitcoin Gold (0.476%) development team will be held in multisignature wallets.    60% of the funds will be time-locked and released in proportional amounts over the course of three years to cover the development costs.    All significant expenditures will be made fully transparent according to the best practices of similar open source projects.    The majority of funds will be allocated as developer bounties, which will be published as issues in the BTCGPU GitHub repository.    Everyone is able to participate in the Bitcoin Gold developer bounty program; to win the bounty, you must provide the open source code that meets the specific requirements.  The Bitcoin community will be able to support these bounties by buying or holding BTG, as the price of the coin will determine how strong of an incentive these bounties are, and how soon these features can be created. Keep in mind that most of these development bounties are designed to benefit the entire Bitcoin ecosystem, not only the Bitcoin Gold fork. Bitcoin Gold itself was designed to be a feature of Bitcoin, not a rival. Some of these essential functions will be performed by full-time employees while others will be outsourced to third-party professional services. All of these expenditures will be made as transparent as possible without compromising operational security.  BITCOIN GOLD 40% BTG Startup expenses Bounties and app collaboration  7%  Pre-fork costs  5%  Community development  3%  Initial reward for core team  5%  Yearly expenses  20%  Total  40%  BITCOIN GOLD 60% BTG Time-locked funds; 20% released per year Development  30%  Ecosystem  15%  Community  15%  Total  60%  Future development:  Core protocol  Lightning network  Bech32 addresses  Sidechains  Cross-chain atomic swaps  Decentralized exchange Operational and infrastructure costs:  Servers: ○ 12+ full nodes on 6 continents ○ 5+ DNS seeds ○ Website  Domain fee  System administration  Security and penetration testing by third-party Future social action:  Economic Development Fund: ○ BTG debit card program (Latin America) ○ Decentralized fiat-crypto brokerage network (Global)  ○  Blockchain Education Fund: Investment in the content creators and influencers who most effectively contribute to rising Bitcoin awareness and adoption.   ○ ○  GPU Mining Infrastructure Fund: Small/mid-scale individual/business loans for GPU mining hardware operations. Developer bounties for user-friendly mining applications that can bring mining to a nontechnical, multi-lingual audience.  Future communication costs:  Meetups and developer conferences  Social media  Design assets  Press releases  Conclusion Bitcoin Gold is a free open source project that was created by a small group of Bitcoin enthusiasts from diverse backgrounds. In contrast to the other prominent Bitcoin forks, Bitcoin Gold was specifically designed from the beginning to inspire innovation in the Bitcoin ecosystem and give value to the vision of decentralization. Whereas the others were born from hostility and an ambition to dominate, Bitcoin Gold arises from a desire to protect Bitcoin and ensure that it not only maintains its position as the dominant cryptocurrency but continues to grow until its liberating roots stretch deep into the economic life of all nations.  THE BITCOIN RM WHITEPAPER  Website  https://www.bitcoinrm.org  Email  team@bitcoinrm.org  Github  https://github.com/BitcoinRM/BCRM  Youtube  https://www.youtube.com/c/BitcoinRM  Discord  https://discord.gg/aj2QVc9  Reddit  https://www.reddit.com/r/BitcoinRM  BitcoinTalk  BitcoinRM  Medium  https://medium.com/@bitcoinrm Last Updated: 9/8/2018  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  EXECUTIVE SUMMARY 1. Bitcoin RM is the first smart fork of Bitcoin. It is a crypto currency for everyone in the world. Its main goal is to empower Retail Merchants. It is a 100% Proof-of-Work coin just like Bitcoin, but with a different symbol: BCRM 2. This is a smart fork, which starts with a clean blockchain instead of copying all the existing Bitcoin blockchain. This makes our blockchain slimmer, cleaner and faster. 3. All users who hold at least 0.001 BTC in their wallets as of the snapshot date are eligible to file a claim for forked coins (BCRM coins). Due to regulatory reasons, we will not allow / approve claims filed by persons or organizations under the jurisdiction of United States of America. 4. The snapshot date will be on August 20, 2018. We expect to launch our fork on August 30, 2018. 5. To file a claim for BCRM coins, one must have all of the following: a. a Bitcoin wallet that allows signing messages with one's Bitcoin private key b. Bitcoin address that begins with 1 (number one) c. official Bitcoin RM wallet d. a valid email address 6. Technical features of Bitcoin RM: a. Unique addresses, wallet file names, net magic and port numbers b. ASIC resistant Equihash 144/5 mining algorithm with custom Blake2b personalization string c. Zawy's LWMA2 difficulty adjustment algorithm to protect against 51% attacks d. Ability to run MAIN net, TEST net and REGR net (Regression Test) all at the same time e. Average Block interval 60 seconds; Max supply 1 Billion coins; Coinbase maturity about 24hrs; Segwit support; Max block size 20MB; Higher data carrier size of 223; Dynamic block reward with weighted average of 339.58 coins per block 7. Bitcoin RM coins that will go to the team: a. 1 million BCRM coins of Premine (0.1% of Maximum coin supply). b. 5% of miner's block reward whenever the reward is at least 100 coins. c. Left over BCRM coins from 21 million Claims Fund (block-1 reward). Coins will be left over due to unclaimed coins, ineligible or unapproved claims and excess coins beyond max Bitcoins in circulation as of snapshot date. 8. Some of the ways in which we plan to empower Retail Merchants: a. Instant Crypto Cashback: Merchants buy discounted coins from us and give them as instant cashback to their customers. b. Atomic cross-chain trading: Users can safely convert their crypto coins from/to BCRM coins if a Page 2 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  Merchant does not accept Bitcoin RM coins. c. Crypto Affiliate Program: Merchants can manage their affiliate network better when they accept Bitcoin RM from users. Affiliates get their commissions faster. d. Lightning Network: It will be even cheaper and faster to send/receive BCRM coins for Retail Merchants and their customers. e. Payment Forwarder: This will automatically forward incoming payments to various partners, affiliates and sales persons of a Retail Merchant. f. Crypto Purchasing Terminals: We will provide incentives to Retail Merchants and Cryptocurrency ATM operators (such as coinme) to maintain a device / tablet to allow their customers to instantly purchase BCRM coins. g. Merchant Account: Using our software, Retail Merchants receive BCRM payments from their customers either directly to their Exchange deposit address or indirectly through our Payment Forwarder. Our software will help merchant to automatically sell some or all of the BCRM payment into fiat currency as soon as possible. We will partner with Exchanges (that support cashing out into fiat) that our Retail Merchants can open business accounts with. If that does not work, We will create our own software to connect to such Exchanges using their API. h. Support for startups: We will provide capital (in the form of BCRM coins) and technical assistance to at least a few users who want to launch small businesses. i. Advertising to support merchants: We will advertise on our website and in our claim processing emails (sent to Bitcoin users who file claim for coins with us) for merchants, government agencies or business development groups. j. The RM Card: Similar to FoundersCard but with no fees. For Retail Merchants and users with intention of starting a small business. Card holders educate community and get discounts. 9. Bitcoin RM will always be a crypto currency to empower Retail Merchants and to give choice and convenience to the entire user community in the world. It will never be a token or a security. 10. We will maintain a dedicated Merchant Advocate on our team to support Retail Merchants and to advise users who want to launch a startup and become a Retail Merchant. 11. If you are merchant, business development organization or a user about to launch a startup, please reach out to our Merchant Advocate to become a BCRM merchant. 12. If you are an advertiser or a government agency interested in promoting your business, please contact the team directly. 13. If you are a business writer who wants to write about hurdles merchants are facing today (e.g. expensive transaction costs, poor affiliate/sales strategy, difficulty to retain customers, inefficient loyalty/coupon strategy), please contact our Community Manager directly. 14. If you are a crypto Exchange that allows fiat withdrawls, and want to gain more customer accounts from small businesses, please contact our Merchant Advocate directly. Page 3 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  INTRODUCTION A. Claiming Bitcoin RM coins 1. All users, except those subject to the jurisdiction of United States of America, who hold at least 0.001 BTC in their wallet are eligible to apply to receive forked coins (Bitcoin RM coins) on a 1 to 1 basis. User must have a Bitcoin Address that begins with 1 (number one) and a wallet that supports signing messages with his/her private key. Users subject to the jurisdiction of United States of America are not eligible, regardless of the amount BTC he/she holds. 2. The user must provide his/her valid email address. It may take upto 7 days to review and approve claim application. The team will review and approve claim applications but reserve the right to refuse any claim application for any reason. Optionally, the team reserves the right to request additional information before approving application for certain claimants. If the Claims Fund is running low due to too many claims for large amounts, we reserve the right to start deducting transaction fees from claim amounts. This is unlikely to happen because our Claims Fund is 21 million BCRM coins. 3. The team will provide a window of at least 60 days to allow all eligible Bitcoin users to file a claim to receive forked coins. 4. The date of snapshot of Bitcoin blockchain is August 20, 2018. Bitcoin RM smart fork will go live on August 30, 2018. 5. Although not required, claimants are encouraged to participate in our Youtube channel and our social media pages such as Reddit and BitcoinTalk. B. Technical Features of Bitcoin RM 1. This is a smart fork. We do not copy existing Bitcoin blockchain. We start with a new, clean and lean blockchain instead. This means that our developers do not copy or spread some offensive content that is part of Bitcoin blockchain. As a result, the Bitcoin RM team is in full compliance with obscenity / trafficking laws, such as, Stop Enabling Sex Traffickers Act (SESTA) in USA. 2. We use our own custom wallet names, net magic numbers and port numbers. Although not needed, we added support for replay protection with our own ForkID. This will come in handy in the unlikely event that we have to relaunch or announce a swap. 3. We are currently running seed nodes and DNS seed servers in multiple locations throughout the globe. Soon, we will more seed nodes in other locations. 4. Our team gets coins through unclaimed coins from Claims Fund (block-1 reward), Premine coins Page 4 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  and Founders' Reward coins. The last part is periodic but only when mining reward is at least 100 coins. 5. We use 100% proof-of-work mining algorithm. The algorithm will be Equihash mining algorithm (Equihash 144/5) with our custom Blake2b personalization string. 6. We use Gitian build to create full node wallets for Linux, Windows and Mac OS operating systems.  TECHNICAL DETAILS 1. Total supply of our coins is 1 billion coins. Because no subsidy is given if it is less than 1, Total supply will never reach but will always be below 1 billion coins. Our average block interval time is 60 seconds. Block size maximum is 20 MB (10 MB if segwit is turned off). Actual block size depends on what options miners are using. If miners do not set block size (which must below our allowed maximum), the defaults are: DEFAULT_BLOCK_MAX_SIZE = 750000; DEFAULT_BLOCK_MAX_WEIGHT = 3000000;  Our maximum allowed SigOps cost (MAX_BLOCK_SIGOPS_COST) is 500KB. Our Coinbase maturity is currently 24 hours. Assuming average size of a transaction to be 495 bytes, max transactions per second = (max block size) / (495 * 60) Even at current default block size of 750000, we can support upto about 25 transactions per second. This is more than enough capacity for us. However, we can enforce higher defaults in the future and ultimately we can support upto about 673 transactions per second (Compare this to Bitcoin's default of upto about 7 transactions per second). As more and more users adopt higher internet speeds and IPv6 gains wider global acceptance, our average block interval time of 60 seconds should work out perfectly to cater to retail merchant transactions. 2. We use a much larger Nonce than Bitcoin's Nonce. Bitcoin default Nonce size is only uint32_t (2**31), about 4 billion possible values. This means bitcoin miners must juggle with different transactions, time stamps, coinbase field etc. to mine it. To avoid such juggling, and to accommodate the increased complexity of equihash mining, we use uint256_t Nonce, 2**255 possible values! 3. Our mining algorithm is 100% Proof of Work, specifically, Equihash 144/5 with our own Blake2b personalization string. All blocks on the blockchain, including the genesis and Premine blocks, will Page 5 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  use this mining algorithm. This is very secure and ASIC resistant and we most likely do not need to worry about ASIC miners at least for the next 8 years. In the event that ASIC miners start exploiting our coin, we anticipate that the price of such ASIC devices to become very affordable such that GPU users can easily afford them, thus preserving our mission of keeping mining fair and decentralized. Zcash uses Equihash 200/9 which takes up 1344 bytes. Using 256-bit nonce instead of default 32-bit nonce takes up extra 28 bytes (1 byte == 8 bits). So, Header size increase for Zcash is 1344 + 28 == 1372 bytes. But in our case, we use Equihash 144/5 which takes up only 100 bytes. So header increase for our coin is only 128 bytes. So, we have much less block size overhead due to Equihash. 4. However, our Equihash 144/5 requires about 2.5GB RAM and fast processor or GPU with significant RAM (recommended) for mining. We use custom Blake2b personalization string and the latest LWMA2 difficulty adjustment algorithm. This means that abusive miners with larging capacity cannot quickly direct their hardware power to launch 51% attack against our coin easily. Zcash uses Equihash 200/9 Bitcoin Gold uses Equihash 200/9 (They too are soon switching to 144/5) MinexCoin uses Equihash 96/5 Zero Currency uses Equihash 192/7 GPU memory requirements: equihash200_9 equihash192_7 equihash96_5 equihash144_5  > > > >  512MB 6GB 32MB 8GB  (solution size: 1344 bytes) (solution size: 400 bytes) (solution size: 68 bytes) (solution size: 100 bytes)  The following script can be used to determine solution size for any Equihash variant. #!/bin/python import sys if len(sys.argv) != 3: print 'Usage: ' + sys.argv[0] + ': N K' exit() N = int(sys.argv[1]) K = int(sys.argv[2]) SolSize = 0 if K < N and N % 8 == 0 and N/(K+1) < 31 and (1 << K)*(N/(K+1)+1) < 15356: SolSize = (1 << K)*(N/(K+1)+1)/8 print 'Equihash variant OK, Solution Size:',SolSize,'bytes' else: print 'NOT a valid Equihash variant'  Page 6 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  The necessary condition for a valid N, K pair is: (CBlockHeader::HEADER_SIZE + equihash_solution_size(N, K))*MAX_HEADERS_RESULTS < MAX_PROTOCOL_MESSAGE_LENGTH – 1000  5. Our wallet file names and config directories (wallet files are inside config directory) are: BCRM.imp testnetBCRM.imp regtestBCRM.imp  (main wallet file) ← Please backup and keep this file safe! (testnet) (regtest)  Windows config directory: Mac OS config directory: Linux config directory:  %APPDATA%\BCRM ~/Library/Application Support/BCRM ~/.bcrm  Optional config file:  bcrm.conf  (inside config directory)  We can launch Mainnet, Testnet and Regtest at the same time. User can override the wallet filename defaults with -wallet command line option. On Linux, the following additional packages are required: fontconfig stix-fonts  xkeyboard-config libX11-xcb (On Debian Linux: libx11-xcb-dev)  6. Although not needed, we incorporated replay protection. In the unlikely event that we have to recall and reissue coins, this comes in handy. Our Fork ID and port numbers are shown below. Fork ID:  94  MAIN net port: MAIN net RPC port:  2094/tcp 2095/tcp  (Main network)  TEST net port: TEST net RPC port:  3094/tcp 3095/tcp  (Test network)  REGR net port: REGR net RPC port:  4094/tcp 4095/tcp  (Regression Test network)  7. Our address prefixes For MAIN net: Regular Address: Script:  Prefix R Prefix M Page 7 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  For TEST net: Regular Address: Script:  Prefix X Prefix x  For REGR net: Regular Address: Script:  Prefix Y Prefix y  8. We also use our own net magic numbers, as shown below. MAIN net: 0x42 0x43 0x52 0x4d (the characters: B C R M) TEST net: 0x43 0x44 0x53 0x4e (shift above characters by 1) REGR net: 0x44 0x45 0x54 0x4f (shift above characters by 1 again) 9. We have seed nodes in various parts of the world. Here are their IP addresses and host names: Los Angeles, USA 167.160.180.109 2607:fcd0:100:1917::b5ce:65f  la.bitcoinrm.org la.bitcoinrm.org  New York, USA 104.168.53.53  ny.bitcoinrm.org  Sao Paulo, Brazil 35.198.37.53  sao.bitcoinrm.org  Montreal, Canada 142.44.240.101 2607:5300:201:3100::34b  montreal.bitcoinrm.org montreal.bitcoinrm.org  Gravelines, France 51.38.187.18 2001:41d0:305:2100::18ef  node0.bitcoinrm.org node0.bitcoinrm.org  Singapore City, Singapore 139.99.99.154 2402:1f00:8000:800::ae4  singapore.bitcoinrm.org singapore.bitcoinrm.org  Sydney, Australia Page 8 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  139.99.195.218 2402:1f00:8100:400::b16  sydney.bitcoinrm.org sydney.bitcoinrm.org  Our Brazil and New York nodes currently do not support IPv6. We plan to change these nodes to better providers and add IPv6 support in the near future. Also, we plan to add new seed nodes in South Africa and India later this year. 10. Miners get dynamic block rewards for mining BCRM coins. The weighted average block reward is 339.58 coins per block. Actual mining reward varies with time. Maximum 64 halvings; i.e. maximum number of intervals: 64, each lasting 1440000 blocks (~2.7 years). If block subsidy is less than 1, no subsidy is actually given to miners. When there is no subsidy, miners only collect transaction fees. After 5 intervals (~13.7 years), there will be no mining subsidy for 240000 blocks (~167 days) and then subsidy begins after that period. We charge 5% Founders' Reward during the times mining subsidy is at least 100 coins. This means, we stop collecting Founders' Reward after about 4 intervals (~10.9 years). All block subsidies stop after 10 intervals (~27 years). 1 000 900 800  Gross Subsidy  700 600 500 400 300 200 100 Days Elapsed  0 0  500  1 000  1 500  2 000  2 500  3 000  3 500  Please refer to the above diagram to understand how mining subsidy varies with days since launch. We chose this type of dynamic reward system to discourage pump and dump schemes, i.e. miners suddenly flock to a new coin and mine it as much as they can and then quickly dump their coins. Soon after, they abandon mining that coin and move on to another newly launched coin. When there are no miners mining a coin, transactions in that coin cannot go through. This impacts the user community. During the first LWMA2 Averaging Window period (warm up period), we start with a reasonable Page 9 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  initial mining difficulty to allow smooth mining and prevent any abuse. To control inflation, we still reduce subsidy by 50% after each halving interval of about 2.7 years. There are no special blocks or super blocks. We don't believe in lottery or surprise block subsidies. Block Number From To 21 120020 120021 240020 240021 360020 360021 480020 480021 600020 600021 720020 720021 840020 840021 960020 960021 1080020 1080021 1200020 1200021 1320020 1320021 1440020 1440021 1560020 1560021 1680020 1680021 1800020 1800021 1920020 1920021 2040020 2040021 2160020 2160021 2280020 2280021 2400020 2400021 2520020 2520021 2640020 2640021 2760020 2760021 2880020 2880021 3000020 3000021 3120020 3120021 3240020 3240021 3360020 3360021 3480020 3480021 3600020 3600021 3720020 3720021 3840020 3840021 3960020 3960021 4080020 4080021 4200020 4200021 4320020  . . .  . . .  Block Subsidy Gross Actual Net 20 20 30 30 45 45 100 95 200 190 400 380 900 855 950 902.5 700 665 500 475 150 142.5 80 80 10 10 15 15 22.5 22.5 50 50 100 95 200 190 450 427.5 475 451.25 350 332.5 250 237.5 75 75 40 40 5 5 7.5 7.5 11.25 11.25 25 25 50 50 100 95 225 213.75 237.5 225.625 175 166.25 125 118.75 37.5 37.5 20 20  . . .  . . . Page 10 of 15  FEATURES: Total Claims Fund: 21 Million coins Maximum Supply: 1 Billion coins Total Premine: 1 Million (0.1%) Average Block Time: 60 seconds Mining Algorithm: Equihash 144/5 Mining Type: 100% Proof-Of-Work Maximum Block Size: 20MB Difficulty Retarget: Each block Difficulty Algorithm: LWMA2 No masternodes; No superblocks  MINING SUBSIDIES: Block subsidy both varies and halves. It varies after each varying interval and halves after each halving interval. Varying interval: 120000 blocks (about 83 days) Halving interval: 1440000 blocks (about 1000 days) There are 12 varying intervals in each halving interval. Block subsidy varies every 120000 blocks (about 83 days). These varying subsidies are also reduced by 50%, every 1440000 blocks (about 1000 days). 5% Founders' Reward is deducted if gross subsidy is at least 100 coins. No subsidy is given if Gross subsidy is less than 1 coin. When Block subsidy is zero, miners will only collect transaction fees if there are transactions. If there are no transactions and block subsidy is zero, miners collect no coins at all.  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  Weighted Avg Block Subsidy: 339.58 coins per block.  Interval 3  Interval 2  Interval 1  Average Block interval: 60 seconds.  Interval 4  1 440 000 489 000 000 2.7 years blocks subsidy total subsidy days days elapsed 20 varies 22 000 000 -1 -1 BEGIN 20 0 0 0 120 000 20 2 400 000 83 83 120 000 30 3 600 000 83 167 120 000 45 5 400 000 83 250 120 000 100 12 000 000 83 333 120 000 200 24 000 000 83 417 120 000 400 48 000 000 83 500 120 000 900 108 000 000 83 583 120 000 950 114 000 000 83 667 120 000 700 84 000 000 83 750 120 000 500 60 000 000 83 833 120 000 150 18 000 000 83 917 120 000 80 9 600 000 83 1 000 120 000 10 1 200 000 83 1 083 120 000 15 1 800 000 83 1 167 120 000 22.5 2 700 000 83 1 250 120 000 50 6 000 000 83 1 333 120 000 100 12 000 000 83 1 417 120 000 200 24 000 000 83 1 500 120 000 450 54 000 000 83 1 583 120 000 475 57 000 000 83 1 667 120 000 350 42 000 000 83 1 750 120 000 250 30 000 000 83 1 833 120 000 75 9 000 000 83 1 917 120 000 40 4 800 000 83 2 000 120 000 5 600 000 83 2 083 120 000 7.5 900 000 83 2 167 120 000 11.25 1 350 000 83 2 250 120 000 25 3 000 000 83 2 333 120 000 50 6 000 000 83 2 417 120 000 100 12 000 000 83 2 500 120 000 225 27 000 000 83 2 583 120 000 237.5 28 500 000 83 2 667 120 000 175 21 000 000 83 2 750 120 000 125 15 000 000 83 2 833 120 000 37.5 4 500 000 83 2 917 120 000 20 2 400 000 83 3 000 . . . . . . . . . . . . . . .  Page 11 of 15  Each halving interval lasts 1440000 blocks (~2.7 years) Block reward for genesis block is zero. After genesis block, we create the first block (block-1) with 21million coins (corresponds to total bitcoin supply). We will premine 19 blocks (after block-1) for 1 million coins. This is 0.1% of Maximum coin supply. There will be 64 halving intervals, each reducing the subsidies of previous interval by 50%. We will only charge Founders' Reward (fixed at 5%) if the subsidy is at least 100 coins. Therefore, Founders' Reward completely stops after 4 halving intervals (~10.9 years) Maximum coin supply is 1 Billion. Because we will not issue mining subsidy in fractional coins (arising out of 50% reduction in each halving interval), the maximum supply will be a bit less than 1 Billion coins. Note: The symbol “~” means “approximately.”  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  PLANNED ROADMAP 1. July 25, 2018 Announce our coin officially and start promoting it online. 2. August 10, 2018 Test and launch miners. Complete premining. Do a test run of claiming. 3. August 20, 2018 Take snapshot of Bitcoin UTXOs. Seek out advertisers. 4. August 30, 2018 Launch MAIN net officially. And begin processing claims from Bitcoin holders. 5. September – December, 2018 Apply and get our coin listed on Exchanges. Stop accepting claims from Bitcoin holders. But Bitcoin holders will retain the option to enjoy any of the offers or promotions sent to them by our advertising partners. 6. December, 2018 Obtain indications of interests from Retail Merchants to use our coin and software. Partner with Exchanges (only those that provide API and support fiat currencies). Merchants will our software to liquidate coins received from their customers into fiat as soon as possible either fully or partially, to mitigate crypto value fluctuation risk. 7. February 15, 2019 Launch Payment Forwarder and Affiliate System for Retail Merchants. Launch The RM Card for community. 8. June 30, 2019 Sign up as many Retail Merchants as possible for our coin. Launch Instant Cash Back system for community. 9. July 30, 2019 Test and implement Lightning Network support. Explore options of installing terminals / ATM systems through partners. 10. December, 2019 Test and implement cross chain atomic swap with Bitcoin and other popular crypto currencies 11. January, 2020 Page 12 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  Launch complete Merchant Account package for Retail Merchants. The Merchant Account package includes all essential services, including The RM Card, for merchants. 12. March, 2020 Launch Startup Support Fund to promote startup businesses. Please note that all the items in Planned Roadmap are subject to change and some items may be completed sooner or later than their listed deadline. Despite such changes, we are committed to bringing value to our retail merchants and our user community.  FREQUENTLY ASKED QUESTIONS 1. Do you have a Premine? Yes. Our team gets coins from unclaimed coins (i.e. when some Bitcoin holders do not get our coins because they did not actually claim or they're ineligible), 0.1% Premine (1000000 coins) and 5% Founders' Reward (only when mining reward is at least 100 coins) for 9.3 years from launch. 2. When is your ICO? Bitcoin RM is a smart fork of Bitcoin. As such, it is a crypto currency only and not a security. Therefore, there is no Initial Coin Offering (ICO). 3. Do you have masternodes? Our coin is a 100% proof of work coin with no masternodes, just like Bitcoin. We also do not have superblocks, ultra blocks or any special blocks or surprise block rewards. 4. Why do you have Premine? It costs a lot of money to pay for infrastructure, software development, testing, marketing and promotion. Additionally, we incur costs in making future enhancements (as outlined in the Roadmap) and bug fixes as needed. Our Premine of 0.1% is one of the most reasonable you will ever find in the crypto industry. 5. Why do you also have Founders' Reward? Our Founders' Reward is only 5%, compared to 20% charged by Zcash. The purpose of Founders' Reward is to provide ongoing incentive to the founders and the team. Premine is a one time revenue and it does not cover ongoing costs incurred by the team. For example, the founders and the team will Page 13 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  hire additional developers, promoters etc. Founders' Reward of 5% is very reasonable, given that a lot of mining software charge 5% Dev fee. Charging miners a Founders' Reward is a way of putting some of the mined coins to work for the overall community. This is how various governments operate. For example, when you start a business, you pay initial filing/registration/license fee and then taxes every year on your business profit. 6. Why another Bitcoin fork? Aren't there too many already? We agree that there are already many Bitcoin forks. However, we are one of the few smart forks out there! As crypto industry matures, needs of the community evolve. Therefore, it is safer to fork and launch a coin specific to those evolving needs. In our case, our fork's purpose is to empower Retail Merchants and embrace all users in the world who support small businesses. Ultimately, it is a matter of choice and every single segment of community gets what it wants. Can you imagine a world with just 10 blogs, or 50 internet companies or 100 movie producers? When consumers have more choices, it ultimately benefits them because increased competition leads to a free market without monopoly. 7. I heard that Alt coins are doomed. Then, why should I care about Bitcoin RM fork? It is true that a lot media outlets, fake crypto experts, promoters and even crypto Exchanges are screaming that we have too many Alt coins. Who are they to decide which Alt coin is better than the others? It is upto the user community to make that decision! Why are these middlemen telling the end users what to do? We do not go through any middlemen. Instead, we reach out to the community directly. It is upto the community to decide which coin is better and which one is not. We believe in our mission to empower merchants because ultimately we all need and depend on a strong economy. If you believe in our mission, please support us. Just like we are reaching out to you directly, we urge you to reach out to us directly with any suggestions, questions or concerns. We will give you all the relevant information and be transparent in what we do. After that, the decision to participate in our project is yours. 8. Isn't it unsafe to claim your forked coins? I don't want to reveal my private keys. You claim coins by a claim procedure which does not involve revealing your Bitcoin private keys. We do not need your private keys. We will never ask for your private keys or money (fiat or crypto currency). 9. How should I buy BCRM coins? We hope to get our BCRM coins listed on Exchanges. We will keep you posted on this. We strongly suggest that you acquire BCRM coins by: a. working for us, b. mining, c. buying on Exchanges. 10. Which wallet should I use?  Page 14 of 15  Bitcoin RM  The Bitcoin RM Team  The First Bitcoin Smart Fork  https://www.bitcoinrm.org  Because we are a smart fork, our blockchain is quite small currently. Therefore, you do not have to wait hours for the full node to synchronize. We strongly recommend that you download our full node on our website and use it's wallet, instead of any third party wallet. Remember to backup and keep your wallet file (BCRM.imp) safe. See item number 5. on page 7. 11. Why a smart fork, instead of a regular fork? A smart fork is, well, smart :) We start with a clean blockchain that is slim. This makes it easy for anybody to download and use our wallet. You don't need a lot of disk space or hours of waiting for synchronization. A smart fork also does not spread any offensive content from Bitcoin, because it does not blindly copy entire existing blockchain of Bitcoin. 12. Where are you located? We are an international team of crypto enthusiasts. We do not do any business in United States of America. However, if you are a resident of USA, you can still work for us as a promoter, developer etc. Please contact us to learn about our current job opportunities. 13. How can I contact you? The best way to reach us is by emailing us or chatting with us on our Discord server. 14. Who are your current investors? The Bitcoin RM team has no investors and no debt. The team hires people as needed for the duration needed and pays the candidates in BCRM coins. Because we have no investors or third parties claiming control, the Bitcoin RM team is completely independent. 15. I currently own Bitcoins. How do I know if I can get BCRM coins when you fork? If you are not connected to USA, you have at least 0.001 BTC in your wallet (i.e. you have the private keys) on snapshot date (currently set to August 20 th, 2018) and your wallet supports message signing, then you are eligible to file a claim for BCRM coins. You will get BCRM coins on 1-to-1 basis (i.e. equal to your BTC balance on snapshot date). 16. How should I mine BCRM coin? Our official pool is pool.bitcoinrm.org. Ideally, you want a good computer with GPU with at least 10 GB GPU memory (such as Nvidia GTX 1080Ti). We will soon release a mining client, which will not charge a Dev fee (although it will not be the fastest!). For best performance, use a commercial miner that supports Equihash 144/5 (e.g. EWBF, miniZ, Funakoshi, Lolminer) with custom personalization string. Our Blake2b personalization string is: BCRMcoin Page 15 of 15  Bitcoin: A Peer-to-Peer Electronic Cash System Satoshi Nakamoto satoshin@gmx.com www.bitcoin.org  Abstract. A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of CPU power. As long as a majority of CPU power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.  1.  Introduction  Commerce on the Internet has come to rely almost exclusively on financial institutions serving as trusted third parties to process electronic payments. While the system works well enough for most transactions, it still suffers from the inherent weaknesses of the trust based model. Completely non-reversible transactions are not really possible, since financial institutions cannot avoid mediating disputes. The cost of mediation increases transaction costs, limiting the minimum practical transaction size and cutting off the possibility for small casual transactions, and there is a broader cost in the loss of ability to make non-reversible payments for nonreversible services. With the possibility of reversal, the need for trust spreads. Merchants must be wary of their customers, hassling them for more information than they would otherwise need. A certain percentage of fraud is accepted as unavoidable. These costs and payment uncertainties can be avoided in person by using physical currency, but no mechanism exists to make payments over a communications channel without a trusted party. What is needed is an electronic payment system based on cryptographic proof instead of trust, allowing any two willing parties to transact directly with each other without the need for a trusted third party. Transactions that are computationally impractical to reverse would protect sellers from fraud, and routine escrow mechanisms could easily be implemented to protect buyers. In this paper, we propose a solution to the double-spending problem using a peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions. The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes.  1  2.  Transactions  We define an electronic coin as a chain of digital signatures. Each owner transfers the coin to the next by digitally signing a hash of the previous transaction and the public key of the next owner and adding these to the end of the coin. A payee can verify the signatures to verify the chain of ownership. Transaction  Transaction  Owner 1's Public Key  Transaction  Owner 2's Public Key  Hash  Owner 3's Public Key  Hash Ver if  Hash Ver ify  y  Owner 0's Signature  Owner 1's Signature gn Si  Owner 1's Private Key  Owner 2's Signature Si  gn  Owner 2's Private Key  Owner 3's Private Key  The problem of course is the payee can't verify that one of the owners did not double-spend the coin. A common solution is to introduce a trusted central authority, or mint, that checks every transaction for double spending. After each transaction, the coin must be returned to the mint to issue a new coin, and only coins issued directly from the mint are trusted not to be double-spent. The problem with this solution is that the fate of the entire money system depends on the company running the mint, with every transaction having to go through them, just like a bank. We need a way for the payee to know that the previous owners did not sign any earlier transactions. For our purposes, the earliest transaction is the one that counts, so we don't care about later attempts to double-spend. The only way to confirm the absence of a transaction is to be aware of all transactions. In the mint based model, the mint was aware of all transactions and decided which arrived first. To accomplish this without a trusted party, transactions must be publicly announced [1], and we need a system for participants to agree on a single history of the order in which they were received. The payee needs proof that at the time of each transaction, the majority of nodes agreed it was the first received.  3.  Timestamp Server  The solution we propose begins with a timestamp server. A timestamp server works by taking a hash of a block of items to be timestamped and widely publishing the hash, such as in a newspaper or Usenet post [2-5]. The timestamp proves that the data must have existed at the time, obviously, in order to get into the hash. Each timestamp includes the previous timestamp in its hash, forming a chain, with each additional timestamp reinforcing the ones before it. Hash  Hash  Block Item  Block Item  ...  Item  2  Item  ...  4.  Proof-of-Work  To implement a distributed timestamp server on a peer-to-peer basis, we will need to use a proofof-work system similar to Adam Back's Hashcash [6], rather than newspaper or Usenet posts. The proof-of-work involves scanning for a value that when hashed, such as with SHA-256, the hash begins with a number of zero bits. The average work required is exponential in the number of zero bits required and can be verified by executing a single hash. For our timestamp network, we implement the proof-of-work by incrementing a nonce in the block until a value is found that gives the block's hash the required zero bits. Once the CPU effort has been expended to make it satisfy the proof-of-work, the block cannot be changed without redoing the work. As later blocks are chained after it, the work to change the block would include redoing all the blocks after it. Block  Block  Prev Hash Tx  Nonce  Tx  Prev Hash  ...  Tx  Tx  Nonce ...  The proof-of-work also solves the problem of determining representation in majority decision making. If the majority were based on one-IP-address-one-vote, it could be subverted by anyone able to allocate many IPs. Proof-of-work is essentially one-CPU-one-vote. The majority decision is represented by the longest chain, which has the greatest proof-of-work effort invested in it. If a majority of CPU power is controlled by honest nodes, the honest chain will grow the fastest and outpace any competing chains. To modify a past block, an attacker would have to redo the proof-of-work of the block and all blocks after it and then catch up with and surpass the work of the honest nodes. We will show later that the probability of a slower attacker catching up diminishes exponentially as subsequent blocks are added. To compensate for increasing hardware speed and varying interest in running nodes over time, the proof-of-work difficulty is determined by a moving average targeting an average number of blocks per hour. If they're generated too fast, the difficulty increases.  5.  Network  The steps to run the network are as follows: 1) 2) 3) 4) 5) 6)  New transactions are broadcast to all nodes. Each node collects new transactions into a block. Each node works on finding a difficult proof-of-work for its block. When a node finds a proof-of-work, it broadcasts the block to all nodes. Nodes accept the block only if all transactions in it are valid and not already spent. Nodes express their acceptance of the block by working on creating the next block in the chain, using the hash of the accepted block as the previous hash.  Nodes always consider the longest chain to be the correct one and will keep working on extending it. If two nodes broadcast different versions of the next block simultaneously, some nodes may receive one or the other first. In that case, they work on the first one they received, but save the other branch in case it becomes longer. The tie will be broken when the next proofof-work is found and one branch becomes longer; the nodes that were working on the other branch will then switch to the longer one.  3  New transaction broadcasts do not necessarily need to reach all nodes. As long as they reach many nodes, they will get into a block before long. Block broadcasts are also tolerant of dropped messages. If a node does not receive a block, it will request it when it receives the next block and realizes it missed one.  6.  Incentive  By convention, the first transaction in a block is a special transaction that starts a new coin owned by the creator of the block. This adds an incentive for nodes to support the network, and provides a way to initially distribute coins into circulation, since there is no central authority to issue them. The steady addition of a constant of amount of new coins is analogous to gold miners expending resources to add gold to circulation. In our case, it is CPU time and electricity that is expended. The incentive can also be funded with transaction fees. If the output value of a transaction is less than its input value, the difference is a transaction fee that is added to the incentive value of the block containing the transaction. Once a predetermined number of coins have entered circulation, the incentive can transition entirely to transaction fees and be completely inflation free. The incentive may help encourage nodes to stay honest. If a greedy attacker is able to assemble more CPU power than all the honest nodes, he would have to choose between using it to defraud people by stealing back his payments, or using it to generate new coins. He ought to find it more profitable to play by the rules, such rules that favour him with more new coins than everyone else combined, than to undermine the system and the validity of his own wealth.  7.  Reclaiming Disk Space  Once the latest transaction in a coin is buried under enough blocks, the spent transactions before it can be discarded to save disk space. To facilitate this without breaking the block's hash, transactions are hashed in a Merkle Tree [7][2][5], with only the root included in the block's hash. Old blocks can then be compacted by stubbing off branches of the tree. The interior hashes do not need to be stored. Block  Block  Block Header (Block Hash) Prev Hash  Nonce  Block Header (Block Hash) Prev Hash  Root Hash  Hash01  Nonce  Root Hash  Hash23  Hash0  Hash1  Hash2  Hash3  Tx0  Tx1  Tx2  Tx3  Hash01  Hash23  Hash2  Hash3  Tx3  Transactions Hashed in a Merkle Tree  After Pruning Tx0-2 from the Block  A block header with no transactions would be about 80 bytes. If we suppose blocks are generated every 10 minutes, 80 bytes * 6 * 24 * 365 = 4.2MB per year. With computer systems typically selling with 2GB of RAM as of 2008, and Moore's Law predicting current growth of 1.2GB per year, storage should not be a problem even if the block headers must be kept in memory.  4  8.  Simplified Payment Verification  It is possible to verify payments without running a full network node. A user only needs to keep a copy of the block headers of the longest proof-of-work chain, which he can get by querying network nodes until he's convinced he has the longest chain, and obtain the Merkle branch linking the transaction to the block it's timestamped in. He can't check the transaction for himself, but by linking it to a place in the chain, he can see that a network node has accepted it, and blocks added after it further confirm the network has accepted it. Longest Proof-of-Work Chain Block Header Prev Hash  Block Header Nonce  Block Header  Prev Hash  Merkle Root  Nonce  Prev Hash  Merkle Root  Hash01  Nonce  Merkle Root  Hash23 Merkle Branch for Tx3 Hash2  Hash3  Tx3  As such, the verification is reliable as long as honest nodes control the network, but is more vulnerable if the network is overpowered by an attacker. While network nodes can verify transactions for themselves, the simplified method can be fooled by an attacker's fabricated transactions for as long as the attacker can continue to overpower the network. One strategy to protect against this would be to accept alerts from network nodes when they detect an invalid block, prompting the user's software to download the full block and alerted transactions to confirm the inconsistency. Businesses that receive frequent payments will probably still want to run their own nodes for more independent security and quicker verification.  9.  Combining and Splitting Value  Although it would be possible to handle coins individually, it would be unwieldy to make a separate transaction for every cent in a transfer. To allow value to be split and combined, transactions contain multiple inputs and outputs. Normally there will be either a single input from a larger previous transaction or multiple inputs combining smaller amounts, and at most two outputs: one for the payment, and one returning the change, if any, back to the sender. Transaction In  Out  In  ...  ...  It should be noted that fan-out, where a transaction depends on several transactions, and those transactions depend on many more, is not a problem here. There is never the need to extract a complete standalone copy of a transaction's history.  5  10. Privacy The traditional banking model achieves a level of privacy by limiting access to information to the parties involved and the trusted third party. The necessity to announce all transactions publicly precludes this method, but privacy can still be maintained by breaking the flow of information in another place: by keeping public keys anonymous. The public can see that someone is sending an amount to someone else, but without information linking the transaction to anyone. This is similar to the level of information released by stock exchanges, where the time and size of individual trades, the "tape", is made public, but without telling who the parties were. Traditional Privacy Model Identities  Transactions  Trusted Third Party  Counterparty  Public  New Privacy Model Identities  Transactions  Public  As an additional firewall, a new key pair should be used for each transaction to keep them from being linked to a common owner. Some linking is still unavoidable with multi-input transactions, which necessarily reveal that their inputs were owned by the same owner. The risk is that if the owner of a key is revealed, linking could reveal other transactions that belonged to the same owner.  11. Calculations We consider the scenario of an attacker trying to generate an alternate chain faster than the honest chain. Even if this is accomplished, it does not throw the system open to arbitrary changes, such as creating value out of thin air or taking money that never belonged to the attacker. Nodes are not going to accept an invalid transaction as payment, and honest nodes will never accept a block containing them. An attacker can only try to change one of his own transactions to take back money he recently spent. The race between the honest chain and an attacker chain can be characterized as a Binomial Random Walk. The success event is the honest chain being extended by one block, increasing its lead by +1, and the failure event is the attacker's chain being extended by one block, reducing the gap by -1. The probability of an attacker catching up from a given deficit is analogous to a Gambler's Ruin problem. Suppose a gambler with unlimited credit starts at a deficit and plays potentially an infinite number of trials to try to reach breakeven. We can calculate the probability he ever reaches breakeven, or that an attacker ever catches up with the honest chain, as follows [8]: p = probability an honest node finds the next block q = probability the attacker finds the next block qz = probability the attacker will ever catch up from z blocks behind  {  q z=  1 if p≤q z q / p if pq  } 6  Given our assumption that p > q, the probability drops exponentially as the number of blocks the attacker has to catch up with increases. With the odds against him, if he doesn't make a lucky lunge forward early on, his chances become vanishingly small as he falls further behind. We now consider how long the recipient of a new transaction needs to wait before being sufficiently certain the sender can't change the transaction. We assume the sender is an attacker who wants to make the recipient believe he paid him for a while, then switch it to pay back to himself after some time has passed. The receiver will be alerted when that happens, but the sender hopes it will be too late. The receiver generates a new key pair and gives the public key to the sender shortly before signing. This prevents the sender from preparing a chain of blocks ahead of time by working on it continuously until he is lucky enough to get far enough ahead, then executing the transaction at that moment. Once the transaction is sent, the dishonest sender starts working in secret on a parallel chain containing an alternate version of his transaction. The recipient waits until the transaction has been added to a block and z blocks have been linked after it. He doesn't know the exact amount of progress the attacker has made, but assuming the honest blocks took the average expected time per block, the attacker's potential progress will be a Poisson distribution with expected value:  =z  q p  To get the probability the attacker could still catch up now, we multiply the Poisson density for each amount of progress he could have made by the probability he could catch up from that point: k  −  {   z−k  if k ≤ z ∑  ke! ⋅ q / p1 if k  z k =0 ∞  }  Rearranging to avoid summing the infinite tail of the distribution... z  1−∑  k =0  k e− 1−q / p z− k  k!  Converting to C code... #include <math.h> double AttackerSuccessProbability(double q, int z) { double p = 1.0 - q; double lambda = z * (q / p); double sum = 1.0; int i, k; for (k = 0; k <= z; k++) { double poisson = exp(-lambda); for (i = 1; i <= k; i++) poisson *= lambda / i; sum -= poisson * (1 - pow(q / p, z - k)); } return sum; }  7  Running some results, we can see the probability drop off exponentially with z. q=0.1 z=0 z=1 z=2 z=3 z=4 z=5 z=6 z=7 z=8 z=9 z=10  P=1.0000000 P=0.2045873 P=0.0509779 P=0.0131722 P=0.0034552 P=0.0009137 P=0.0002428 P=0.0000647 P=0.0000173 P=0.0000046 P=0.0000012  q=0.3 z=0 z=5 z=10 z=15 z=20 z=25 z=30 z=35 z=40 z=45 z=50  P=1.0000000 P=0.1773523 P=0.0416605 P=0.0101008 P=0.0024804 P=0.0006132 P=0.0001522 P=0.0000379 P=0.0000095 P=0.0000024 P=0.0000006  Solving for P less than 0.1%... P < 0.001 q=0.10 z=5 q=0.15 z=8 q=0.20 z=11 q=0.25 z=15 q=0.30 z=24 q=0.35 z=41 q=0.40 z=89 q=0.45 z=340  12. Conclusion We have proposed a system for electronic transactions without relying on trust. We started with the usual framework of coins made from digital signatures, which provides strong control of ownership, but is incomplete without a way to prevent double-spending. To solve this, we proposed a peer-to-peer network using proof-of-work to record a public history of transactions that quickly becomes computationally impractical for an attacker to change if honest nodes control a majority of CPU power. The network is robust in its unstructured simplicity. Nodes work all at once with little coordination. They do not need to be identified, since messages are not routed to any particular place and only need to be delivered on a best effort basis. Nodes can leave and rejoin the network at will, accepting the proof-of-work chain as proof of what happened while they were gone. They vote with their CPU power, expressing their acceptance of valid blocks by working on extending them and rejecting invalid blocks by refusing to work on them. Any needed rules and incentives can be enforced with this consensus mechanism.  8  References [1] W. Dai, "b-money," http://www.weidai.com/bmoney.txt, 1998. [2] H. Massias, X.S. Avila, and J.-J. Quisquater, "Design of a secure timestamping service with minimal trust requirements," In 20th Symposium on Information Theory in the Benelux, May 1999. [3] S. Haber, W.S. Stornetta, "How to time-stamp a digital document," In Journal of Cryptology, vol 3, no 2, pages 99-111, 1991. [4] D. Bayer, S. Haber, W.S. Stornetta, "Improving the efficiency and reliability of digital time-stamping," In Sequences II: Methods in Communication, Security and Computer Science, pages 329-334, 1993. [5] S. Haber, W.S. Stornetta, "Secure names for bit-strings," In Proceedings of the 4th ACM Conference on Computer and Communications Security, pages 28-35, April 1997. [6] A. Back, "Hashcash - a denial of service counter-measure," http://www.hashcash.org/papers/hashcash.pdf, 2002. [7] R.C. Merkle, "Protocols for public key cryptosystems," In Proc. 1980 Symposium on Security and Privacy, IEEE Computer Society, pages 122-133, April 1980. [8] W. Feller, "An introduction to probability theory and its applications," 1957.  9  Colored Coins whitepaper This document describes the original concept of Colored Coins. To view the updated specs please visit the official  Coloredcoins.org Github (​Coloredcoins.org​) Yoni Assia yoni.assia@gmail.com - Vitalik Buterin v@buterin.com m liorhakiLior Hakim@gmail.com - Meni Rosenfeld ​meni@bitcoil.co.il​ - Rotem Lev Rotem@colu.co​.  Abstract Bitcoin is the world's first decentralized digital currency, allowing the easy storage and transfer of cryptographic tokens, using a peer-to-peer network to carry information, hashing as a synchronization signal to prevent double-spending, and a powerful scripting system to determine ownership of the tokens. There is a growing technology and business infrastructure supporting it. By the original design bitcoins are fungible, acting as a neutral medium of exchange. However, by carefully tracking the origin of a given bitcoin, it is possible to "color" a set of bitcoins to distinguish it from the rest. These bitcoins can then have special properties supported by either an issuing agent or by public agreement, and have value independent of the face value of the underlying bitcoins. Such colored bitcoins can be used for alternative currencies, commodity certificates, smart property, and other financial instruments such as stocks and bonds. Because colored coins make use of the existing Bitcoin infrastructure and can be stored and transferred without the need for a third party, and even be exchanged for one another in an atomic transaction, they can open the way for the decentralized exchange of things that are not possible by traditional methods. In this paper we will discuss the implementation details of colored coins and some of their use cases.  Background The advent of Satoshi Nakamoto's Bitcoin in 2009 has fundamentally enlarged the scope of what can be done with cryptography. Before Bitcoin, any attempt to manage any kind of digital assets online without a centralized authority always ran into a fundamental problem that was thought to be intractable: the double-spending threat. If a digital asset is nothing but a file consisting of ones and zeroes, the theory went, what stops anyone from simply making a hundred copies of that file and thereby multiplying their own wealth? With a central authority, the problem is easy; the central authority can simply keep track of who owns what quantity of the digital asset. However, centralization is problematic for many reasons: a centralized authority creates a single point of failure which can be hacked, shut down due to regulatory pressure, or even leverage its privileged status to benefit itself at the wider community's expense. Although some abortive attempts were made by Wei Dai and Nick Szabo in 1998 and 2005 as well as A ​ dam Back's hashcash​, Bitcoin marks the first time that anyone has created a fully functional, viable decentralized solution - a solution which is rapidly gaining popularity and mainstream acceptance all around the world. Because of its fully digital and decentralized design, Bitcoin offers a number of advantages over existing payment systems. Its transactional irreversibility provides for a high degree of security, allowing merchants to considerably cut costs in high-risk industries. Its lack of a centralized authority has rendered it invulnerable to so-called "financial censorship" attempts such as the financial blockade of Wikileaks by banks and credit card companies in 2011; while donations through the traditional financial system dropped by 95% as a result of the blockade, Bitcoin donations kept coming in at full steam the whole way through​[1]​. Its technical ease of use is allowing entrepreneurs around the world to start up prediction markets, notarization services, gambling sites, paywalls and many other types of businesses with zero startup cost in only a few weeks' worth of development work. Finally, most interestingly of all, the public nature of Bitcoin transactions potentially allows for a degree of radical transparency for businesses and nonprofit organizations that is simply not possible any other way; although this area has not yet been fully developed in practice, one early example of such a system being implemented in practice is the "provably fair gambling" systems used by sites like SatoshiDice, allowing users to cryptographically verify that the site is not cheating on bets. Given all of these advantages, the natural question is: is it possible to use the same functionality for other applications as well? The answer, it turns out, is yes. The fundamental innovation behind Bitcoin, that of using cryptographic proof of work to maintain a secure distributed database, is good for more than just the single limited-supply currency originally envisioned by Satoshi Nakamoto in 2009; exactly the same technology can be used to maintain ownership of company shares, "smart property", alternative currencies, bank deposits and much more. Anything which is (1) representable as a digital asset, and (2) a "rivalrous good", meaning that only one  person can own it at a time, is potentially fair game for representation in the Bitcoin blockchain. However, Bitcoin does not include any facilities for doing this by default; to do any of these things, an additional protocol is required. And BitcoinX intends to accomplish just that.  Overview The idea is to open up Bitcoin as we know it now into two separate layers, being, the underlying transactional network based on its cryptographic technology, and an overlay network of issuance of distinct instruments encapsulated in a design we call 'colored coins.' If we can issue many distinct instruments within the Bitcoin ecosystem, there are many potential use cases :  A company might want to ​issue shares​ using colored coins, taking advantage of the Bitcoin infrastructure to allow people to maintain ownership of shares and trade shares, and even allow voting and pay dividends over the Bitcoin blockchain.  Smart property​: suppose there is a car rental company. The company can release one colored coin to represent each car, and then configure the car to turn on only if it receives a message signed with the private key that currently owns the colored coin. It can then release a smartphone app that anyone can use to broadcast a message signed with their private key, and put up the colored coins on a trading platform. Anyone will be able to then purchase a colored coin, use the car for whatever period of time using the smartphone app as a "car key", and sell the coin again at their leisure.  A local community might wish to create a ​community currency​, using the Bitcoin infrastructure to securely store funds.  A company may wish to create a ​corporate currency​, such as Air Miles rewards points, or even plain ​coupons​.  An issuer might wish to release a coin to ​represent deposits​, allowing people to trade, for example, "Bitstamp-USD coins" or some gold storage company’s "gold coins".  Decentrally managing ownership of ​digital collectibles​ such as original artworks just like art collectors buy and sell original copies of famous paintings for millions of dollars today, colored coins allow us to do the same with purely digital objects, such as songs, movies, e-books and software, as well, by storing the current ownership of the work as a colored coin on the blockchain.  Using colored coins to trade and manage​ access and subscription services​. For example, a museum, a subway or an online service like Netflix may issue passes as colored coins and release a smartphone app that can be used to make a signature proving ownership of a pass in person, allowing these passes to be simultaneously transferable, fully digital and securely uncopyable.  Technical background: Bitcoin transactions The fundamental object in the Bitcoin network is called a Bitcoin transaction. A Bitcoin transaction contains: 1. A set of ​inputs​, each of which contains (i) the ​transaction hash​ and ​output index of one of the outputs of a previous transaction, and (ii) a ​digital signature​ which serves as cryptographic proof that the recipient of the previous transaction output authorizes this transaction. 2. A set of ​outputs​, each of which contains (i) the ​value​ (amount of BTC) going to the output, and (ii) a ​script​, which determines the conditions under which the output can be spent.  (an example of transactions illustrating inputs and outputs) The precise nature of scripts and signatures is not important for the purposes of the colored coins protocol; a useful simplification is to think of the output script as a public key and the signature as a standard cryptographic digital signature made with the corresponding private key. Every output script maps to a unique Bitcoin address, and it is possible to efficiently convert back and forth between the two representations. The algorithm for validating a transaction is as follows: INPUT_SUM = 0 for input in inputs: prevtx = get_tx_from_blockchain(input.prev_out.hash) if prevtx == NULL:  return FALSE if is_output_already_spent(prevtx,input.prev_out.index): return FALSE ​if not verify(input.sig, prevtx.outputs[input.prev_out.index].script): return FALSE INPUT_SUM += prevtx.outputs[input.prev_out.index].value OUTPUT_SUM = 0 for output in outputs: OUTPUT_SUM += output.value if INPUT_SUM < OUTPUT_SUM: return FALSE return TRUE Note that Bitcoin does not recognize the concept of "address balances"; that is simply a layer of convenience added on top by Bitcoin software. If your Bitcoin wallet tells you that you have 50 BTC, what that means is that there are 50 BTC worth of transaction outputs (perhaps one output of 50 BTC, perhaps four of 12.5 BTC; any combination works) whose output scripts correspond to your address. But what if there is no set of transaction outputs that you can combine into a transaction to send exactly what you want? In that case, Bitcoin uses a concept called change. Essentially, you put more than you need to into the transaction, but you send the excess back to yourself in one of the outputs. Usually, change goes into a new address to preserve privacy. Using these transactions, it's possible to maintain user account balances and send to other users any amount desired.  (example of sending transaction with change)  Genesis transactions In order to issue a new color, one must release the coins of that color by creating a genesis transaction. A colored coin genesis transaction has specific rules that its inputs and outputs must follow. Inputs For the inputs of a genesis transaction, there are two cases to consider:     Non-reissuable colors​ - in the non-reissuable case, the inputs are irrelevant; the issuer will not have any power once the transaction is issued, so all that matters is the transaction itself (specifically, its outputs). Reissuable colors​ - here, the issuer should choose one secure address as an “issuing address”, and set input 0 of the transaction to come from that address. Later on, the issuer will be able to issue more units of the color by creating another genesis transaction with the same address at input 0. Inputs 1+ of a genesis transaction are always irrelevant from the perspective of the colored coins protocol; the only purpose of adding additional inputs is if you do not have a single input capable of paying for the colored coin genesis all by itself. Also, note that one issuing address can be responsible for at most one reissuable color, and if an issuing address is responsible for any reissuable colors it cannot be responsible for any non-reissuable colors.  Outputs  The outputs of a colored coin genesis consist of a set of outputs sending the colored coins to their original owners, followed by one OP_RETURN data output, followed by one or more “change” outputs to send excess uncolored bitcoins back to the issuer. An OP_RETURN output is allowed to have 40 bytes to remain as a standard transaction, and all of the data identifying the transaction as a colored coin transaction will be inside this data field. The data field will be organized as follows: [0...4]: [0,67,67,80,0] (that’s “CCP” padded with a zero-byte on both sides) [5...6]: protocol version number (currently 1) [7...8]: reissuance policy (0 for non-reissuable, 65535 for infinitely reissuable from the genesis address) [9..39]: optional (data about the color) The scheme is intended to be hierarchical; in the future, “COLOR” will always remain in bytes 0...4, and a version number will always be in bytes 5...6, but a future version may have a different intention for bytes 7-39 than to describe a re-issuance policy and optional data. Even in the scope of version 0, a future reissuance policy may specify some data in bytes [9...39]. Currently, the data in bytes 9...39 can be in any format; one possibility is for the issuer to include the color’s name and URL. Note that, in the case of re-issuable colors, a genesis address should be kept highly secure, since if an attacker manages to gain access to the genesis they will have unlimited power to issue new colored coins of that color. Thus, it is very strongly recommended that the address should be at least a 2-of-3 multisignature address, or perhaps even 3-of-5, with private keys held in cold storage. In the case of non-reissuable colors, the issuer needs no security unless they themselves intend to hold on to some portion of the colored coins, in which case the same security considerations apply to them as for any other digital assets. Full clients that download the entire blockchain should attempt to process every genesis, and store information about as many colors as possible. Light clients, including web-based clients, should require users to import colors by submitting the transaction hash of the color genesis, at which point they query a server for the genesis transaction to extract the information about the color. Example inputs: [ 17ztLiaGdWcWFX8CgYqWGQPEizepPLsSrb : 500000, 19GAFukX9ixSDLy1p2UEed2mpDQ69QPUXr : 2125735, 1MyK5te6U6zGoom68nVEzTU2x949ReNySN : 2500000 ] outputs: [  1LQhUnH2UY9cuLh8qvHUNPL3GgLVWU5ziD 1PEFUd66e2Q7w4w4KisZRSaupNJz7K7dpL 1PmqmJGRxpKx45FHLTBmteKQxViHgm4Mxi 1DCo3v1gSqU6g2GGDz9d4BD35h9cARe5dy OP_RETURN : [ 0 67 67 80 0 0 0 255 1MyK5te6U6zGoom68nVEzTU2x949ReNySN  : 1000000, : 1000000, : 1000000, : 2000000, 255 ] + [0] * 71 : 115735  ] First, we look at the outputs of the transaction. Note that the OP_RETURN output appears in position 5, and its first five bytes are ​[ 0 67 67 80 0 ]​ as specified by the protocol, so the transaction is a color genesis transaction and the outputs in positions 0-4 are all colored coin outputs. Hence, addresses ​1LQhU​,​ 1PEFU​,​ ​and ​1Pmqm​ will all have a colorvalue of 990000, and address ​1DCo3​ will have a colorvalue of 1990000 (the reason why 10000 is subtracted from each colorvalue will become evident in the “transfer transaction” section; for now accept the idea that 10000 satoshis of “padding” is required for each output as a rule). Then, look at the OP_RETURN itself. Bytes 7-8 are ​[ 255 255 ]​, so the color is reissuable. The address of input 0 is ​17ztLiaGdWcWFX8CgYqWGQPEizepPLsSrb​, so the color will be reissuable by any future genesis transaction which also has ​17ztLiaGdWcWFX8CgYqWGQPEizepPLsSrb​ as the address of its input 0.  Transfer transactions In order to transfer colored coins, one must create a transfer transaction. A transfer transaction sends colored coins from one address to another, although transfer transactions can also be more complex, involving different colors and uncolored coins in one transaction. This version of the protocol will define a transfer algorithm known as ​tagging-based coloring,​ using the sequence number field on each input as a “tag” determining which outputs the coins from that input will go to. Specifically, the bottom binary digits of the sequence number are treated as a bit field; eg. if the third digit from the right in the binary expansion of the sequence number is one, then colorvalue from that input will go to output 2, otherwise it will not. Thus, each input ends up associated with a subset of the outputs. The algorithm then has colorvalue “flow” from each input to each of its associated outputs in order. If an output ends up completely filled with colorvalue of one color, it is treated as colored; otherwise, the output is uncolored. The tagging-based algorithm also makes use of a concept called “padding”, a potentially adjustable parameter which is set to 10000 by default. The idea behind padding is that the first 10000 satoshis of each output are treated as uncolored, and the remainder is colored. This gets around anti-dust rules that prevent transaction outputs below a certain size. The precise algorithm specification is as follows:  padding = 10000 for output in outputs: output.colorvalue = 0 output.space = output.value - padding for i in [0 ... length(inputs) - 1]: inputs[i].space = inputs[i].value - padding if inputs[j].sequence < 4294967295: for j in [0 ... length(outputs) - 1]: if [inputs[i].sequence / (2 ^ j)] % 2 == 1: transfer = min(inputs[i].space,outputs[j].space) if inputs[i].colored: outputs[j].colorvalue += transfer outputs[j].space -= transfer inputs[i].space -= transfer for output in outputs: if outputs[j].colorvalue == outputs[j].value - padding: outputs[j].colored = true else: outputs[j].colored = false  Examples: inputs: [ { value: { value: { value: { value: ] outputs: [ { value: { value: { value: { value: ]  10005, seq: 2, colored: true }, 10010, seq: 6, colored: true }, 10020, seq: 1, colored: false }, 100000, seq: 4294967295, colored: false }  10020 10010 10005 90000  }, }, }, }  For the sake of human-readability and intuitiveness, we won’t quite follow the algorithm in the code; instead, we’ll break the process down into two parts. First, we’ll initialize all of the space and colorvalue variables, and determine which outputs each input goes to based on its  sequence number. Space is always initialized to value minus padding for inputs and outputs, and colorvalue is initialized to zero for all of the outputs. To show how sequence numbers turn into output sets, look at input 1 for example. Input 1’s sequence number, in binary form, is 00000000 00000000 00000000 00000110​, of which only bits 1 and 2 are set to 1. If the sequence number is 4294967295, we treat that as being equivalent to a sequence number of zero (ie. no outputs). This special rule is added so that non-colored-coin transactions, where the sequence number is nearly always 4294967295, can be safely ignored by the protocol. inputs: [ { space: { space: { space: { space: ] outputs: [ { space: { space: { space: { space: ]  5, outputs: [1], colored: true }, 10, outputs: [1, 2], colored: ture }, 20, outputs: [0], colored: false }, 90000, outputs: [0, 1, 2, 3], colored: false }  20, cv: 0 }, 10, cv: 0 }, 5, cv: 0 }, 80000, cv: 0 }  Now, we process input 0. With input 0, only output 1 needs to be considered. With this input/output pair, we see that ​min(inputs[0].space,outputs[1].space)​ is 5, so we subtract 5 space from both and add 5 colorvalue to input 1. For the sake of brevity, we’ll show the intermediate state in a more compressed form: inputs: [ (0,[1],t) (10,[1,2],t) (20,[0],f) (90000,[0,1,2,3],f) ] outputs: [ (20, 0) (5, 5) (5, 0) (80000, 0) ] Next, we process input 1. With input 1, we first process output 1. ​min(inputs[1].space, outputs[1].space)​= 5, so we change the state to: inputs: [ (0,[1],t) (5,[1,2],t) (20,[0],f) (90000,[0,1,2,3],f) ] outputs: [ (20, 0) (0, 10) (5, 0) (80000, 0) ] With output 2, we see that both the input and the output have 5 space, so we go to: inputs: [ (0,[1],t) (0,[1,2],t) (20,[0],f) (90000,[0,1,2,3],f) ] outputs: [ (20, 0) (0, 10) (0, 5) (80000, 0) ] Now, on to input 2. Input 2 has 20 space and output 0 has 20 space, so we subtract 20 space from both. However, input 2 is uncolored, so we add no colorvalue to output 0.  inputs: [ (0,[1],t) (0,[1,2],t) (0,[0],f) (90000,[0,1,2,3],f) ] outputs: [ (0, 0) (0, 10) (0, 5) (80000, 0) ] Input 3 has 90000 space, and matches no outputs because its sequence number is 4294967295, so we skip it. And we’re done. We now look at the outputs: outputs: [ { value: { value: { value: { value: ]  10020, 10010, 10005, 90000,  colorvalue: colorvalue: colorvalue: colorvalue:  0 }, 10 }, 5 }, 0 }  Outputs 1 and 2 have colorvalue matching the value - padding formula, and so are colored. Outputs 0 and 3 have no colorvalue, so they are uncolored. Fortunately, we have no outputs that are partially filled with colorvalue - if there were, that colorvalue would be destroyed. So what is this transaction? From our point of view, we see that colorvalue from inputs 0 and 1 flows to outputs 1 and 2. We also see that space flows from input 2 to output 0, but not colorvalue, and we see that space from input 3 goes nowhere. Chances are that this transaction is actually a trade, simultaneously moving the color that we are looking at from the owner of inputs 0 and 1 to the owner of outputs 1 and 2, and some other color that we are unaware of from input 2 to output 0. To see this more clearly, try running through the algorithm but with only input 2 colored. The space disappearing from input 3 is junk; that input exists simply to pay the transaction fee, and output 3 exists only to collect the change. Reasoning for Choice of Tagging-Based Coloring This tagging-based coloring algorithm has several advantages: 1. ​Minimal per-transaction metadata​ - with each transaction output, you do not need to store its colorvalue independently of its value, as colorvalue is always either ​value - padding​ if the output is colored or zero if it is uncolored. Additionally, in a multicolor context, the algorithm is designed in such a way that a transaction output can never have more than one color. Thus, the only metadata that should be stored with each transaction output is just its color (or, potentially, “uncolored”). 2. ​Multicolor support ​- a transaction can be made containing satoshis of multiple colors, and the transaction can be processed even if you are not aware of some of the colors that are in the transaction. In fact, it may even be possible to use backward scanning (see next section) to automatically detect and import new colors upon seeing a transaction containing them. 3. ​No minimum sending amount ​- thanks to the padding parameter, Bitcoin’s dust rule is circumvented, and a transaction can send even 1 colorvalue if desired. 4. ​Efficiency ​- if one is trying to determine the color of an output without any prior data, one would need to employ a backward scanning algorithm with potentially exponential growth in the  number of outputs needed to scan over time. Because the sequence based coloring algorithm ignores all default uncolored transactions, this process is massively sped up. Weaknesses: 1. ​Bitcoin price dependence​ - if the Bitcoin price jumps by 100x, and the dust minimum (currently 5430 satoshis) changes to compensate, the padding amount will become expensive to send. Alternatively, if the price falls by 100x and the dust minimum increases, the padding will become insufficient. 2. ​Efficiency​ - although this algorithm makes massive improvements over more naive approaches such as order-based coloring, the backward scanning is still theoretically exponential (as one output can have multiple parent inputs), so a secure “light client” is still much less efficient with colored coins than with pure Bitcoin. We know of only two ways of alleviating this problem. First, one can have a color scheme that allows splitting but not recombining outputs, relying on an active issuer to do the recombination. By limiting each output to having one input as a parent, this solves the exponential growth problem. Second, one can move beyond colored coins to a separate specialized network, most likely a new cryptocurrency (“altcoin”) merged-mined with Bitcoin. 3. ​SIGHASH_ANYONECANPAY​ does not work​. ​SIGHASH_ANYONECANPAY​ is an “opcode” that the signer of an input can add which, if translated into English, reads roughly as “I agree to send the funds contained in this input to the specified outputs, and I do not care what the other inputs are as long as they are sufficient to pay for the outputs”. Normally, a signature signs the entire transaction, including specific inputs and outputs. With colored coins, SIGHASH_ANYONECANPAY​ does not work, since Bitcoin’s scripting language is not “color-aware” so there is no way to demand that the other transaction outputs be of any specific color. This is a fundamental limitation of colored coins 4. ​Maximum 31 outputs per transaction​ - technically, 32 are possible, although the code would need to be more complicated to avoid accidentally making an input with sequence number 4294967295 (binary, ​11111111 11111111 11111111 11111111​) with the intention of sending that input to all outputs. Backward Scanning: TODO  Decentralized Exchange with Colored Coins One beneficial feature of colored coins is the ability to easily do atomic swaps - that is, trade X units of coin A for Y units of coin B without either party needing to trust the other. The idea behind this is simple: have a multi-color transaction with an input of X units of coin A from party 1 and Y units of coin B from party 2, and an output of X units of coin A to party 2 and Y units of  coin B to party 1. With OBC or padded OBC, this simply consists of putting the inputs and outputs in order - input and output 0 for coin A and input and output 1 for coin B.  Here is a more precise example in plain OBC, including a 10000-satoshi fee: { inputs: [ { color: { color: { color: ], outputs: [ { color: { color: { color: ]  'green', address: [A], value: 60000 }, 'blue', address: [B], value: 20000 }, 'uncolored', address: [B], value: 70000 }  'green', address: [B], value: 60000 }, 'blue', address: [A], value: 20000 } 'uncolored', address: [B], value: 60000 }  } However, this is not enough to make a full exchange; a full exchange requires the ability for users to post orders (eg. "I want to sell up to 50 X at a ratio of at least 3.3 Y for 1 X") and have those orders be enforceable. If an order is not enforceable, then anyone can spam the order book with false orders and thereby either manipulate or shut down the market outright. With colored coins, unfortunately, orders are not enforceable - anyone can make an order and refuse to sign any specific exchange transactions. Partially signed transactions with ​SIGHASH_ANYONECANPAY​ are powerless to solve the problem in most cases; if you are trying to sell colored coins for BTC, it is a viable solution, but if you are trying to buy colored coins anyone can defraud you by sending in uncolored coins, which the blockchain cannot distinguish from colored coins. Thus, a centralized, or at least semi-centralized, service is still required to act as a spam filter to make colored coin exchange a reality. However, the level of centralization is much less than between Bitcoin and other currencies, as the centralized service does not need to actually store any funds the only task that it needs to perform is that of screening users to see if anyone is making false orders, and then forbid them from making further orders.  References 1. http://yoniassia.com/coloredbitcoin/​ Initial blog post on coloredcoins 2. https://bitcointalk.org/index.php?topic=101197.0​ Initial discussions on coloredcoins in bitcointalk 3. https://bitcoil.co.il/BitcoinX.pdf​ Overview of colored coins by - Meni Rosenfeld 4. http://nwotruth.com/wikileaks-boycotts-too-big-to-fail-with-bitcoin/​ Wikileaks boycotts: too big to fail with Bitcoin 5. https://github.com/bitcoin/bitcoin/pull/2738​ Bitcoin 0.9 change 6. https://github.com/bitcoinx/colored-coin-tools/wiki/colored_coins_intro​ Colored coins intro (describes OBC and POBC) 7. https://coloredcoins.org 8. https://github.com/killerstorm/BitcoinArmory/blob/color/p2ptrade.py​ P2Ptrade 9. https://bitcointalk.org/index.php?topic=112007.0​ Discussion on atomic coin swapping 10. https://en.bitcoin.it/wiki/Smart_Property​ Smart property 11. http://brownzings.blogspot.com/2013/06/an-alternate-more-simple-implementation.h tml​ An Alternate More Simple Implementation of Colored Coins, Paul Snow 12. http://vbuterin.com/coloredcoins/valuecalc.py​ Dynamic programming algorithm for sending a desired number of satoshis under value-based coloring 13. order-based weak coloring Overview and call for ideas: 14. Alex’s ​order-based weak coloring​: 15. https://github.com/killerstorm/BitcoinArmory​ Armory version supporting colored coins 16. http://www.bitcoinx.org​ web site of a growing community around a multiple currency on top of bitcoin payment system 17. http://szabo.best.vwh.net/idea.html 18.  https://www.usenix.org/legacy/publications/library/proceedings/ec98/fujimura.html  Appendix: Alternative coloring algorithms The more challenging part of developing the colored coins protocol is to determine a way of transferring colored coins from one address to another. For transactions with one input and one output, the problem is trivial; the color of the output can simply be the same as the color of the previous output that is spent by the input. If a transaction has multiple inputs and outputs, however, the situation is different; how do we know which inputs correspond to which outputs? As it turns out, figuring out a way of specifying this is more challenging than it seems; although several solutions do present themselves, they are all imperfect in their own ways. Formally, what we care about is defining a function, ComputeOutputColors, that takes a transaction containing inputs and outputs, of the form: inputs = [ { value: [integer], colorvalue: [integer], }, ... ] outputs = [ { value: [integer] }, ... ] And computes the "​colorvalue​" property of each output. Note that in this model we are looking at one color at a time. In most of the coloring schemes described here, it is possible to process many colors simultaneously, but that is not a requirement for a coloring scheme  and we will show the one-color version for all coloring schemes for simplicity. In simpler coloring schemes, we have ​colorvalue = value​ for colored transactions (and of course ​colorvalue = 0​ for uncolored transactions), but this does not always hold true for more complex schemes. The difference between value and colorvalue is this: an output's value is the number of satoshis in the output, according to the plain Bitcoin protocol, whereas its "colorvalue" is the number of colored satoshis that it has. For example, in padded order based coloring (see below) a transaction may have 60000 regular satoshis, but only 50000 colored satoshis. It could also go the other way around; under certain tagging-based coloring schemes, an output might have 10000 satoshis according to the Bitcoin protocol, but 10 million satoshis according to the coloring rules. A full colored coins implementation, including the genesis transaction protocol and the transfer transaction protocol, should ideally have the following properties:  Simplicity​ - the protocol should be as simple and easy to implement as possible. Ideally, a human should be able to look through an ordinary blockchain explorer and trace colored coins of a specific color down the transaction graph.  Backward scanning​: it should be possible to determine the colorvalues of the inputs to a transaction from the colorvalues of the outputs.  Non-restrictiveness​ - as much of what one can do with bitcoins as possible should also be doable with colored coins. The transfer transaction protocol should not impose so many restrictions that it becomes impossible to, for example, make a multisignature transaction, a partially signed transaction or a time locked transaction. Unfortunately, 100% non-restrictiveness is impossible given the current protocol; for example, a trust-free assurance contract, done by publishing a transaction with a given output and allowing anyone to add and sign their own input using ​SIGHASH_ANYONECANPAY​ to pool together funds to pay for the output, cannot work, as there is no way to distinguish a colored output from an uncolored output in scripts. However, the protocol should leave as much room as possible.  Atomic tradeability​ - it should be possible to create a single transaction sending ​X units of color ​C1​​ ​from ​A​ to ​B​ and Y ​ ​ units of color ​C2​​ from ​B​ to ​A​ for any ​C1​​ and ​C2​​ provided that either (i) ​C​1​ and ​C​2​ use the same color protocol, or (ii) ​C​2​ = uncolored bitcoins. Ideally, color protocols should also be compatible with each other, but this is not critical.  Efficiency​ - it should be possible to efficiently determine the colorvalue of a given transaction output, ideally starting with zero information except the output and the color definition itself. Efficiency in this context does not have a clear definition; it is simply an empirical question of practicality (eg. does it take 100 blockchain queries or 200000; can the queries be parallelized?)  Space efficiency​ - colored coins transactions should impose a minimal amount of extra bloat on the Bitcoin blockchain compared to regular transactions.  Order-based Coloring Order-based coloring is the original, and simplest, coloring algorithm. The best intuitive explanation for order-based coloring is this: say that every transaction has a "width" proportional to its total input value. One side of the transaction will have inputs, with each input having a width proportional to its value, and the other side will have outputs, with each output having a width proportional to its value. Now, suppose that colored water is flowing down (or right) in a straight line from the inputs to the outputs. The color of an output will be the color of the water that flows into the output, or uncolored if an output receives water from multiple colors.  Formally, the algorithm is defined as follows: i = 0 offset = 0 for j in [0 ... length(outputs) - 1]: col = true while offset < outputs[j].value: if inputs[i].colorvalue == 0: col = false i += 1 if col: outputs[j].colorvalue = outputs[j].value else: outputs[j].colorvalue = 0 offset -= outputs[j].value return outputs Evaluation of OBC​:  Simplicity​: the algorithm is easy to code, and follows an intuitive model of the way that coloring "should" work.   Non-restrictiveness​: Aside from ​SIGHASH_ANYONECANPAY​, there are no restrictions.  Atomic tradeability​: atomic trades are very easy: input 0 and output 1 belong to A, and input 1 and output 0 belong to B.  Efficiency​: efficiency is very problematic. There are two ways to calculate the color of an output: top-down, calculating the color of every transaction output starting from the genesis output by recursively applying the OBC algorithm, and bottom-up, starting from the output and looking back through as few ancestors as possible to conclusively determine the color. The top-down algorithm is obviously not efficient without precomputation. The bottom-up algorithm is also not efficient, as every output can have multiple parents, in some circumstances causing the number of ancestors that needs to be searched to blow up exponentially. Uncolored outputs are especially difficult to prove, as the entire blockchain would potentially need to be scanned all the way back to the coinbase transactions.  Space efficiency​: OBC transactions take up exactly as much space as regular Bitcoin transactions. Special considerations: Order-based coloring also has another major weakness: the dust limit. Bitcoin enforces the requirement that all outputs in a standard Bitcoin transaction must contain at least 5430 satoshis (0.0000543 BTC), and transactions that fall afoul of this requirement will take days to confirm. This introduces the following problems:  Unique or near-unique colored coins​ (eg. smart property, original art, digital baseball cards,etc) need to be treated specially, since it is very difficult to send an output of exactly one satoshi. The likely solution is to create a 6000-satoshi genesis output, and then treat the 6000 satoshis together as a single colored coin; anything less than 6000 in a single output would count as nothing. However, there is a choice to be made here: can the 6000 satoshis be split and then put back together again? If so, then there is no final criterion for coin destruction, which is potentially very problematic. If not, then the implementation does not constitute pure order-based coloring.  Stocks or currencies issued on top of colored coins would either have high minimum transaction values, or be ​expensive to set up​. For example, if a company makes an IPO worth 10000 BTC on top of 100 BTC, the minimum amount of the stock that could be transacted would be worth 0.00543 BTC (~$1 as of the time of this writing). Note that on a lower level this is a problem for all coloring schemes described here; it is very expensive to create a color with more granularity than a few million units. Theoretically, however, it is possible to create coloring schemes that artificially add more granularity if the need arises.  Padded Order-Based Coloring  Padded order-based coloring is a somewhat more complicated approach that addresses some of the concerns around order-based coloring. Essentially, the idea is to treat every transaction output as having a "padding" of a certain number of uncolored satoshis, with the colored satoshis following. Aside from this, the algorithm is similar to OBC, although it is more restrictive.  The algorithm works by splitting up the list of transaction inputs and outputs into "sequences", where a sequence consists of a set of inputs and outputs that perfectly line up with each other. For example, if a transaction has inputs of size ​6,6,6,7,5,3​ (not including padding) and outputs of size ​9,9,10,2,2,1​, then the sequences would be [[6,6,6],[9,9]]​, ​[[7,5],[10,2]]​ and ​[[3],[2,1]]​. Within each sequence, if all inputs have the same color then the outputs are of that color, and otherwise the outputs are uncolored. This extra layer of restrictiveness is added to allow clients to recognize a large subset of normal Bitcoin transactions as uncolored and therefore immediately discard them. The code is as follows: j = 0 offset = 0 padding = 10000 sequences = [] current_input_seq = [] current_output_seq = [] for i in [0 ... length(inputs) - 1]: if inputs[i].value < padding: break offset += (inputs[i].value - padding) while offset > 0: if outputs[j].value < padding: break  offset -= (outputs[j].value - padding) current_output_seq.append(outputs[j]) j += 1 if outputs[j].value < padding: break current_input_seq.append(inputs[i]) if offset == 0: sequences.append({ inputs: current_input_seq, outputs: current_output_seq }) current_input_seq = [] current_output_seq = [] for output in outputs: output.colorvalue = 0 for seq in sequence: col = true if (seq.inputs[0].colorvalue > 0) else false for input in seq.inputs: if !input.colorvalue: col = false if col: for output in seq.outputs: output.colorvalue = output.value - padding Evaluation:  Simplicity​: the algorithm is considerably more complex, and less intuitive, than plain OBC. Furthermore, creating transactions with more outputs than inputs (or vice versa) adds an extra layer of complexity, as a transaction may need to have an extra input or output to pay for or collect excess padding.  Non-restrictiveness​: in some ways, padded OBC is actually slightly less restrictive than plain Bitcoin, as it becomes practical to send a single satoshi of a given color. On the other hand, ​SIGHASH_ANYONECANPAY​ is still unusable.  Atomic tradeability​: atomic trades are very easy.  Efficiency​: padded OBC wins over traditional OBC on one count: it can sometimes detect non-colored-coin transactions, thereby in many cases prematurely ending the search. This usually happens when, in non-colored-coin transactions with multiple inputs and outputs, the number of inputs and outputs does not match up, leading to the coloring algorithm discovering no sequences. For even greater efficiency, one can always add more restrictions, for example outright mandating that the number of inputs and outputs must be the same, and forbidding outputs smaller than the padding. Aside from this, however, the fact that a potentially exponential upward search may be required still remains.   Space efficiency​: OBC transactions take up exactly as much space as regular Bitcoin transactions.  Alternative Tagging-based Coloring There is also an alternative coloring algorithm, which uses tagging to determine not the permutation, but rather the coloring itself. The last two digits of the output value are used to represent the color of the output. However, because there is a risk of collisions (in fact, with 101 colors at least two must have the same two-digit ID), we assign to each color ten two-digit IDs, and give the user’s client the freedom to pick a color ID that is unique to a particular color in the event of an atomic swap. The version described here is a formalization and minor modification of an algorithm described by Paul Snow.​[8] padding = 10000 for output in outputs: tag = output.value % 100 output.color = "uncolored" if tag > 0: for i in [0 ... 9]: if SHA256(get_color_definition(color)+i) == tag: if output.value >= padding: output.colorvalue = output.value - padding Unlike the other algorithms, this one does need to be followed up by a value consistency check: cv = 0 for input in inputs: cv += input.colorvalue for output in outputs: cv -= output.colorvalue if cv < 0: for output in outputs: output.colorvalue = 0 Evaluation:  Simplicity​: the algorithm is easy to implement. However, there is one complication: an output can have multiple colors at the same time. Surprisingly though, even if that does happen it is possible to construct a transaction to split the multicolored output into separate single-colored outputs.  Non-restrictiveness​: one could maliciously construct a color which cannot be exchanged for a given other color in 10^20 steps (or two colors which are mutually incompatible in 10^10 steps). Also, not all transaction sizes are possible. However,  statistical tests show that any amount of more than 2000 satoshis can nearly always be built from multiple outputs, and an efficient dynamic programming algorithm exists to find a way of doing so​[9]​.  Atomic tradeability​: atomic trades are very easy.  Efficiency​: the algorithm will nearly always be able to detect and avoid processing uncolored transactions, so the upward search will be limited to transactions of a given color. This essentially provides the maximum possible efficiency with any output-based coloring scheme.  Space efficiency​: no extra data cost required.  Per-Satoshi Tracking Per-satoshi tracking is another form of order-based coloring, but which is very limited in scope. Unlike OBC or tagging-based coloring, which seek to assign colors to outputs, per-satoshi tracking assigns a color to each satoshi in an output. Algorithm: def getParent(tx, index, offset): total_offset = 0 for i in [0 ... index-1]: total_offset += tx.outputs[i].value total_offset += offset j = 0 while total_offset > tx.inputs[j].value: total_offset -= tx.inputs[j].value: j += 1 return (get_previous_tx(tx.inputs[j]), j, total_offset) getChild​ works similarly but in reverse. Note that these functions do not compute colors; they simply track a single satoshi as it makes its way down the blockchain, with the understanding that one particular satoshi will always have the same color. The main advantage of this is efficiency: there is no risk of exponential blowup, as each satoshi has only one parent. However, for most cases this protocol also introduces an impractical amount of inefficiency, as most outputs have many thousands, or even millions, of satoshis. Thus, this is a special-purpose protocol, limited to unique colored coins such as collectibles and smart property. However, for that purpose, it provides an incredibly useful and light-weight implementation of colored coins.  Since individual satoshis cannot practically be sent due to the dust limit, a colored satoshi would likely come pre-packaged with 10000 satoshis of padding. A genesis transaction would, by convention, publish one satoshi per output, always at offset zero, and then the satoshi would be passed around along with its padding.  CryptoKitties: Collectible and Breedable Cats Empowered by Blockchain Technology W H IT E P A - P U R R  VERSION 2.0  Abstract As blockchain technology continues to dominate headlines, cryptocurrencies—especially their valuations, and potential to disrupt the financial industry—are of increasing interest. However, the average consumer doesn’t understand what a cryptocurrency is or why it matters, let alone how the technology behind it works. As a result, the public perception of blockchain application is increasingly narrow-minded and short-sighted. Likewise, the technology’s potential and long-term implications remain esoteric and largely ignored. CryptoKitties will make blockchain technology accessible to the average consumer through four main tactics: • • • •  Gamifying features that leverage blockchain’s unique applications An approachable, consumer-facing brand based on a genuine passion for blockchain technology An open platform inclusive to users of all levels of technical knowledge A sustainable revenue-based model (as opposed to an ICO)  On a more technical level, we plan to innovate within the blockchain space through practical experimentation and application of digital scarcity, digital collectibles, and non-fungible tokens. By normalizing the practical application of smart contracts and cryptocurrency transactions, we will empower everyday consumers with a basic fluency in distributed ledger technology. Likewise, by showcasing a practical use for blockchain technology outside of the financial industry, we hope to broaden the public’s understanding of the technology and its potential application. Note that this whitepaper is provided for informational purposes only, and does not and will not create any legally binding obligation on the authors or on any third party. For specific legal terms governing the use of the CryptoKitties app, please view the Terms of Use here: https://www.cryptokitties.co/terms-of-use.  2  CryptoKitties: Collectible and Breedable Cats Empowered by Blockchain Technology  1  Abstract  2  1. Motivation  4  1.1 Public perception of blockchain technology  4  1.2 Practical and sustainable application of blockchain technology  4  1.3 Meaningful innovation of blockchain technology via digital scarcity  5  2. The Product  6  2.1 Education through gamification  6  2.2 Pawsitive perception and broad appeal  6  2.3 Proven mechanics and practical testing  7  2.4 A sustainable revenue model  7  2.5 Innovation via implicit mechanics  8  Conclusion: The Future is Meow  9  3  1. Motivation In developing CryptoKitties, we were motivated by blockchain’s public perception and how the following issues are potentially detrimental to the technology’s potential: 1. The public’s understanding of blockchain technology is limited and interest is typically tied to the headline-grabbing cryptocurrency valuations 2. ICOs are a powerful funding tool, but abuses with the model and a lack of practical use cases are sowing mistrust in the technology they’re supposed to empower These two issues contribute to a larger issue: a shortage of meaningful innovation with blockchain technology. To that end, our product aims to not only address these broad issues, but it aims to innovate within the space by practically exploring: 3.  Digital scarcity, digital collectibles, and non-fungible tokens.  1.1 Public perception of blockchain technology Distributed ledger technology has the potential to be the information age’s biggest revolution since the Internet. Its potential applications are varied and its implications reach across numerous industries. However, the general concept of blockchain technology, especially in the mind of the mass consumer, is esoteric. Existing blockchain projects typically limit their audiences to early investors or a relatively small group of people with highly specialized knowledge or interests. Even then, most of these projects are either concepts or works in progress: their practical product remains nebulous.  1.2 Practical and sustainable application of blockchain technology Initial Coin Offerings (ICOs) have proven themselves a viable funding model for blockchain projects. However, while this model intends to open up funding to investors outside of the venture capitalist sphere of influence, it can create obstacles for other audiences. What’s more concerning are the ICOs conducted in bad faith. These token sales amount to little more than scams, creating mistrust in the model, projects, and technology associated with them. Because of the headline-grabbing focus of cryptocurrency valuations and the potential disruption to the financial industry, innovations in blockchain technology have been relatively stymied, instead focusing on the “low-hanging fruit” that is a cryptocurrency-adjacent application. Likewise, the “cryptocurrency,” as a concept often goes over the head of the average consumer; they don’t understand the implications of blockchain technology beyond simple trading and investment. While it’s understandable that new developments will follow public interest to some degree, it may limit blockchain innovation in both the short and long term.  4  1.3 Meaningful innovation of blockchain technology via digital scarcity Finally, an area of substantial experimentation that continues to go “unsolved” is the concept of digital scarcity and digital collectibles. Digital goods have seen real-world valuation, from World of Warcraft’s gold farmers to the Steam platform’s online marketplace (where users can buy and sell in-game items across their PC’s video game collection). However, these niche instances are limited to video games and lack any security or protections. There have been numerous examples of hacking, cheating, or developers influencing the ecosystem and larger economy. Digital collectibles hold immense potential, but they haven’t proven successful for three main reasons: Central Issuing Authority When digital collectibles are created and issued, and the most rare or popular collectibles are identified, there is nothing stopping the creator from simply creating more. When this happens, it diminishes the value of the original collectibles, potentially making them worthless. Provider Dependency The existence of a digital collectible is dependent upon the existence of the issuing authority. If a digital collectible is created and said creator ceases to exist, your digital collectibles also cease to exist. Lack of Function Physical collectibles are popular because of their intended purpose. Art is a great example: people collect it, it can be worth a lot of money, and it serves a purpose by hanging on the wall as a thing of beauty. Current digital collectibles don’t serve a purpose and don’t have a function. This is evidenced by the initial interest shown in digital collectibles such as Cryptopunks, but that interest waned quickly. We believe this was, in part, due to their lack of functionality. The only reason our previous examples have not been susceptible to these issues is because of their large user base, their central authority in the form of a large developer beholden to its large user base, and the “function” of these collectibles via their respective videogame application. The size, scope, and long-term pedigree of these platforms can alleviate fears associated with provider dependency, but it doesn’t solve it. Because these problems exist, people aren’t willing to invest in digital collectibles, outside of these niche examples. If digital collectibles held their value the same way a physical collectible would, this problem is eliminated and an entirely new world of collecting would come to life. 5  2. The Product CryptoKitties are digital, collectible cats built on the Ethereum blockchain. They can be bought and sold using ether, and bred to create new cats with exciting traits and varying levels of cuteness. At launch, 50,000 “Gen 0” cats (colloquially referred to as “Clock Cats”) will be stored in a smart contract on the Ethereum blockchain. These Clock Cats will be distributed automatically via smart contract at a rate of one cat every 15 minutes. Each cat will be sold by auction. CryptoKitties are unique in appearance, with a distinct visual appearance (phenotype) determined by its immutable genes (genotype) which are stored in the smart contract. By allowing the cats to breed, they aren’t just a digital collectible. CryptoKitties is an exciting, self-sustaining community where users can create new collectibles and trade them on the ethereum blockchain. Two CryptoKitties can breed to produce a new cat that is the genetic combination of its parents. Anticipating the outcome is exciting and the possibilities for new and rare genetic makeups of a CryptoKitty are endless. In each breeding pair, one cat acts as the sire and will have a recovery period before it can breed again. The second cat incubates the kitten, during which time it cannot engage in another pairing. There is no limit to the number of times a CryptoKitty can breed, but the recovery and gestational period increases the more they mate.  2.1 Education through gamification CryptoKitties’ key game mechanics are tied to actions associated with cryptocurrencies and smart contracts. In doing so, previously esoteric concepts are normalized and users are empowered with a basic fluency in the technology. Because blockchain is an emerging technology, there are more obstacles to user onboarding than we consider ideal. At launch, a user will require a MetaMask wallet with ether in it. However, we plan to explore alternatives to MetaMask as a login solution, as well as and shortcuts for converting traditional currencies into ether. For now, we’ve set up straightforward guides to make the process as easy as possible.  2.2 Pawsitive perception and broad appeal The CryptoKitties brand is incredibly approachable to consumers. By using colourful palettes, cute cat puns, and cat humour, the brand stands out in a space that is typically dominated by lacklustre, business-to-business branding. Outside of a few specific experiments, few blockchain projects have marketed themselves beyond high-value investors or crypto experts. The CryptoKitties marketing plan also leverages influencers in various communities (e.g. celebrity cats, tech authorities, etc.) to design custom art for a litter of “Fancy Cats,” which are CryptoKitties with custom art. This is yet another opportunity to broaden our audience and  6  introduce new users to CryptoKitties. It also creates long-term content marketing touchpoints associated with each Fancy Cat collaboration, release, and discovery.  2.3 Proven mechanics and practical testing A major challenge associated with smart contracts is their immutable nature. The same concepts that empower trustless transactions and security make a smart contract flaw outright deadly. While a traditional software project can iterate or fix core issues, a blockchain project cannot edit its smart contract once it launches. To this end, the economy and revenue model that we are exploring cannot be artificially influenced, for good or ill. With that in mind, it was crucial that we thoroughly test and explore our product’s mechanics. It’s also worth noting that we believe that proof of concept is necessary to ensure trust in both a project and the technology behind it. We limited our “build in public” approach until we had a minimum viable product (in this case, an alpha build) to showcase to initial users. We tested the concept and initial mechanics in a live, three-day alpha test at ETHWaterloo, the world’s largest Ethereum hackathon. This alpha helped us foster our initial community and product advocates. It also provided insight into how to best utilize our game’s core mechanics. (Our team also won the ETHWaterloo Hackathon with Rufflet, a mixpanel application for smart contracts.) A month later, we deployed our initial “beta” smart contract to Axiom Zen, our parent company. Unfortunately, there was a fatal flaw in the contract, so we terminated it. We then launched our smart contract to an early access closed beta. This version passed with flying colours and catapulted CryptoKitties to the third most active Ethereum project on the network within three hours of the closed beta launch. Minor tweaks to breeding prices ensured a stable launch the following week.  2.4 A sustainable revenue model Instead of pursuing an ICO, CryptoKitties operates on a sustainable revenue model. We receive a small percentage (3.75%) of each transaction conducted on its marketplace. Like any other user on the CryptoKitties platform, we also receive the revenue from our cat sales. However, these cat sales are limited to the 50,000 “Clock Cats” that are released every 15 minutes via the CryptoKitties smart contract. A Clock Cat’s price is determined by the average price of the last five CryptoKitty sales, plus 50%. They are sold by a descending clock auction and their actual valuation will be determined by the marketplace’s ecosystem.  7  2.5 Innovation via implicit mechanics CryptoKitties provides a practical use case for digital scarcity and digital collectibles by pioneering ERC-721, a non-fungible token protocol.  https://github.com/ethereum/EIPs/issues/721 2.5.1 Non-Fungible Tokens In general, tokens on the blockchain are fungible. The value of every token is the same and, similar to cash, it doesn’t matter what token you receive. Because of this, blockchains track counts of tokens instead of the specific tokens themselves. This works well for things like stocks or currencies, but because CryptoKitties are genetically unique and breedable, we needed to create a non-fungible token environment. To do this, we pioneered the ERC-721 protocol for non-fungible tokens, the standard for transactions and ownership of non-fungible assets on the blockchain. Using this protocol, CryptoKitties provides both an interface to browse unique items and robust smart contracts to conduct transactions (breeding) once both parties agree.  2.5.2 Descending Clock Auctions While creating CryptoKitties, we needed to find a way to auction non-fungible tokens. Because each CryptoKitty is genetically unique, the current process for selling tokens on the blockchain fails because the value of each token could be different. Each token must have its own bespoke auction, like in an auction on a platform such as eBay. Since CryptoKitties runs on the Ethereum blockchain, the typical English auction (where a suggested opening bid is made and buyers can increase their offer as time goes on) doesn’t work either. The Ethereum blockchain requires each transaction placed on the network to pay gas fees, so users would be required to pay simply to place a bid. This is a poor user experience, so we needed to find a way to reduce the number of on-chain transactions. To mitigate this, we created a descending clock auction. Sellers choose a high opening bid, a minimum closing bid, and a timeframe for which they’d like their auction to run. Buyers are able to choose their purchase price along that spectrum by purchasing when the price aligns with their perceived value of the CryptoKitty being sold – as long as someone else doesn’t buy it before them. Buyers pay gas when they complete a purchase and sellers pay gas to initiate an auction. The same process is used for breeding. CryptoKitty owners may place their cats available for sire by selecting a maximum opening bid for siring, a minimum closing bid, and timeframe for the auction. Owners who want to breed their cats may do so by choosing a sire and paying the current descending clock auction bid associated with the sire they want to breed with.  8  Conclusion: The Future is Meow Our team believes in the potential of blockchain technology, but we’re frustrated by the state and direction of its public perception. We genuinely believe that education, an accessible user experience, and practical application are the missing factors for broader adoption. We strive to communicate this belief with CryptoKitties, Rufflet (our winning project at the ETHWaterloo Hackathon), and any any future blockchain projects our team works on. We still see numerous opportunities to innovate with blockchain technology. With CryptoKitties, we’re exploring digital scarcity, an exciting but underexplored concept that blockchain empowers. CryptoKitties is the world’s first game built on the Ethereum blockchain. It makes the blockchain approachable for the everyday consumer and brings us but a small step closer to widespread adoption of cryptocurrencies and blockchain technologies. In our cheekily-titled “catifesto,” we state that we didn’t want to change the future, but rather, we wanted to have fun with it. That’s not entirely true - we genuinely believe the power of fun and games can shape blockchain’s future for the better. We can’t be sure that CryptoKitties will be a runaway success, a flash in the pan, or a blip on blockchain’s biography. Like all things related to blockchain, the future is in flux. But we do believe in a few things: • • • •  We believe in blockchain’s potential We believe in this product We believe in our team We believe in our community  Onwards and upwards, The CryptoKitties Team  9  White paper  Decentraland  A blockchain-based virtual world  Esteban Ordano esteban@decentraland.org  Ariel Meilich ari@decentraland.org  Yemel Jardi yemel@decentraland.org  Manuel Araoz manuel@decentraland.org  Abstract Decentraland is a virtual reality platform powered by the Ethereum blockchain. Users can create, experience, and monetize content and applications. Land in Decentraland is permanently owned by the community, giving them full control over their creations. Users claim ownership of virtual land on a blockchain-based ledger of parcels. Landowners control what content is published to their portion of land, which is identified by a set of cartesian coordinates (x,y). Contents can range from static 3D scenes to interactive systems such as games. Land is a non-fungible, transferrable, scarce digital asset stored in an Ethereum smart contract. It can be acquired by spending an ERC20 token called MANA. MANA can also be used to make in-world purchases of digital goods and services. People are spending increasingly more time in virtual worlds, for both leisure and work1. This occurs predominantly in 2D interfaces such as the web and mobile phones. But a traversable 3D world adds an immersive component as well as adjacency to other content, enabling physical clusters of communities. Unlike other virtual worlds and social networks, Decentraland is not controlled by a centralized organization. There is no single agent with the power to modify the rules of the software, contents of land, economics of the currency, or prevent others from accessing the world.  Decentraland 1/14  This document lays out the philosophical underpinnings, technical foundations, and economic mechanisms of Decentraland.  1 The State of Mobile, Flurry Analytics Blog. https://flurrymobile.tumblr.com/post/155761509355  We would like to thank the following reviewers, whose contributions and feedback made this document possible:  Jake Brukhman (Founder, CoinFund) Luis Cuende (Project Lead, Aragon) Simon de la Rouviere (ConsenSys) Diego Doval (Founder of n3xt, previously CTO, Ning) Michael Bosworth (Google) Jesse Walden (Mediachain/Spotify) Chris Burniske (ex ARK Invest) Guillermo Rauch (CEO, Zeit) Joe Urgo (Co-founder, District0x) David Wachsman (Wachsman PR) Jon Choi (Dropbox)  Decentraland 2/14  Allen Hsu  Table of Contents  Decentraland 3/14  1 Introduction 1.1 Rationale 1.2 History 1.3 A Traversable World 1.4 Foundations for an In-world Economy 1.5 Use Cases  4 4 5 6 6 7  2 Architecture 2.1 Consensus Layer 2.2 Content Distribution Layer 2.3 Real-Time Layer 2.4 Payment Channels 2.5 Identity System  8 9 9 10 11 11  3 Economy 3.1 LAND and the MANA Token 3.2 Fostering the Network  12 12 13  4 Challenge 5 Summary  13 14  01 Introduction Decentraland provides an infrastructure to support a shared virtual world, also known as a metaverse2. It consists of a decentralized ledger for land ownership, a protocol for describing the content of each land parcel, and a peer-to-peer network for user interactions.  1.1 Rationale The development of large proprietary platforms, such as Facebook, has allowed hundreds of millions of users to gather, interact, share content, and play games. Their network eﬀects helped cultivate vast online communities and gaming companies. These platforms, controlled by centralized organizations, manage the network’s rules and content flow, while extracting significant revenue from the communities and content creators who drive traﬀic to the platforms. Decentraland aims to establish a network that allows its content creators to own and capture the full value of their contributions. The current team began working on Decentraland in 2015. At the time, the adoption of cryptoassets was still in its infancy, as much of the blockchain-based infrastructure necessary for a consumer-oriented platform was lacking. Since then, the rate of consumer adoption and infrastructure creation has exploded. For instance, by July 2017, Coinbase alone reached 8.4 million user accounts, with half of these added over the past 12 months4. This growth has given rise to a pool of users large enough to fuel the decentralized commerce that will take place in a virtual world like Decentraland. While blockchain infrastructure, spearheaded by Ethereum, is now more widely available, the lack of an eﬀicient method to quickly process micropayments constrains the throughput of network transactions.  01. Introduction 4/14  The maturation of cryptocurrencies as a global, instant, and low cost payment method is still evolving. Payment transactions will need to occur oﬀ-chain to achieve short- to medium-term scalability in blockchain payment networks. Solutions such as Bitcoin's Lightning Network5 or Ethereum's state channels6 are on the verge of enabling a fast, global payment system with low fees.  2 “The Metaverse”. Wikipedia. https://en.wikipedia.org/wiki/Metaverse 3 http://www.kccllc.net/thq/document/1213398130702000000000003 4 https://www.coinbase.com/about 5 “Lightning Apps and the Emerging Developer Ecosystem on LND”. Lightning Network’s Blog. http://lightning.community/software/lnd/lightning/2017/07/05/emerging-lightning-developer-ecosystem/ 6 “Sprites: Payment Channels that Go Faster than Lightning”. A. Miller, I. Bentov, R. Kumaresan, P. McCorry, 2017. https://arxiv.org/pdf/1702.05812.pdf  More centralized solutions can work today7, although at the expense of operability with other systems, privacy, and standardization. Decentraland is built on the premise that low cost, direct payments between content creators and users will radically change internet commerce.  1.2 History Decentraland began as a proof of concept for allocating ownership of digital real estate to users on a blockchain. This digital real estate was initially implemented as a pixel on an infinite 2D grid, where each pixel contained metadata identifying the owner and describing the pixel's color. The experiment was entitled Decentraland’s Stone Age.  Figure 1: Stone Age. A visual representation of the blockchain state. Users could mine, transfer, and change the color of pixels they own.  In late 2016, the team started developing the Bronze Age, a 3D virtual world divided into land parcels. The owner of each parcel was able to associate it with a hash reference to a file, using a modified Bitcoin blockchain. From this reference, users exploring the virtual world could use a Distributed Hash Table (DHT) and BitTorrent to download the file containing the parcel’s content, which defines the models and textures to be displayed at that location. We hosted the first world viewer at decentraland.org/world. Any enthusiast can run a node, download and verify the blockchain, and explore the world by following more advanced instructions8.  01. Introduction 5/14  Figure 2: Bronze Age. Structures created by the community around the Genesis parcel, located at coordinates (0,0)  7 “Near-zero fee transactions with hub-and-spoke micropayments”. Peter Todd, bitcoin-development mailing list, December 2014. https://www.mail-archive.com/bitcoin-development@lists.sourceforge.net/msg06576.html 8 https://github.com/decentraland/bronzeage-node#run-a-node  The next version of Decentraland, the Iron Age, will create a social experience with an economy driven by the existing layers of land ownership and content distribution. In the Iron Age, developers will be able to create applications on top of Decentraland, distribute them to other users, and monetize them. The Iron Age will implement peer-to-peer communications, a scripting system to enable interactive content, and a system of fast cryptocurrency payments for in-world transactions. A communication layer is essential for social experiences, providing positioning, postures, voice chat, and more; Decentraland achieves this with a P2P network. The scripting system is the tool that landowners will use to describe the behavior and interactions of 3D objects, sound, and applications running on land parcels. Finally, a payment system with low fees is key to developing an economy in the quick environment of a virtual world.  1.3 A Traversable World The adjacency of land makes Decentraland parcels unique from web domains. New land parcels must be contiguous to existing ones. This adjacency allows for spatial discovery of new content and the creation of districts devoted to a special topic or theme. While each web domain can have an unlimited number of hyperlinks to other content, parcels in Decentraland have a fixed amount of adjacencies. Additionally, the content of adjacent parcels can be seen from a distance. For content creators, the establishment of districts provides access to targeted traﬀic; for end users, it enables discovery of themed experiences. Users can travel through neighborhoods and interact with applications that they stumble upon. This discovery by adjacency is at odds with having infinite land: in that scenario, users would have a hard time finding relevant content by traveling through it. With scarce land, developers can acquire users by purchasing land in high-traﬀic areas. This will allow secondary markets to develop around land ownership and rentals, as is already happening on district0x.io.9  01. Introduction 6/14  1.4 Foundations for an In-world Economy Decentraland’s value proposition to application developers is that they can fully capitalize on the economic interactions between their applications and users. To allow those economic interactions, the platform must allow three things to be traded: currency, goods, and services. Decentraland will integrate a core system that allows global, instant, and cost-eﬀective payments between any two users on the internet. Cryptocurrencies allow for trustless payment channels to be established between parties, with low-trust hub-and-spoke systems already possible. 9 https://blog.district0x.io/decentraland-districts-40b9ada0431b  For services to be provided on Decentraland, we are developing a scripting system that enables developers to program the interactions between users and applications. This scripting system runs exclusively on the client side but allows for diﬀerent data flow models: from mere local eﬀects and traditional client-server architectures, to P2P interactions based on state channels. Developers programming in it will benefit from the availability of fast, cheap micropayments, provably fair games, decentralized storage, and other features enabled by the advent of cryptographic techniques using blockchain-based smart contracts. To foster the exchange of virtual goods, economic incentives must be in place to ensure the continued creation and distribution of avatars, items, and scripts. Because static content can be arbitrarily copied, the user experience should empower social agreements that recognize original creations. By implementing an identity system to establish authorship, users will be able to track and verify an author’s consent through cryptographic signatures. These experiments are already happening, as in the case of Rare Pepes10.  1.5 Use Cases Applications The Decentraland scripting language will allow the development of applications, games, gambling, and dynamic 3D scenes. This scripting language will be designed to handle a wide range of capabilities, including creating objects, loading textures, handling physics, encoding user interactions, sounds, payments, and external calls, among others.  Content Curation Users in Decentraland will gather around neighborhoods of shared interest. Being located near high-traﬀic hubs will drive users to the landowners’ content.  Advertising  01. Introduction 7/14  Brands may advertise using billboards near, or in, high-traﬀic land parcels to promote their products, services, and events. Some neighborhoods may become virtual versions of Times Square in New York City. Additionally, brands may position products and create shared experiences to engage with their audience.  Digital Collectibles We expect users to publish, distribute, and collect rare digital assets issued on the blockchain by their creators. Just as it occurs today in other virtual worlds or through online forums, these digital assets will be traded inside this world through the scripting system and be backed by the aforementioned naming system.  10 “Rare Pepe”. Fred Wilson. https://avc.com/2017/05/rare-pepe/  Social Groups that currently gather in online forums, chat groups, or even other centralized multiplayer games could port their communities into Decentraland. Oﬀline communities could also find in Decentraland a space to gather.  Other use cases There are no technical specifications to what could be built in Decentraland. Therefore, other use cases could emerge, such as training and professional development, education, therapy, 3D design, and virtual tourism, among others.  02 Architecture The Decentraland protocol is comprised of three layers: 1) Consensus layer: Track land ownership and its content. 2) Land content layer: Download assets using a decentralized distribution system. 3) Real-time layer: Enable users’ world viewers to connect to each other. Land ownership is established at the consensus layer, where land content is referenced through a hash of the file’s content. From this reference the content can be downloaded from BitTorrent or IPFS. The downloaded file contains a description of objects, textures, sounds, and other elements needed to render the scene. It also contains the URL of a rendezvous server to coordinate connections between P2P users that are exploring the tile simultaneously. Figure 3 shows a diagram of the steps the Decentraland clients execute to provide the experience of a shared virtual world in a decentralized way.  02. Architecture 8/14  Figure 3: The Decentraland protocol for simultaneous users in a decentralized virtual world.  Additionally, two other systems are key for Decentraland’s economy to develop: Payment Channel Infrastructure for fast payments with low fees. Identity System that allows users to establish ownership over original creations.  2.1 Consensus Layer Decentraland will use an Ethereum smart contract to maintain a ledger of ownership for land parcels in the virtual world. We call these non-fungible digital assets LAND: each LAND has unique (x, y) coordinates, an owner, and a reference to the content description file, which encodes what the landowner wants to serve there. Decentraland clients will connect to the Ethereum network to fetch updates to the state of the LAND smart contract. LAND is bought by burning MANA, a fungible ERC20 token of fixed supply. This token serves as a proxy for the cost of claiming a new parcel. The LAND contract uses a burn function to destroy MANA and create a new entry in the LAND registry. New parcels need to be adjacent to a non-empty parcel.  2.2 Content Distribution Layer Decentraland uses a decentralized storage system to distribute the content needed to render the world. For each parcel that needs to be rendered, a reference to a file with the description of the parcel’s content is retrieved from the smart contract. The current solution uses the battle-tested BitTorrent and Kademlia DHT networks by storing a magnet link for each parcel. However, the Inter-Planetary File System (IPFS)11 provides a compelling alternative as its technology matures.  02. Architecture 9/14  This decentralized distribution system allows Decentraland to work without the need of any centralized server infrastructure. This allows the world to exist as long as it has users distributing content, shifting the cost of running the system to the same actors that benefit from it. It also provides Decentraland with strong censorship-resistance, eliminating the power of a central authority to change the rules or prevent users from participating. However, hosting these files and the bandwidth required to serve this content has significant costs. Currently, users of the Decentraland P2P network are seeding the content without compensation and out of goodwill. However, in the future, this infrastructure cost can be covered by the use of protocols like Filecoin12. Until this technology becomes available, automated micropayments can be used to pay for quality of service. The proceeds of Decentraland’s continuous sale of MANA can cover these costs over the long run (see Section 3.2). 11 https://ipfs.io 12 https://filecoin.io/  The description of a parcel will contain a list of diﬀerent files required to render it, a list of services hosted by the landowner, and an entry point to orchestrate the placement of objects and their behavior. This document must declare: Content files References to, or blobs with, 3D meshes, as well as textures, audio files, and other relevant content required to render the parcel. These are specified so that the client knows what contents the renderization will need, without any instructions on how to place them. Scripting entry point The scripting system controls how the content is placed in the parcel, as well as its behavior. This enables applications and animations to take place within the parcel. It also coordinates behaviors such as the positioning and movement of objects, the timing and frequency of sounds played, the possible interactions with users, among other features. P2P interactions This allows the client to connect to a server that bootstraps user-to-user connections, coordinates positions and postures, and enables voice chat and messaging.  2.3 Real-Time Layer Clients will communicate with each other by establishing peer-to-peer connections with the help of servers hosted by landowners or third parties. Without a centralized server, peer-to-peer connections are needed to provide social interactions between users, as well as applications that the landowner wants to run inside the parcel. To coordinate the bootstrap of peer-to-peer connections, landowners will have to provide rendezvous servers or understand that users will not be able to see each other in their parcel.  02. Architecture 10/14  The maintenance of these servers can be incentivized the same way as content servers. When lightweight protocols like STUN13 can cover the functionality required from the server, the costs would be fairly low. But for more advanced features, such as a voice chat between multiple concurrent users or network traversal services, micropayments can be used to cover the operating costs. The social experience of users in Decentraland will include avatars, the positioning of other users, voice chat, messaging, and interaction with the virtual environment. The diﬀerent protocols used to coordinate these features can work on top of existing P2P solutions like Federated VoIP or WebRTC.14  13 https://tools.ietf.org/html/rfc5389 14 https://webrtc.org/  2.4 Payment Channels General purpose, public, and distributed HTLC networks like Lightning may be at least one year away from materializing, but low-trust hub-and-spoke payment channel networks allow for fast and low-cost transactions that can be implemented today. Payment channels are key for Decentraland for two reasons: In-world purchases Incentivizing quality of service of content and P2P servers Today, platforms mitigate the risk inherent in credit card payments: users trust the platform, rather than the application, with their payment details. With payment channels, they could make direct purchases to the developer with no risk of identity theft. Some parts of Decentraland’s infrastructure can be paid for with micropayments. These costs include hosting content, serving it, and running P2P protocols like spatial audio processing for multiple users. The marginal cost of running applications on Decentraland for developers, given a market of incentivized servers to provide the infrastructure, approaches its real cost as this becomes essentially commoditized. However, in order to have no barrier to entry for incoming developers, Decentraland will subsidize these services with the proceeds of selling MANA tokens.  2.5 Identity System Decentraland’s ownership of land is one kind of identity system, where credentials are the coordinates of one’s land. Economic incentives are also necessary to ensure that the creators of avatars, items, and scripts continue building and distributing them. Since content can be arbitrarily copied, we must rely in social agreements to enforce retribution to the creator.  02. Architecture 11/14  Social agreements can make digital scarcity possible. In centralized systems, this scarcity is defended by the company that creates the platform. For Bitcoin and other proof-of-work blockchains, scarcity is enabled by a computational puzzle and the fact that mining blocks requires an onerous economic sacrifice. Decentraland can use decentralized identity systems to create a layer of ownership over in-world items. This system must allow users to easily verify the consent of an author by linking public keys and signatures with human-readable names.  Projects like uPort15 or the Ethereum Name Service16 can be used to do this. Social reputation is also needed to facilitate contributions to the author. The ability to incentivize content creation on decentralized economies is evolving quickly, with multiple projects working in the space directly or indirectly. Potential solutions include Mediachain17, Curation Markets18, Rare Pepes19, and more.  03 Economy In Section 1.1, we make a case for how the increasing adoption of cryptocurrencies creates the necessary conditions for the emergence of a distributed platform for a virtual world. Below, we introduce the utility of LAND and the MANA token, how their strategic allocation can help bootstrap the utility of the network, and outline how the issuance of MANA will be conducted.  3.1 LAND and the MANA Token With the launch of the Iron Age, we are introducing two digital assets: LAND, the non-fungible parcels in which the virtual world is divided; and MANA, an ERC-20 token that is burned to claim LAND, as well as to make in-world purchases of goods and services. The utility of LAND is based on its adjacency to other attention hubs, its ability to host applications, and also as an identity mechanism. Developers and other content creators will demand LAND so that they can build on top of it and reach their target audience. Although every unclaimed LAND can be purchased at the same exchange rate (1000 MANA = 1 LAND), LAND parcels are distinguishable from each other, potentially trading at diﬀerent prices on a secondary market due to diﬀerences in adjacencies and traﬀic.  03. Economy 12/14  On the other hand, MANA serves as a proxy to asses the price of a new parcel of LAND. Also, MANA used to buy goods and services in the virtual world creates utility value for the token.  15 https://www.uport.me/ 16 The Ethereum Name System is one of the most successful projects in this area: https://ens.domains/ 17 http://www.mediachain.io/ 18 “Curation Clubs: Tokenizing & Incentivizing Public Funding With Curation Markets”. Simon de la Rouviere. 2017. https://medium.com/@simondlr/276613e641f0 19 http://avc.com/2017/05/rare-pepe/  3.2 Fostering the Network Land ownership is acquired through an ERC-20 token called MANA. This token will serve to bring value into the network, and to acquire a new parcel of land. MANA can also be used to purchase in-world goods and services. In order to jumpstart the network, developers and content creators will be rewarded to set up shop in Decentraland. The Foundation will hold contests to create art, games, applications, and experiences, with prizes contingent on meeting a set of milestones. At the same time, new users will be assigned allowances, allowing them to participate in the economy immediately. These financial incentives will help bootstrap the utility value of the network until it independently attracts users and developers.  04 Challenges Decentralized Content Distribution Content distribution through a P2P network has two main issues. The first involves download speed: retrieving a file from a DHT or distributed peer-to-peer storage system has traditionally been too slow. Specially, in a graphical application like Decentraland, users will be adverse to using a system that does not load the experience quickly. The second issue involves availability: ensuring that content is suﬀiciently distributed around the network without loss. IPFS and the upcoming FileCoin protocol are addressing these issues and we’re looking forward to when they become production ready.  Scripting Scripting will be the most important element used to create valuable experiences for users in Decentraland. Its APIs will need to be secure enough for clients to hold private keys and authorize micropayments frequently. Ease-of-use is also critical to penetrate a broad audience of developers.  04. Challenges 13/14  Content Curation The issue of filtering content for mature audiences (like pornography, violence, or gambling) is diﬀicult to solve within a decentralized network. We expect a market to emerge here: with a reputation-based approach, users could select one or more providers of whitelists/blacklists that track the type of content being served on each parcel.  05 Summary  05. Summary 14/14  Decentraland is a distributed platform for a shared virtual world that enables developers to build and monetize applications on top of it. The scarcity of land, on top of which applications can be built, creates hubs that capture user attention, which drives revenue to content creators. MANA tokens will be used to purchase land, goods, and services in-world. MANA tokens will also be used to incentivize content creation and user adoption, therefore bootstrapping the first decentralized virtual world.  EO COINS SALE TERMS AND CONDITIONS The following Terms and Conditions (the "Terms") govern the Purchaser’s use of https://eo.trade/ (the “Website”) and the purchase of crypto coins ("EO Coins") created by ExpertOption Ltd., a Saint Vincent International Business Company with registration number 22863 IBC 2015 (the “Company”). The Purchaser and the Company may be referred to herein individually as a “Party” and collectively as the “Parties”. By accessing and using the Website or purchasing EO Coins, the Purchaser agrees to comply and be legally bound by these Terms. These Terms do neither represent nor constitute in any way a solicitation for investment and/or an offering of securities in any jurisdiction. IMPORTANT DISCLAIMER: BY PURCHASING EO COINS, THE PURCHASER ACKNOWLEDGES THAT HE/SHE HAS FULLY READ, UNDERSTOOD AND AGREED TO THESE TERMS, AND THAT HE/SHE HAS THE NECESSARY LEGAL CAPACITY TO COMMIT HIMSELF/HERSELF TO THE OBLIGATIONS STATED IN THESE TERMS. IF THE PURCHASER DOES NOT AGREE WITH ANY OF THE PROVISIONS OF THESE TERMS, THE PURCHASER SHALL NEITHER PURCHASE EO COINS, NOR USE THIS WEBSITE OR SERVICES OFFERED ON THE WEBSITE. PRIOR TO PURCHASING EO COINS, THE PURCHASER SHOULD CAREFULLY CONSIDER THESE TERMS AND IF NECESSARY, GET ADVICE FROM AN APPROPRIATE LAWYER, ACCOUNTANT, OR TAX PROFESSIONAL. INITIAL EO COINS SALE EO COINS WILL NOT BE AVAILABLE FOR SALE TO THE CITIZENS OR RESIDENTS OF THE USA OR ANY OTHER COUNTRY WHERE TRANSACTIONS IN RESPECT OF, OR WITH USE OF, DIGITAL TOKENS FALL UNDER THE RESTRICTIVE REGULATIONS OR REQUIRE FROM THE COMPANY TO BE REGISTERED OR LICENSED WITH ANY GOVERNMENTAL OR LICENSING AUTHORITY. The Company is conducting an initial coin sale for funding further development of token-based accounts, crypto wallet (“EO.Finance Wallet”) and crypto exchange (“EO.Trade Exchange”) that are described in more detail in EO Whitepaper https://static.expertoption.com/ico/eo-whitepaper.pdf Both EO.Finance Wallet and EO.Trade Exchange will be managed and monitored by a separate legal entity that will be registered and licensed in Estonia. Operations of EO.Finance Wallet and EO.Trade Exchange will depend on existence and circulation of EO Coins that will have attributes of discount on EO.Finance Wallet and EO.Trade Exchange services and an internal payment unit. EO Coins will give their holders discount rights and access to services and products as outlined in these Terms. The Company will create and issue in total 1,000,000,000 EO Coins. 70% of issued and created EO Coins will be available for crowdsale. No further EO Coins will be created and issued and all unsold EO Coins will be burnt and will not be in circulation. All purchases of EO Coins are final and, therefore, non-refundable.  In order to participate in the Initial EO Coins Sale, the Purchaser must have a wallet that supports ERC20 tokens. The Purchaser is solely liable for issues arising from the use of a non-compatible system or wallet. EO Coins bought by the Purchaser will be sent to his/her Ethereum-based personal account/wallet. Purchasers shall be responsible for implementing reasonable measures for securing their wallets, including any requisite private keys or other credentials necessary to access such storage mechanism. If the Purchaser loses private keys or other access credentials, the Purchaser may lose access to EO Coins bought. The Initial Sale of EO Coins launches on the 16th of July, 2018 and shall run until the 30th of August, 2018 as described in the EO Whitepaper. The dates of the Initial EO Coins Sale may be changed or the duration of the Initial EO Coins Sale may be extended for any reason, including the unavailability of the website https://eo.trade/ or other unforeseen security or procedural issues. EO Coins Sale Phases: 1. Private Presale The private presale will be conducted from 30 of March, 2018 till 12 of April, 2018. During the private presale, the price of 1 EO Coin will be $0.20 or its equivalent in another fiat or crypto currency that will be automatically converted at the prevailing exchange rate at the time of purchase. The Purchaser shall buy minimum 500 EO Coins. Accepted currencies: Fiat, Bitcoin, Ethereum, Litecoin, Bitcoin cash, Bitcoin Gold 2. Pre-Initial Coin Sale The private pre-sale will be conducted from 16 of April, 2018 till 29 of June, 2018. During the pre-initial coin sale, the price of 1 EO Coin will be $0.20 or its equivalent in another fiat or crypto currency that will be automatically converted at the prevailing exchange rate at the time of purchase. The Purchaser shall buy minimum 500 EO Coins. Accepted currencies: Fiat, Bitcoin, Ethereum, Litecoin, Bitcoin cash, Bitcoin Gold 3. Initial Coin Sale The initial coin sale will be conducted from 16 of July, 2018 till 30 of August, 2018. During the initial coin sale, the price of 1 EO Coin will be $0.20 or its equivalent in another fiat or crypto currency that will be automatically converted at the prevailing exchange rate at the time of purchase. The Purchaser shall buy minimum 500 EO Coins. Accepted currencies: Fiat, Bitcoin, Ethereum, Litecoin, Bitcoin cash, Bitcoin Gold  KYC POLICY The Company reserves the right to conduct “Know your customer” and/or “Anti-money laundering” checks and procedures on the Purchasers if it becomes required by the applicable laws. The Purchaser undertakes to provide all the information requested by the Company in respect of conducting KYC or AML checks/procedures. In the event that the Purchaser fails or refuses to provide the specific and necessary information requested by the Company in due time, the Company shall have the power to terminate the Purchaser’s rights to use the Website and the power to stop providing services to that Purchaser without any obligation from the Company to refund or indemnify the Purchaser. REPRESENTATIONS AND WARRANTIES The Company makes no representations or warranties, whether express or implied, and assumes no liability or responsibility for the proper performance of any services, online cryptocurrency services, assets or platforms and/or the information, images or audio contained or related to the EO Project. The Purchaser uses all of these services including but not limited to the Website services, online cryptocurrency services, assets or platforms at his/her own risk. The Purchaser agrees not to hold the Company, including its affiliates, directors, employees, agents, contractors and service providers, liable for any losses or any special, incidental, or consequential damages arising from, or in any way connected to the sale of EO Coins including losses associated with these Terms. By accepting these Terms, the Purchaser guarantees that he/she is neither resident nor citizen of a country whose laws and regulations ban or limit the purchase and/or use of cryptographic coins. The Purchaser represents and warrants that the Purchaser has an understanding of the usage and intricacies of blockchain-based assets, like EO Coins, and blockchain-based software systems. The Purchaser represents and warrants that he/she is aware of all the merits, risks and any restrictions associated with cryptographic coins, cryptocurrencies and blockchain-based system, and that he/she is knowledgeable as to their management. The Purchaser further represents and warrants that he/she will take sole responsibility for any restrictions and risks associated with the purchase of EO Coins. The Purchaser understands and accepts that EO Coins are not an investment, currency, security, commodity, a swap on a currency, security or commodity, or any other kind of financial instrument. The Purchaser understands and accepts that EO Coins do not have any rights, uses, purpose, attributes, functionalities or features, express or implied, outside EO Project.  The Purchaser understands and accepts that purchase and possession of EO Coins shall not give/grant the right to the Purchaser to exercise any control over the Company or other aspects of EO Project. The Purchaser represents and warrants that the Purchaser is of legal age to buy EO Coins. The Purchaser represents and warrants that the Purchaser is legally permitted to buy EO Coins in the Purchaser’s jurisdiction. The Purchaser accepts and warrants that he/she bears a sole responsibility for determining if the acquisition, allocation, use or ownership of EO Coins, potential appreciation or depreciation in the value of EO Coins over time, the sale and purchase of EO Coins and/or any other action or transaction related to EO Coins has tax implications. The Purchaser represents and warrants that the Purchaser is not buying for EO Coins for the purpose of speculative investment. The Purchaser acknowledges and accepts that EO Coins are purchased on an “as is” and “under development” basis. Therefore, provided the Company acts in good faith, the Purchaser accepts that the Company is providing EO Coins without being able to provide any warranties, including, but not limited to title, merchantability or fitness for a particular purpose. The Purchaser warrants and represents that he/she fully agrees with all the information related to EO Coins, whether provided in these Terms, in the Whitepaper or in any other documentation or information made available by the Company. The Purchaser warrants and represents that he/she shall comply with all the provisions set out in both these Terms and Whitepaper, in addition to any other documents published by the Company in connection with the Initial EO Coins Sale. The Purchaser understands and accepts that the Company does not guarantee that EO Coins will be listed or traded on any exchange. The Purchaser understands and accepts that the Company does not guarantee that the EO Coins will hold its value or increase in value in the future. The Purchaser warrants and represents that he/she does not intend to hinder, delay or defraud the Company or any other Purchasers of the Website, as well as that the Purchaser’s participation in the Initial EO Coins Sale is not connected to engaging in any illegal conduct and/or unlawful activity. The Purchaser further undertakes not to use EO Coins should their use not be legal in the applicable jurisdiction. The Purchaser warrants and represents that any and all information provided in connection with his/her participation in the Initial EO Coins Sale is accurate, up to date and complete, and that it does not impinge on the rights of any third party. The Purchaser undertakes to notify the Company should any of the information provided in relation to the participation in the Initial EO Coins Sale changes, becomes outdated  or is no longer accurate or complete. The Purchaser shall be fully responsible for ensuring that any credentials or information linked to the participation in the Initial EO Coins Sale remain confidential and are not used by any third party. The Purchaser warrants that he/she shall neither misuse the Website by knowingly introducing viruses, worms, logic bombs or other material which is malicious or technologically harmful, nor attempt to gain unauthorised access to the Website, computer or database connected to the same, nor violate or attempt to violate the security of the Website, nor access information or data to which he/she has not been expressly granted a right to access. The Company warrants that it has taken all necessary measures, within its reasonable control and the state of the art, in order to guarantee the proper functioning of the Website and to minimize system errors, both from a technical point of view and material published, as well as to prevent the existence and transmission of viruses and other harmful components to the computer systems of Purchasers; however, the Company does not guarantee that the Website shall be fully exempt of errors, failures or malign components (of any nature) at all times. The Company does not guarantee the lawfulness, reliability and usefulness of the contents supplied by third parties through the Website. If any Purchaser becomes aware of the existence of any content that is illegal, unlawful or infringing of the rights of third parties, he/she shall immediately notify the Company so that we can proceed with the adoption of appropriate measures. By purchasing EO Coins, the Purchaser acknowledges that the Company, including its affiliates, directors, employees, agents, contractors and service providers, is not required to provide a refund for any reason, and that the Purchaser is not entitled to receive money or any other form of compensation for any EO Coins that are not used or not suitable for their intended purpose. FORCE-MAJEURE Neither the Purchaser nor the Company, including its affiliates, directors, employees, agents, contractors and service providers, shall be held liable towards the other party for any failure to perform any obligation, if such failure is caused by circumstances beyond the reasonable control of either the Company or the Purchaser failing to fulfil its obligations. For the purpose of these Terms a force-majeure shall include, but not limited to lightning, flood, exceptionally severe weather, fire, explosion, war, civil disorder, industrial disputes, acts or omissions of persons for whom we are not responsible, acts of government or other competent authorities (including telecommunications and internet service providers). The Purchaser indemnifies and holds the Company, including its affiliates, directors, employees, agents, contractors and service providers, harmless against all and any losses, liability, actions, suits, proceedings, costs, demands and damages of all and every kind, (including direct, indirect, special or consequential damages), and whether in an action based on contract, negligence or any other action, arising out of or in  connection with the failure or delay in the performance of the EO Project, whether due to the Company’s, including its affiliates, directors, agents, contractors and service providers, negligence or not. LIMITATION OF LIABILITY Subject to any Applicable Laws and Regulations, the Purchaser agrees that the Company shall not be liable for any costs, claims, damages (including, without limitation, indirect, extrinsic, special, penal, punitive, exemplary or consequential losses (such as loss of profits, business, goodwill, revenue or anticipated savings) or damage of any kind), penalties, actions, judgments, suits, expenses, disbursements, fines or other amounts that you or any third party might suffer that relate to or arise from these Terms or purchase and use of EO Coins, or termination of these Terms, for any reason, whether or not anyone anticipated or should have anticipated that the damages would occur. Subject to any Applicable Laws and Regulations, the Purchaser shall not have any claim of any nature whatsoever against the Company for any failure by the Company to meet any of the Company’s obligations under these Terms as a result of causes beyond the Company’s control. Subject to any Applicable Laws and Regulations, the Purchaser agrees to indemnify and hold the Company harmless in respect of any claim that a third party might bring against the Company that relates to or arises from these Terms or purchase of EO Coins. Any party related to the Initial EO Coins Sale shall not be liable for any losses howsoever caused as a result of, arising from, or in connection with, whether directly or indirectly, the following: -  the Purchaser’s use of the Website or unavailability, failure of performance, error, omission, interruption, defect, delay in operation or transmission, computer virus or line or system failure of the Website or any linked website;  -  the Purchaser’s reliance on the information provided through the Website;  -  the Purchaser’s participation in the Initial EO Coins Sale and/or his/her use and/or possession of, and reliance on, EO Coins he/she receives as a result of the same;  -  any results that the Purchaser envisages that he/she might obtain from his/her participation in the Initial EO Coins Sale;  -  unauthorised access to or alteration of the Purchaser’s transmissions or data;  -  statements or conduct of any third party on the Website; and  -  any other matter relating to the Website and to any of the services and goods available through the same.  The Purchaser acknowledges and accepts that cryptocurrencies are volatile and a fluctuating good based on technology and a supply and demand model, and therefore, the Company shall not accept any liability for any depreciation of EO Coins and any losses that the Purchaser may suffer as a result. The Company shall not be held liable for, the accuracy, usefulness or correctness of all information and documents published on the Website. The Company does not guarantee that EO Coins are reliable or error-free. The Purchaser shall indemnify, defend and hold the Company, including its subsidiaries, affiliates, directors, officers, employees, agents, representatives, assignees and successors, harmless from and against any and all claims, damages, losses, actions, demands, proceedings, expenses and/or liabilities filed or incurred by any third party against the Company arising out of a breach of any warranty, representation or obligation under these Terms. SEVERABILITY These Terms set forth the entire understanding between the Purchaser and the Company with respect to the purchase and sale of EO Coins. If any term, covenant, condition, or provision of these Terms shall be invalid, illegal or unenforceable in any jurisdiction shall, as to such jurisdiction be ineffective to the extent of such invalidity, illegality or unenforceability without affecting the validity, legality and enforceability of the remaining terms and conditions; and the invalidity of a particular provision in a particular jurisdiction shall not invalidate such provision in any other jurisdiction. The Purchaser agrees that these Terms govern the sale of EO Coins and supersede any public statements about EO Coins sale made by third parties or by EO Team or individuals associated with the EO Team, before and during EO Coins Sale. WAIVER The failure of the Company to require or enforce strict performance by the Purchaser of any provision of these Terms or the Company’s failure to exercise any right under these Terms shall not be construed as a waiver or relinquishment of the Company's right to assert or rely upon any such provision or right. The express waiver by the Company of any provision, condition, or requirement of these Terms shall not constitute a waiver of any future obligation to comply with such provision, condition or requirement. Except as expressly and specifically set forth in these Terms, no representations, statements, consents, waivers, or other acts or omissions by the Company shall be neither considered as an amendment of these Terms nor be legally binding. The Company reserves the right to amend these Terms and Conditions at any time with immediate effect by publishing the updated Terms and Conditions on the Website. All such changes will take effect once they have been posted on the Website and the User will be deemed to have accepted any such changes by the User’s use of the Website from such time. AMENDMENTS AND UPDATES  The Company reserves the right, at its sole discretion, to change, modify, add, or remove parts of the Whitepaper and/or Terms at any time during the Initial EO Coins Sale by posting the amended versions of the Whitepaper and/or Terms on the Website. The Purchaser acknowledges and accepts that these Terms are subject to changes, modifications, amendments, alterations or supplements at any time, which will be effective immediately upon publication on the Website. The new and amended Terms shall apply to any purchase of EO Coins made by the Purchaser after the new Terms have been published. The Purchaser agrees to be bound by any such update or change if the Purchaser continues to use the Website and/or purchase and hold EO Coins. These Terms may be amended from time to time in the following circumstances: -  changes in methods of accepting payments for EO Coins from the Purchaser; changes in methods of valuating EO Coins; changes in Applicable Laws and Regulations, if any regulatory authority requires to make changes to these Terms or any aspects of EO Project or the Company’s business practices in terms of Applicable Laws and Regulations. USE OF THE WEBSITE  The Purchaser agrees that in using the Website the Purchaser shall not: - use the Website in any way that may lead to the encouragement, procurement or carrying out of any criminal or unlawful activity; - transfer files that contain viruses or other harmful programs; - seek to bypass or interfere with any security features of the Website, or interfere with any of the Company’s websites, servers, or networks. The Company reserves the right to suspend, restrict or terminate the Purchaser’s access to this Website at any time without notice, at the Company’s discretion, if the Company has reasonable grounds to believe the Purchaser has breached any of the restrictions above. The Company may, at its sole discretion, restrict access to some parts of this website, or the entire website, to limited groups of Purchasers. INTELLECTUAL PROPERTY The Company is the sole owner of the rights and titles over the Website and any and all of its contents and information provided, used or published, including, but not limited to, the website itself, texts, photographs,  illustrations, logos, trademarks, graphics, designs, interfaces, software, technology, and any other information or content, and the services available through the Website. The Purchaser acknowledges that all intellectual property rights over the contents of the Website are vested in the Company. It is expressly forbidden to modify, copy, reproduce, publicly communicate, transform or distribute, through any means, all or part of the contents published on the Website without the prior, written and express consent of the Company. By accessing the Website, the Purchaser is granted the right to use the content published and available on the Website solely for the purposes of participating in the Initial EO Coins Sale and limited to a personal and non-commercial use by the Purchaser. It is strictly forbidden to use any trademarks, trade names or similar distinctive signs included or referred to on the Website, whether owned by the Company or any other third party, without the Company’s or the relevant third party’s consent. The Purchaser shall not use any of the Company’s intellectual property for any reason without Company’s prior written consent. INDEMNITY The Purchaser hereby agrees that subject to Applicable Laws and Regulations, the Company will not be liable for: •  any interruption, malfunction, downtime, off-line situation or other failure of any cryptocurrency or virtual currency trading platforms or online services provided by any third parties, including any third party's system, databases or any of its components;  •  regulatory compliances measures, notices or actions imposed or any tax liability incurred by the Purchaser;  •  any loss or damage with regard to the Purchaser’s data or other data directly or indirectly caused by malfunction of any third party systems, power failures, unlawful access to or theft of data, computer viruses or destructive code on any third party systems or programming defects; and/or  •  any interruption, malfunction, downtime or other failure of services provided by third parties, including, without limitation, third party systems such as the public switched telecommunication service providers; internet service providers, electricity suppliers, relevant local authorities and certification authorities; or any other event over which the Company has no direct control. SECURITY  The Purchaser shall take proper measures to protect his/her personal wallet keys, without which the Purchaser will not be able to access his/her personal wallet and use EO Coins, which have been purchased during the Initial EO Coins Sale. The Purchaser expressly acknowledges that he/she understands, accepts and agrees that the Company, including its affiliates, directors, employees, agents, contractors and service providers, will not be required or able to re-issue the keys to access his/her personal wallet. The Purchaser further accepts and agrees that without the required keys and login details the Purchaser’s EO Coins will be inaccessible and considered as unusable. The Company will not be liable to refund the Purchaser due to this failure from the Purchaser’s side. COMMUNICATIONS The Purchaser agrees that the Company may communicate with, and/or otherwise provide notifications to the Purchaser, via email and other forms of electronic communications, by sending a paper document, or by posting on the Website. LANGUAGE Only English versions of any EO Team communications shall be considered official communications of the Company. In case of any differences the English version of any communications as well as these Terms shall prevail. GOVERNING LAW Any matters arising from these Terms and Conditions, shall be governed by and interpreted in accordance with the laws of Saint Vincent and the Grenadines. If a dispute arises between the Purchaser and the Company, the Purchaser is strongly encouraged to first contact the Company directly to seek amicable resolution. The courts of Saint Vincent and the Grenadines will have exclusive jurisdiction to settle any disputes which may arise out of or in connection with these Terms, or use of the Website. FEEDBACK If you have any questions about these Terms and Conditions, please contact us by e-mail help@expertoption.com  EO Ecosystem W H I T E PA P E R  eo.trade 1  Table of Contents Introduction.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 4 ExpertOption.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 5 The EO Ecosystem. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 7 The ExpertOption Trading Platform .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 8 EO.Finance Wallet .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 8 EO.Trade Crypto Exchange.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 8 EO.News Portal .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 8  Milestones. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 9 Online Trading Market.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 11 ExpertOption Platform.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 12 Token-Based Accounts .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 13 Payments on the ExpertOption Platform.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 13 Evolution of the ExpertOption Platform.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 14 ExpertOption App .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 14  EO.Finance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Currencies on EO.Finance. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 17 What Makes EO.Finance Special?. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 17 EO.Finance App .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 18  Cryptocurrency Market.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 19 EO.Trade. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 20 Why Develop a New Crypto Exchange?. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 The Market Gap.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 21 How will EO.Trade Overcome These Obstacles?.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 22 Better. Faster. Stronger. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 23 Fastest KYC Process. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 26 Proven Customer Care .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 27 Coin Listing Process. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 28 EO.Trade App.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 28  EO.News. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 29 The EO Coin. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 30 EO Technical Structure.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 32 Frontend.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 33 Mobile Applications.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 34 Backend. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 34 Deployment, Testing and Security. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 35 2  EO Marketing.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 37 Marketing Strategy .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 38 Affiliate Program .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 38 A Strong Marketing Team.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 39 Helping Investors.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 40  EO Bounty.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 41 Referral Program.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 41 Airdrop .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 41  Fund Allocation.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 42 EO Roadmap. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 43 EO Group Management.  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 45  3  Introduction The EO ecosystem was designed to bridge the gap between traditional online trading and Crypto investments. The EO Coin will power three platforms: an exchange, a wallet and online trading on derivatives. All three platforms will allow fiat and crypto transactions. Currently the 5 trillion dollar per day online trading market and the multi-billion dollar crypto market are still separated. Making the jump from online trading to crypto is complicated and managing fiat finances is difficult for crypto investors. Our project will create a financial management hub with a seamless process where buying, storing, exchanging and trading on both fiat and cryptos is available. In short, investors can manage all their fiat and crypto finances on the same platforms, with major benefits for EO Coin holders. This whitepaper will describe the products, details and vision of the project and explain how we will make it become a reality.  4  ExpertOption ExpertOption has been a leading broker in online trading for years. Our success has been achieved by a strong team of professionals who have excelled in the development of the platform, the marketing which has turned the company into a global giant, the support of millions of clients around the clock and the overall management of such a large institution. Our confidence and hard work has resulted in a unique award-winning platform with close to 8 million registered accounts from more than 150 countries and featured in more than 15 languages. Our operations have resulted in having more than 100 employees with offices in six different countries; Estonia, Cyprus, Russia, Ukraine, Thailand and India. The ExpertOption platform won the Best Trading Platform award at the China Forex Expo Shenzhen, in May of 2017. The platform is available in multiple forms including Web, Windows, MacOs, iOS, Android and PWA. The app has been downloaded on iOS and Android more than 3,500,000 times. From the very beginning we believed in the power of crypto and so have designed a platform which accepts crypto payments and offers trading on Bitcoin, Ethereum, Litecoin and more than a dozen altcoins, therefore making us one of the first brokers to put cryptocurrencies at the forefront of trading. The platform offers more than 100 assets to trade, including the world’s most precious commodities; gold and oil, the biggest stocks like McDonald’s and Coca-Cola, and the most liquid currencies including majors, minors and exotics. ExpertOption continues to be a leading global brand for online trading with millions of traders. The creators of the ExpertOption brand and the team who lead it to success are going to design and work on the EO ecosystem and all its platforms.  5  2014  Company founded  6  Countries  100+ Employees  7 500 000+ Accounts  100+ Assets  30 000 000+ Deals executed monthly  Partnerships PARTNERSHIPS  Available on Web, PC, Mac, iPhone, iPad and Android Licensed and Regulated Broker  6  The EO Ecosystem Following the success of the ExpertOption brand and platform we are now creating an Ecosystem around both crypto and fiat trading, which will act as a one-stop-shop to buy, store, sell, exchange and trade on fiat and crypto currencies. The ecosystem will be fuelled by the EO coin, built on the Ethereum network and designed for four platforms. The coin will have real-life value and offer its holders benefits on the ExpertOption trading platform, EO.Finance and EO.Trade.  7  The ExpertOption Trading Platform The ExpertOption trading platform offers trading on more than 100 assets including currencies, stocks, commodities and cryptos with high profit percentage on trades. The EO coin will introduce token-based accounts which are fully funded by cryptos but will allow trading on all asset classes. Additionally, token-based accounts will have higher profit  EO.Finance Wallet We will build our own crypto wallet which will support more than 20 cryptocurrencies and allow instant transactions through a web, desktop and mobile app, both designed to make blockchain transactions simple. The wallet will allow fiat to crypto transactions.  EO.Trade Crypto Exchange Our biggest project will be the design and creation of a new crypto exchange that the blockchain world has yet to see. We will build an exchange made for the current and future demand of the market which will continue to add pressure on platforms.  EO.News Portal We will develop a pristine news portal which will provide reliable and continuous updates on both market and crypto news, on technical analysis and also on a large number of financial tools to ensure support for all types of investors.  8  Milestones Our vision is to create four platforms which will form a fully rounded financial hub for crypto and fiat investors. We will only deliver the highest quality platforms on the market which will require an adequate amount of funding. This is why we have set milestones for our sale. Every milestone will allow us to develop a futher layer of the project. The order of the platforms is entirely based on the funding each will require.  Soft cap $10 million We have set a soft cap at $10 million. The project must reach this amount in order to proceed with any parts of the ecosystem. If the soft cap is not reached all funds will be returned to investors.  1st Milestone - $10 million EO.Finance Wallet EO.Finance will be an all-inclusive financial wallet which will allow the direct purchase of cryptos with fiat currencies. The wallet will host 20 cryptocurrencies and act as an online wallet for fiat currencies. It will allow investors to manage their fiat and crypto currencies from the same place. EO.Finance can be used to buy, store, exchange and pay with crypto and fiat currencies. EO Coin holders can pay 50% lower commission on transaction fees with the EO coin.  2nd Milestone - $35 million EO.Trade Exchange EO.Trade will be the fastest and strongest crypto exchange on the market. Its ability to handle the rapidly growing market demand, which has broken many crypto exchanges, will make this exchange truly stand out. Our engine will be designed not only for the present market, but also for the anticipated size the market will grow into throughout the next few years EO Coin holders can pay 50% lower commissions on transaction fees with the EO coin. EO.Trade will offer the fastest KYC and withdrawal process in the current markets, which is thanks to our skilled back office team which is also be prepared to expand when needed.  9  3rd Milestone - $40 million EO.News portal We will design, develop and market on online news portal for the financial markets and crypto world. The platform will include technical analysis, market updates and financial tools which will support every investor and trader around the clock.  4th Milestone - $45 million Token-based Accounts Token-based accounts will be added to the ExpertOption existing trading platform. These accounts can be opened with the EO coin. Token-based accounts will allow the full security of blockchain. Deposits and withdrawals will be instant. Profit percentages on the platform will be higher.  5th Milestone - $70 million Licensing and Supervision If we reach the $70 million mark we will use the additional funds to apply for two licenses: The Electronic Money Institution and The EU Payment Service Provider  Hard cap $100 million We have set a hard cap at $100 million. This is the amount we need to develop all parts of the ecosystem. Once the hard cap is reached the sale will end, even if it is prior to the set date. The level the project reaches is entirely determined by the investors of the EO coin. We have already envisioned an ecosystem where the four platforms work hand-in-hand to create a one-stop-shop for both fiat and crypto investors. With enough confidence from investors we can make it a reality.  10  Online Trading Market The online trading market has an average of $5.2 trillion in daily turnover making it is the biggest industry in the world. Online trading continues to excel in growth based on its technological advancements. Without the rise of the internet it would have remained a privileged investment for the rich, controlled by stock brokers who manipulated market prices and had full control on traders’ orders. But at the dawn of the internet in the late 90s, trading relocated from physical trading floors to independent online platforms, which then allowed any individual to open an account within minutes, control their own trades and make profits directly from the global market with small initial investments. The rise of cryptocurrencies offered an additional set of assets which could be traded online through traditional brokers. Bitcoin, Ethereum, Litecoin and hundreds of altcoins started becoming available for online trading, therefore attracting a new wave of traders who have tuned into the block chain era and are becoming more familiar with cryptocurrencies than traditional trading. After all, cryptocurrencies cannot be taken lightly; they are an economic innovation with the potential to revolutionize the economic structure and the way all financial assets operate. As an industry which has always stood at the forefront of innovation, online trading will naturally evolve as the market demands. More brokers are now accepting payments for trading accounts with cryptocurrencies, and are adding cryptos to their trading options. At ExpertOption we are ahead of the crowd, integrating crypto payments and trading derivatives in cryptocurrencies on our platform. Our early experience in digital assets has enabled our management and teams to gain enough experience. We are now ready to make our platform match present demands of the market and be ahead of future demands as the market inevitably grows.  11  ExpertOption Platform The ExpertOption platform was developed and implemented in-house, providing a web based platform, as well as a desktop and mobile application. It supports more than 100 trading assets which include currencies, commodities such as gold and oil, stocks and of course cryptocurrencies.  The award-winning platform has built-in trading tools, video tutorials, social trading and supports more than 30 funding methods including Bitcoin payments. User experience was of utmost importance in its design, which is why all functionalities are simple and understandable even to the least experienced trader. The platform is also equipped with all the features required for experienced investors. Multilingual support is available around the clock and withdrawals are among the fastest in the market thanks to our professional team who is prepared to operate smoothly even through the busiest times.  12  Token-Based Accounts The ExpertOption platform will advance to offer token based accounts lead by the use of our very own EO coin. Currently, online trading is an advanced version of currency exchange which has existed since the beginning of the economic system. Through online platforms traders have access to control their trades, enjoy much faster execution and full transparency. Today we are moving to an even more advanced version of trading; block chain. Token-based accounts can be opened using the EO coin and will offer the benefits of block chain transactions with instant deposits and withdrawals. ExpertOption traders will always have access to traditional trading accounts but the option to trade on token based accounts will offer extra benefits: Higher profit percentages Instant deposits and withdrawals Much higher speed of execution Token based accounts will allow trading on any of our 100+ assets including currency pairs, commodities, metals, stocks and cryptocurrencies. Altcoins have always been an important part of the ExpertOption platform and we already accepted payments through cryptos on our traditional online trading accounts. Now, our traders can own the EO coin and enjoy a fully immersed block chain experience with our token based accounts.  Payments on the ExpertOption Platform The ExpertOption platform offers 7 base currencies with which accounts can be opened. Other currencies are accepted for payments but will have to be transferred to one of the base currencies in order to be used on a trading account. The base currencies are subject to change when we deem it more effective and helpful for our traders.  13  The current base currencies on the ExpertOption platform are: USD US dollar CNY Chinese Yuan IDR  Indonesian Rupiah  INR  Indian Rupee  THB Thai Baht KRW South Korean Won VND Vietnamese Dong The EO coin will become the base currency for token-based accounts. When using the EO coin accounts can be opened purely with crypto and be used to trade derivatives on currencies, commodities, stocks and other cryptos.  Evolution of the ExpertOption Platform The ExpertOption platform has been offering exceptional conditions for online traders, including trading derivatives on cryptocurrencies. More than 7 million traders have chosen ExpertOption as their trusted platform, and now with the platform’s evolution, traders can own the EO coin to trade on token-based accounts and receive special benefits on the platforms. The ExpertOption platform will offer traditional online trading on more than 100 assets. In this case traditional trading means opening an account with one of the fiat base currencies and using fiat to trade on other assets. At the same time the platform will offer token-based accounts using the EO coin. Token based accounts can be opened with the EO coin and the EO coin can be used to trade derivatives on the same 100+ assets which are offered on the platform.  ExpertOption App The ExpertOption trading platform is a web, desktop and mobile app. Our app was built inhouse in native iOS and Android and has been the primary choice for our traders. Knowing that we live in a time where apps are at the forefront of any company, we have made sure that our app was designed to support all the features and allow a full rounded trading experience. This includes the, opening, adjusting and closing of positions, account 14  account history and details, deposit and withdrawal methods and all types of charts, indicators and trading tools. As the platform advances so does the app. All the functionalities which will be applied to the existing ExpertOption platform, including token-based-accounts, will be upgraded in the app. Existing and new traders can perform all their trading through the app.  15  EO.Finance EO.Finance is a crypto-fiat online wallet designed to integrate the shift between traditional online investments and the block chain revolution. Crypto Investors The financial world has been experiencing a shift from traditional bank accounts to crypto wallets. That is of course due to the increasing use of cryptocurrencies. Cryptocurrencies however do not eliminate the need and use of fiat currencies. Fiat Investors Investors who want to buy cryptos for the first time need to invest in one of the big digital currencies such as Bitcoin or Ethereum first, and buy another altcoin using their BTC or ETH. This makes the process lengthy, expensive and complicated for a newcomer to crypto. Crypto and Fiat Investors Investors who choose to use both fiat and cryptos need two separate wallets to manage their finances. EO.Finance EO.Finance is an online wallet which will be designed with an easy and user-friendly interface, offering both fiat and cryptocurrency management and making block chain transactions simple. Furthermore, the EO wallet will allow investors to buy cryptocurrencies using fiat currencies and withdraw cryptocurrencies into fiat currencies. Holders of the EO coin can use the coin to pay 50% lower commissions on transaction fees. The wallet can be used to store more than 20 cryptocurrencies (Bitcoin, Ethereum, Litecoin, EO Coin and more) which can be used to make payments anywhere online for any products, services or investments. At the same time investors who choose to open accounts on multiple EO platforms will only need one set of credentials for ease of use.  16  The EO.Finance wallet will be used to buy, store, exchange and pay with both crypto and fiat currencies. The wallet will not be crypto-to-crypto and fiat-to-fiat based. All currencies, both fiat and digital, will be interchangeable making all financial transactions easy and seamless. EO.Finance is a valuable wallet which can be used by fiat investors, crypto investors and anyone who uses both or would like to transition from one to the other.  Currencies on EO.Finance In the beginning the wallet will support 20 cryptocurrencies and a large number of fiat currencies. The cryptocurrencies will be carefully selected based on currency liquidity and demand to ensure that all the cryptos on EO.Finance are highly active and valuable to users of the wallet. In the future, based on market needs, we could expand the number of currencies the wallet supports.  What Makes EO.Finance Special? EO.Finance is designed to make block chain transactions easy for everyone. Cryptocurrencies are leading the future and everyone wants access even though many parts of the process can be confusing for inexperienced investors - after all it is a new technology and most investors are indeed inexperienced. EO.Finance will be an easy platform where all functionalities and payments are clarified and made simple. EO.Finance will be a financial hub where investors can manage all their crypto and fiat finances. EO.Finance will offer the following uses: Buy cryptocurrencies with fiat currencies Withdraw fiat from cryptocurrencies Buy fiat and cryptocurrencies Store fiat and cryptocurrencies Make payments with fiat and cryptocurrencies Exchange fiat and cryptocurrencies Pay 50% less commission with EO Coin The wallet will be available globally in a large range of languages. Once an investor has an account with EO.Finance they will receive instant access to round the clock 24 hour support by our experienced team who is ready to assist with all transactions and questions.  17  Additionally, we will prepare video tutorials explaining all the processes to ensure that all our investors are fully comfortable with making secure and fast block chain transactions.  EO.Finance App Most crypto wallets are web-based whereas the EO wallet will be one of the few designed as a mobile app too, so users can have access to their cryptocurrencies wherever they are. iOS and Android apps will feature all functionalities of the wallet including live market rates, buying, selling and withdrawing.  18  Cryptocurrency Market No one can argue against the strength of the existing Cryptomarket. At the time of writing the article the market cap was valued at 263 billion dollars. Cryptocurrencies challenge existing economic structures, and allow instant transactions without the interference of any third parties like banks or governments. The largest cryptocurrency is still the very first one, Bitcoin. Its value had spiked to $20,000 in 2017 and during the first couple of months of 2018 it has been fluctuating around $10,000. Considering the magnitude of cryptocurrencies, crypto exchanges have risen accordingly. Once these cryptos go into the market they become tradable on an exchange a lot like traditional exchanges for fiat currencies and other assets. Demand for this market is high, some crypto exchanges have hit more than a billion dollars in daily turnover over a couple of months. This proves how a young market is climbing the financial ladder much faster than any of its predecessors.  19  EO.Trade The EO.Trade Crypto Exchange will redefine crypto trading. Might sound too ambitious but this is the only reason we decided to introduce a new crypto exchange to a market which is flooded by malfunctioning exchange platforms that are lagging the evolution of crypto trading. Our exchange will provide a much better and user-friendly interface with advanced graphs. A much faster service which will never delay orders or withdrawals. A much stronger engine which can handle existing and future market demand without faltering. The platform will be designed by an experienced development team, which has already developed the successful ExpertOption platform, and who understands the product and the industry. Our team has already developed a solid plan and an architecture based on which EO.Trade will be built in order to be the robust platform we promise. EO.Trade will be a powerful crypto exchange which is merged in a crypto-fiat ecosystem and surrounded by multiple platforms including a trading platform, a wallet and an advanced news portal. Given all the above, we are confident that EO.Trade will stand out among its peers in both the crypto and finance world.  Why Develop a New Crypto Exchange? Crypto trading is a much bigger financial evolution than the public has fathomed so far. We know that because we have witnessed the previous financial revolution at the dawn of the internet when online trading emerged. It took years until the world realized that online trading was no longer the ‘next big thing’ but it had already become the biggest industry in the world. While everyone keeps calling cryptos the next big thing, we are preparing for this industry to be the current biggest thing with the biggest market in the world. How is this relevant? This means we are building an exchange designed for the existing vast market and all its demands. With ExpertOption we provided traders with a strong, user-friendly, fully equipped platform to help them succeed. Now, we can see investors being let down by their Crypto Exchanges 20  simply because they were not built to handle this market. We are working hard to create the EO.Trade exchange to offer every investor around the globe the reliable exchange they need and deserve to manage their crypto trading without any worries. EO.Trade will allow investors to rely on a crypto exchange the same way online traders can rely on their trading platforms.  The Market Gap The market is that big that there are over 200 crypto exchanges online at the time of writing this article, yet demand is still exceeding supply. A large number of crypto exchanges were nowhere near prepared for the magnitude of the market, even the biggest exchanges out there have faced big problems because of the amount of users. According to the Financial Times article, released on the 8th of February 2018, the biggest crypto exchange currently on the market had to close down due to the large volume of users. The company stated “Due to a significant increase in users and trading activity, [we] will need to extend the system upgrade. Withdrawals and trading during this period will remain suspended”. Current exchanges on the market are clearly struggling to address the exponentially increasing demand from investors. This is due to extensive pressure from users on the platform, with delays being prominent for account opening and withdrawals, due to strict Know Your Customer (KYC) and Anti-Money Laundering (AML) systems which have to be implemented on exchanges. KYC and AML procedures are important to maintain the security of accounts and works for the customers and not against them. Failing to segregate fraudulent accounts from legitimate users could lead to an unreliable trading environment, This however means that each user application must be manually approved and verified, which requires significant effort from employees of the exchange. Up to 100,000 users have been added to an exchange in one day. Using this example adds up to an average of hundreds of thousands of account approvals per month. This takes up a large amount of employee hours and the numbers are continuously growing. This is making it very difficult for existing exchanges to keep up and maintain a steady and reasonable registration and withdrawal time and near impossible for newcomers to keep up.  21  How will EO.Trade Overcome These Obstacles? When we developed and launched ExpertOption we dealt with an overwhelming number of new users, which quickly taught us that our customer support is like an elastic band which should always be ready to easily expand based on supply and demand and always fit customer needs like a glove. That is why the ExpertOption client care team is made up of more than 30 individuals in six different countries covering more than 15 languages. With the upcoming crypto exchange we are ready to double, triple, and expand the team to any size necessary to ensure quality is balanced with quantity. Meanwhile, our development team has also observed existing exchanges and the challenges they face. The team has learnt from our predecessors mistakes and has already built a solid plan to create a much stronger platform. Our experience has prepared us to take on this challenge and succeed.  22  Better. Faster. Stronger  Better The definition of ‘better’ is more desirable, satisfactory, or effective. We plan to be more of all three. This is a big statement to make considering the market already has highly successful crypto exchanges however ‘better’ comes from acknowledging an existing state of being and deciding to add onto it. We acknowledge existing crypto exchanges and the benefits they offer investors we just don’t think it’s enough. EO.Trade will be a web, desktop and mobile app designed for easy and user friendly transactions in both crypto and fiat currencies powered by the EO Coin. We have learnt from our predecessors’ mistakes and have combined the knowledge, along with our experience in designing and developing the ExpertOption platform - which has led to a list of pros and cons, to continue to build on making our platform superior to existing platforms and better than anything we have ever built before. Special Benefits with EO Coin Paying commission on transaction fees with the EO Coin costs 50% less. In addition, the EO Coin will be highly supported by its increased use on the exchange.  23  Access to New Altcoins New altcoins can be listed on EO.Trade after a scanning process which will determine their authenticity, security and potential in the market, as well as through a voting process. Access to the Best Altcoins Investors will discover only the best of the newest altcoins on EO.Trade. While we will encourage new altcoins to be listed on our platform, we will ensure they are liquid, active and have potential. High Liquidity We will progress to become an exchange with one of the largest volumes in the world, where buyers and sellers can make fast transactions at market price. Airdrops We will reward our investors with airdrops and extra free EO Coins. More Currency Options We plan on integrating a large number of both fiat and cryptos and will continue increasing this number based on market demand to offer the biggest exchange options. Affiliate Program We will launch an affiliate program on the platform which will allow affiliates to benefit from referring new users to EO.Trade.  Faster Speed is an important factor for investors. Speed in everything related to their investments; registering, getting approved, placing orders, support, withdrawal and everything in between. One second for an investor is measured differently than in the real world. Every delay is a potential expense. EO.Trade will cover all investor needs by ensuring all parts of the process are as swift as possible. Fast KYC Process The KYC process which can take up to 1 week on most existing crypto exchanges will require a much shorter time frame with EO.Trade.  24  Fast Withdrawal Process Withdrawals are delayed in most existing crypto exchanges as they pend approval. We will offer the fastest withdrawal process among crypto exchanges thanks to a large and experienced team we have onboard. Seamless Interface Lagging, delays on chart data and notifications, freezing screen, slow functionality or any such problems are already addressed in the development and will continue being monitored to ensure their complete elimination. No Delay in Orders We are building the architecture of our system adhering to the principal of unlimited horizontal scaling which means we can adjust the amount of hardware required as the number of users expands, therefore ensuring that orders are always made on time.  Stronger A crypto exchange requires strong technical back up to handle hundreds of thousands of users per day and millions of active users at a time. It’s a challenging web operation which requires a powerful dedicated server cluster and constant maintenance to make the platform scalable for hundreds of millions of users. This doesn’t just include registrations but also ongoing use of the website and constant transactions of all types. The platform has to be built right from the start in an efficient and practical manner, which allows it to handle the pressure but also have a manageability system which is easy to operate, maintain and update. This way any potential problems will be easy to diagnose and fix before they affect users. EO.Trade will be the strongest crypto exchange based on an efficient and practical platform which will ensure the highest quality of trading. Availability The website and platform will always be available, regardless of how many users are simultaneously registering or making transactions. We do not claim to know the future, but we do know that technology only fails those who fail to prepare.  25  Performance The platform will be completely scaled which means its behaviour is maintained as the load increases. This is achieved by network and server utilization. Reliability The platform will always be reliable for users. This doesn’t only include availability but also trusting that when information is stored in the system it will be there for them to access again without any glitches. Latency The platform will have ultra-low latency thanks to a strong hardware and software architecture which will ensure users never face glitches or freezing, which could affect their experience and most importantly transactions. Back-end support Back-end processing needs to be performed to create the contents of the page and process requests of users. Back-end has to always be supported to keep up with user demand. Cost Ensuring a platform of this magnitude achieves the above includes big costs for development, operation, maintenance, adding more servers in the future and more. We have predicted the expenses and added a flexible increase for the expected expansions we will have to make as the platform grows.  Fastest KYC Process KYC (Know Your Client) is a standard security procedure of identifying and verifying the identity of clients. In this process companies filter their customers to ensure there is no fraud on their platforms. The EO.Trade crypto exchange will require a KYC process to maintain a safe environment for all investors. Current crypto exchanges are facing criticism for their lengthy KYC process which delays investors from accessing the platform, and withdrawing their money. Due to overwhelming demand some clients have had to wait up to a week to get verified. In traditional online trading platforms the process does not require more than 48 business hours. The KYC process is manual and in order for it to be secure, details have to be checked by a real person rather than an automated program. Logically this makes it a lengthy process 26  when hundreds or thousands of documents are pouring in on a daily basis. Any ill-prepared project would struggle under such pressure. EO.Trade will promise the fastest KYC procedure by eliminating all and any delays for investors. The crypto world is rapidly expanding and should offer the same benefits as online trading and cease to keep clients waiting a week or even several days for their documents to be checked. How will EO.Trade achieve this timing when some of the biggest crypto exchanges can’t? It is simple. We will expand our team as much as needed to ensure all areas are covered and there is enough manpower to support manual work. Most crypto exchanges surfaced on the crypto market without any previous experience in client care. They jumped on the bandwagon and in some cases succeeded. Our team has years of experience in supporting ExpertOption clients and going through millions of KYC documents. We understand the process because we deal with it on a daily basis and we know exactly how to level out supply with demand. The EO.Trade exchange will offer processes which match and exceed traditional trading platforms, making investments in crypto as seamless as any other investment.  Proven Customer Care Support is one of the most vital departments in any company, even more so in an exchange where the KYC process delays registrations and withdrawals. Most crypto exchanges currently on the market emerged out of an ICO which means their support teams were built for the specific projects and were thrown into the deep end as soon as the platform was launched. They have had no experience in dealing with millions of clients on a daily basis. When the concept of the EO project came about, we consulted with our team to ensure they have enough resources to support large numbers of new clients. The existing team has been prepared and organized to manage all aspects of support and ensure a fast KYC and withdrawal process. Most importantly a strategy has been put in place to allow the expansion of the team in a very short time. This way we will always keep up with client demand and offer high quality services.  27  Coin Listing Process We want to create an exchange which offers a large variety of crypto and fiat currencies while maintaining a highly active trading volume. We will set requirements for accepting new altcoins to ensure we keep our exchange liquid. Applicants must go through a verification process to identify the creators of the coin and the authenticity of the project. Applicants must also go through a due diligence process for a comprehensive appraisal of the business to establish a clear understanding of its assets and liabilities and evaluate its potential. Every established and new token can submit an application to be listed on EO.Trade. Additionally, we will include the crypto community in the selection of new altcoins to be added to the exchange. This will happen through a voting process during which existing members of the exchange can vote using their EO coin.  EO.Trade App The EO.Trade crypto exchange will be designed as a native app for iOS and Android. The EO.Trade app will include all functionalities of the platform in a user-friendly app downloadable on all devices. This includes full charting options optimized for the app. Market watch. Buying and selling fiat and crypto currencies. Deposits and withdrawals. Customer service. And all other functionalities. We have taken into consideration the fact that a large percentage of our users will come through the mobile app and hence we have built the app to equally withstand large numbers of active users as the web platform.  28  EO.News EO.News is a platform we will develop to provide news for the trading community. The news portal will be pumped with up-to-date market news, analysis, streaming quotes and charts, technical data and financial tools to keep investors updated about the global financial markets.  The platform will not be bound to crypto or fiat but much like the concept of our whole ecosystem, it will help all types of investors and give them a portal to the markets to reinforce their trading and investments. EO.News will be available for viewers from across the globe with no charge or the need to be associated with the EO ecosystem. It will be an open news source with the purpose of helping investors make better trading decisions. In the future we will include more languages to make it easier for international audiences to follow up the news. As a one-stop-shop the EO ecosystem will benefit from a powerful financial tool which will grow to become one of the most trusted sources of market news in the world.  29  The EO Coin The EO coin is a utility token built on the Ethereum block chain and follows the ERC20 token standard. The coin has three forms of utility on three platforms: the EO.Trade crypto exchange, the EO.Finance wallet and to power token-based accounts on the ExpertOption trading platform. Most prominently it will be the underlying gas to power the EO ecosystem. The EO coin will be listed on the EO.Trade exchange and it will be paired with all other currencies, cryptos and fiat. This will make the exchange to any other currency using EO Coin simple. The EO coin is a strong coin which will enable its holders to trade on the EO.Trade crypto exchange and use the EO.Finance wallet with 50% discounts on transaction fees. The coin will also offer higher profit percentages to traders on the ExpertOption platform. The coin can be used to pay for practically anything related to crypto on our platforms. A few of these functionalities are: Buy cryptos on EO.Trade or EO.Finance Open trades on token based accounts on the ExpertOption platform Pay fees for transactions on the EO.Trade exchange Pay fees for transactions on the EO.Finance wallet Pay online for any product or service which accepts cryptos  EO Coin Discount Structure Paying commission for transactions with the EO coin comes with discounts on EO.Finance and EO.Trade. The discount amount will be reduced annually in the following manner:  Discount Rate  1st year  2nd year  3rd year  4th year  50%  25%  10%  5%  On the 5th year the discount will be eliminated. The years are counted based on the launch of the products. 30  31  EO Technical Structure We use a huge variety of technologies. The main goal for us is to be secure and fast. We don’t rely on white labels and outsourcing. Everything is built in-house so we are prepared to make this project happen.  32  The EO Ecosystem consists of several layers:  Front-end The core approach of building client-side implementation of the platform is a single page application (SPA technology). This ensures several aspects of the project including the following: Fast loading Application loads fast, even on a poor 2G internet connection GPU rendering Our web application uses WebGL technology for drawing charts, it works very fast and reduces the CPU load. And makes our interface responsive, and user experience more fluent. Reactive interface We care about great user experience, no more waiting for loading pages. Offline first Applications can run without internet; you can access your data when you offline. Cutting Edge Technologies In our development we use cutting edge technologies such as; WebGL, WebSocket, Webpack and TypeScript. Our web applications use high performance modern stack of technologies, patterns and libraries. That gives the best experience to our customers and ease of fast production to our web development team. The heart of all of our web applications is Facebook’s React framework. WebGL WebGL is a Javascript API for direct communication with graphics processing units. This graphics library allows us to take maximum control over 2D and 3D rendering to get fast responsive user interface, which determinants fluent user experience. WebSocket To ensure a reliable high-speed full-duplex communication channel between the service and clients, WebSocket is used to guarantee the delivery of messages, to ensure minimum access time, customers choose the fastest access point from the geographically distributed 33  pool. Thus each client is guaranteed to be connected to the most optimal access point in terms of the speed and quality of the channel. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C. React We use React library which helps us implement the View part of MVC pattern (Model View Controller). React allows us to create large web-applications that use data and can change over time without reloading the page. It aims primarily to provide speed, simplicity, and scalability. Obfuscation Algorithms Helps us to keep code confidence, and protect code from compromisation. It enables the detection and removal of code injections, Man-In-The-Browser (MITB) attacks, DOMtampering and data exfiltration on the client-side in real-time.  Mobile Applications We use the most progressive technologies for building our mobile applications: OpenGL and asynchronous GPU rendering for smoother user experience Kotlin on Android and Swift/ObjC on iOS Code sharing via writing some parts of our apps in C++ Realm mobile database Asynchronous interface via Texture for achieving superior performance For several applications we use React Native framework, where fast variability of code is vitally important.  Backend The system is designed for high load and international audiences, and also requires rapid response and continuity in the operation of services, so we strictly follow several basic architectural principles. The basis is a micro-service architecture, which allows to improve and update key nodes of the system without loss of the provided service. The system is designed with the possibility of unlimited horizontal scaling to avoid indeterminable bottlenecks. We use a geographically distributed network of front-end nodes for the fastest possible delivery of content to users anywhere in the world. We stick to the asynchronous model of interaction between client applications and servers. This allows to achieve high 34  performance and responsiveness of communication channels. For data storages we use both relational databases and NoSQL solutions, like PostgreSQL, Redis, Couchbase, Elasticsearch. We use Hadoop Framework for Big Data, and processes it through machine learning based on neural networks. Backend implementation is based on a set of solutions: Python, Go, NodeJS, Java and C++ in sensitive places. Blockchain layer is serviced by Geth and Parity. The basic mathematical algorithms are realized on the basis of modern highly effective mathematical frameworks and libraries, such as TALib, NumPy, Pandas, SciPy, scikit-learn. Most of the calculations are performed in concurrency mode to increase the calculated system capacity, including on many servers (within a cluster consisting of several servers) Architecture is initially designed to take into account further expansion. All bottlenecks are organized with the possibility of parallel execution on multiple servers. That is what helps us to organise horizontal scaling.  Deployment, Testing and Security We use a set of DevOps practices, aimed at actively interacting developers with IT specialists and the mutual integration of their work processes into each other. It is based on the practices of world leaders in software development. Our team has successful extensive international experience in ensuring the quality of software products. We have a strong competence in secure storing user data and financial information. The platform testing takes place at expert level and corresponds to ISO / IEC TR 19759: 2005. The testing process includes: functional testing system testing performance testing regression testing security testing localization and usability testing automatic and unit testing  35  Our data centers have Tier III (availability of 99,982%) and Tier IV (99.995% availability) certification. Timely and fast software updates to exclude the possibility of exploiting vulnerabilities, including 0-day vulnerabilities Our goal is to eliminate the human factor as much as possible and use utomatic monitoring systems. We use trained neural networks to detect deviant behaviour of systems, including services, processes, and user activity. But we also have security monitoring personnel, who works along with automatic monitoring systems. To ensure security, all servers use full data encryption compatible with AES FIPS PUB 197 HTTP/2: We use the most advanced http protocols to increase speed, availability and security For API layer we use HTTP Secure through TLS connections only. This means that all data transferred between client-side applications and servers is encrypted. We also use websocket with binary data, to avoid man-in-the-middle vulnerability. Geographically distributed CI system, ensuring the timeliness, security and integrity of deployment of the developed software. We obfuscate core client-side scripts to reduce potential attacks to client applications and to keep internal user data processing secret and secure. Two factor Authentication helps us make our applications more secure.  36  EO Marketing Marketing a product always derives from its sellability potential and the gap it fills in the market, if there is one. A product must offer value and fulfill a need or desire in consumers. An existing market makes targeting much easier. Our marketing efforts are covered from all sides. Market Gap The market is not lacking trading platforms, wallets, crypto exchanges or coins, we know that. What the market does not have is a series of all four in a connected ecosystem where both fiat and cryptocurrencies are interchangeable and can be managed, transferred, exchanged, bought, saved and traded using the same coin. Consumer Need Traders who want to switch from fiat to crypto need a platform through which they can do it when they still do not own any cryptocurrencies, and have no experience in doing so. EO.Finance will offer direct access to buy cryptocurrencies with fiat. Consumer Desire Traders who manage both crypto and fiat in most cases do it on separate platforms. With our ecosystem they will no longer need separate wallets. They will be able to use our platforms to manage all their finances seamlessly. Existing Market Traditional online trading has the largest market in the world. In 2016 it was officially announced that the industry makes an average of $5.2 trillion in daily turnover. This number has been consistently growing over the past three years. Online trading is a tool for the largest financial institutions including central banks like Deutsche Bank which provides 21% of market liquidity. On the 11th April of 2018 coinmarketcap.com reported a market cap of $270 billion in the cryptocurrency market.  37  Marketing Strategy Based on sellability factors and our target audience we will market the EO ecosystem to investors and traders in both online trading and crypto investments. We will utilize all marketing channels, both organic and paid. Our community managers will expand our organic reach within the crypto community, starting with existing ExpertOption traders who have already trusted our company and experienced our superior and extensive services which have never failed them. We will inform the community about our work through Telegram, Medium, Bitcointalk, Facebook, Twitter and Reddit to begin with. We will use these platforms to communicate with traders and investors allowing our client care to help interested parties understand our ecosystem and its products. Our marketing team will take over the creative development of the brand which will determine our positioning in the market. Through digital marketing we will push the ecosystem internationally with well-crafted, consistent messages directed at the right audience. Relationship marketing and PR will reinforce organic marketing.  Affiliate Program ExpertOption runs a successful affiliate program which has attracted more than 8730 affiliates so far. The program is designed to invite anyone from across the globe to sign up at no cost and make profits for every investment made from their referrals. The affiliate program helps in spreading the word about the brand through thousands of people, each of which will promote the brand to thousands more people rapidly expanding the circle. This has worked successfully for ExpertOption. We will run similar affiliate programs for both EO.Trade and EO.Finance encouraging investors to attract new clients and get rewarded for it. The program will support affiliates with promotional and marketing material as well as customer support. All affiliates will have accurate tracking of their referrals. Affiliates will be rewarded from direct clients and clients of their sub affiliates. Rewards will be CPA based.  38  A Strong Marketing Team Our marketing team is made up of handpicked specialists and experienced individuals who single handedly led ExpertOption to global success. We confidently state that our marketing team can compete with any global marketing agency in all aspects of marketing. The team covers all areas of digital and direct marketing allowing us to control everything in-house ensuring brand quality, clarity and accuracy of the messages we send through carefully managed high quality brand and community management. Our marketing team consists of several well organized segments. The segments are divided by focused tasks but united in one big open-plan department as one team. The team has already proven its success with the ExpertOption brand. Below is a simplified version of how the segments of our marketing team work. Digital Marketing We have a highly capable group of marketers who have mastered the beast known as digital marketing. Online digital marketing enables us to have global reach and target the right audience. It requires creative, technical and analytical skills wrapped with marketing intelligence. Our Google, Facebook, media buying, analytics, SEO, campaign, and BI experts are masters at their profession and have developed their abilities around the ExpertOption brand magnifying the results of their work. Creative Development Our creative team is responsible for every audio-visual aspect of the brand. Creative development has expanded a lot in the last couple of years and marketing has become highly immersed in video and visual production. In order to keep up with today’s marketing demands, the creative team has to be artistic, corporate, and highly technical. Luckily we have gathered a group of individuals who are young and creative but also aware of corporate needs and branding. Graphic design, video production, video editing, motion graphics, art direction and content writing collaborate to deliver the brand of EO. Relationship Marketing Our relationship marketing team is a savvy and well connected one. Relationship marketing smoothly builds a bridge between marketing and customer care ensuring branding and brand quality is translated in every form of direct or digital marketing. Under the umbrella of relationship marketing we include inbound marketing, social media, public relations and 39  brand awareness. Our community and social media managers, email specialists, and public relations officers maintained the ExpertOption brand for years and they are onboard the EO project to do the same.  Helping Investors Strong marketing does not only work in the best interest of the company but also in the best interest of investors. When EO has a strong marketing team, every EO Coin holder is backed by the same team which is working hard to maintain and increase the success of the EO ecosystem and coin. And that works in the best interest of everyone who is part this community. Marketing is important for any successful business. It helps introduce a product or service to the market but also to continue promoting it to any potential customers. Successful campaigns keep investing in marketing and we will do the same for the EO ecosystem. We intend to become the most successful in every section of our ecosystem, and build a new community of investors who are looking for a place to manage their finances both fiat and crypto in one place. Our marketing will boost our name, reputation, sales and overall value. Our marketing team has your back.  40  EO Bounty The EO bounty program will consist of two sections, a referral program and airdrop. 5% of overall coins will be allocated for the bounty.  Referral Program The EO referral program will allow all registered users to refer the EO coin through any online means and make a profit out of their referrals’ investment. The link can be shared through social media, on a private website or anywhere online. Every link will have a unique identifier code. When a registration comes from the link it is automatically placed under the referrer’s name. When an investment is made, the referer receives a bonus. The bonus is extra EO coins added to his/ her account. The amount of tokens will be calculated as 10% of the investment made by the referral. The amount will not be taken from the referral’s investment. It will be extra tokens added to the referrer’s account from EO. The referral program does not cost anything for the referrer and can be used by registered clients even if they have not made an investment in the EO coin.  Airdrop Airdrops will be scheduled in Q4 2018 to offer investors who have bought the EO coin extra free coins as a reward. The airdrop amount will depend on the performance of the referral program.  41  Fund Allocation Funds will be allocated based on our cost expectation and ensuring all sectors of the project are supported. The sectors will be funded as follows:  Software Development 50% 50% of funds will be dedicated to developing the three new platforms, EO.Trade, EO.Finance and EO.News as well as developing token based accounts on the existing ExpertOption platform.  Marketing 35% 35% of funds will be dedicated to marketing costs including but not limited to media buying, brand awareness, public relations, ad campaigns, social media campaigns, newsletters, videos, and any other forms of marketing deemed necessary by our marketing strategy, to expand the brand and reach our goals.  Operational Costs 15% 15% of funds will be dedicated to operational costs which will cover the costs of applying for and obtaining licensing and day-today operations for expanding our teams in order to offer an optimal and fast experience for all our investors.  42  EO Roadmap 2014 The EO Group is established  2015 ExpertOption.com trading platform is launched  2016 Google and Facebook partnerships Oﬃces in Ukraine and Russia opened  2017 Expansion to Asian market Oﬃces in Thailand and India opened  January 2018 7 500 000+ accounts 3 500 000+ apps downloads 100+ employees in 6 countries  30 March - 12 April 2018 EO Coin private presale for ExpertOption traders  16 April - 29 June 2018 EO Coin public presale  Q3 2018 Obtaining License for crypto payments and exchange  July 2018 EO.Finance wallet launch  16 July – 31 August 2018 Second part of the EO Coin sale  43  December 2018 EO.Trade crypto exchange launch  Q4 2018 Airdrop release  January 2019 EO.Trade iOS and Android apps launch  Q1 2019 ExpertOption.com Token-based accounts launch  Q2 2019 EO.News market news portal launch  44  EO Group Management The EO team is made up of more than 100 dedicated professionals with experience in the industry and with each other. The same team worked on the highly successful ExpertOption brand and are now developing the EO ecosystem. Their proven commitment and measurable results are a testament to the effect they will have on our new products and the ecosystem as a whole.  Ivan Opriya CHIEF EXECUTIVE OFFICER of EO group Ivan Opriya is the CEO of the EO Group and the mastermind behind the ExpertOption brand. His vision lead to the award winning platform with millions of clients and his vision is leading the creation of the EO ecosystem.  Heinz Grünwald  Kenji Cheung  GLOBAL MARKETING ADVISOR  ASIAN MARKET ADVISOR  Grünwald is a world renowned marketer and an  Cheung is an entrepreneur, founder and renowned  advanced crypto advisor and investor. With more than 12  crypto investment advisor. He co-founded and launched  years of experience in digital marketing Grünwald has  two successful companies; Space Capital Group Limited  become an expert at driving growth in any business,  and Crypto28. Prior to that he mastered the ﬁnancial  especially in blockchain related ventures. Grünwald is the  industry through high ranked jobs for close to a decade.  founder of several companies including New Challenge.  Cheung is considered one of the top advisors for the blockchain industry.  45  Ivan Dashkevich  Vladimir Arsenev  CHIEF TECHNICAL OFFICER  CHIEF MARKETING OFFICER  Ivan Dashkevich is the technical leader at ExpertOption  Vladimir Arsenev is the marketing leader at ExpertOption  and the EO platforms. He has more than 5 years  and EO. He is one of the brightest minds in marketing  experience of building and leading eﬀective development  who helped turn ExpertOption into one of the biggest  teams (up to 50 developers). He developed successful  ﬁntech platforms on the market. His savvy technical  proven high load projects with multi-million daily active  knowledge combined with his natural creative instinct  audience. He is also a crypto expert with more than 3  has lead to measurable results which prove his abilities.  years of experience in blockchain.  Dmitrij Nikitin  Dmitry Ochkas  HEAD OF CUSTOMER CARE  CHIEF HR OFFICER  Dmitrij Nikitin leads the EO customer care team and  Dmitry leads the EO Human Resources department and  overlooks support quality and customer satisfaction at all  focuses on talent acquisition which he excelled at when  stages of the process. He has a history in the investment  handpicking the ExpertOption team. He has more than  banking industry and excels in many sectors. His vast  10 years of experience in helping businesses build  knowledge enables him to combine knowledge from all  eﬃcient teams in the gaming and ﬁntech industries. His  the departments to handle all customer queries. Dimitrij  personal interest in cryptos enhances his contribution to  lead the ExpertOption team to handle more than 7 million  the EO project.  traders and will do the same for EO.  Risk Disclaimer Trading comes with a high level of risk, and its execution can be very risky. In case of purchase of financial instruments offered by the Website and the Services, you may incur significant losses of investment or even lose all funds on your Account.  46  EOS TOKEN PURCHASE AGREEMENT Last Updated: September 4, 2017 This EOS Token Purchase Agreement (this “Agreement”) contains the terms and conditions that govern your use of the EOS distribution smart contract (the “EOS Distribution Contract”); use of the related ERC-20 EOS token smart contact (the “EOS Token Contract”); and purchase of the related ERC-20 compatible tokens distributed on the Ethereum blockchain (the “EOS Tokens”) and is an agreement between you or the entity that you represent (“Buyer” or “you”) and block.one (“block.one,” together with its parent company, subsidiaries and affiliates, “Company”). Buyer, block.one and Company are herein referred to individually as a “Party” and collectively, as the “Parties”. NOW, THEREFORE, in consideration of the mutual representations, warranties and agreements contained in this Agreement, and for other good and valuable consideration, the receipt and sufficiency of which are hereby acknowledged, Company and Buyer hereby agree as follows: IMPORTANT INFORMATION: PLEASE READ THIS AGREEMENT CAREFULLY AND IN ITS ENTIRETY. Buyer acknowledges, understands and agrees to the following: • MATTERS RELATING TO EOS.IO SOFTWARE AND EOS PLATFORM: 1. block.one is developing the EOS.IO software (the “EOS.IO Software”) as further described in the EOS.IO Technical White Paper (as it may be amended from time to time) (the “White Paper”); 2. at the end of its development stage, block.one will be releasing the EOS.IO Software it has developed under an open source software license; 3. Company will not configure and/or launch any public blockchain platform adopting the open source EOS.IO Software (the “EOS Platform”) for any purpose; 4. any launch and implementation of the EOS Platform may occur by third parties unrelated to Company; 5. third parties launching the EOS Platform may delete, modify or supplement the EOS.IO Software prior to, during or after launching the EOS Platform; and 6. Company will have no control over when, how or whether the EOS.IO Software is adopted or implemented, or how, when or whether the EOS Platform is launched. •  BINDING AGREEMENT: Buyer understands and agrees that Buyer is subject to and bound by this Agreement by virtue of Buyer’s purchase of EOS Tokens.  •  NO U.S. OR CHINESE BUYERS: EOS Tokens are not being offered or distributed to U.S. persons (as defined below) or Chinese persons (as defined below). If you are citizen, resident of, or a person located or domiciled in, the United States of America including its states, territories or the District of Columbia or any entity, including,  without limitation, any corporation or partnership created or organized in or under the laws of the United States of America, any state or territory thereof or the District of Columbia (a “U.S. person”), or, if you are citizen, resident of, or a person located or domiciled in, or any entity, including, without limitation, any corporation or partnership created or organized in or under the laws of the People’s Republic of China (a “Chinese person”), do not purchase or attempt to purchase EOS Tokens. •  EOS TOKENS HAVE NO RIGHTS, USES OR ATTRIBUTES. The EOS Tokens do not have any rights, uses, purpose, attributes, functionalities or features, express or implied, including, without limitation, any uses, purpose, attributes, functionalities or features on the EOS Platform. Company does not guarantee and is not representing in any way to Buyer that the EOS Tokens have any rights, uses, purpose, attributes, functionalities or features.  •  NOT A PURCHASE OF EOS PLATFORM TOKENS. EOS Tokens purchased under this Agreement are not tokens on the EOS Platform. Buyer acknowledges, understands and agrees that Buyer should not expect and there is no guarantee or representation made by Company that Buyer will receive any other product, service, rights, attributes, functionalities, features or assets of any kind whatsoever, including, without limitation, any cryptographic tokens or digital assets now or in the future whether through receipt, exchange, conversion, redemption or otherwise.  •  PURCHASE OF EOS TOKENS ARE NON-REFUNDABLE AND PURCHASES CANNOT BE CANCELLED. BUYER MAY LOSE ALL AMOUNTS PAID.  •  EOS TOKENS MAY HAVE NO VALUE.  •  COMPANY RESERVES THE RIGHT TO REFUSE OR CANCEL EOS TOKEN PURCHASE REQUESTS AT ANY TIME IN ITS SOLE DISCRETION.  •  PLEASE READ THE RISKS SET FORTH IN SECTION 7 CAREFULLY AND IN THEIR ENTIRETY.  •  THIS AGREEMENT INCLUDES PRE-DISPUTE RESOLUTION IN SECTION 9.1 AND REQUIRES ARBITRATION IN SECTION 9.2. ARTICLE ONE: ACCEPTANCE OF AGREEMENT AND PURCHASE OF EOS TOKENS  1.1.  This Agreement shall be effective and binding on the Parties when Buyer: (a) clicks the check box on the official https://eos.io/ website (the “Website”) to indicate that Buyer has read, understands and agrees to the terms of this Agreement; or, if earlier (b) upon Company’s receipt of payment from Buyer. Buyer agrees to be bound on this basis, and confirms that Buyer has read in full and understands this Agreement and the terms on which Buyer is bound.  1.2.  Website Terms of Use. Company has established Terms of Use, as may be amended from time to time, for the Website located at https://eos.io/terms-of-use.html, which are hereby incorporated by reference. Buyer has read, understands and agrees to those terms. 2  1.3.  White Paper. Company has prepared the White Paper, which is available at https://github.com/EOSIO/Documentation/blob/master/TechnicalWhitePaper.md, describing matters relating to the EOS.IO Software. The White Paper, as it may be amended from time to time, is hereby incorporated by reference. Buyer has read and understands the White Paper and its contents.  1.4.  EOS Tokens. a. No Purpose. As mentioned above, the EOS Tokens do not have any rights, uses, purpose, attributes, functionalities or features, express or implied. Although EOS Tokens may be tradable, they are not an investment, currency, security, commodity, a swap on a currency, security or commodity or any other kind of financial instrument. b. Company’s Use of Proceeds. Buyer acknowledges and understands that the proceeds from the sale of the EOS Tokens will be utilized by Company in its sole discretion. ARTICLE TWO: EOS TOKEN DISTRIBUTION  2.1.  Allocation and Distribution of EOS Tokens. block.one intends to allocate and distribute EOS Tokens (the “EOS Token Distribution”) in accordance with the material specifications as set forth in Exhibit A to this Agreement which includes details regarding the timing (the “EOS Distribution Period”) and pricing of the EOS Token Distribution and the amount of EOS Tokens that will be distributed. During the EOS Distribution Period, block.one will provide specific procedures on how Buyer should purchase EOS Tokens through the official Website. By purchasing EOS Tokens, Buyer acknowledges and understands and has no objection to such procedures and material specifications. Failure to use the official Website and follow such procedures may result in Buyer not receiving any EOS Tokens. Any buyer of EOS Tokens may lose some or all of the amounts paid in exchange for EOS Tokens, regardless of the purchase date. The access or use of the EOS Distribution Contract, access or use of the EOS Token Contract and/or the receipt or purchase of EOS through any other means other than the official Website are not sanctioned or agreed to in any way by the block.one Parties. Buyer should take great care that the website used to purchase EOS Tokens has the following universal resource locator (URL): https://eos.io/.  2.2.  No U.S. or Chinese Buyers. The EOS Tokens are not being offered to U.S. persons or Chinese persons. U.S. persons and Chinese persons are strictly prohibited and restricted from using the EOS Distribution Contract, using the EOS Token Contact and/or purchasing EOS Tokens and Company is not soliciting purchases by U.S. persons or Chinese persons in any way. If a U.S. person or a Chinese person uses the EOS Distribution Contract, uses the EOS Token Contract and/or purchases EOS Tokens, such person has done so and entered into this Agreement on an unlawful, unauthorized and fraudulent basis and this Agreement is null and void. Company is not bound by this Agreement if this Agreement has been entered into by a U.S. person or a Chinese person as Buyer or Buyer has entered into this Agreement or has purchased EOS Tokens on behalf of a U.S. person or a Chinese person, and Company may take all necessary and appropriate actions, in its sole discretion, to invalidate this Agreement, including referral of information to the appropriate authorities. Any U.S. person or Chinese person who uses the EOS Distribution Contract, uses the EOS Token 3  Contract and/or purchases EOS Tokens or enters this Agreement on an unlawful, unauthorized or fraudulent basis shall be solely liable for, and shall indemnify, defend and hold harmless block.one and block.one’s respective past, present and future employees, officers, directors, contractors, consultants, equity holders, suppliers, vendors, service providers, parent companies, subsidiaries, affiliates, agents, representatives, predecessors, successors and assigns (collectively, the “block.one Parties”) from any damages, losses, liabilities, costs or expenses of any kind, whether direct or indirect, consequential, compensatory, incidental, actual, exemplary, punitive or special and including, without limitation, any loss of business, revenues, profits, data, use, goodwill or other intangible losses (collectively, the “Damages”) incurred by a block.one Party that arises from or is a result of such U.S. person’s or Chinese person’s unlawful, unauthorized or fraudulent use of the EOS Distribution Contract, unauthorized use of the EOS Token Contract and/or the receipt or purchase of EOS Tokens. 2.3.  Allocation and Sale of EOS Tokens to block.one Parties. Buyer understands and consents to the participation of the Company’s past, present and future employees, officers, directors, contractors, consultants, equity holders, suppliers, vendors and service providers in the purchase of EOS Tokens, including people who may work on the development and implementation of the EOS.IO Software or who may work for block.one’s future businesses which block.one may establish with a portion of the proceeds from the EOS Token Distribution. All such block.one Parties will participate on the same terms as every other buyer of EOS Tokens and will be bound by this Agreement.  2.4.  No Representations and Warranties. The EOS Tokens will be distributed to buyers thereof pursuant to the EOS Distribution Contract and the EOS Token Contract. None of the block.one Parties makes any representations or warranties, express or implied, including, without limitation, any warranties of title or implied warranties of merchantability or fitness for a particular purpose with respect to the EOS Distribution Contract, the EOS Token Contract or the EOS Tokens or their utility, or the ability of anyone to purchase or use the EOS Tokens. Without limiting the foregoing, none of the block.one Parties represent or warrant that the process of purchasing the EOS Tokens or receiving the EOS Tokens will be uninterrupted or error-free or that the EOS Tokens are reliable and error-free. As a result, Buyer acknowledges and understands that Buyer may never receive EOS Tokens and may lose the entire amount Buyer paid to Company. Buyer shall provide an accurate digital wallet address to Company for receipt of any EOS Tokens distributed to Buyer pursuant to the EOS Distribution Contract and the EOS Token Contract.  2.5.  Not an Offering of Securities, Commodities, or Swaps. The sale of EOS Tokens and the EOS Tokens themselves are not securities, commodities, swaps on either securities or commodities or a financial instrument of any kind. Purchases and sales of EOS Tokens are not subject to the protections of any laws governing those types of financial instruments. This Agreement and all other documents referred to in this Agreement including the White Paper do not constitute a prospectus or offering document, and are not an offer to sell, nor the solicitation of an offer to buy an investment, a security, commodity, or a swap on either a security or commodity.  2.6.  Not an Investment. Buyer should not participate in the EOS Token Distribution or purchase EOS Tokens for investment purposes. EOS Tokens are not designed for 4  investment purposes and should not be considered as a type of investment. Within twenty-three (23) hours from the end of the EOS Distribution Period, all EOS Tokens will no longer be transferable and the EOS Token Contract will prevent all further transfers and public key mappings. At this point, the distribution of EOS Tokens will be complete. Buyer acknowledges, understands and agrees that Buyer should not expect and there is no guarantee or representation or warranty by Company that: (a) the EOS.IO Software will ever be adopted; (b) the EOS.IO Software will be adopted as developed by block.one and not in a different or modified form; (c) a blockchain utilizing or adopting the EOS.IO Software will ever be launched; and (d) a blockchain will ever be launched with or without changes to the EOS.IO Software and with or without a distribution matching the fixed, non-transferable EOS Token balances. Furthermore, EOS Tokens will not have any functionality or rights on the EOS Platform and holding EOS Tokens is not a guarantee, representation or warranty that the holder will be able to use the EOS Platform, or receive any tokens utilized on the EOS Platform, even if the EOS Platform is launched and the EOS.IO Software is adopted, of which there is no guarantee, representation or warranty made by Company. 2.7.  Not for Speculation. Buyer acknowledges and agrees that Buyer is not purchasing EOS Tokens for purposes of investment, speculation, as some type of arbitrage strategy, for immediate resale or other financial purposes. ARTICLE THREE: NO OTHER RIGHTS CREATED  3.1.  No Claim, Loan or Ownership Interest. The purchase of EOS Tokens: (a) does not provide Buyer with rights of any form with respect to the Company or its revenues or assets, including, but not limited to, any voting, distribution, redemption, liquidation, proprietary (including all forms of intellectual property), or other financial or legal rights; (b) is not a loan to Company; and (c) does not provide Buyer with any ownership or other interest in Company.  3.2.  Intellectual Property. Company retains all right, title and interest in all of Company’s intellectual property, including, without limitation, inventions, ideas, concepts, code, discoveries, processes, marks, methods, software, compositions, formulae, techniques, information and data, whether or not patentable, copyrightable or protectable in trademark, and any trademarks, copyright or patents based thereon. Buyer may not use any of Company’s intellectual property for any reason without Company’s prior written consent. ARTICLE FOUR: SECURITY AND DATA; TAXES  4.1.  Security and Data Privacy. a. Buyer’s Security. Buyer will implement reasonable and appropriate measures designed to secure access to: (i) any device associated with Buyer and utilized in connection with Buyer’s purchase of EOS Tokens; (ii) private keys to Buyer’s wallet or account; and (iii) any other username, passwords or other login or identifying credentials. In the event that Buyer is no longer in possession of Buyer’s private keys or any device associated with Buyer’s account or is not able to provide Buyer’s login or identifying credentials, Buyer may lose all of Buyer’s EOS Tokens and/or access to Buyer’s account. Company is under no obligation to recover any EOS Tokens and Buyer acknowledges, understands and agrees that all purchases of 5  EOS Tokens are non-refundable and Buyer will not receive money or other compensation for any EOS Tokens purchased. b. Additional Information. Upon Company’s request, Buyer will immediately provide to Company information and documents that Company, in its sole discretion, deems necessary or appropriate to comply with any laws, regulations, rules or agreements, including without limitation judicial process. Such documents include, but are not limited to, passport, driver’s license, utility bills, photographs of associated individuals, government identification cards, or sworn statements. Buyer consents to Company disclosing such information and documents in order to comply with applicable laws, regulations, rules or agreements. Buyer acknowledges that Company may refuse to distribute EOS Tokens to Buyer until such requested information is provided. 4.2.  Taxes. Buyer acknowledges, understands and agrees that: (a) the purchase and receipt of EOS Tokens may have tax consequences for Buyer; (b) Buyer is solely responsible for Buyer’s compliance with Buyer’s tax obligations; and (c) Company bears no liability or responsibility with respect to any tax consequences to Buyer. ARTICLE FIVE: REPRESENTATIONS AND WARRANTIES OF BUYER  By buying EOS Tokens, Buyer represents and warrants to each of the block.one Parties that: 5.1.  Not a U.S. Person or Chinese Person: Buyer is not a U.S. person or a Chinese person.  5.2.  Authority. Buyer has all requisite power and authority to execute and deliver this Agreement, to use the EOS Distribution Contract and the EOS Smart Contract, purchase EOS Tokens, and to carry out and perform its obligations under this Agreement. a. If an individual, Buyer is at least 18 years old and of sufficient legal age and capacity to purchase EOS Tokens. b. If a legal person, Buyer is duly organized, validly existing and in good standing under the laws of its domiciliary jurisdiction and each jurisdiction where it conducts business.  5.3.  No Conflict. The execution, delivery and performance of this Agreement will not result in any violation of, be in conflict with, or constitute a material default under, with or without the passage of time or the giving of notice: (a) any provision of Buyer’s organizational documents, if applicable; (b) any provision of any judgment, decree or order to which Buyer is a party, by which it is bound, or to which any of its material assets are subject; (c) any material agreement, obligation, duty or commitment to which Buyer is a party or by which it is bound; or (d) any laws, regulations or rules applicable to Buyer.  5.4.  No Consents or Approvals. The execution and delivery of, and performance under, this Agreement require no approval or other action from any governmental authority or person other than Buyer.  5.5.  Buyer Status. Buyer is not subject to any of the disqualifying events listed in Rule 506(d)(1) of Regulation D under the Securities Act of 1933 (a “Buyer Event”), and 6  there is no proceeding or investigation pending or, to the knowledge of Buyer, threatened by any governmental authority, that would reasonably be expected to become the basis for a Buyer Event. 5.6.  Buyer Knowledge and Risks of Project. Buyer has sufficient knowledge and experience in business and financial matters, including a sufficient understanding of blockchain or cryptographic tokens and other digital assets, smart contracts, storage mechanisms (such as digital or token wallets), blockchain-based software systems and blockchain technology, to be able to evaluate the risks and merits of Buyer’s purchase of EOS Tokens, including but not limited, to the matters set forth in this Agreement, and is able to bear the risks thereof, including loss of all amounts paid, loss of EOS Tokens, and liability to the block.one Parties and others for its acts and omissions, including with limitation those constituting breach of this Agreement, negligence, fraud or willful misconduct. Buyer has obtained sufficient information in order to make an informed decision to purchase EOS Tokens.  5.7.  Funds; Payments. a. Funds. The funds, including any fiat, virtual currency or cryptocurrency, Buyer uses to purchase EOS Tokens are not derived from or related to any unlawful activities, including but not limited to money laundering or terrorist financing, and Buyer will not use the EOS Tokens to finance, engage in, or otherwise support any unlawful activities. b. Payments. All payments by Buyer under this Agreement will be made only in Buyer’s name, from a digital wallet or bank account not located in a country or territory that has been designated as a “non-cooperative country or territory” by the Financial Action Task Force, and is not a “foreign shell bank” within the meaning of the U.S. Bank Secrecy Act (31 U.S.C. § 5311 et seq.), as amended, and the regulations promulgated thereunder by the Financial Crimes Enforcement Network, as such regulations may be amended from time to time.  5.8.  Miscellaneous Regulatory Compliance. a. Anti-Money Laundering; Counter-Terrorism Financing. To the extent required by applicable law, Buyer complies with all anti-money laundering and counterterrorism financing requirements. b. Sanctions Compliance. Neither Buyer, nor any person having a direct or indirect beneficial interest in Buyer or EOS Tokens being acquired by Buyer, or any person for whom Buyer is acting as agent or nominee in connection with EOS Tokens, is the subject of sanctions administered or enforced by any country or government (collectively, “Sanctions”) or is organized or resident in a country or territory that is the subject of country-wide or territory-wide Sanctions. ARTICLE SIX: DISCLAIMERS  6.1.  Buyer expressly acknowledges, understands and agrees that Buyer is using the EOS Distribution Contract, the EOS Token Contract and purchasing EOS Tokens at the Buyer’s sole risk and that the EOS Distribution Contract, the EOS Token Contract and EOS Tokens are each provided, used and acquired on an “AS IS” and on an “AS 7  AVAILABLE” basis without representations, warranties, promises or guarantees whatsoever of any kind by Company and Buyer shall rely on its own examination and investigation thereof. 6.2.  No Representation or Warranty. (A) COMPANY DOES NOT MAKE AND EXPRESSLY DISCLAIMS ALL REPRESENTATIONS AND WARRANTIES, EXPRESS, IMPLIED OR STATUTORY; AND (B) WITH RESPECT TO THE EOS DISTRIBUTION CONTRACT, THE EOS TOKEN CONTRACT AND THE EOS TOKENS, COMPANY SPECIFICALLY DOES NOT REPRESENT AND WARRANT AND EXPRESSLY DISCLAIMS ANY REPRESENTATION OR WARRANTY, EXPRESS, IMPLIED OR STATUTORY, INCLUDING WITHOUT LIMITATION, ANY REPRESENTATIONS OR WARRANTIES OF TITLE, NONINFRINGEMENT, MERCHANTABILITY, USAGE, SUITABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE, OR AS TO THE WORKMANSHIP OR TECHNICAL CODING THEREOF, OR THE ABSENCE OF ANY DEFECTS THEREIN, WHETHER LATENT OR PATENT. ARTICLE SEVEN: RISKS  EOS TOKENS MAY HAVE NO VALUE. BUYER MAY LOSE ALL AMOUNTS PAID. Buyer has carefully reviewed, acknowledges, understands and assumes the following risks, as well as all other risks associated with the EOS Tokens (including those not discussed herein), all of which could render the EOS Tokens worthless or of little value: 7.1.  No Rights, Functionality or Features. EOS Tokens have no rights, uses, purpose, attributes, functionalities or features, express or implied. EOS Tokens do not entitle holders to participate on the EOS Platform, even if the EOS Platform is launched and the EOS.IO Software’s development is finished and the EOS.IO Software is adopted and implemented.  7.2.  EOS Platform. Buyer should not purchase EOS Tokens in reliance on the EOS Platform because EOS Tokens are not usable on the EOS Platform and do not entitle Buyer to anything with respect to the EOS Platform.  7.3.  Purchase Price Risk. The distribution of EOS Tokens will occur at the end of each set period during the EOS Distribution Period. The purchase price a buyer receives for EOS Tokens depends upon the actions of all other users sending ether (“ETH”) to the EOS Token Contract during the same period. Everyone sending ETH during the same period receives the same price. It is possible for other people to send in a large amount of ETH after Buyer and dramatically increase the price Buyer and everyone else pays per EOS Token received. There are no guarantees as to the price of EOS Tokens purchased by Buyer and no guarantees that the price per EOS Token determined each period by the market will be equal to or higher in the subsequent periods of the EOS Distribution Period. There is the possibility that the price per EOS Token in subsequent periods of the EOS Distribution Period falls below the price paid by initial buyers of EOS Tokens during the EOS Distribution Period. block.one reserves the right to change the duration of the EOS Distribution Period for any reason, including, without limitation, bugs in the EOS Distribution Contract or the EOS Token Contract or the unavailability of the Website or other unforeseen procedural or security issues.  8  7.4.  Blockchain Delay Risk. On the Ethereum blockchain, timing of block production is determined by proof of work so block production can occur at random times. For example, ETH contributed to the EOS Smart Contract in the final seconds of a distribution period may not get included for that period. Buyer acknowledges and understands that the Ethereum blockchain may not include the Buyer’s transaction at the time Buyer expects and Buyer may not receive EOS Tokens the same day Buyer sends ETH.  7.5.  Ethereum Blockchain. The Ethereum blockchain is prone to periodic congestion during which transactions can be delayed or lost. Individuals may also intentionally spam the Ethereum network in an attempt to gain an advantage in purchasing cryptographic tokens. Buyer acknowledges and understands that Ethereum block producers may not include Buyer’s transaction when Buyer wants or Buyer’s transaction may not be included at all.  7.6.  Ability to Transact or Resell. Buyer may be unable to sell or otherwise transact in EOS Tokens at any time, or for the price Buyer paid. By using the EOS Distribution Contract or the EOS Token Contract or by purchasing EOS Tokens, Buyer acknowledges, understands and agrees that: (a) EOS Tokens may have no value; (b) there is no guarantee or representation of liquidity for the EOS Tokens; and (c) the block.one Parties are not and shall not be responsible for or liable for the market value of EOS Tokens, the transferability and/or liquidity of EOS Tokens and/or the availability of any market for EOS Tokens through third parties or otherwise.  7.7.  Token Security. EOS Tokens may be subject to expropriation and or/theft. Hackers or other malicious groups or organizations may attempt to interfere with the EOS Distribution Contract, the EOS Token Contract or the EOS Tokens in a variety of ways, including, but not limited to, malware attacks, denial of service attacks, consensusbased attacks, Sybil attacks, smurfing and spoofing. Furthermore, because the Ethereum platform rests on open source software and EOS Tokens are based on open source software, there is the risk that Ethereum smart contracts may contain intentional or unintentional bugs or weaknesses which may negatively affect the EOS Tokens or result in the loss of Buyer’s EOS Tokens, the loss of Buyer’s ability to access or control Buyer’s EOS Tokens or the loss of ETH in Buyer’s account. In the event of such a software bug or weakness, there may be no remedy and holders of EOS Tokens are not guaranteed any remedy, refund or compensation.  7.8.  Access to Private Keys. EOS Tokens purchased by Buyer may be held by Buyer in Buyer’s digital wallet or vault, which requires a private key, or a combination of private keys, for access. Accordingly, loss of requisite private key(s) associated with Buyer’s digital wallet or vault storing EOS Tokens will result in loss of such EOS Tokens, access to Buyer’s EOS Token balance and/or any initial balances in blockchains created by third parties. Moreover, any third party that gains access to such private key(s), including by gaining access to login credentials of a hosted wallet or vault service Buyer uses, may be able to misappropriate Buyer’s EOS Tokens. Company is not responsible for any such losses.  7.9.  EOS Tokens Will Become Non-Transferable. Buyer acknowledges and understands that EOS Tokens will become non-transferrable within twenty-three (23) hours after the end of the EOS Distribution Period. At this time, Buyer will no longer be able to map a public key to Buyer’s account and Buyer will not be able to transfer EOS Tokens 9  on the Ethereum blockchain. Some cryptocurrency exchanges may on their own accord enable EOS Tokens to continue trading, but the exchanges will be unable to accept new deposits or authorize withdrawals of EOS Tokens. 7.10. New Technology. The EOS.IO Software and the EOS Platform and all of the matters set forth in the White Paper are new and untested. The EOS.IO Software might not be capable of completion, implementation or adoption. It is possible that no blockchain utilizing the EOS.IO Software will be ever be launched and there may never be an operational EOS Platform. Buyer should not rely on the EOS.IO Software or the ability to receive tokens associated with the EOS Platform in the future. Even if the EOS.IO Software is completed, implemented and adopted, it might not function as intended, and any tokens associated with a blockchain adopting the EOS.IO Software may not have functionality that is desirable or valuable. Also, technology is changing rapidly, so the EOS Tokens and any tokens transferable on the EOS Platform may become outdated. 7.11. Reliance on Third-Parties. Even if completed, the EOS.IO Software will rely, in whole or partly, on third parties to adopt and implement it and to continue to develop, supply, and otherwise support it. There is no assurance or guarantee that those third parties will complete their work, properly carry out their obligations, or otherwise meet anyone’s needs, all of might have a material adverse effect on the EOS.IO Software and EOS Platform. 7.12. Failure to Map a Public Key to Buyer’s Account. Failure of Buyer to map a public key to Buyer’s account may result in third parties being unable to recognize Buyer’s EOS Token balance on the Ethereum blockchain when and if they configure the initial balances of a new blockchain based upon the EOS.IO Software of which Company makes no representation or guarantee. 7.13. Exchange & Counterparty Risks. If Buyer sends ETH to the EOS Token Contract from an exchange or an account that Buyer does not control, pursuant to the EOS Token Contract, EOS Tokens will be allocated to the account that has sent ETH; therefore, Buyer may never receive or be able to recover Buyer’s EOS Tokens. Furthermore, if Buyer chooses to maintain or hold EOS Tokens through a cryptocurrency exchange or other third party, Buyer’s EOS Tokens may be stolen or lost. In addition, third parties may not recognize Buyer’s claim to any derivative tokens if and when launched by third parties according to the distribution rules set in the EOS.IO Software. By using the EOS Distribution Contract, using the EOS Token Contract and/or by purchasing EOS Tokens, Buyer acknowledges and agrees that Buyer sends ETH to the EOS Token Contract through an exchange account and/or holds EOS Tokens on a cryptocurrency exchange or with another third party at Buyer’s own and sole risk. 7.14. Changes to the EOS.IO Software. The EOS.IO Software is still under development and may undergo significant changes over time. Although Company intends for the EOS.IO Software to have the features and specifications set forth in the White Paper, Company may make changes to such features and specifications for any number of reasons, and any party that adopts the EOS.IO Software and launches the EOS Platform also may make changes, any of which may mean that the EOS Platform does not meet Buyer’s expectations.  10  7.15. Risk of Alternative Blockchains based on EOS.IO Software. The EOS.IO Software will not likely be licensed under an open source license until after the end of the EOS Distribution Period; however, it is possible somebody will not respect the EOS.IO Software copyright or will modify the EOS.IO Software after it has been released under an open source license. Therefore, it is possible for someone to utilize the EOS.IO Software to build and launch blockchain protocols using a token distribution other than the one intended for the EOS Tokens pursuant to the EOS.IO Software both prior to or after the EOS.IO Software has become licensed as open source. 7.16. Risk of Lack of Transferability in Blockchain Cryptographic Token. The EOS.IO Software is built such that any blockchain that adopts the EOS.IO Software will require approval of holders of not less than 15% of the total issued and outstanding EOS Tokens before tokens on such blockchain (the “Blockchain Tokens”) can be transferred. In other words, if the EOS.IO Software is adopted, it will be the responsibility of holders holding at least 15% of the issued and outstanding EOS Tokens to adopt one or more blockchains in order for Blockchain Tokens received on such blockchains to be transferrable. Buyer acknowledges, understands and agrees that if the EOS.IO Software is adopted and the requisite vote described above is not obtained, Buyer may not be able to transfer any Blockchain Tokens Buyer receives. 7.17. Project Completion. The development of the EOS.IO Software may be abandoned for a number of reasons, including, but not limited to, lack of interest from the public, lack of funding, lack of commercial success or prospects, or departure of key personnel. 7.18. Lack of Interest. Even if the EOS.IO Software is finished and adopted and the EOS Platform is launched, the ongoing success of the EOS Platform relies on the interest and participation of third parties like developers. There can be no assurance or guarantee that there will be sufficient interest or participation in the EOS Platform. 7.19. Uncertain Regulatory Framework. The regulatory status of cryptographic tokens, digital assets and blockchain technology is unclear or unsettled in many jurisdictions. It is difficult to predict how or whether governmental authorities will regulate such technologies. It is likewise difficult to predict how or whether any governmental authority may make changes to existing laws, regulations and/or rules that will affect cryptographic tokens, digital assets, blockchain technology and its applications. Such changes could negatively impact EOS Tokens in various ways, including, for example, through a determination that EOS Tokens are regulated financial instruments that require registration. Company may cease the distribution of EOS Tokens, the development of the EOS.IO Software or cease operations in a jurisdiction in the event that governmental actions make it unlawful or commercially undesirable to continue to do so. 7.20. Risk of Government Action. As noted above, the industry in which Company operates is new, and may be subject to heightened oversight and scrutiny, including investigations or enforcement actions. There can be no assurance that governmental authorities will not examine the operations of Company and/or pursue enforcement actions against Company. Such governmental activities may or may not be the result of targeting Company in particular. All of this may subject Company to judgments, settlements, fines or penalties, or cause Company to restructure its operations and activities or to cease offering certain products or services, all of which could harm Company’s reputation or lead to higher operational costs, which may in turn have a 11  material adverse effect on the EOS Tokens and/or the development of the EOS.IO Software. ARTICLE EIGHT: LIMITATION OF LIABILITY; INDEMNIFICATION 8.1.  Limitation of Liability. To the fullest extent permitted by applicable law, Buyer disclaims any right or cause of action against the block.one Parties of any kind in any jurisdiction that would give rise to any Damages whatsoever, on the part of any block.one Party. Each of the block.one Parties shall not be liable to Buyer for any type of Damages, even if and notwithstanding the extent a block.one Party has been advised of the possibility of such Damages. Buyer agrees not to seek any refund, compensation or reimbursement from a block.one Party, regardless of the reason, and regardless of whether the reason is identified in this Agreement.  8.2.  Damages. In no circumstances will the aggregate joint liability of the block.one Parties, whether in contract, warrant, tort or other theory, for Damages to Buyer under this Agreement exceed the amount received by Company from Buyer.  8.3.  Force Majeure. Buyer understands and agrees that Company shall not be liable and disclaims all liability to Buyer in connection with any force majeure event, including acts of God, labour disputes or other industrial disturbances, electrical, telecommunications, hardware, software or other utility failures, software or smart contract bugs or weaknesses, earthquakes, storms, or other nature-related events, blockages, embargoes, riots, acts or orders of government, acts of terrorism or war, technological change, changes in interest rates or other monetary conditions, and, for the avoidance of doubt, changes to any blockchain-related protocol.  8.4.  Release. To the fullest extent permitted by applicable law, Buyer releases the block.one Parties from responsibility, liability, claims, demands, and/or Damages (actual and consequential) of every kind and nature, known and unknown (including, but not limited to, claims of negligence), arising out of or related to disputes between Buyer and the acts or omissions of third parties.  8.5.  Indemnification. a. To the fullest extent permitted by applicable law, Buyer will indemnify, defend and hold harmless and reimburse the block.one Parties from and against any and all actions, proceedings, claims, Damages, demands and actions (including without limitation fees and expenses of counsel), incurred by a block.one Party arising from or relating to: (i) Buyer’s purchase or use of EOS Tokens; (ii) Buyer’s responsibilities or obligations under this Agreement; (iii) Buyer’s breach of or violation of this Agreement; (iv) any inaccuracy in any representation or warranty of Buyer; (v) Buyer’s violation of any rights of any other person or entity; and/or (vi) any act or omission of Buyer that is negligent, unlawful or constitutes willful misconduct. b. Company reserves the right to exercise sole control over the defense, at Buyer’s expense, of any claim subject to indemnification under this Section 8.5. This indemnity is in addition to, and not in lieu of, any other indemnities set forth in a written agreement between Buyer and Company. 12  ARTICLE NINE: DISPUTE RESOLUTION 9.1.  Informal Dispute Resolution. Buyer and Company shall cooperate in good faith to resolve any dispute, controversy or claim arising out of, relating to or in connection with this Agreement, including with respect to the formation, applicability, breach, termination, validity or enforceability thereof (a “Dispute”). If the Parties are unable to resolve a Dispute within ninety (90) days of notice of such Dispute being received by all Parties, such Dispute shall be finally settled by Binding Arbitration as defined in Section 9.2 below.  9.2.  Binding Arbitration. Any Dispute not resolved within 90 days as set forth in Section 9.1 shall be referred to and finally resolved by arbitration under the London Court of International Arbitration (LCIA) rules in effect at the time of the arbitration, except as they may be modified herein or by mutual agreement of the Parties. The number of arbitrators shall be one who shall be selected by Company. The seat, or legal place, of arbitration shall be London, England. The language to be used in the arbitral proceedings shall be English. The governing law of the Agreement shall be as set forth in Section 10.1 herein. The arbitration award shall be final and binding on the Parties (“Binding Arbitration”). The Parties undertake to carry out any award without delay and waive their right to any form of recourse insofar as such waiver can validly be made. Judgment upon the award may be entered by any court having jurisdiction thereof or having jurisdiction over the relevant Party or its assets. Company and Buyer will each pay their respective attorneys’ fees and expenses. Notwithstanding the foregoing, Company reserves the right, in its sole and exclusive discretion, to assume responsibility for any or all of the costs of the arbitration.  9.3.  No Class Arbitrations, Class Actions or Representative Actions. Any dispute arising out of or related to this Agreement is personal to Buyer and Company and will not be brought as a class arbitration, class action or any other type of representative proceeding. There will be no class arbitration or arbitration in which an individual attempts to resolve a dispute as a representative of another individual or group of individuals. Further, a dispute cannot be brought as a class or other type of representative action, whether within or outside of arbitration, or on behalf of any other individual or group of individuals. ARTICLE TEN: MISCELLANEOUS  10.1. Governing Law and Venue. This Agreement shall be governed in all respects, including as to validity, interpretation and effect, by the laws of the Cayman Islands, without giving effect to its principles or rules of conflict of laws, to the extent such principles or rules are not mandatorily applicable by statute and would permit or require the application of the laws of another jurisdiction. 10.2. Assignment. Buyer shall not assign this Agreement without the prior written consent of block.one. Any assignment or transfer in violation of this Section 10.2 will be void. Company may assign this Agreement to an affiliate. Subject to the foregoing, this Agreement, and the rights and obligations of the Parties hereunder, will be binding upon and inure to the benefit of their respective successors, assigns, heirs, executors, administrators and legal representatives.  13  10.3. Entire Agreement. This Agreement, including the exhibits attached hereto and the materials incorporated herein by reference, constitutes the entire agreement between the Parties and supersedes all prior or contemporaneous agreements and understandings, both written and oral, between the Parties with respect to the subject matter hereof, including, without limitation, any public or other statements or presentations made by any block.one Party about the EOS Tokens, the EOS.IO Software, the EOS Platform, Blockchain Tokens or any other tokens on the EOS Platform. 10.4. Severability. If any provision of this Agreement is determined by a court of competent jurisdiction to be invalid, inoperative or unenforceable for any reason, the provision shall be modified to make it valid and, to the extent possible, effectuate the original intent of the Parties as closely as possible in an acceptable manner in order that the transactions contemplated hereby be consummated as originally contemplated to the fullest extent possible. 10.5. Modification of Agreement. Company may modify this Agreement at any time by posting a revised version on the Website, available at https://eos.io/purchase_agreement. The modified terms will become effective upon posting. It is Buyer’s responsibility to check the Website regularly for modifications to this Agreement. This Agreement was last modified on the date listed at the beginning of this Agreement. 10.6. Termination of Agreement; Survival. This Agreement will terminate upon the completion of all sales in the EOS Token Distribution. Company reserves the right to terminate this Agreement, in its sole discretion, in the event that Buyer breaches this Agreement. Upon termination of this Agreement: (a) all of Buyer’s rights under this Agreement immediately terminate; (b) Buyer is not entitled to a refund of any amount paid; and (c) Articles 3, 4, 6, 7, 8, 9, and 10 will continue to apply in accordance with their terms. 10.7. No Waivers. The failure by Company to exercise or enforce any right or provision of this Agreement will not constitute a present or future waiver of such right or provision nor limit Company’s right to enforce such right or provision at a later time. All waivers by Company must be unequivocal and in writing to be effective. 10.8. No Partnership; No Agency; No Third Party Beneficiaries. Nothing in this Agreement and no action taken by the Parties shall constitute, or be deemed to constitute, a partnership, association, joint venture or other co-operative entity between the Parties. Nothing in this Agreement and no action taken by the Parties pursuant to this Agreement shall constitute, or be deemed to constitute, either Party the agent of the other Party for any purpose. No Party has, pursuant to this Agreement, any authority or power to bind or to contract in the name of the other Party. This Agreement does not create any third party beneficiary rights in any person. 10.9. Electronic Communications. Buyer agrees and acknowledges that all agreements, notices, disclosures and other communications that Company provides Buyer pursuant to this Agreement or in connection with or related to Buyer’s purchase of EOS Tokens, including this Agreement, may be provided by Company, in its sole discretion, to Buyer, in electronic form. 14  EXHIBIT A EOS TOKEN DISTRIBUTION The EOS Token Distribution will take place over 341 days starting on June 26, 2017 at 13:00 UTC. One billion (1,000,000,000) EOS Tokens will be distributed according to the schedule below: 1. 200,000,000 EOS Tokens (20% of the total amount of EOS Tokens to be distributed) will be distributed during a 5 day period beginning on June 26, 2017 at 13:00 UTC and ending on July 1, 2017 at 12:59:59 UTC. 2. 700,000,000 EOS Tokens (70% of the total amount of EOS Tokens to be distributed) will then be split evenly into 350 consecutive 23 hour periods of 2,000,000 EOS tokens each beginning on July 1, 2017 at 13:00:00 UTC. 3. 100,000,000 EOS (10% of the total amount of EOS Tokens to be distributed) will be reserved for block.one and cannot be traded or transferred on the Ethereum network. At the end of the 5 day period and at the end of each 23 hour period referred to above, the respective set number of EOS Tokens set forth above will be distributed pro rata amongst all authorized purchasers, based on the total ETH contributed during those periods, respectively, as follows: 𝑏  𝑁𝑢𝑚𝑏𝑒𝑟 𝑜𝑓 𝐸𝑂𝑆 𝑇𝑜𝑘𝑒𝑛𝑠 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑒𝑑 𝑡𝑜 𝑎𝑛 𝑎𝑢𝑡ℎ𝑜𝑟𝑖𝑧𝑒𝑑 𝑝𝑢𝑟𝑐ℎ𝑎𝑠𝑒𝑟 = 𝑎 × ( 𝑐 ) Where: a = Total ETH contributed by an authorized purchaser during the period. b = Total number of EOS Tokens available for distribution in the period. c = Total ETH contributed by all authorized purchasers during the period. As an example: 1. 20 EOS Tokens are available during a period. 2. Bob contributes 4 ETH and Alice contributes 1 ETH during the period. The period ends. 3. As a total of 5 ETH were contributed for 20 EOS Tokens during the period, 1 EOS Token will be distributed for every 0.25 ETH contributed. Therefore, Bob receives 16 EOS Tokens and Alice receives 4 EOS Tokens. If an EOS Platform adopting the EOS.IO Software is launched, the default EOS.IO Software configuration developed by block.one will lock new founders tokens distributed pursuant to such EOS Platform in a smart contract and release 10,000,000 (10%) of such tokens to block.one at the end of each one year anniversary of the genesis block over a period of 10 years. The EOS.IO Software configuration of the EOS Platform will be ultimately determined by a third party who initializes a genesis block and starts the EOS Platform.  15  White Paper 1.11.2017  Table of Contents  1 About this document  5  2 Why is insurance a candidate for decentralization?  6  3 Analysis of Basic Insurance Paradigms  7  3.1 Overview  7  3.2 Principles of insurance  8  3.2.1 Sharing the expected value of risk  9  3.2.2 Sharing the long-tail-risks  10  3.2.3 Sharing the transaction costs  10  3.2.4 Information asymmetry  11  3.2.5 Summary  11  3.3 Blockchain can help to solve issues of traditional insurance  12  3.4 Requirements and consequences of a decentralized implementation  14  3.4.1 General requirements for decentralized insurance  14  3.4.2 Requirements for token  15  3.5 Protocol  16  3.5.1 Owner of the protocol, governance  16  3.5.2 Outline of workflow elements of the protocol  17  3.6 Community of customers, users and companies  17  4 The DIP protocol token  19  4.1 Protocol & platform  19  4.2 Role of the protocol token  19  4.2.1 Example  20  4.2.2 Decomposing the value chain  21  4.3 Participants on the platform and their use of the token  22  4.3.1 Customers  22  4.3.2 Risk Model Providers and Actuaries  22  4.3.3 Data providers and oracles  23  4.3.4 Sales agents  23  4.3.5 Claims agents and Prediction markets  23  4.3.6 License providers  23  4.3.7 Product Managers, Business Developers, Application builders.  24  4.4 The DIP token: a protocol token 5 Tokenize Risk with Risk Pool Tokens  24 25  5.1 General Concept  25  5.2 Calculating the required capital  26  Page 2  5.3 Risk Management for Flight Delay Insurance  28  5.4 Target Parameters of the Risk Model  29  6 Etherisc the company  30  6.1 Business Plan  30  6.2 Team  30  6.2.1 Founders  30  6.2.2 Advisors  31  6.3 Legal & Regulatory Strategy  32  6.3.1 Legal structure  32  6.3.2 General regulatory strategy  34  6.3.3 Approach in Malta  35  6.3.4 Approach in the UK  35  6.4 Risk Management 7 Token Sale  35 36  7.1 Token Sale Structure and Timeline  36  7.2 Meeting capital requirements  36  7.3 Deduction of token sale range  39  7.4 Protection of Participants and Transparency  40  7.5 Migration of RSC Tokens  40  7.6 Token Sale contract and audits  40  8 Appendix  41  8.1 Example application to the use of an oracle in an insurance context  41  8.2 Credit Risk Model  44  Page 3  Version Control Version  Date  Author  Remarks  0.5  15.04.2017  Christoph Mussenbrock  Initial (from old WP 0.3)  0.6  04.10.2017  Christoph Mussenbrock  Rework structure, integrate Token mechanics.  0.7  16.10.2017  Christoph Mussenbrock  Rewrite, compact, extend  1.0  01.11.2017  Christoph Mussenbrock  Ready for publishing  1.01  11.07.2018  Stephan Karpischek  Fixed introduction  Acknowledgements The ideas in this paper are a collaborative result of many talks and discussions, online and in person, with some brilliant minds of the crypto-economic space. The Etherisc team would like to say “thank you” especially to the following people, who added considerable value to the draft: Ron Bernstein, Jake Brukhman, Alexander Bulkin, Alex Felix, William Mougayar, Micah Zoltu and last but not least all those members of our gitter channel, who gave valuable feedback and criticism and encouraged us to follow our path.  Page 4  1 About this document As a result of the Nov/Dec 2016 Hackathon, the team wrote a White Paper, which was released to public in its Version 0.3. Focus of the hackathon was the attempt to outline the core of a blockchain based reinsurance market. In the meantime, some things have changed and clarified. First, we have seen that the development of an open protocol will be at the core of our efforts. This leads to a much broader approach, needs different legal entities and more funding. We have consequently increased our funding goal. Second, we have seen that a reinsurance market will most probably not be the first step. Trading of risks (and tokenization of risks as a precondition) is a different regulatory area and probably more difficult to achieve; furthermore, without first establishing a regular insurance business we don’t have suitable risks to trade. Third, we have seen that we need to apply for an insurance license in some relevant jurisdictions, namely in the DACH area (Germany, Austria, Switzerland) or in the European Union (Malta). Additionally we also try to establish an insurance business in the US. These three cornerstones don’t reflect in the former White Paper, which was centered around the tokenization of risks. This White Paper is the attempt to structure the overall picture of decentralized insurance along the new insights.  Page 5  2 Why is insurance a candidate for decentralization? The multi-trillion dollar insurance industry is dominated by huge corporations, weighed down by heavy regulation and plagued by misalignments of company and consumer incentives. The insurance world has devolved into an inefficient, expensive and ultimately frustrating industry. When customers most need help, they can end up fighting in vain for reimbursement from companies whose profits too often depend on avoiding paying out. Etherisc is building a platform for decentralized insurance applications. With visionaries like you, we can create a platform full of opportunities across the industry’s value chain. Corporates, large and small, not-for-profit groups and insurtech startups can all come together to provide better products and services. We aim to use blockchain technology to help make the purchase and sale of insurance more efficient, enable lower operational costs, provide greater transparency into the industry and democratize access to reinsurance. Blockchain can provide the means to disintermediate the market with a peer-to-peer risk platform that helps insurance return to its roots as society’s safety net. We even envisage new groups building their own bespoke insurance risk pools and services on the platform. And Etherisc will be a fully-compliant, fully licensed insurance platform for the emerging blockchain economy. In short, Etherisc can deliver the insurance industry the modernization customers are crying out for. We have assembled an award-winning team of experts, experienced in delivering innovative products. We have already demonstrated the use-case for decentralized insurance applications with a successful flight-delay DApp that debuted at one of blockchain’s biggest international conferences. This was the first insurance product live on a public blockchain. With your support, we can now build out our open-access platform and help make one of the globe’s biggest industries finally work the way it should – for everyone, everywhere.  Page 6  3 Analysis of Basic Insurance Paradigms 3.1 Overview Since our early prototype in September 2016, we analysed the basic principles of insurance and developed a token system on top of these principles, which is sustainable and sound (later in this paper we will describe which kind of tokens we consider to be “sustainable” and “sound”). First, we analyze insurance and and break costs and capital flows down into three elements: 1. Expected value of risk 2. Capital costs for long tail risks 3. Transaction costs We show that the first isn’t a source of profit, because it is only a redistribution of capital corresponding to sharing risks among the participants. The second are a source of fixed income, at a certain risk. Capital has to be locked for a certain period in time, and there is a potential risk of losing the capital provided, e.g. in the case of a rare but catastrophic event, also known as “black swan event”. Capital providers are compensated for this risk. This compensation is calculated based on the lock-up time and on the risk of what is being insured. The third are a source of entrepreneurial revenue and increase with higher efficiency of the business processes. We argue that today insurance companies are the predominant way to organize these elements and that blockchain technology provides an opportunity to replace insurance firms by decentralized structures using a standardized protocol. Capital and revenue streams can then be represented by tokens. Our conclusion from this analysis is that we need two types of tokens. The first one supports the coordination and economical incentivization of actors in a decentralized insurance system. This is the token to be discussed for a token sale to fund the development of a protocol and platform for decentralized insurance. We call these “protocol tokens”. The second type of token represents risks - this type will come as a collection of similar tokens, one for each risk pool, we call those “risk pool tokens”. These “risk pool tokens”  Page 7  will be discussed in a separate document, as they underlie a different economic dynamic.1 In a distributed environment with many participants, building products as a collaborative effort, the protocol token serves as glue, as collateral, and as representation of the material and immaterial value of the network, much as Ether serves as a means to secure the stability of the Ethereum Blockchain. In Chapter 3, we detail the DIP protocol token. Chapter 4 shows a concrete example of the use of the token in an insurance context.  3.2 Principles of insurance Lots of literature has been written on the theory of insurance, but the basic principles are simple. Let's start with an example . The example is of course simplified, and serves the sole purpose to explain the principle. We consider a homeowners insurance. Insurance is about probabilities of losses, so it would be interesting to see what the probability of a damage is. A homeowners insurance typically covers a number of perils, including fire, natural disasters, water, and even falling objects2. But it is difficult to obtain real numbers, as insurance companies are not very transparent with their fundamental data3. We will assume that, for our example, the probability is 0.1%. For our fictional example, let’s assume insurance had not been invented yet. In this fictional world, Alice owns a house. The house is worth $100K. The probability of a complete disaster is 0.1% per year (that is one devastating event in 1,000 years). Alice wants to ensure that she has access to enough funds to get a new house in the case of a disaster. So she decides to get a loan of $100K and has to pay redemption (also called principal) and interest rate. Additionally, she pays an interest rate of maybe 1%, so she has yearly costs of $1,100 ($100,000 loan * 1% interest rate plus $100 annual redemption = $1100.00). Now we show how pooling risks in an insurance scheme reduces these costs drastically.  In ​https://www.etherisc.com/whitepaper​, we already described a possible implementation of a “risk pool token”, which aggregate similar risks and which can be sold and traded to provide the necessary funds to cover ​“long-tail-risks”​. 2 ​Allstate.com: What Perils Are Typically Covered By A Homeowners Insurance Policy? 3 A quick market survey in Germany shows that you get a homeowners insurance for considerably less than 0.1% of the value. For simplicity, we’ll assume that the premium is 0.1% plain and we don’t take insurance taxes etc. into account. From the relation premium/value, we can easily estimate an upper bound for the probability. One of the most fundamental principles of insurance is that the expected losses should not surpass the collected premiums (“Risk loading” - cf. ​http://www.wiley.com/legacy/wileychi/eoas/pdfs/TAP027-.pdf​). The expected losses are - simplified - number of policies multiplied with the probability of loss multiplied with the loss (which is equal to the value), and collected premiums are number of policies multiplied with premium per policy. It follows that the probability can be approximated by premium/value, which is lower than 0.1% in our market test. 1  Page 8  3.2.1 Sharing the expected value of risk Assume 100,000 homeowners are coming together in a pool. Again, everybody pays a $100 share; this amount is now called the “premium”. They collect a total of $10,000,000 in premiums. But now there is a difference to Alice, who takes care only for herself: because of the law of large numbers4, with a very high probability there will only be about 100 fires, causing a damage of about $10,000,000! And because the sum of all premiums is also $10,000,000, the whole damage can be paid out of the collected premiums, there is no need for every house owner to take on a loan. (Because premiums are collected at the beginning of the year, and all the houses “expected” to burn don’t all burn at the beginning of the year, but more or less are equally distributed over the year(s), there is a so called “float”5 of liquidity which can also generate a significant revenue. For simplicity, we won’t focus on this effect in this paper. So the costs for each single house owner are now reduced from $1,100 to $100! This difference asks for an economical explanation. Let’s have a closer look into it. First of all, if all house owners would follow Alice’s example, they would need a huge amount of loans, from which only a tiny part of 0.1% would been needed in the average. It is clear that providing unused liquidity is costly. Pooling of risks in an insurance optimizes the use of capital, and the participants benefit from the reduced costs, not to speak from the difficulties to obtain a loan without collateralization! Second, if everybody only cares for himself, only a tiny fraction of participants are struck by disaster, and have the burden of actually paying back their loan. The others can pay back without loss, as soon as they don’t need protection. In an insurance collective, we have solidarity: with the premiums, everybody pays for the damages of the others. To summarize, the risk pool offers three advantages for the participants: 1. Building a large liquidity pool. 2. Guaranteed access to this liquidity in case of a damage. 3. Mutual subsidizing of damages. Such a pool may be designed solely to benefit its’ participants, and to not make any “profit”. If the pool did generate profits, these profits could be distributed back to the participants, effectively reducing the premiums again to a level where no profits are generated. Such an insurance would have a loss ratio of 100%, because all premiums are used to pay the losses. This is the very basic effect of risk transfer in insurance. Please note that the effect increases with the pool size. But still, this is not the whole story.  4 5  https://en.wikipedia.org/wiki/Law_of_large_numbers http://www.npr.org/sections/money/2010/03/warren_buffett_explains_the_ge.html Page 9  3.2.2 Sharing the long-tail-risks In some years, there are more fires, in other years, less. To account for these variations in damages, the whole pool has again to raise some money, e.g. $10M, to cover the unlikely event of a burst of many fires in one particular year. And let’s suppose that the interest rate for this capital is even particularly high, e.g 20%. We will have total costs for this capital of $2M. The interest rate for the capital is a function of the risk and the riskless interest rate on the capital market; in an efficient market, the interest rate will compensate for the higher risk in comparison with a risk-free investment and will also contain a fair profit. So basically, this is where profits are generated for providing capital in an insurance structure. The overall costs of $2M are distributed among all house owners, yielding an additional cost of $20 per house owner per year, which is added to the premium. So after this, there is also a protection against “long tail risks” or “black swan events”, at a cost of $20 per house owner. Again, the risk diversification effect increases with the pool size. Overall, participants now pay $120 per year for their house insurance. The loss ratio is now reduced to 83% because of the capital costs of protecting the long tail risks6.  3.2.3 Sharing the transaction costs To organize 100,000 people in a pool, a professional structure is needed, otherwise, every single participant would have to talk to every other, which would simply be impossible. The operation of this professional structure adds transaction costs to the premium. This is the reason why insurance companies have come into existence: They provide a way to decrease transaction costs for the participants of the pool, creating an economy of scale and coordinating a huge number of participants and employees7. The effect is considerable and enables the modern form of insurance with huge customer bases and a capitalization which can cover even global catastrophic events like hurricanes and earthquakes. However, the remaining transaction cost are still considerable: a recent study by KPMG shows the impact on the loss ratio, which is about 66% in the average8.  6  $100 for covering the risk against $120 premium => 100/120 loss ratio = 83% The downside of this is the fact that inefficiencies tend to hide in the organization. The bigger the organization, the lesser people are doing the real work (people at the “rim” of the organization) and the more people are needed in the center to organize the people at the rim (the “management”). Furthermore, to limit internal inefficiencies, companies need a plethora of control mechanisms (that’s the old style) or complicated incentive systems (that’s the more modern way). 7  8  https://assets.kpmg.com/content/dam/kpmg/au/pdf/2016/general-insurance-industry-review-2016.pd f Page 10  3.2.4 Information asymmetry Together with the reduction of transaction costs comes an asymmetry of information, which leads to a further increase of costs and to incredible profits for the big insurance companies. The unbounded collection of customer data and the exclusive exploitation of this data is a consequence of this imbalanced relationship. It creates an “unfair competitive advantage” for existing companies: companies with big data vaults can offer better products, and thus further optimize their data base. One of the core goals of a decentralized insurance platform is the disruption of this circle, giving back to customers the ownership of their data.  3.2.5 Summary The three elements described above; pooling or risk, risk transfer, and efficient administration are necessary. You can’t have insurance without each of them. For the purposes of this paper, I will call them: 1. expected value of the risk 2. capital costs for long tail risks 3. transaction costs As we have seen, a community may not wish to generate profit from the first element. The second element yields a risk fee for binding capital which depends on the structure of the particular risk: It is typically lower if the risks are granular and uncorrelated; it is typically higher if the risks are clustered or correlated. The third one depends on the complexity of the process. A simple and highly standardized insurance “product” has a smaller transaction complexity than a more complicated, non-standardized product. This will reflect in lower transaction costs. The three elements are completely independent of the underlying technology, economic environment or currencies. They are the atomic building blocks of every risk-sharing system9. As an additional aspect we have seen the information asymmetry which is inherent in the traditional insurance systems, and which is undesirable. The distribution of expected value (element 1) and capital costs for long-tail-risks among participants (element 2) is inevitable and not specific for a blockchain solution. Therefore, let’s focus on the third element.  9  There is a fourth element - reinsurance. The purpose of reinsurance is to reduce the cost of risk diversification by categorizing and securitizing different risks. Reinsurance and “wholesale” risk transfer enabled by reinsurance adds another layer of complexity, and therefore we won’t discuss reinsurance in this paper. Page 11  Blockchain is essentially - among other aspects - a way to solve the transaction cost problem without firms. Without the “design pattern”10 of firms, transaction costs are subject to combinatorial explosion. The coordination costs for n participants are roughly of Order O(n​2​) and firms reduce this to O(n). Because of this huge gain in efficiency, firms have many ways to hide profits in the transaction costs, and on the other side internal inefficiencies don’t show up fast. Transaction costs also appear in another context: regulations, which are deemed necessary to protect customers in a context with built in conflicts of interest. Regulations form a very effective “competitor” barrier to entry. While insurance companies often complain about the burdens of regulations, they actually don’t have much interest in reducing these burdens, as they discourage new competitors from entering the market.  3.3 Blockchain can help to solve issues of traditional insurance While the current insurance business has evolved over centuries, and is optimized in many aspects, we have seen that it has severe shortcomings to the disadvantage of customers. We will outline some properties of an alternative system, which remedies these shortcomings. First, an alternative system should of course offer the basic ingredients of any insurance system: covering expected losses, covering long tail risks, and covering of necessary transaction costs. Obviously, we need ways to capitalize such a system, and we need a system to reduce transaction costs to a minimum. Transaction costs cannot be eliminated completely. But open markets have proven to be a solution for these challenges, and therefore, we propose a market-based approach with two components: - an open marketplace for capitalization of risks - an open marketplace for insurance related services This is where blockchain comes into play: a decentralized solution on blockchain can implement such open marketplaces in a way that is collusion resistant and has no single points of failure. We can watch the emergence of many such marketplaces for different domains, like computation, file storage, exchange of assets; and insurance is just another domain in this respect. More specific, blockchain can help to solve four main problems which pile up costs in traditional insurance companies: 1. 2. 3. 4.  Coordination (“managerial”) costs. Conflict of interest between customers and company. Information asymmetry between customers and company. Access to risk pools  Advantage 1.​ In traditional firms, you have two types of employees: the first group is doing the actual work, the second group is coordinating the whole system. The larger a 10  On the importance of design patterns, see also: Design Patterns, by the “Gang of Four” https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusa ble-Object-Oriented-Software/PGM14333.html# Page 12  company grows, the more energy flows in the second group (like a circle, the first group forms the rim of the circle, the second the area; the larger the circle, the less efficient are the processes, and the more energy flows into the coordination of the coordinators). Blockchain can help reduce these coordination costs. Instead of a posse of managers, “smart contracts”11 can act as trustless hubs between the agents at the rim of the system, and thus eliminating most of the costs and the inefficiency of the management. Advantage 2.​ In a traditional insurance company, the company “owns” the whole process, including the tasks which tend to raise conflicts of interest between customer and company. An obvious example is claims management: The claims manager has the explicit goal of minimizing payouts for damages, because he is employee of the insurance provider! Of course there is a guild of “independent” appraisers and experts, but who pays their bills? Blockchain can solve this conflict of interest, by enabling truly independent experts (who for example may be publicly ranked by their reputation for efficiency or fairness), and whose work is independent of the insurance provider, as well as being transparent and auditable by the whole community. The same is valid for another area, where the conflict of interest is (intentionally) not obvious; consider Product Design. An insurance company has a big advantage over customers, because they can design products in a way which perhaps unfairly maximizes revenues (sales) and minimizes payouts (expenses). For example if a customer expects a payout from an insurance policy they bought for a particular “event” but the insurance company does not provide the payout because the company maintains that the policy bought doesn't actually cover that “event”, the customer experience is severely degraded and trust is eroded between consumers and insurance providers. Advantage 3.​ Information asymmetry is in itself a source of inefficiency and high transaction costs. Insurance companies gather data and information in huge private silos by proprietary means and the data is often not shared., This data and the companies’ experience in analyzing the data is considered one of the main differentiators in the market. The reasoning behind decisions made based on this data is opaque and difficult to challenge. In a blockchain environment, all fundamental data and the decisions based on the data can be transparent and objectively validated.  11  Some blockchains like Ethereum (which we use) enables programs (called “smart contracts”) that are un-censorable, immutable, and permanent. These smart contracts can interact with each other to perform a wide variety of actions, including financial and escrow transactions. This makes possible direct and transparent interactions between two parties who may be and may remain anonymous, that previously required a third-party intermediary to be effective. The term was originally coined by Nick Szabo, but in a slightly different meaning. Note: The above definition was thankfully supplied by Ron Bernstein, who was not successful in finding the original author - please contact us if you are the author. Page 13  Advantage 4.​ The risk pools of traditional insurances are attractive investment vehicles, but currently, they are not open to the public, and the profits generated benefit only a small circle of investors. Blockchain can democratize the access to similar instruments, by tokenizing risks with “Risk Pool Tokens”, see our ​2016 whitepaper​ for details. We will consider issuing of such tokens at a later point of time.  3.4 Requirements and consequences of a decentralized implementation To offer an alternative to traditional company-centric insurance systems, we can identify some requirements and consequences for implementing a decentralized insurance protocol.  3.4.1 General requirements for decentralized insurance 1. We need a protocol and not just an (decentralized) application. Insurance is way too complex to be covered by a single application, and needs some means to incentivize participants to use it. Fostering “Network Effects”12 is one such mean and can lead to a sustainable and growing user base. While a single contract can handle a single product, this singularity will not generate the network effects which are desirable to form multiple large pools of similar risks needed to get the benefits of the “law of large numbers” working. Decentralized insurance will work only if the value chain is decomposed and there is a way different participants can cooperate on the process in an interoperable way. A standardized protocol defines this way. The (architecture of the) protocol is the sole “central” part in the model. 2. A decentralized insurance protocol can replace “the firm”, by implementing a standardized set of rules for how stakeholders in the system interact with smart contracts and with each other using the protocol. By this, most of the coordination costs are replaced by autonomous and automated contracts and procedures and enforce efficiency by open market mechanisms. At the same time, a protocol does not impose a fixed set of code to the participants, but allows for flexible extension and interpretation of the basic rules. 3. The development and operation of a protocol needs funding. Even if we can drastically reduce the coordination costs, there are still the costs for the initiation of the system - e.g. acquisition of licenses, development of smart contracts, audits, as well as costs for agents at the “rim” of the system which we cannot eliminate completely. Therefore we need a way to collect these costs from the ultimate customers and distribute them amongst these agents.  ​Network effect​ is described as the effect that one user of a good or service has on the value of that product to other people. The classical example is telephone: the more people use it, the more valuable the telephone is for all. 12  Page 14  4. We also need a way to calculate and distribute the expected value of the risk and the capital costs for covering long tail risks amongst the customers.  3.4.2 Requirements for token 1. Tokenization may be the solution for these requirements - but only if the token is intrinsically required for the protocol to operate efficiently; i.e. “baked into” the protocol itself and u ​ sage of the protocol is only possible via tokens. If the token were not intrinsic to the use of the platform, then some new actor could replicate the protocol except without the token, and migrate users to the new protocol without the friction of a purely "rent seeking" token. 2. Protocol tokens: For the d ​ istribution of the transaction costs​ we need a different type of token. This token has to be designed in a way that incentivizes the use and the efficiency of the protocol: the revenue associated with this token or its price should increase with the efficiency and use of the underlying processes. In the next chapter, we describe a proposal for a token with these properties. 3. Risk Pool tokens: For the distribution of the e​ xpected value​, and for the ​distribution of the capital costs for covering long-tail-risks​, we need a type of token which generates a foreseeable profit. The profit solely depends on the underlying risk structure, the number of risks, their correlation, and so on. The value therefore depends only on the knowledge of the risk parameters (which can be incomplete) and mathematics. These tokens will e.g. yield a fixed revenue or generate an equivalent rise in price for their owners (which is equivalent). This type of token will be implemented in a second step. 4. Now that we have elaborated the necessary token types, we can backtest if these tokens are “necessary” Etherisc will build an economic space for decentralized insurance The space will have a broad set of participants, customers, service providers, risk carriers, etc., the goal is to incentivize these participants to cooperate and behave well, and in line with the interests of the whole space. This space is difficult to build. It comes at a cost.  What adds value to the space:  Page 15  Building Block  Consists of  Resistance against forks & copycats  Licenses  Formal approval by authorities  Cannot be copied  Operational Model  Infrastructure to run a business  Cannot be copied  Products  Code (Frontend/backend) infrastructure Certifications/Audits Developers Product managers  Tech can be copied, but products are micro-ecosystems with development roadmap, user base, customer support, core development team, supporters and contributors  Users  Customers Supporters Contributors  Cannot be copied  Network  Formal or informal Relations to other projects,  Cannot be copied. Relations to other projects are based on common vision.  5. Conclusion: only tech can be copied easily. Most of the value-bearing components of the economical space (the value that participants bring) can’t be copied easily The economical space will offer opportunities to generate profits. These profits should benefit those who have participated to build up the space, and they will expect the platform to protect their participation. Reason: If you have two identical platforms, one platform with some kind of protection mechanism for its creators and contributors and one platform without such protection. The platform with protection will of course attract more contributors. It will have the stronger network effects. A platform without protection is subject to the “Tragedy of the Commons” In the prospect of decentralized exchanges the use of a token is no longer a barrier.  3.5 Protocol 3.5.1 Owner of the protocol, governance As an open standard, the protocol will be a common good, it can be used and implemented by whoever likes it. We will take care that the entry barriers are as low as possible. However, for some portions of the protocol a certification will be necessary, to reflect regulatory obligations and restrictions.  Page 16  We propose a swiss based foundation as legal body, which formally holds the IP rights of the protocol and ensures that the protocol can be used freely. We will establish a continuous, community driven protocol improvement process similar to the EIP process for the Ethereum Platform.  3.5.2 Outline of workflow elements of the protocol            Application for policy Process of offering a product and applying Underwriting Process of accepting a policy Collection of premiums Payment process, one-time and regular payments Submitting of claims Process of submitting a claim, via oracle or manually Claims assessment Process of assessing a claim, via oracle or manually. A claims verification process allows the system to determine which policies are legitimately claimed and to propagate agreed payments to claimants. In the case of parametric insurance, this process references data feeds about insurable events and is (fully) automated. Identity Management & Privacy Process of KYC and AML, respecting privacy. This may involve private chains or off-chain storage of data. Admission / Certification Admission of participants to offer products and perform parts of the protocol Asset Management As funds flow in, we have to responsible use funds which are not immediately needed.  3.6 Community of customers, users and companies The success of the platform will depend of a vivid community of users and companies. The token model should reflect and support this community. This community will play a central role in the realignment of incentives. Via tokens, customers can “own” their insurance. The community model should facilitate the development of future mutuals and P2P-Insurance models. A community cannot be build from the outside, it has to grow from the inside. However, experience shows that there are some success criteria for communities. Famous open source pioneer Pieter Hintjens, h ​ ttp://hintjens.com/blog:10​ has drafted some which we consider to be helpful for an in-depth discussion:   Quality of mission A community can only grow pursuing a worthwhile goal. The goal must be super-individual and  Page 17         Freedom of access. The community should not have barriers or walls, it should welcome those of good will and encourage participation. Well-written rules. If rules are necessary, they should be carefully written and obvious. Strong neutral authority. To resolve conflicts, a strong but neutral authority should be in place, which can also be incorporated by some kind of governance mechanism. Proportional ownership. "You own what you make"  Page 18  4 The DIP protocol token The proposed protocol token is an integral part of a decentralized insurance platform. It will have some desirable properties: It will not introduce additional fees. The usage of the token is free, and owners of a token do not receive a revenue from the use of the platform. However, participants can use the token in their profit-seeking enterprises. Therefore, there is no incentive to fork the protocol, as you can’t save costs by doing so.  4.1 Protocol & platform The Protocol is a collection of Smart Contract Templates, Rulebooks, Standards, Best Practices which are developed and maintained by the community. There are many possible governance schemes for such a protocol; we intentionally don’t make a prejudice on which model should be chosen, this will be part of the protocol development. The governance should fit to the participants using it. Of course, meanwhile blockchain offers some interesting tools to formalize governance, but that should be left to developers and users. The platform is the community of all entities which make use of the protocol, and which are connected by a common economic interest. Providing insurance is a complex process, involving possibly many participants, as we have seen above.  Customers of an insurance need to rely on the smooth operation of these participants.  Fees have to be distributed along the value chain, but only if all parts of a process have been appropriately fulfilled  Participants supplying critical parts (e.g. a risk model) have to assume liability for their work.  Some services are needed by many participants, so it makes sense to offer them as shared services. The platform will serve as marketplace for insurance-related services, which are offered according to the open protocol standard and which are therefore always interoperable. Protocol relates to platform like chess rules to board & figures. For clarity, we will use the term “protocol” exclusively.  4.2 Role of the protocol token To make long story short, we need some strong economic principles to ensure the proper working of all participants and their cooperative, mutually supportive behaviour. Therefore, we have designed the protocol tokens to bind participants to the platform and to assure the quality of the provided services. We are effectively implementing what is known as "Staking", focusing on the specifics of the risk transfer.  Page 19  4.2.1 Example Traditionally, economic relations are coded in form of legal contracts, which often have the form of “if-then” statements. “If you pay me $5.000, I’ll sell you my car”. In business contexts, we often have long-term contracts, like supply contracts or contracts for work and labor. These provide either a reward for a delivered good or service, a penalty for not delivering, or both. In our FlightDelay Insurance, we also have such a contract: we use oraclize to obtain provable data from our data provider, flight stats. Oraclize charges us with some finneys for calling their contract, but we have no guarantee that Oraclize will deliver. We have two options to incentivize Oraclize to provide their service properly: 1. in a “buyers market” (i.e. a market with many competing oracles) we could demand Oraclize to put some tokens in a “staking contract”, which will return the tokens if they deliver in time and forward the tokens to us in case they miss their obligations. 2. in a “sellers market” (i.e. a market with only one or few oracles) we can offer Oraclize an additional profit, again by staking tokens in a “staking contract”, but with reversed roles: Etherisc will stake tokens, and Oraclize will earn these tokens if they deliver, and in case they don’t deliver, the tokens are returned to us. 3. Of course, both options can be combined: both parties staking tokens, and the contractor earning tokens according to his performance. The “staking contract” is very simple, it’s signature is contract TokenStake { [...] function stakeFor(address _staker, uint256 _value) public returns (bool); function stake(uint256 _value) public returns (bool); function releaseFor(address _beneficiary, uint _value) internal returns (bool); function release(uint _value) internal returns (bool); }  The ​stakeFor​ and s ​ take​ functions put some tokens in the contract where they are locked, until some predefined condition meets, in which case ​releaseFor​ or release are called and return the tokens to the respective ​beneficiary​:  Page 20  Example use cases for s ​ take​, ​stakeFor​, ​release​, ​releaseFor​: 1. “​stake​”: A contractor stakes tokens as collateral for providing a service at a certain quality / service level. 2. “​stakeFor​”: A commissioner stakes tokens as reward for contractor, for providing a service at a certain quality / service level. 3. “​release​”: Tokens are released to staker in case the condition is fulfilled / quality is proven / service level met. 4. “​releaseFor​”: Tokens are returned to commissioner or “slashed” in case quality is not proven / service level not met.  4.2.2 Decomposing the value chain Similar, a bunch of other contractual relationships could be modeled - each with a variant for “buyers” or “sellers” markets. . Thus, we would like to decompose the value chain as far as possible and to engage market mechanisms to select those participants which offer a service at the best value. The illustration shows the roles typically found in an insurance value chain, and which roles are needed for a particular step in the chain:  Basically, you can separate each role as an independent business, which can work together flexibly and bind themselves via reward-or-stake token contracts. This is quite similar to the operating mode of a blockchain: Miners have an economic incentive for cooperative behaviour. Some aspects of “good-behavior” comprise stability properties like:  Promise to offer a certain service over a certain time (service stability)  Promise to offer a certain service in a certain quality / with a certain SLA (quality stability)  Promise to offer a certain service at a certain price (price stability)  Promise to take a certain liability for a service (guarantees) We propose to secure the platform and the products built on that platform via the protocol token. Participants (not customers) will need a certain amount of tokens to enter the platform “ecosystem”. These tokens can be locked as collateral or offered as a  Page 21  reward. Depending on the service offered, a different number of tokens will be required or offered to use the platform or provide services on the platform.. Simple services require a small number of tokens, complex or critical services will require a higher number of tokens. The amount of tokens which have to be provided as collateral or reward will correlate to the potential damage from participant misbehavior or from the violation of the platform terms. These parameters may be subject to a platform governance model (in the future) where participants have voting power based upon tokens owned. Or, governance may be conducted automatically by the use of smart contracts. The proceeds from token sale(s) are used to nurture the development of the platform and to establish or provide central services as long as there is no independent participant providing them. A certain insurance product needs a collection of services chained together to some business process. Participants offering these services can organize to offer such a product (maybe there is a market for such services and a “product management service” doing the coordination work). It is even possible that the fees for some of the services offered by participants in the ecosystem may be negotiated on an open-market platform. The protocol will offer ways to distribute the premium to the various risk pools and to the participants who provide product “processing”.  4.3 Participants on the platform and their use of the token 4.3.1 Customers Customers can buy insurance using the token. For convenience, third parties can offer payment gateways and integrations which remove the necessity to own cryptocurrency from the end customer. Furthermore, participants can choose to offer insurance products in any native currency - be it a cryptocurrency, a token or a fiat currency. Use of token: Universal currency to buy insurance products.  4.3.2 Risk Model Providers and Actuaries Risk models are fundamental for any insurance product. The correctness of the model is precondition for the economic success of the product. With great impact comes great responsibility. Generally, because of the magnitude of value affected by errors and deviations in the model, a Risk Model Provider won’t take responsibility for the economic outcome of his model, but rather for his adherence to principles and established guidelines in his trade. E.g. a risk model should be built on a clear specification, and it should be validated by acknowledged testing methods before it is put into production. A risk model provider should therefore be rewarded according to such benchmarks. The economic risk of a model will usually stay with the party who runs the risk pool. Use of token: Staking/Reward for providing or updating risk models.  Page 22  4.3.3 Data providers and oracles One of the most promising application of a decentralized insurance space is the way data is collected and managed. Customer data should remain in the control of the customer, and blockchain technology offers new ways of monetization of data. Currently, data is collected together with the application for an insurance, and the insurance company “owns” the data - even after the insurance contract is no longer valid. In a blockchain decentralized environment, the collection of data could be separated. Customers could get paid for voluntarily offering their data to a data pool, which in turn can sell this to interested parties, leaving the ownership of the data completely with the customer. Like a certificate in Keybase.io, data could be revoked at any time. For the area of parametric insurance, oracles act as gateways to the physical world, providing provable and reliable ways to transfer data to smart contracts. Use of token: Reward for giving data. Reward for giving access to data pools. Staking / Reward for providing reliable oracles.  4.3.4 Sales agents Decentralized distribution will emerge as the blockchain space emerges as a whole. Sales agents can offer insurance products to business or end customers, receiving a share of the profit. The token can be used as a means to distribute revenue and profits among all parties involved in the production of a specific insurance product. Use of token: Reward for distribution of products; means for distribution of revenue & profits.  4.3.5 Claims agents and Prediction markets While the area of parametric insurance is rapidly growing in the ascent of IoT and the explosion of available data, there will remain many cases where an automatic detection and processing of claims is not possible, e.g. because the derivation of the loss from the event is to complicated or dependent on manual assessment. Specialized and sometimes independent claims agents already exists e.g. in the area of car insurance, where they help insurers to process claims in shorter time. These claims agents can immediately use a decentralized platform, as soon as adequate products are available. For other use cases, prediction markets could be used to decide on certain relevant triggers for specialized insurance contracts, like cat bonds13. Use of token: Reward for the provided service. Incentive to start bids on a prediction market.  4.3.6 License providers For the foreseeable future, insurance in most developed countries will depend on a proper license, which can be very difficult and costly to obtain. In some countries 13  https://en.wikipedia.org/wiki/Catastrophe_bond Page 23  however, specialized companies offer “Protected Cell Company” model. In such a model, a license provider acts as an intermediary to regulators, bundling capital of many smaller projects to meet the minimum capital requirements. Use of tokens: Staking tokens to provide capital for a license provider, paying fees for licenses.  4.3.7 Product Managers, Business Developers, Application builders. Product managers and Business Developers scan the market for opportunities and orchestrate the necessary participants needed to build the actual product. Application builders can offer single products or even complete development kits where you can “your own insurance product” from some predefined templates. Use of token: Reward for the provided service, fee for using application.  4.4 The DIP token: a protocol token Etherisc is building a platform for decentralized insurance applications. Corporates, large and small, not-for-profit groups and insurtech startups can all come together to provide better products and services across the whole insurance value chain. We aim to use blockchain technology to help make the purchase and sale of insurance more efficient; enable lower operational costs; provide greater transparency into the industry compared to traditional operations; and democratize access to reinsurance instruments. Blockchain can provide the means to disintermediate the market with a peer-to-peer risk platform that helps insurance return to its roots as society’s safety net. We even envisage new groups building their own bespoke insurance risk pools and services on the platform. And Etherisc will be a fully-compliant, fully licensed insurance platform for the emerging blockchain economy. The DIP token is a protocol token. A protocol token (also known as an ‘Appcoin’ or ‘coin’) is an electronic asset that underlies a network. Tokens perform all kinds of functions depending on the network or platform they back, i.e. users use filecoin to store files on a distributed file-storage network and entities with open hard drive space earn filecoin for storing files. Tokens are an exciting new way to incentivize distributed networks and many uses have yet to be invented!14 The DIP token is the building block for the emerging decentralized insurance economy on blockchain. Etherisc builds a decentralized insurance network which does not rely on an oligarchy of big parties, which control most of the business, like in the traditional insurance business. Instead, many participants can collaborate on new insurance products. Cooperation is welcome, also competition; but there won’t be artificial moats and barriers protecting some big players. The notorious market entry barriers like high capital requirements and regulatory obligations are removed. The DIP token enables the economy on this network: participants in the decentralized insurance network cooperate in building insurance products. With the token, you can: 14  From the CoinList FAQ: What is a protocol token? Page 24   stake tokens as collateral  buy insurance products  interact with other participants to build decentralized insurance products  pay the necessary fees and capitalization to obtain insurance licenses  incentivize quality and proper behavior  distribute revenues and profits among participants  reward the provision of data by customers and participants  pay oracles and prediction markets to resolve claims The token itself does not add noteworthy friction to the network. The token does not generate revenue or profit. The use of the protocol itself is free, the protocol open source, this is guaranteed by a swiss-based not-for-profit foundation, the DI Foundation. In a second step, separate “Risk Pool Tokens” will enable the capitalization of risk pools for specific insurance risks and will provide new instruments, which will further drive the adoption of the platform.  5 Tokenize Risk with Risk Pool Tokens We propose a token model which enables participants to buy and trade the “long-tail” risks of a decentralized insurance portfolio and to gain exposure to its revenue as an income stream. Together with the consumer-facing insurance application, this forms a complete and fully functioning “trustless” insurance system on the blockchain. These risk tokens can be traded on an an end-to-end automated insurance and reinsurance marketplace. This platform will require no human intervention, and will be highly transparent to both customer and participant sides of the marketplace. We expect these tokens to be true “securities”, because they will generate profits which are directly related to the managerial efforts of the creator, who provides the risk model for these tokens. Due to the significant complexity of regulations we will build such a token system as a second step after establishing the core operational insurance business. Nevertheless, we will give an outline of such a token system in the next sections.  5.1 General Concept Conceptually, the platform has several components: 1. A r​ isk pool​, which holds a certain amount of reserve collateral used to issue and underwrite insurance policies against a predefined set of insurable events, within the framework of an ​insurance model​. 2. A r​ einsurance pool​, which holds extra collateral and reinsures the risk pool against catastrophic long-tail events which unexpectedly deplete the risk pool or render it unable to issue additional insurance. 3. A ​ risk management system​, which is a set of rules that governs the issuance, supply, inflation, and deflation of a digital token. For the FlightDelay Dapp (FDD)  Page 25  we suggest to name the token RSC-FDD. Tokens are sold to collateralize the reinsurance pool and entitle holders to dividends from the risk pool's revenue stream. 4. A t​ oken marketplace​, which allows participants to purchase and redeem tokens at economically fair and transparently calculated prices. Under normal operation, the reinsurance pool holds a non-zero amount of collateral. The system is designed to constrict the total amount of risk underwritten to an amount no greater than the amount of collateral held by the reinsurance pool. At the outset, the reinsurance collateral is gathered through an offering of an initial fixed supply of RSC-FDD tokens (a crowdsale), and thus the upper bound of the number of policies that can be underwritten with 100% collateral backing is established. The system can be tuned toward a desired m ​ aximum liability​ level where the total risk of the insurance portfolio is capped considerate of market forces. In turn, the risk pool automatically underwrites policies until this upper bound of policies is reached, and then ceases to underwrite policies. This is intended to ensure that every insurance policy is 100% collateralized and no customer can lose a payout to which she is entitled. (If this upper bound is reached but there is further demand for policies, the system's maximum liability parameter can be adjusted higher, and the system will automatically issue and sell tokens to support new policies with minimal dilution.) Also note that a $1M capitalization of the reinsurance pool will support a vastly larger throughput of policies than will likely be required in the early stages of the project. To support normal operation, a minimal collateral reserve is required to be held in the risk pool, and this value is determined by the insurance model. Insurance premiums are calculated as a function of this required collateral, the insurable event in question, and the desired payout for the policy at claim time. The exact calculation is specific to the model, but note that the risk pool is able to subsidize premiums by reserving excess collateral through a variety of means, such as seeding the pool with initial auxiliary capital or retaining revenue in the risk pool. At the time a customer purchases a premium, a 5-10% fixed fee will be assessed on the premium and allocated toward operational costs.  5.2 Calculating the required capital The primary concern of any insurance model is to calculate the reserve capital required to guarantee solvency of the risk pool to some arbitrary and high confidence level, such as 99.99%.15 Under normal circumstances this results in an automated system where risk is shared among policy holders. Since the actual collateralization of the risk pool is Note that in many jurisdictions, regulations require lower confidence levels. For instance, in the EU Solvency II requires a “a confidence level of 99.5% over a one-year period”. See http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32009L0138​ Article 101. 15  Page 26  usually higher than the actual number of claims that must be paid, the risk pool has a positive probability expectation of revenue. When a policy expires without a claim, its premium becomes revenue and it is allocated as follows:16 1. 10% is reserved in the risk pool to subsidize premiums. 2. 20% is paid to the reinsurance pool to subsidize long-tail risk collateral 3. 70% is paid pro rata to the holders of RSC-FDD tokens as dividends. In exceptional circumstances, an outsized number of policies are claimed and this can result in depleting and exceeding the collateral reserved in the risk pool. In this case, the claim liability is paid out to customers from the reinsurance pool, whose precise function is to service this long-tail risk. An event which depletes the reinsurance pool in this way results in a level of collateral below the targeted liability level desired by the business, and the system will issue new RSC-FDD tokens in order to replenish the pool accordingly. The reinsurance pool is also replenished through the revenue flow described above, and tokens are automatically purchased back from the RSC-FDD token marketplace when the reinsurance capital exceeds the targeted capitalization. This, in turn, results in deflation of the RSC-FDD token supply (or an increase in potential acceptable business risk liability) and a token supply which remains “managed”, increasing only at the rate by which the business is able to increase its throughput of policies underwritten. This proposed economics has several desirable properties: 1. Solvency Guarantees. N ​ o customer can lose money as insurance policies are underwritten against 100% collateralization in the risk and reinsurance pools. No insurance policy will ever be issued that is not fully backed by collateral. 2. Natural Scalability. I​ f the demand for policies exceeds the available collateralization, the system has a natural mechanism to scale up to meet the desired demand through additional RSC-FDD token issuance. In the same way, it can naturally scale down to adjust to decreasing demand. 3. Fair Token Pricing. ​The fair price of tokens is transparent, as it is the present perpetuity value of a measurable dividend stream which is itself well-defined by the probability model of the insurance portfolio. Given a reasonable risk-free rate and the observed recurring revenue stream of the risk pool, the price of tokens can estimated without resorting to speculative markets for pricing. 4. Value Proposition for Crowdsale Participants. ​Under reasonable risk-free rates available on cryptocurrency-focused markets, such as the Poloniex BTC  Note that the following allocations are subject to change at any time prior to launch based on new modeling. 16  Page 27  lending rate17, and assuming modest utilization of the proposed insurance product, the fair pricing of tokens results in substantial incentives for crowdsale participants. 5. Low Dilution. ​Under reasonable risk-free rate assumptions and even modest utilization of the proposed scheme, dilution is likely to be low. This is due to the fact that tokens gain a substantial increase in value after an end-to-end beta product has been delivered without exceptional occurrences and expects a non-zero future revenue stream. The following diagram outlines the components and value flows of the proposed system.  5.3 Risk Management for Flight Delay Insurance An insurance risk is a future liability for payment which occurs with a certain probability. In the case of the flight delay insurance, the probability is calculated from historical flight data. Such data is readily available from multiple trusted sources.18 We assume the data quality for flight delay data to be sufficiently accurate for our purposes, considering that airlines are legally obligated to provide such data and it is highly audited by various businesses which rely on it. Insurance risks associated with flight delays are ideal for a proof of concept product, as individual premiums represent very See ​https://www.poloniex.com/lending#BTC​ and h ​ ttps://cryptolend.net/rates.html​ for current and historic rates. 18 For instance, FlightStats (http://flightstats.com). 17  Page 28  small financial risks to consumers and participants and under normal circumstances flight delays are well-approximated by independent probability events. A simple algorithm for premium calculation is that the average payout for claims is covered by the net premiums collected19. This works well in most cases, but there can be periods where - due to statistical fluctuations - the payout is higher, sometimes even much higher than the average. This is compensated by other periods in which the payout is lower, but we have to provide enough funds that under “most circumstances” all payouts can be made. “Most circumstances” can be refined to a “level of confidence”: a level of confidence of e.g. 99.9% means that in 99.9% of all distinct periods of time the probabilistic gross payout is smaller than the sum held in the risk pool. In our model, we calculate with an even higher level of confidence of 99.99%. Put into simple words, that means that in the average, for every 10,000 years there will be one year in which the risk pool doesn’t have enough funds to pay all claims. Such a level of confidence exceeds the legal requirements e.g. in Europe by far - the “Solvency II” regulations require only a level of 99.5% probabilistic reserve. Providing a risk pool at such a high level of confidence comes at a cost. Participants have to provide the funds and expect a fair return for taking the risk and binding their capital. This return is paid from excess premiums, which were not needed to pay claims, and in the end the revenue from the risk pool is designed to provide a certain surplus in excess of the net premiums collected to pay the average expected claims.  5.4 Target Parameters of the Risk Model Participants will ask for the key parameters of our model, therefore we provide some estimates which we will elaborate more precisely in the future. Note that all values are subject to change.  Parameter  Estimate  Risk pool solvency confidence level  99.99%  Fixed service fee on premiums  5-10%  Target return rate on reinsurance pool  5-10%  Target maximum liability at launch  $1M max  Target policy throughput  2000 concurrent policies @ $500 average payout  19  Net premiums are the premiums paid by the customer net of any fixed service fees. Page 29  6 Etherisc the company 6.1 Business Plan A business plan for our first product, the FlightDelay Insurance, is available as separate Document.  6.2 Team 6.2.1 Founders Christoph Mussenbrock Christoph has a long record of accomplishment in the cooperative banking sector in Germany. After several years on the board of a cooperative bank, he switched to the IT segment and became Chief Program Manager Credit Solutions and Chief of Strategy Development at Fiducia & GAD IT AG – one of Germany’s biggest IT Service Providers. Since 2015, he has been CEO of parcIT GmbH, one of Germany’s best-known companies specialized in risk management solutions. Due to his many years of working in the field of banking and insurance, Christoph is highly experienced in all matters concerning regulatory frameworks. He also co-founded Progeno Wohnungsgenossenschaft eG, a housing cooperative in Munich, which has successfully crowdfunded a large residential project. Christoph has a master’s degree in mathematics and wrote his thesis on formal soft- and hardware verification. Stephan Karpischek Stephan has more than 20 years experience in IT businesses and advises finance and telecom enterprises on digital strategy. In 2015, he was part of the of the UBS crypto 2.0 innovation lab at Level39 in London, applying blockchain to banking use cases. Stephan has been involved with digital currencies since 2008 and has a PhD in information management from Swiss federal technical university ETH Zürich. He wrote his thesis on mobile applications for the Internet of Things. Renat Khasanshyn Before joining Etherisc, Renat was Venture Partner at Runa Capital and CEO of Altoros. Most recently, Renat led from its inception, the insurance practice at Altoros together with its key customers Allianz, Allstate and Liberty Mutual, focusing on canonical use cases of blockchain, reinsurance and insurance securitization (catastrophe bonds). Renat co-founded Altoros, a 250+ employee strong software and research laboratory in the area of distributed databases, container orchestration & developer marketplaces. Renat began his career in 2001 as a software engineer at a regulated insurance intermediary in Tampa, Florida. Using Perl/CGI/LAMP, he built one of the first online Page 30  distribution portals in the insurance industry. Real-time quoting, payment and policy issuance gave uninsured consumers in the US same-day access to a network of 30,000 doctors. In 2007, Renat co-authored Apatar, - GPL-licensed, 100% open source data integration tool, and co-founded Belarusian Java User Group. Renat studied Engineering at Belarusian National Technical University.  6.2.2 Advisors Ron Bernstein Ron is the CEO of AugmentPartners Limited, a private software development company focused on decentralized trading dApps, market liquidity, and blockchain data management. Ron is also an early advisor to the Augur Project -- a decentralized Prediction Market built on the Ethereum blockchain. Previously, Ron founded Intrade.com and Tradesports.com. Intrade was the world's most popular (centralized) Prediction Market from 2007 until 2012. Prior to focusing his attention of crypto assets, Ron traded commodity options and derivatives for more than 25 Years on the trading floors in NYC and London. Ralf Glabischnig Entrepreneurs. Invested. Involved. This is the core mindset that Ralf embodies as Co-Founder of Lakeside Partners, a leading early-stage investment company in Crypto Valley. With 20 years of experience as a business- and IT-consultant and in his role as Managing Partner at inacta AG, a major Swiss Information Management solution provider, he possesses extensive expertise in transforming the insurance industry, as well as a diverse entrepreneurial background stemming from several ventures and advisory board positions. Ralf brings a passion for innovation and first-hand knowledge of the Swiss business landscape in his role as strategic advisor to the Etherisc project. William Mougayar William is a Toronto-based investor, researcher, blogger, and author of The Business Blockchain (Wiley, 2016). He is a direct participant in the crypto-technology market, currently on the Board of Directors of OB1, the OpenBazaar open source protocol that is pioneering decentralized peer-to-peer commerce, a former Board Advisor to the Ethereum Foundation, board member at Stratumn, a member of OMERS Ventures Board of Advisors, an Advisory Board member to the Coin Center, Bloq and other leading blockchain organizations. He blogs regularly about the present and future of blockchains at Startup Management. Jake Brukhman Jake is Co-Founder and Managing Partner at CoinFund LLC, a blockchain advisory company and a cryptofund operating since July of 2015. Jake has 9 years of experience in pure and financial technology, a background in computer science and mathematics, and an avid interest in blockchain and financial technology. He is co-author on multiple blockchain whitepapers including Etherisc, Kin, Sweetbridge, and others. Previously, Jake was Partner & CTO at Triton Research, a technical product manager and engineer at Amazon.com, and spent five years as a financial technologist at Highbridge Capital Management and as a quantitative researcher at Kohera. Page 31  Tobias Noack Since 1991, insurance broker with ATS Insurance Brokers. Subsequently, power of attorney and shareholder role. Acquisition by Aon Risk Solutions (ARS) in 2004. Since then held positions as Regional Manager for ARS Berlin (Germany East), as well as in Key Accounting and Sales. Member of the ARS Operational Board. Since 2016, Special Projects role.  6.3 Legal & Regulatory Strategy 6.3.1 Legal structure One of the core concepts of the Etherisc Approach is the “Two-Folded Approach” with an independent, not-for-profit foundation (the “DI-Foundation”) and commercial entity/entities (bundled in an “Etherisc Holding AG”).  For the success of the token sale, but also for the success of the vision, and last but not least for the legal setup, it is crucial to have a good understanding of the “Why” of this construction. First, we believe in decentralization as one of the core concepts of blockchain. As Fred Ehrsam states: “So the biggest question I ask myself when evaluating a blockchain idea is: "is this uniquely enabled by the new paradigm?" https://twitter.com/FEhrsam/status/902584358770434048  Page 32  To be able to execute this statement, we need the power to do so. Clearly, the success of any decentralized organization cannot be enforced by central entities, but to avoid the “Tragedy of the Commons”, central entities can help, if they are clearly dedicated to the common goal. (again, a recent tweet from Fred Ehrsam: “As a result, Ethereum is starting to suffer from a tragedy of the commons problem: while lots of people own ETH and would benefit from Ethereum improving, the economic reward for any single individual improving it is low.” h ​ ttps://twitter.com/FEhrsam/status/900745083426791425​) The one and only reason for our two-folded approach is the attempt to give an answer to this question. Now, what’s the answer? The Tragedy of the Commons is a Di-Lemma. On one side, you need common goods which are accessible without fees by everyone (of course there are some rules of the game, but no fees, and no profit). On the other side, the common goods need to be nurtured which comes at a cost. Our solution: First, a decentralized foundation, which is strictly neutral and has only one purpose: to develop the open platform and keep it open for all times. Second, a commercial entity which acts as first mover on the platform, shows its feasibility and earns money which will ensure the sustainable development of the platform. Now that we have solved the tragedy of the commons from the start, we have to ensure that this will endure. The Foundation is bound by its codified purpose, which is “eternal” (cannot be changed easily). But it its a “lame duck” by purpose - it has to stay neutral, and its funds are limited. Provided the foundation is equipped with enough funds from the beginning, this can be tolerated, but it will inevitably come up again after some time. The commercial entity is more difficult. The owners can always change its purpose, the direction of its commercial activities and the use of its profits. There is only one way to ensure that this commercial entity is bound “forever” to the purpose we give it at the start: at least a blocking minority has to be under control of an entity which itself is bound - which is very naturally the Foundation. This also solves the problem of the foundation mentioned above, having no natural income otherwise. Having a blocking minority in the commercial entity has a second advantage. The foundation can take care that the commercial entity is always serving the open platform and not cannibalizing it. The commercial entity will thus act as a Shared Service Provider for the open platform. Especially, in our case of a decentralized insurance platform, the commercial entity will reduce the regulatory moat for all participants, offering a simple access to licensed insurance products and structures, in a way which could be similar to the Malta “Protected Cell” structures.  Page 33  Last tweet of Fred Ehrsam on this: “Ideas uniquely enabled by a new paradigm have never been seen before. So we have to remix current ideas to get there.” https://twitter.com/FEhrsam/status/902583309972103168 If you ask “Can this work?”, there is one big example for such a construction, which is very successful: the German Cooperative Banking System. The cooperative banks where a rock of stability in the 2008 financial crisis, and they have exactly this setup. All these banks are legally independent, but working in close cooperation. They have an not-for-profit association, which per definition is neutral regarding commercial strategies, but helps to formulate and cultivate such strategies. E.g. they do a lot of market research, organize larger projects. More important, they have common Shared Service Companies, which perform tasks a single cooperative bank is not able to perform itself, like international payments, IT Operations and the very german concept of a “cooperative building society” (Bausparkasse). Even an insurance is part of the system, and it is one of the biggest. The whole cooperative sector has 18 million members, and a market share of > 30%. The Shared service companies are always among the biggest in the respective market. There is one important caveat: While this is very stable, there is a natural - but creative tension between the Shared Service Companies and the cooperative banks, because naturally, the Shared Service Companies seek their opportunities and this can lead to a conflict of interest. Therefore, a very clear governance is needed, and of course a strong and common vision.  6.3.2 General regulatory strategy Etherisc aims to enable fully-licensed and fully-regulated insurance products on its decentralized insurance platform. To achieve this ambitious goal, we have been in contact with regulators in multiple jurisdictions to educate them on the role and benefits of blockchain technology in the insurance space. We strongly believe regulatory safety is an essential component of a decentralized insurance platform, and we are working with both regulators and insurance partners in the major markets to be able to roll out commercial products. Acquiring proper authorizations in every market where we will be selling insurance is critical, and we expect to be authorized as an insurance company in one of our top target markets sometime between Q4 2017 and Q2 2018. Obtaining authorization to underwrite insurance is a collaborative process involving multiple partners and specialized service providers. Earlier this year, we initiated an evaluation of insurance management companies to support our application for authorization. In July 2017, the team selected a service provider in the EU and began the process of authorization. We also plan to share insurance licenses with other insurtech startups as licensing will be one of the services that the platform provides.  Page 34  Our regulation and licensing plan is a multi-step process: For the first stage, until we have our own license, we will use insurance partners to sell insurance products. We will also apply for our own insurance license, and engage a reinsurance partner to sell insurance products. Our longer-term vision is to replace reinsurance with a decentralized insurance market.  6.3.3 Approach in Malta We are partnering with Malta-based Atlas Insurance Malta. The Atlas Group of Companies forms one of Malta’s foremost insurance and financial services organisations with a staff complement of over 170. The flagship company of the Group is Atlas Insurance PCC Limited (“Atlas”), a general business insurer operating an extensive intermediary network across the islands. Atlas was the first direct insurer to convert to a Protected Cell Company (PCC). Atlas gives promoters the opportunity to own their own EU insurance vehicle with less capital and less cost, also avoiding fronting requirements. Atlas is an independent PCC giving the option to promoters to outsource cell management to authorised insurance managers. Cells in Atlas can also write third party risks, where our substantial active core provides added security and flexibility. The company maintains substantial surplus funds over regulatory requirements. Starting in October 2017, Atlas provides us with capacity to start a small pilot for selling FlightDelay insurance. In 2018, we will formally establish a Protected Cell entity to explore the “Protected Cell” model as role-model for decentralized insurance.  6.3.4 Approach in the UK In the UK, we have established Etherisc Ltd. as a legal body to apply for a FCA sandbox license. Extensive talks with the FCA, the PRA and our insurance partner in the UK have led to a preliminary acceptance of Etherisc Ltd. to the sandbox program in early 2017. We plan to become part of the third cohort of companies to obtain a license to sell insurance products in the UK.  6.4 Risk Management Etherisc will establish a risk management system in the DI Foundation as well as in the commercial entities, bundled in the Etherisc Holding AG. We have inventoried the main risks which come along with the disruption of a century old business in a completely new technological context. The risk monitoring and managing process will implement the requirements of the respective jurisdictions. The expertise for setting up a risk management system which fulfills the regulatory requirements is available in the founders’ team.  Page 35  7 Token Sale 7.1 Token Sale Structure and Timeline The terms & conditions of the token sale as well as the exact timeline will be described in a separate document, which will be published at the time of the announcement of the token sale.  7.2 Meeting capital requirements The funds collected from the token sale will be transferred in a swiss-based DI Foundation. This transfer - which is, technically speaking, a donation - will enable the DI Foundation to accomplish its goals, which are hard-coded in its purpose. The “Eidgenössische Stiftungsaufsicht” will then supervise the operations of the DI Foundation and enforce the execution of the Foundation's’ purpose. While the foundation will keep its own business lean and cost-effective, it will use its funds in two main areas: 1. Financing the development of the Decentralized Insurance Protocol and the community of customers, users and participants 2. Establishing commercial insurance companies and providing the capitalization for these companies, either alone or together with other partners, preferably from the insurance business. We can deduce the funding goals for our token sale from these two fields, which require and empower each other. Of course, there is a large bandwidth for these numbers. In what follows, we will give lower and upper bounds together with indications how a higher funding will enable a broader or faster approach to our overall objectives. The dependency of the scope from the achieved funding is different for the two areas mentioned above. For the first field - the development costs - and for simplicity and better understanding, we can organize our estimates in “Levels” (Bronze, Silver, Gold, Platinum) with the following meaning: Level  Name  Description  1  Bronze  Accomplishing basic objectives. Basic protocol components. Deliver single working parametric product. Achieve self-sustainability. Regulated entities in cooperations e.g. dependent PCCs  Cost USD 5.0 M  FTE 15  Page 36  2  Silver  Accomplish basic + some advanced objectives: Full protocol components. Deliver several working parametric products. Achieve revenue stream for foundation. Regulated entities in cooperations e.g. dependent PCCs  7.0 M  25  3  Gold  Fully accomplish all objectives. Full protocol. Gateways & interaction with other protocols. Deliver first example of non-parametric product. Achieve revenue stream for foundation. Independent primary insurer.  9.0 M  30  4  Platinum  11.0 M  35  Extended operations: Full protocol & gateways. Risk trading platform with Risk Pool Tokens. Broad palette of products parametric / non parametric. Revenue stream for foundation. Independent primary insurer and reinsurer.  The funds will be spend over a period of 2 years. We calculate a FTE (Full Time Equivalent) with average cost of USD 100K per year and a relation of HR to other costs of 2:1. The costs will be distributed between DI Foundation and the commercial entities in a ratio of roughly 1:1. For the second field - the capitalization of commercial entities, which act as subsidiaries of the foundation, the metric for the capitalization is different. Due to the basic regulatory approach the business volume of an insurance company is roughly proportional to its capitalization, because in insurance, business volume means risks, and risks need to be covered by own funds. To get an estimate, we can examine some quantitative material, e.g. the results of the famous “Quantitative Impact Study 5”20 which was conducted prior to the introduction of Solvency II. It gives a very rough estimate for the ratio between the “SCR” - the Solvency Capital Requirement - and the sum of individual risks taken by an insurance company.  20  ​https://eiopa.europa.eu/publications/reports/qis5_report_final.pdf Page 37  From the graphic, we learn that for € 1M insured risks we need a minimum of about 41.2% = € 412K own funds to fulfill the Solvency II capital requirements. The remaining risk is covered by diversification (35.1%) and sharing (23.7%). This number, however, is a minimum - most insurance company grossly overachieve this by factors up to more than 400%:  Furthermore, due to the operational complexity of an insurance business, an insurance company is difficult to operate at the bare minimum of capital. Economies of scale become effective at larger scales, and therefore we estimate the minimum solvency capital required to run a sustainable insurance at about USD 10M. Solvency capital is, however, only one part of the equation. Regulations in most countries require the provision of a separate “organizational fund” to finance the operational costs of ramping up an insurance business. The organizational fund is typically between 25-50% of the necessary solvency capital. In total, we can estimate the minimum total capital requirement for starting an insurance business between USD 12,5M - USD 15M.  Page 38  This number is plausible, because its in the same magnitude as the bare minimum capital required to establish an insurance company in Germany and Switzerland, and it corresponds as well to the capitalization of some newly founded insurance startups in Germany21. However, the same report shows that most promising startups are much better capitalized. Lemonade e.g. commands USD 60M after their third round, trov has USD 84 M and indian based Digit Insurance has another USD 60M.  7.3 Deduction of token sale range From what has been said, we have a range of USD 5-10M for development costs and a starting point for capitalization of USD 12.5 - 15M. Coming to Etherisc, we have several additional factors to take into consideration: 1. Different from the startups mentioned above, which are typically financed over various rounds, Etherisc will need to capitalize in only one round. 2. Etherisc is operating in a completely new field, with less mature systems, with unproven economics. We are pioneers in every aspect of our model, and for this systemic risk an adequate capitalization is mandatory. 3. For the same reason, we expect that regulators will demand higher capital requirements than an equivalent insurance operating in a traditional model. 4. Ramping up our business can last longer than expected, and it is not yet clear how fast mass adoption of crypto currencies and crypto business models will start. 21  ​http://i3analytics.com/wp-content/uploads/2017/07/CB-Insights_Insurance-Tech-Q2-2017.pdf Page 39  We therefore target a hard cap of USD 50M, but we can as well take off with as much as USD 20M. Reaching the hard cap would give us the runway to develop protocol and community more organically and would significantly reduce risks. But for every amount north of the minimum we can start as well.  7.4 Protection of Participants and Transparency Etherisc is dedicated to a high degree of transparency, as long as legitimate interests of participants, customers and employees are taken into account. To work not only on Etheriscs own transparency policy, but also enhance the transparency of the whole blockchain sector, Etherisc has joint “Project Transparency”22, a joint effort of some of the best reknown projects in the space, including Aragon, Lykke, Maecenas, to name only a few. Etherisc and the other members of Project Transparency follow a self-inflicted policy of making public the purpose of every use of funds which exceeds 0.5% of collected funds.  7.5 Migration of RSC Tokens We use RSC-DST to denominate the RSC tokens which are issued in the DST contract of hack.ether.camp, and DIP for the future tokens which are issued to finance development of the decentralized insurance protocol. Proceeds from selling RSC-DST tokens have been used to back research and development and initial operational costs. RSC-DST token holders will receive a fair share of DIP-tokens in the upcoming “Token Generating Event” (TGE) for their engagement in an early stage of Etherisc. The conditions will be published together with the tokensale document mentioned in chapter 7.1.  7.6 Token Sale contract and audits The token sale smart contracts have been written by the team. The code has undergone three independent audits of well-known solidity experts, which will be published as soon as the final version of the contract is considered stable.  22  www.projectransparency.org Page 40  8 Appendix 8.1 Example application to the use of an oracle in an insurance context The TokenStake contract serves as an abstract interface. It provides four functions: two functions for staking and two functions for releasing: 23 contract TokenStake { function function function function  stakeFor(address _staker, uint256 _value) public returns (bool) ; stake(uint256 _value) internal returns (bool); releaseFor(address _beneficiary, uint _value) internal returns (bool); release(uint _value) internal returns (bool);  }  The ​staking​ functions are public, anyone can transfer tokens to the contract. To notify the contract about the incoming tokens, this has to be done in two steps: 1. The token owner​ ​approves​ ​the TokenStake contract over the sum of tokens to be staked. 2. The token owner calls s ​ take​ or s ​ takeFor​. The TokenStake contract then transfers​ the token to the TokenStake contract and records the tokens in an internal ledger. The release functions are internal, therefore the TokenStake contract itself has not much utility: If you t ​ ransfer​ tokens to the original Tokenstake contract, the tokens will be locked forever. The TokenStake contract becomes useful if you extend it with some additional logic, which binds the r ​ elease​ functions to some condition. This condition can be arbitrarily implemented. A typical use would be a time lock: the r ​ elease​ function can be called after a certain block or time is reached. As a more complex example, we present another use case where the TokenStake contract is used to reward an oracle for providing the information in due time. First, we present the basic skeleton of a blockchain oracle: contract usingOracle { event Query(string _request, bytes32 _id); uint256 count; function query(string _request) internal returns (uint256) { count += 1; Query(string _request, id); return count; }  23  we show only the function signature, the complete code can be found here: https://github.com/etherisc/tokensale/blob/develop/contracts/protocol/TokenStake.sol Page 41  function __callback(string _result, uint256 _id) public { } }  A contract using this oracle will extend this interface. It will then call ​query() ​with a string containing the actual request (e.g. a http url with an api call, or an sql query). The actual oracle will watch the ​Query​ event, perform the query offchain, and then call __callback​ with the result. The i ​ d​ parameter is used to discriminate parallel calls to the oracle. We can now extend this basic interface with a reward mechanism: the oracle receives 10 token for each _ ​ _callback​ which is performed within 30 minutes of the respective query: contract usingIncentivizedOracle is usingOracle, TokenStake { mapping(uint256 => uint256) public deadlines; uint256 deadline constant = 30 minutes; address constant contractor = 0x1234abcd1234abcd1234abcd1234abcd1234abcd; uint256 constant reward = 10; function query(string _request) internal returns (bytes32 _id) { _id = super.oraclize_query(_request); deadlines[_id] = now + deadline; return _id; } function __callback(bytes32 id, string _result) public { if (now < deadlines[id]) { releaseFor(contractor, reward); } } }  Finally, we put everything together in a simple insurance contract. We assume that somebody has already approved the insurance for the transfer of a number of tokens. In case of a timely delivery of the request, the oracle (with address ​contractor​) receives 10 tokens. The oracle - simplified - checks the “flight” and yields “ok” if the flight can be insured: contract Insurance is usingIncentivizedProxy { function Insurance (uint256 _stakedTokens) { stake(_stakedTokens); } function newPolicy(string _flight) public { id = query(_flight);  Page 42  // store policy, … perform further checks ... } function __callback(uint256 _id, string _result) public { super.__callback(_id, _result); if (_result == ‘ok’) { underwrite(_id, _result); // notify customer that the policy has been accepted. } } }  Page 43  8.2 Credit Risk Model  Page 44  arXiv:1909.05821v1 [cs.DC] 12 Sep 2019  Flow: Separating Consensus and Compute  Dr. Alexander Hentschel alex.hentschel@dapperlabs.com  Dieter Shirley  Layne Lafrance  dete@dapperlabs.com  layne@dapperlabs.com  Abstract Throughput limitations of existing blockchain architectures are well documented and are one of the most significant hurdles for their wide-spread adoption. Attempts to address this challenge include layer-2 solutions, such as Bitcoin’s Lightning or Ethereum’s Plasma network, that move work off the main chain. Another prominent technique is sharding, i.e., breaking the network into many interconnected networks. However, these scaling approaches significantly increase the complexity of the programming model by breaking ACID guarantees (Atomicity, Consistency, Isolation, and Durability), increasing the cost and time for application development. In this paper, we describe a novel approach where we split the work traditionally assigned to cryptocurrency miners into two different node roles. Specifically, the selection and ordering of transactions are performed independently from their execution. The focus of this paper is to formalize the split of consensus and computation, and prove that this approach increases throughput without compromising security. In contrast to most existing proposals, our approach achieves scaling via separation of concerns, i.e., better utilization of network resources, rather than sharding. This approach allows established programming paradigms for smart contracts (which generally assume transactional atomicity) to persist without introducing additional complexity. We present simulations on a proof-of-concept network of 32 globally distributed nodes. While the consensus algorithm was identical in all simulations (a 2-step-commit protocol with rotating block proposer), block computation was either included in a consensus nodes’ regular operations (conventional architecture) or delegated to specialized execution nodes (separation of concerns). Separation of concerns enables our system to achieve a throughput increase by a factor of 56 compared to conventional architectures without loss of safety or decentralization.  1  Contents 1 Introduction 1.1 Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3 4 4  2 Architecture Overview  6  3 Theoretical Performance and Security Analysis 3.1 Special Role of Consensus Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  8 11  4 Performance Simulations 4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Experiment (I): Flow PoS network with the split of consensus and compute 4.1.2 Experiment (II): PoS network of nodes with diverse performances . . . . . . 4.1.3 Experiment (III): PoS network of nodes with uniform performance . . . . . 4.2 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  14 14 15 16 16 16  . . . . .  5 Further Work  17  6 Conclusions  17  Acknowledgments  19  References  19  2  1  Introduction  The original Bitcoin blockchain uses a consensus model referred to as Nakamoto consensus [1]. It uses a sequential model in which a block is built, mined, and verified, and consensus about it is formed by nodes building subsequent blocks on top of it. While the proof-of-work challenge (mining) that must be solved for every block provides tamper-resistance for the chain, the associated computational effort limits block-production rate and transaction throughput. The throughput limitations of existing blockchain architectures are well documented and among the most significant hurdles for the wide-spread adoption of decentralized technology [2, 3, 4]. The leading proposals for removing the overhead of proof-of-work adapt Byzantine Fault Tolerant (BFT) consensus algorithms [5, 6]. Blockchains using proof-of-work (PoW) require exceptional computational effort and, subsequently, electric power. In contrast, finalizing blocks through BFT consensus is highly efficient but requires a known set of participants, a super-majority of which must be honest. Combining BFT consensus with proof-of-stake (PoS) [7] allows for the creation of a permissionless network with strong security properties. Under PoS, all participating nodes are required to deposit a (financial) stake that can be taken away if they violate the protocol’s rules. The amount of influence given to each node is proportional to its fraction of total stake. Then the economic pressure (i.e., the stake at risk) to follow the protocol is correlated with a node’s influence. In addition, the deposited stake has the added benefit of preventing Sybil attacks [8, 9, 10]. PoS systems promise to increase the throughput of the chain while also decreasing the total capital costs associated with maintaining the security of the chain. Even including the performance increases through adopting PoS, throughput restrictions are remaining the major challenge for wide-spread adoption [11, 12]. In this paper, we explore a novel approach to increasing the throughput of PoS blockchains. While PoS blockchain proposals remove proof-of-work as the dominant sink of computational effort, they tend to inherit most of their architecture from the proof-of-work systems of the previous generation. In particular, every full node in the network is required to examine and execute each proposed block to update their local copy of the blockchain’s state. As every transaction needs to be processed by every single node, adding nodes to the system provides no benefit in throughput or scale. Instead, adding nodes reduces the throughput of most BFT consensus protocols, because the message complexity to finalize a block increases super-linearly with the number of consensus nodes (see section 3.1 for details). Consequently, most PoS blockchains have to make a trade-off between a small consensus committee (weakening security) or a low block production rate (decreasing throughput). For networks unwilling to compromise either security or decentralization, the most common approach to addressing the scaling problem has been through sharding [2] or moving work off the main chain (e.g. Lightning [13] or Plasma [14] network). Both of these approaches, unfortunately, place significant limitations on the ability of transactions to access state that is distributed throughout the network [15]. These limitations dramatically increase the complexity required for developers who wish to deploy smart contract-based applications. Our proposal, the Flow architecture, addresses these limitations by fundamentally changing how the blockchain is formed. Flow decouples the selection and ordering of transactions from their execution so that both processes can run in parallel. The decoupling enables significantly higher transaction throughput than other blockchains architectures, without undermining security or participation rates. 3  Traditional blockchain architectures require a commitment to the result of each block’s state update to be included as part of the consensus process. As a result, every node must reproduce the state-update computation before it can finalize a block. Our finding is that consensus on the order1 of transactions in the block is all that is required. Once that order is fixed, the resulting computation is determined even though it may not necessarily be known. Thereby, the computational effort of participating in consensus is significantly reduced, even for a very large number of transactions. Once the transaction order is determined, ensuing processes can be delegated to perform the computation itself, without affecting decentralization of the system. Our main body of work is section 3, where we discuss and prove our central theorem, which states that one can separate the majority of computation and communication load from consensus without compromising security. Section 4 complements the theoretical discussion by benchmarking the Flow architecture on an experimental network. We conclude the paper by outlining the future work that would be required to implement a system based on these ideas in section 5. The focus of this paper is to formalize the split of consensus and computation and prove that this approach increases throughput while maintaining strong security guarantees. A blockchain designed around the principles outlined in this paper would need to specify a variety of additional mechanisms including: • a full protocol for verifying computation results, • details of the consensus algorithm, • and adequate compensation and slashing mechanics to incentivize nodes to comply with the protocol. Detailed formalization and analysis of these topics is reserved for follow-up publications.  1.1  Terminology  While most current blockchains focus solely on processing financial transactions, we consider a blockchain as a general, Turing-complete, distributed computing platform. Instead of referring to the blockchain as a ledger, we adopt the terminology of a distributed state machine wherein transactions describe transitions between computational states. Furthermore, we use the term consensus to refer only to linearizing the order of state transitions (but do not consider agreement about the computational result as a part of consensus).  1.2  Related Work  Blockchains supporting Turing-complete computation generally impose an upper limit on the computation within one block, such as Ethereum’s gas limit. Such a gas limit, in turn, introduces undesired throughput restrictions. One reason for imposing a gas limit in the first place is to avoid the Verifier’s Dilemma [16]. By setting the gas limit low enough, the time investment for verification is negligible compared to solving the PoW challenge for mining the block. Thereby, the gas limit ensures that performing the verification work does not introduce a pivotal disadvantage for a node to successfully mine the next block. 1  In the full Flow architecture, Consensus Nodes work with transaction batches (collections). A block contains collection hashes and a source of randomness, which the Execution Nodes use to shuffle the transactions before computing them. While Consensus Nodes don’t directly compute the transaction order for a block, they implicitly determine the order by specifying all the inputs to the deterministic algorithm that computes the order. The detailed protocol is specified in a follow-up publication.  4  For PoS blockchain, the Verifier’s Dilemma persists when incentives are given for speedy operation. Especially for high-throughput blockchains, the verification of a large number of transactions consumes significant computational resources and time. By separating consensus and computation, the Verifier’s Dilemma for checking correctness of the execution result is of no concern anymore to the consensus nodes2 . The maximum amount of computation in a block (the block’s gas limit) can now be increased without the consensus nodes affected by Verifier’s Dilemma or slowed down by the computation work. However, the Verifier’s Dilemma still needs to be solved for the verifiers of the computation. Potential solutions include zkSnarks [17], Bulletproofs [18], and TrueBit’s approach [19]. For Flow, we developed Specialized Proofs of Confidential Knowledge (SPoCKs) to overcome the Verifier’s Dilemma, which is described in detail in a follow-up paper. Another limitation for blockchains are the resource demands to store and update the large computational state. In Ethereum, the gas limit is also used to control the rate at which the state grows [20]. By delegating the responsibility to maintain the large state to specialized nodes, hardware requirements for consensus nodes can remain moderate even for high-throughput blockchains. This design increases decentralization as it allows for high levels of participation in consensus by individuals with suitable consumer hardware on home internet connections. The concept of separating the issue of transaction ordering from the effort of computing the results of the computations has been previously explored in the context of distributed databases [21, 22]. Here, transactions are ordered into a log through a quorum (consensus), and subsequently each node can independently resolve transaction effects. However, these systems are designed for operation in well-maintained data-centers where Byzantine faults are not concern. Specifically, the number of participating nodes is small, and node failure modes are restricted to dropouts. Within the blockchain space, the Ekiden [23] paper describes a system where consensus is separated from computation, with the goal of preserving the privacy of the contract execution environment. The paper, which explains part of the Oasis blockchain technology [24], notes that this approach leads to improved performance. But it does not quantify the performance gain or prove that the resulting system maintains security.  2  The Verifier’s Dilemma (checking the correctness of the execution result) is of no concern anymore to the Consensus Nodes. However, checking cryptographic signatures and proofs in PoS can still require noticeable computational work. Delay through verifying signatures might induce a Verifier’s Dilemma for the consensus nodes Though, on a much smaller scale compared to requiring the consensus nodes to re-execute all transactions.  5  2  Architecture Overview  The Flow architecture is founded on the principle of ‘separation of concerns’. The network has specialized roles: Consensus Nodes and Execution Nodes. The core differentiation between the two node types is Objectivity vs. Subjectivity. Objective tasks are those for which there is an objectively correct answer. Any traditional mathematical calculation is objective; you don’t need an authority or expert to confirm the correctness of 2 + 2 = 4, or to confirm that an actor that claims 2 + 2 = 5 is Byzantine. Subjective tasks have no such deterministic solution. Most human governance systems (“laws”) typically deal with subjective issues. At different times in different societies, the rules about who can do certain things can be very different. The definition of the word consensus means the agreement on subjective problems, where there is no single correct answer. Instead, one answer must be selected through mutual agreement. Blockchains combine objective rules with a decentralized mechanism for resolving subjective problems. One example is if two transactions are submitted at the same time that try to spend the same coins (e.g., no double-spends), which one resolves correctly, and which one fails? Traditional blockchain architectures ask the nodes participating in the network to solve both kinds of problems at the same time. In Flow, the Consensus Nodes are tasked with all subjective questions, while the Execution Nodes are responsible solely for fully deterministic, objective problems. While we reserve a detailed discussion of the nodes’ roles, tasks, and interactions for the follow-up paper, we briefly define the Consensus Role and Execution Role for nodes. For an illustration, see Figure 1. Consensus Role Consensus Nodes form blocks from transaction data digests. Essentially, Consensus Nodes maintain and extend the core Flow blockchain. An agreement to accept a proposed block needs to be reached by many nodes which requires a Byzantine-Fault-Tolerant (BFT) consensus algorithm [5]. It should be noted that the results in this paper hold for any BFT consensus algorithm with deterministic finality. In Flow, a block references its transaction and defines their execution order. However, a block contains no commitment to the resulting computational state after block execution. Accordingly, Consensus Nodes do not need to maintain the computational state or execute transactions. Furthermore, Consensus Nodes adjudicate slashing requests from other nodes, for example claims that an Execution Node has produced incorrect outputs. Execution Role Execution Nodes provide the raw computational power needed to determine the result of the transactions when executed in the order determined by the Consensus Nodes. They produce cryptographic attestations declaring the result of their efforts in the form of Execution Receipts. These receipts  Transaction Data (digest)  Consensus Nodes  Finalized Blocks  Execution Nodes  Execution Receipts  Figure 1: Overview of the message flow through Consensus and Execution Nodes. For brevity, only the messages during normal operation are shown. Messages that are exchanged during the adjudication of slashing requests are omitted.  6  can be used to challenge the claims of an Execution Node when they are shown to be incorrect. Also they are used to create proofs of the current state of the blockchain once they are known to be correct. The verification process – by which Byzantine Receipts are rejected (and the Execution Nodes which produced them are slashed), and by which valid receipts are accepted (and shared with observers of the network) – is outside the scope of this paper and will be addressed in future work.  7  3  Theoretical Performance and Security Analysis  In this section, we present a theoretical derivation that one can separate the majority of computation and communication load from consensus nodes without compromising security. Furthermore, we provide an analysis that explains the source of the experimentally observed throughput increase. Flow is designed to guarantee3 that any error introduced by the Execution Nodes maintains four critical attributes: • Detectable: A deterministic process has, by definition, an objectively correct output. Therefore, even a single honest node in the network can detect deterministic faults, and prove the error to all other honest nodes by pointing out the part of the process that was executed incorrectly. • Attributable: The output of all deterministic processes in Flow must be signed with the identity of the node that generated those results. As such, any error that has been detected can be clearly attributed to the node(s) that were responsible for that process. • Punishable: All nodes participating in a Flow network, including Execution Nodes, must put up a stake that can be slashed if they are found to have exhibited Byzantine behaviour. Since all errors in deterministic processes are detectable and attributable, those errors can be reliably punished via slashing. • Recoverable: The system must have a means to undo errors as they are detected. The property serves to deter malicious actors from inducing errors that benefit them more than the slashing penalty. An important property of this design is that for each system-internal operation, the participants are accountable. Specifically, for all operations except for the Consensus Nodes, the execution of each operation is delegated to a subset of nodes, the operation processors. Verifying the outcome is assigned to a disjoint node set, the operation verifiers. Informally, the protocol works as follows: • Both operation processor and operation verifier groups are chosen at random. The selection of nodes uses a verifiable random function [25], such that the outcome is deterministic but resistant to hash grinding. • The inclusion probability for a node in either group is proportional to its stake. This enforces that Byzantine actors must lock up a significant amount of stake in order to have a nonnegligible probability of affecting the system. Specifically, this hardens the system against Sybil attacks [26]. • The required amount of stake for both groups is set sufficiently high such that the probability of sampling only Byzantine actors in both groups is sufficiently small. • As long as at least one honest node is involved in either group, the honest node will detect and flag any error. • If a potential error is flagged, the case is adjudicated by the Consensus Nodes, malicious nodes are slashed, and the operation’s outcome is rejected if faulty. The process above guarantees that malicious Execution Nodes are slashed with near certainty. Furthermore, it is nearly impossible for the malicious actors to succeed with introducing an error. A question that might arise is why Flow has separate groups of operation processors and operation verifiers instead of the operation processors verifying each other’s results. We chose this separation of concern to address the Verifier’s Dilemma [16]. Without a dedicated verifier role, there is a 3  In Flow, guarantees are probabilistic. Specifically, errors are detected and corrected with probability p = 1 − ε for 0 < ε  1. Through system parameters, ε is tuned such that the desired properties hold with near certainty.  8  conflict of interest for an operation processor to compute the next block vs. verifying (re-computing) the last block’s result. In Flow, this dilemma is alleviated by dedicated operation verifiers who are compensated solely for verification. The technical details of our block verification protocol are presented in the follow-up paper, including a solution to the ‘freeloader problem’ (verifiers just approving any result without doing the actual computation) and ‘maliciously flagging’ (verifiers challenging correct results to congest the network). The following theorem formalizes the security guarantees of the Flow architecture and proves that introducing an error into the system by publishing or approving faulty results is economically infeasible. Theorem 1 (Probabilistic security for delegation of work to small groups) Introducing an error into the system by deliberately publishing or approving faulty results is economically infeasible, if the following conditions hold for any operation, except for those from the Consensus Nodes. 1. The operation is delegated to two sets of randomly selected nodes: (a) set of operation processors: members of this group execute the operation and provide cryptographically secure commitments to their result (b) set of operation verifiers: members of this group verify the operation’s result and provide cryptographically secure commitments to the result if they approve 2. Both groups can be relatively small as long as the probability of choosing only Byzantine actors in both groups at the same time is sufficiently low. 3. At the time the operation processors generate the result, the membership of the operation verifier group is unknown to them. 4. Consensus Nodes (a) either verify that a significant majority have committed to the published outcome and there are no objections raised by participating nodes (b) or adjudicate objections, determine the faulty nodes (attributable), and slash them (punishable). It is essential to highlight that Consensus Nodes are not required to check the correctness of the results of an operation. Instead, they ensure that other nodes with sufficient stake are accountable for the verification. Furthermore, Theorem 1 holds for any BFT consensus algorithm with deterministic finality. Proof of Theorem 1: We will show that Theorem 1 can always be satisfied under realistic conditions where • at least one of the two groups is sampled from a large population with a super-majority of honest nodes; • Byzantine actors cannot suppress communication between correct nodes, i.e., if there is one honest node objecting to the result and proving its faultiness, the erroneous nodes will be slashed.  9  Specifically, let us consider a population of N nodes from which we want to randomly draw a subset with n nodes. Furthermore, we assume that there are at most M < N/3 Byzantine nodes. In the following, we focus on the case where all nodes are equally-staked4 , i.e., their inclusion probabilities are identical. Drawing an n-element subset falls in the domain of simple random sampling [27] without replacement. The probability of drawing m ≤ n Byzantine nodes is given by the hypergeometric distribution5   M N −M Pn,N,M (m) =  m  n−m  N n  .  (1)  The probability of a successful attack, P (successful attack), requires that there is no honest node that would contradict a faulty result. Hence, P (successful attack) ≤ Pn,N,M (n),  (2)  where Pn,N,M (n) is the probability of sampling only Byzantine nodes. M ! (N − k)! for k ≤ M N ! (M − k)! Pn,N,M (n) (N − n)! (M − (n + 1))! N −n = = >1 Pn+1,N,M (n + 1) (N − (n + 1))! (M − n)! M −n Pk,N,M (k) =  ⇒  (3) (4)  As eq. (4) shows, the probability of sampling only Byzantine nodes is strictly monotonously decreasing with increasing n. Eq. (4) states that the larger the sample size n, the smaller the probability to sample only Byzantine node. For a node to deliberately attack the network by publishing a faulty result or approving such, we assume the existence of some reward r which the node receives in case its attack succeeds. However, if the attack is discovered, the node is slashed by an amount ξ (by convention positive). The resulting statistically expected revenue from attacking the network is  revenue = P (successful attack) · r − 1 − P (successful attack) · ξ (5) (2)  ≤ Pn,N,M (n) · r − (1 − Pn,N,M (n)) · ξ  (6)  !  For the attack to be economically viable, one requires 0 ≤ revenue, which yields the central result of this proof: r 1 ≥ − 1. ξ Pn,N,M (n) 4  (7)  The argument can be extended to nodes with different stakes. In this case, each node would have an inclusion probability equal to its fraction of total stake. However, the probability of sampling fully Byzantine groups is depending on the specific fractions of total stake for the individual nodes. A basic solution for allowing nodes with different stakes is to introduce a unit quantity % of stake. For a node with the stake s, the multiplicity k = bs/%c represents how many full staking units the node possesses. For operational purposes (including voting and node selection), the blockchain treats the node identically to k independent nodes each having stake %. 5 For conciseness, we only handle the case m ≤ M . For m > M , Pn,N,M (m) = 0 per definition.  10  Furthermore, eq. (4) implies that r/ξ increases strictly monotonously with increasing n. Using results from [28], one can show that r/ξ grows exponentially with n for n < N/2. The left-hand side of equation (7), r/ξ, is a measure of security as it represents the statistical cost to attack the system in the scenario where an attacker bribes nodes into byzantine behavior. As an example, let us consider the case with N = 1000, M = 333, n = 10. For simplicity, assume that for publishing or approving a faulty result, the node’s entire stake is slashed. Then, for an attack to be economically viable the success reward r would need to be 65 343 times the node’s stake. If the operation verifiers staked $1000 each, an attacker would have to expend $65.3 million on average to cover all the slashing costs. It would be cheaper for the attacker to run the entire pool of operation verifiers instead of attempting to slip an error past the honest verifiers. When increasing n even further to n = 20, an attacker would need to expend r/ξ = 5.2 · 109 times the stake to slip a single error past a super-majority of honest verifiers. In summary, we have analyzed the case where either processing an operation or verifying its result is delegated to a small, random subset of a much larger population of nodes. We have shown that under realistic assumptions, introducing an error into the system by publishing or approving faulty results is economically infeasible. Note that this result only covers node types other than Consensus Nodes. Hence, it is sufficient for the Consensus Nodes to check that enough nodes have participated in executing the operation as well as verifying it. However, they do not need to check the result itself to problematically guarantee its integrity.  Theorem 1 is a key insight, as it allows us to: • separate the majority of computation and communication load from consensus; • develop highly specialized nodes with distinct requirement profiles (as opposed to having one node type that has to have outstanding performance in all domains or otherwise diminish the network throughput); While other nodes verify each others’ operations in small groups, the entire committee of Consensus Nodes audits themselves.  3.1  Special Role of Consensus Nodes  Consensus Nodes determine the relative time order of events through a BFT consensus algorithm. While our results hold for any BFT consensus algorithm with deterministic finality, HotStuff [29, 30] is the leading contender. However, we continue to assess other algorithms such as Casper CBC [31, 32, 33] or Fantômette [34]. In contrast to the operations of other nodes, which requires an auditing group to approve the result, Consensus Nodes finalize blocks without external verification. While the contents of a block can be verified and Consensus Nodes punished if they include invalid entries, blocks are not rebuilt in this scenario, unlike other verification processes. External parties can inspect the finalized blocks after the fact. However, in the event of a adversarial attack forking the chain, a double-spend attack might have already succeeded at this point. To increase the resilience of the entire system, the committee of Consensus Nodes should consist of as many staked nodes as possible. For a simple BFT algorithm, the message complexity η per block (i.e., the total number of messages sent by all N nodes) is O(N 2 ) [7]. More advanced protocols achieve η ∈ O(N log N ) [35] or η ∈ O(c · N ) [36, 37], for c  N an approximately constant value for large N . The overall  11  bandwidth load B [MB/s] for the entire consensus committee is B = β · b · η,  (8)  for β the bock rate [s−1 ], b the message size [MB/s]. For a node receiving a message m and processing it, imposes a latency L(m) = f (b) + process(m),  (9)  where f denotes the network transmission time for receiving m and process(m) represents the computation time for processing m. It is apparent that both B and L strongly impact the throughput of the consensus algorithm. The specific details are highly dependent on the chosen BFT protocol, as well as the employed gossip topology [38]. Other factors include delays or bandwidth limitations in the underlying network hardware. Currently, we are targeting a consensus committee on the order of several thousand nodes. To support decentralization and transparency, hardware requirements for Consensus Nodes should be limited such that private groups can still afford to run a node and participate in consensus. Hence, given a desired minimal block rate (e.g., β = 1 block s ) and an environment-determined function f , the consensus committee can be increased only by decreasing message size b or process(m). For completeness, we provide a brief outlook on how we simultaneously reduce b and process(m) in Flow. For the detailed mechanics of the respective processes, the reader is referred to subsequent papers. • We delegate the computation of the transactions to the specialized Execution Nodes. The delegation removes the need for Consensus Nodes to maintain and compute state, which significantly reduces process(m). • Consensus Nodes do not require the full transaction texts during normal operation. Instead, specialized Collector Nodes receive transactions from external clients and prepackage them into batches, called collections. Consensus Nodes only order collection references (hashes), which substantially reduces the messages size b compared to handing individual transaction texts. We conclude this section by comparing our apLow Latency Finality Low Overhead proach with the ‘triangle of compromises’ proSmall Number of Nodes posed by Vlad Zamfir [39], which we re-create in Figure 2. The triangle illustrates Zamfir’s impossibility conjecture for proof of stake systems. While the conjecture has not been proven formally, we concur that the compromises are correctly identified. However, Flow optimizes where these compromises are made: • Consensus Nodes work as part of a large Low Latency Finality High Latency Finality consensus committee for maximal secu- High Overhead Low Overhead Large Number of Nodes rity. To ensure security and fast genera- Large Number of Nodes tion of finalized blocks, we accept a higher Figure 2: Zamfir’s triangle of compromises; communication overhead (the bottom-left orange: Consensus Nodes; blue: Execution Nodes. corner of the triangle). However, unlike other blockchains, this consensus only determines the order of transactions within a block, but not the resulting state. The architecture compensates for the resulting bandwidth higher 12  overhead by minimizing message size through the use of collection references (references to batches of transactions) instead of full individual transactions. To further increase throughput and reduce communication latency, all possible computation is delegated to other node types. • The Execution Nodes unpack the collections and process individual transactions. Without a large number of Byzantine actors involved, this can be handled directly between the Collector and Execution Nodes without involving Consensus Nodes. Furthermore, tasks can be parallelized and processed by small groups which hold each other accountable for correct results. Only during Byzantine attacks, malicious actions would be reported to Consensus Nodes to adjudicate slashing.  13  4  Performance Simulations  The theoretical analysis presented in section 3.1 suggests that transaction throughput can be increased by separating consensus about the transaction order from their execution. However, the theoretical analysis makes no assertion as to what the realistically achievable speedup is, as throughput heavily depends on a variety of environmental parameters such as message round-trip time, CPU performance, etc. Therefore, we have implemented a simplified benchmark network that solely focuses on transaction ordering (consensus) and transaction execution.  4.1  Experimental Setup  In a 2015 study analyzing the distribution of computational power in the Bitcoin network [40], the authors estimated that 75% of the mining power was provided by roughly 2% of the nodes. We simulated a system whose centralization metrics are roughly half of the Bitcoin scores. In our simulations, roughly 38% computational power is provided by the fast nodes, which represent approximately 6% of the nodes. For the remaining 62% of the network’s total computational power, we have applied a less-extreme ratio: two-third of the nodes (slow nodes) hold one-third of the remaining computational power (i.e., 62%/3 ' 20% of the total computational power). The remainder is assigned to medium nodes. To assign the Execution role to nodes with the most computation power requires incentive mechanisms that compensate nodes for the resources used by the network. Assuming the existence of such incentive mechanisms, it is economically rational for a fast node to stake as an Execution Node. In any other role, its resources would not be utilized to the maximum potential leading to diminished revenue. Hence, we assumed that the most powerful nodes would stake specifically to become Execution Nodes. We conducted three different experiments. The common characteristics of all simulations are described in the following. Section 4.1.1 to 4.1.3 present the specific details for each individual experiment. Figure 3 illustrates the different setups. For each experiment, the network resources (node types) and the assignment of responsibilities are given in Table 1. Slow Nodes  Medium Nodes  Fast Nodes  Experiment (I):  number of nodes role of nodes  20 consensus  10 consensus  2 compute  Experiment (II)  number of nodes  20 consensus and compute  10 consensus and compute  2 consensus and compute  role of nodes Experiment (III)  number of nodes role of nodes  32 consensus and compute  Table 1: Network configuration for each experiment.  14  slow node  medium node  fast node  ...  ...  ...  ...  ...  ...  consensus compute  consensus and compute  consensus and compute  (I)  (II)  (III)  Figure 3: Illustration of experiments. (I) Flow network with the division of consensus and compute; (II) Conventional PoS network containing nodes with different performance levels; (III) Conventional PoS network containing only slow nodes.  Common Characteristics Transactions: For simplicity, our network was processing benchmark transactions, which all had identical complexity (number of instructions). Batches of benchmark transactions were directly generated by the Consensus Nodes instead of integrating a dedicated submission process into the simulation. Network: We implemented a relatively small network of 32 nodes, were each node ran on a dedicated Google Cloud instance. The nodes were spread over 8 data centres across the world to provide somewhat realistic latencies. Nodes: Depending on the experiment (see Table 1), transactions were executed on nodes with different hardware: • slow nodes: process a benchmark transaction in approximately 10ms • medium nodes: five times as fast as slow nodes, i.e., process a benchmark transaction in 5ms • fast nodes: 25 times as fast as slow nodes, i.e., process a benchmark transaction in 2.5ms To facilitate decentralization, an ideal network should allow any participant to join, requiring only a minimal node performance. Consequently, a realistic network will contain a majority of slow nodes, some medium nodes and very few fast nodes. Consensus: We implemented a Tendermint-inspired consensus algorithm with a rotating bock proposer. As our goal was to benchmark achievable throughput in the absence of a large-scale Byzantine attack, our benchmark network only consists of honest nodes. The proposed blocks contain a variable number of t benchmark transactions, where t is drawn uniform randomly from the integer interval [240, 480]. However, for repeatability, we seeded the random number generator such that in all experiment, the same sequence of 20 blocks was proposed and finalized. 4.1.1  Experiment (I): Flow PoS network with the split of consensus and compute  This experiment simulates a network with the division of consensus and compute: • 20 slow nodes and 10 medium nodes form the consensus committee. They agree on the order of transactions within a block but don’t store or update the chain’s computational state. • Two fast nodes execute the transactions in the order they are finalized in the blocks. They do not participate in consensus. For further illustration, see Figure 3(I) and Table 1. In the Flow network, blocks only specify the transaction order, but there is no information about the resulting state included. As consensus in this model only covers the transaction order, consensus nodes are oblivious about the computational state. 15  4.1.2  Experiment (II): PoS network of nodes with diverse performances  Experiment (II) closely models conventional BFT consensus systems such as Tendermint [41, 42, 43] or Hot-Stuff [29]. The network is identical to Experiment (I), i.e., it consists of exactly the same types and numbers of nodes (see Table 1 and Fig. 3(II) for details and illustration). Though, blocks contain the transaction order and a hash commitment to the resulting computational state. Due to this result commitment, each consensus node must repeat the computation of all transactions for a proposed block to validate its correctness. In essence, there is only one role for a node: to participate in the consensus algorithm, which includes the task of updating the computational state. 4.1.3  Experiment (III): PoS network of nodes with uniform performance  Experiment (III), illustrated in Figure 3(III), simulates a network of 32 nodes with uniform computational performance. As in Experiment (II), all nodes execute the same algorithm which combines consensus about transaction ordering with their computation.  4.2  Experimental Results  Our simulations aim at benchmarking the transaction throughput. For each experiment, we sent 7995 benchmark transactions through the network and measured the corresponding processing time. The results are summarized in Table 2. Experiment (I) and (II) were executed with the same network configuration. The only difference was the separation of consensus and compute in the experiment (I), while both were combined in the experiment (II). In our moderately simplified model, separating compute from consensus increased the throughput approximately by a factor of 56. Comparing experiment (II) and (III) illustrates the limited impact of increasing network resources compared. In terms of instructions per seconds, the network in the experiment (II) is 3.75 times more powerful6 than in the experiment (III). However, the throughput of (II) increased only by 0.7% compared to the experiment (III). As the results show, separation of consensus and compute allows utilizing network resources more efficiently. In a PoS network with combined consensus and compute, deterministic block finalization requires a super-majority of nodes to vote in favor of a candidate block to be finalized. For may 6  Let a slow node process x instructions per second. Hence, under ideal resource utilization, the network in the experiment (III) can process 32x instructions. In contrast, the network in the experiment (III) processes 20x + 10 · 5x + 2 · 25x = 120x.  Experiment (I) Experiment (II) Experiment (III)  Processing Time [s]  Throughput [TX/s]  5.14 291 293  1555.4 27.5 27.3  Table 2: Network performances. Processing time for 7995 transactions in seconds [s] and the resulting transaction throughput [TX/s]. While we only conducted one-shot experiments, we have repeatedly observed throughout implementation that processing times fluctuate only on the sub-second scale.  16  BFT protocols, such as [29, 42, 44, 45], finalization requires supporting votes with an accumulated fraction of stake S > 2/3. Though, some protocols have other limits, e.g., S > 80% for [46, 47]. All of these protocols have in common that consensus nodes are obliged to execute the computation as a sub-task to verifying the proposed block. Therefore, the time for finalizing a block is bound by the fastest S th percentile of nodes. Less formally, the slowest nodes determine the throughput of the entire system. Consequently, running the network on stronger nodes leaves throughput unchanged as long as the slowest (1 − S)-fraction of nodes do not receive performance upgrades. In contrast, separation of consensus and compute significantly shifts computational load from the consensus nodes to the fastest nodes in the network.  5  Further Work  A blockchain designed around the principles outlined in this paper would need to address additional problems, the most notable being the mechanism that verifies computation outputs. On the one hand, splitting consensus and compute work boosts the throughput of Flow. On the other hand, special care has to be taken that the resulting states are correct as consensus nodes do not repeat the computation. Furthermore, in Flow, blocks no longer contain a hash commitment to the resulting state after computing the block. Therefore, a node that receives data from a block state cannot verify the validity of the received data based on the information published in the block. Nevertheless, a hash commitment for the result of a previous block can be published in a later block after passing verification. We will present the technical details of the block verification and commitment to the computation results (referred to as block sealing) in the follow-up papers. The presented simulations provide experimental evidence to support the theoretical work of this paper. While the theoretical results (section 3) stand on their own without experimental validation, the experiments could be extended significantly. For example, we have not accounted for the extra steps required to verify computational states and commit them into the chain. Another aspect is the size of the consensus committee. It would be interesting to study the scaling of transaction throughput with different committees sizes of consensus and execution nodes. However, we have decided to prioritize implementing the Flow architecture over benchmarking a simplified model system. Throughput and other performance characteristics will be measured and published as soon as a full-fledged implementation is completed.  6  Conclusions  In this proof-of-concept work, we have demonstrated that a separation of consensus and compute can lead to significantly increased resource utilization within PoS blockchain networks. For conventional round-based PoS networks, where one block is finalized before the subsequent block is proposed, the throughput is limited by a small fraction of the slowest nodes. In contrast, separation of consensus and compute significantly shifts computational load from the consensus nodes to the fastest nodes in the network. We have shown in Theorem 1 that such separation of concern does not compromise the network’s security. First experiments suggest that the throughput improvements enabled by such a separation of concerns are drastic. In a moderately simplified model, our simulations show  17  a throughput increase by a factor of 56 compared to architectures with combined consensus and block computation. One way to substantially increase the throughput of existing blockchains, such as Ethereum, could be to increase the gas limit. However, this would accelerate the rate at which the state grows making it harder for new nodes to join the system. While in conventional proof-of-work blockchains the computational load to maintain and update the state is uniform across all (full) nodes, the large majority of the computation resources are concentrated in a small fraction of mining nodes [40]. The Flow architecture utilizes the resource imbalance naturally occurring within a network ecosystem. The few data-center-scale nodes with massive computational and bandwidth capacities can stake to become Execution Nodes to contribute their resources most efficiently. In contrast, Consensus Nodes do not store or maintain the state and, therefore, can be run on off-the-shelf consumer hardware. With such separation of concerns, sharing a large state with new Execution Nodes joining the system should not pose a substantial challenge given the operational resources available to nodes with this role.  18  Acknowledgments We thank Dan Boneh for many insightful discussions, J. Ross Nicoll for contributions to an earlier draft, and Nick Johnson, Alex Bulkin, Karim Helmy, Teemu Paivinen, Travis Scher, Chris Dixon, Jesse Walden, Ali Yahya, Ash Egan, Casey Taylor, Joey Krug, Arianna Simpson, as well as Lydia Hentschel for reviews.  References [1] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system, 2009. [2] Kyle Croman, Christian Decker, Ittay Eyal, Adem Efe Gencer, Ari Juels, Ahmed Kosba, Andrew Miller, Prateek Saxena, Elaine Shi, Emin Gün Sirer, Dawn Song, and Roger Wattenhofer. On scaling decentralized blockchains. In Jeremy Clark, Sarah Meiklejohn, Peter Y.A. Ryan, Dan Wallach, Michael Brenner, and Kurt Rohloff, editors, Financial Cryptography and Data Security, pages 106–125, Berlin, Heidelberg, 2016. Springer Berlin Heidelberg. [3] BBC News. Cryptokitties craze slows down transactions on ethereum. 2017. https://www.bbc.com/news/ technology-42237162. [4] Richard Dennis and Jules Pagna Diss. An Analysis into the Scalability of Bitcoin and Ethereum, pages 619–627. 01 2019. [5] M. Pease, R. Shostak, and L. Lamport. Reaching agreement in the presence of faults. J. ACM, 27(2):228–234, April 1980. [6] Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. ACM Trans. Program. Lang. Syst., 4(3):382–401, July 1982. [7] Miguel Castro and Barbara Liskov. Practical byzantine fault tolerance. In Proceedings of the Third Symposium on Operating Systems Design and Implementation, OSDI ’99, pages 173–186, Berkeley, CA, USA, 1999. USENIX Association. [8] Scott Nadal Sunny King. Ppcoin: Peer-to-peer crypto-currency with proof-of-stake. 2012. http://www. peercoin.net/. [9] Ittai Abraham and Dahlia Malkhi. The blockchain consensus layer and BFT. Bulletin of the EATCS, 123, 2017. [10] Diksha Gupta, Jared Saia, and Maxwell Young. Proof of work without all the work. In Proceedings of the 19th International Conference on Distributed Computing and Networking, ICDCN ’18, pages 6:1–6:10, New York, NY, USA, 2018. ACM. [11] Shehar Bano, Alberto Sonnino, Mustafa Al-Bassam, Sarah Azouvi, Patrick McCorry, Sarah Meiklejohn, and George Danezis. Consensus in the age of blockchains. arXiv:1711.03936, 2017. https://arxiv.org/abs/1711. 03936. [12] Jason Spasovski and Peter Eklund. Proof of stake blockchain: Performance and scalability for groupware communications. In Proceedings of the 9th International Conference on Management of Digital EcoSystems, MEDES ’17, pages 251–258, New York, NY, USA, 2017. ACM. [13] Joseph Poon and Thaddeus Dryja. The bitcoin lightning network: Scalable off-chain instant payments. 2016. https://lightning.network/lightning-network-paper.pdf. [14] Joseph Poon and Vitalik Buterin. Plasma: Scalable autonomous smart contracts. White paper, pages 1–47, 2017. [15] Mahdi Zamani, Mahnush Movahedi, and Mariana Raykova. Rapidchain: A fast blockchain protocol via full sharding. IACR Cryptology ePrint Archive, 2018:460, 2018. [16] Loi Luu, Jason Teutsch, Raghav Kulkarni, and Prateek Saxena. Demystifying incentives in the consensus computer. In Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS ’15, pages 706–719, New York, NY, USA, 2015. ACM. [17] Nir Bitansky, Ran Canetti, Alessandro Chiesa, and Eran Tromer. From extractable collision resistance to succinct non-interactive arguments of knowledge, and back again. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS ’12, pages 326–349, New York, NY, USA, 2012. ACM. [18] B. Bünz, J. Bootle, D. Boneh, A. Poelstra, P. Wuille, and G. Maxwell. Bulletproofs: Short Proofs for Confidential Transactions and More. In 2018 IEEE Symposium on Security and Privacy (SP), pages 315–334, May 2018.  19  [19] Jason Teutsch and Christian Reitwießner. A scalable verification solution for blockchains, March 2017. Accessed:2017-10-06. [20] Vitalik Buterin’s comment on medium article “The Ethereum-blockchain size has exceeded 1TB, and yes, it’s an issue”. 2018. https://medium.com/hackernoon/the-ethereum-blockchain-size-has-exceeded-1tb-and-yesits-an-issue-2b650b5f4f62. [21] Matt Freels. FaunaDB: An Architectural Overview. 2018. https://www2.fauna.com/FaunaDB_Tech_WP. [22] Alexandre Verbitski, Tengiz Kharatishvilli, Xiaofeng Bao, Anurag Gupta, Debanjan Saha, James Corey, Kamal Gupta, Murali Brahmadesam, Raman Mittal, Sailesh Krishnamurthy, and Sandor Maurice. Amazon aurora: On avoiding distributed consensus for i/os, commits, and membership changes. pages 789–796, 05 2018. [23] Raymond Cheng, Fan Zhang, Jernej Kos, Warren He, Nicholas Hynes, Noah M. Johnson, Ari Juels, Andrew Miller, and Dawn Song. Ekiden: A platform for confidentiality-preserving, trustworthy, and performant smart contract execution. CoRR, abs/1804.05141, 2018. [24] Dawn Song. Oasis: Privacy-preserving smart contracts at scale, 2018. Microsoft Research Faculty Summit. [25] Silvio Micali, Salil Vadhan, and Michael Rabin. Verifiable Random Functions. In Proceedings of the 40th Annual Symposium on Foundations of Computer Science, FOCS ’99, pages 120–, Washington, DC, USA, 1999. IEEE Computer Society. [26] John R. Douceur. The sybil attack. In Peter Druschel, Frans Kaashoek, and Antony Rowstron, editors, Peerto-Peer Systems, pages 251–260, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg. [27] Frank Olken. Random sampling from databases. PhD thesis, University of California, Berkeley, 1993. http: //db.cs.berkeley.edu/papers/UCB-PhD-olken.pdf. [28] Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58(301):13–30, March 1963. [29] Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham. HotStuff: BFT Consensus in the Lens of Blockchain. 2018. http://arxiv.org/abs/1803.05069. [30] Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham. HotStuff: BFT Consensus with Linearity and Responsiveness. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, PODC ’19, pages 347–356, New York, NY, USA, 2019. ACM. [31] Vlad Zamfir. A Template for Correct-By-Construction Consensus Protocols. 2017. https://github.com/ ethereum/research/tree/master/papers/cbc-consensus. [32] Vlad Zamfir. Casper the Friendly Ghost: A ‘Correct By Construction’ Blockchain Consensus Protocol. 2017. https://github.com/ethereum/research/blob/master/papers/CasperTFG. [33] Vlad Zamfir, Nate Rush, Aditya Asgaonkar, and Georgios Piliouras. Introducing the “Minimal CBC Casper” Family of Consensus Protocols. 2018. https://github.com/cbc-casper/cbc-casper-paper. [34] Sarah Azouvi, Patrick McCorry, and Sarah Meiklejohn. Betting on Blockchain Consensus with Fantômette. 2018. http://arxiv.org/abs/1805.06786. [35] Jing Liu, Wenting Li, Ghassan O. Karame, and N. Asokan. Scalable byzantine consensus via hardware-assisted secret sharing. IEEE Transactions on Computers, 68:139–151, 2019. [36] Guy Golan-Gueta, Ittai Abraham, Shelly Grossman, Dahlia Malkhi, Benny Pinkas, Michael K. Reiter, DragosAdrian Seredinschi, Orr Tamir, and Alin Tomescu. SBFT: a scalable decentralized trust infrastructure for blockchains. CoRR, abs/1804.01626, 2018. https://arxiv.org/abs/1804.01626. [37] Mohammad M. Jalalzai, Costas Busch, and Golden Richard III. Proteus: A scalable BFT consesus protocol for blockchains. 2019. https://arxiv.org/abs/1903.04134. [38] Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms. IEEE/ACM Trans. Netw., 14(SI):2508–2530, June 2006. [39] Vlad Zamfir. Fundamental tradeoff in fault tolerant consensus protocols. https://twitter.com/vladzamfir/ status/942271978798534657, December 2017. [40] Ann Elizabeth Miller, James Litton, Andrew Pachulski, Neal Gupta, Dave Levin, Neil Spring, and Bobby Bhattacharjee. Discovering Bitcoin’s Public Topology and Influential Nodes. 2015. [41] Jae Kyun Kwon. Tendermint : Consensus without mining. 2014. https://github.com/cosmos/cosmos/tree/ master/tendermint. [42] Ethan Buchman. Tendermint: Byzantine Fault Tolerance in the Age of Blockchains, Jun 2016.  20  [43] Jae Kwon and Ethan Buchman. Cosmos - A Network of Distributed Ledgers. 2016. https://cosmos.network/ cosmos-whitepaper.pdf. [44] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer. Consensus in the presence of partial synchrony. J. ACM, 35(2):288–323, April 1988. [45] Miguel Castro and Barbara Liskov. Practical byzantine fault tolerance and proactive recovery. ACM Trans. Comput. Syst., 20(4):398–461, November 2002. [46] Jean-Philippe Martin and Lorenzo Alvisi. Fast byzantine consensus. IEEE Trans. Dependable Secur. Comput., 3(3):202–215, July 2006. [47] Dillkötter David. The ripple protocol consensus algorithm. Ripple Labs Inc., 2014.  21  Flow: Separating Consensus and Compute  arXiv:2002.07403v1 [cs.DC] 18 Feb 2020  – Block Formation and Execution –  Dr. Alexander Hentschel alex.hentschel@dapperlabs.com  Dr. Yahya Hassanzadeh-Nazarabadi  Ramtin Seraj  yahya@dapperlabs.com  ramtin.seraj@dapperlabs.com  Dieter Shirley  Layne Lafrance  dete@dapperlabs.com  layne@dapperlabs.com  Abstract Most current blockchains are built as a homogeneous system, comprised of full nodes, which are responsible for all tasks: collecting the transactions, block formation, consensus, and transaction execution. Requiring all full nodes to execute all tasks limits the throughput of existing blockchains, which are well documented and among the most significant hurdles for the widespread adoption of decentralized technology [1, 2, 3]. This paper is a follow-up on our previous proof-of-concept, Flow [4], a pipelined blockchain architecture, which separates the process of consensus on the transaction order from transaction computation. As we experimentally showed in our previous white paper, this provides a significant throughput improvement while preserving the security of the system. Flow exploits the heterogeneity offered by the nodes, in terms of bandwidth, storage, and computational capacity, and defines the roles for the nodes based on their tasks in the pipeline, i.e., Collector, Consensus, Execution, and Verification. While transaction collection from the user agents is completed through the bandwidth-optimized Collector Nodes, the execution of them is done by the compute-optimized Execution Nodes. Checking the execution result is then distributed among a more extensive set of Verification Nodes, which confirm the result is correct in a distributed and parallel manner. In contrast to more traditional blockchain architectures, Flow’s Consensus Nodes do not execute the transaction. Instead, Verification Nodes report observed faulty executions to the Consensus Nodes, which adjudicate the received challenges and slash malicious actors. In this paper, we detail the lifecycle of the transactions from the submission to the system until they are getting executed. The paper covers the Collector, Consensus, and Execution role. We provide a protocol specification of collecting the transactions, forming a block, and executing the resulting block. Moreover, we elaborate on the safety and liveness of the system concerning these processes.  1  Contents 1 Introduction 1.1 Architecture Overview . . . . 1.2 Design Principles . . . . . . . 1.2.1 Safety and Liveness . 1.3 Assumptions . . . . . . . . . 1.4 Flow’s Roles . . . . . . . . . . 1.4.1 Collector Role . . . . 1.4.2 Consensus Role . . . . 1.4.3 Execution Role . . . . 1.4.4 Verification Role . . . 1.5 States, Staking, and Slashing  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  . . . . . . . . . .  4 4 4 5 5 6 7 8 9 9 9  2 Preliminaries 2.1 Adversarial Model . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 HotStuff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Distributed Random Beacon . . . . . . . . . . . . . . . . . . . . 2.3.1 DRB Random Generation Phase (Threshold Signatures) 2.3.2 DRB Setup Phase (Distributed Key Generation) . . . . 2.4 Distributed Random Beacon Setup in Flow . . . . . . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  . . . . . .  11 11 11 12 13 15 16  . . . . .  17 17 17 19 20 22  . . . . . . . . . .  25 25 25 27 27 28 28 28 29 30 30  . . . .  32 32 33 33 34  3 Collection Formation 3.1 Overview . . . . . . . . 3.2 Cluster Formation . . . 3.3 Transaction submission 3.4 Collection Formation . . 3.5 Liveness . . . . . . . . .  . . . . .  . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . . .  4 Block Formation 4.1 Overview . . . . . . . . . . . . . . . . . . . . 4.2 Data Structures . . . . . . . . . . . . . . . . . 4.3 Block formation process . . . . . . . . . . . . 4.3.1 Block Proposal . . . . . . . . . . . . . 4.3.2 Block Proposal Evaluation and Voting 4.3.3 Finalizing a Proto Block . . . . . . . . 4.3.4 Source of randomness Attachment . . 4.4 Protocol State Updates . . . . . . . . . . . . 4.5 Adjudicating Slashing Challenges . . . . . . . 4.6 Correctness Proof . . . . . . . . . . . . . . . . 5 Execution 5.1 Overview . . . . . . . . 5.2 Collection Retrieval . . 5.3 Block Execution . . . . 5.3.1 Execution Result  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  . . . .  2  . . . .  . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . . . . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  . . . . .  . . . . . . . . . .  . . . .  5.4  5.3.2 Specialized Proof of Confidential Knowledge (SPoCK) . . . . . . . . . . . . . 35 5.3.3 Execution Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Correctness Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39  6 Mitigating Attack Vectors 40 6.1 Byzantine Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 6.2 Faulty Computation Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 Acknowledgments  41  References  41  3  1  Introduction  1.1  Architecture Overview  In most conventional blockchains, all the operational tasks are executed by full nodes, i.e., each full node is expected to collect and choose the transactions to be included in the next block, execute the block, come to a consensus over the output of the block with other full nodes, and finally sign the block, and append it to the chain. The structure resembles the single-cycle processing architecture of the early computers, where one instruction (i.e., block) is executed per step. On the other hand, the pipelined structure offered by Flow resembles the pipelined architecture of modern CPU design. While some unit of Flow is executing the current block, the pipeline architecture allows another unit to generate the next block, which results in an improvement over the throughput. In contrast to the majority of existing blockchains, which operate on the assumption that nodes are homogeneous, Flow considers resources heterogeneous in their network bandwidth, computation power, and storage capabilities. The Flow blockchain is built on a diverse set of heterogeneous nodes, each performing a specific set of tasks associated with their role(s). Specifically, there are four different roles: Collector Nodes, Consensus Nodes, Execution Nodes, and Verification Nodes, which handle the collection of transactions, generating blocks, executing blocks, and verifying the execution results, respectively. Flow allows a high level of participation in Consensus and Verification by requiring only a high-end consumer internet connection while leveraging large-scale data centers to operate as Execution Nodes. All types of nodes in Flow receive compensation through crypto-economic incentives. Prior research indicates that exploiting the heterogeneity of the nodes and taking the pipelined approach results in a throughput improvement by a multiplicative factor of 56 [4] compared to homogeneous architectures using only full nodes. We also showed that the decentralization and safety of the network is preserved by the Flow architecture [5]. It is worth noting that the separation of consensus from execution is done based on the separation of the subjective from objective operations. The Subjective operations are those that do not have a deterministic result. Instead, they require a consensus to be reached over the result. An example of a Subjective act is the set of transactions to be included in the next block. On the other hand, the Objective operations are the ones with a deterministic result and do not require the consensus over their result. An example of an Objective act is the execution of a transaction that transfers some tokens between two accounts. In Flow, the separation of consensus and execution is done by assigning the Subjective tasks to the Consensus Nodes, and the Objective tasks to the Execution Nodes. Hence, even a few data centers may run the Execution Nodes. Nevertheless, their computation results are deterministic, verifiable, attributable, punishable, and recoverable, and do not compromise the decentralization of the system. We iterate over these terminologies in the rest of this section.  1.2  Design Principles  For Flow to be secure against Byzantine failures and attacks, we identify the following core architectural principles. In this paper, by an honest actor, we mean a node that exactly follows protocol associated with its role and never deviates from the protocol. • Detectable: Any honest actor in the network can detect deterministic faults and prove the fault to all other honest nodes. The proof only requires asking other honest nodes to redo the faulty parts. 4  • Attributable: For performance reasons, many tasks in Flow are assigned randomly to a group of nodes. While the assignments are random for security reasons, it is based on Verifiable Random Functions (VRFs) [6]. Hence, any fault upon detection is also attributable to the responsible node(s) of the associated task. • Punishable: Every node participating in the Flow network must put up a stake, which is slashed when a detected fault is attributed to the node. Reliably punishing errors via slashing is possible because all errors in deterministic processes are detectable1 and attributable. • Recoverable: The Flow protocol contains specific elements for result verification and resolution of potential challenges. These elements act as countermeasures against the attempts of the malicious nodes to inject faults into the system. The probability of successfully injecting such faults is negligible. 1.2.1  Safety and Liveness  As a distributed multi-process system, the correctness of Flow protocol sets are evaluated against their safety and liveness [7]. A distributed multi-process system is safe against a set of features, if it provides guarantees that those features never happen in the system, unless with a negligible probability in some specific system parameter. Similarly, the system shows liveness respect to a set of features, if it provides guarantees that those features always persist, unless with a negligible probability in some specific system’s parameter. In this paper, we formally identify the liveness and safety features of the introduced protocols. We prove Flow is safe and live in the presence of a limited fraction of Byzantine actors in each role. In the design of Flow, we prioritize safety over liveness in case of a network split. A network split happens when at least a non-empty subset of nodes in the system is not accessible by rest of the nodes. In the extremely unlikely circumstance of a large-scale network split, where no connected subset of the network includes enough nodes to make forward progress, we allow the network to halt. This preserves the safety of the network even in the face of intentional network disruption, such as with an Eclipse attack [8].  1.3  Assumptions  The Flow architecture makes the following set of assumptions: • Consensus and Authentication – All nodes participating in the system are known to each other. – Each node is authenticated through its unforgeable digital signature. – To reduce the computational cost and preserve the sustainability of the system, the consensus protocol of Flow is based on Proof of Stake (PoS). • Participation in network 1  In Flow, nodes check protocol-compliant behavior of other nodes by re-executing their work. In most cases, verification is computationally cheap, with the noticeable exception of computing all transactions in a block. We describe in our previous white paper the procedure of distributing and parallelizing the verification of the computation of the blocks [5]. Hence, each Verification Node only performs a small fraction of the overall block computation.  5  – The evolution of the Flow blockchain is comprised of fixed intervals2 , called epochs. – To participate in the network, a node must put up the minimum required stake for that role in a specific epoch – A Flow node may participate over multiple epochs. • Source of randomness – Flow requires a reliable source of randomness for seeding its pseudo-random number generators. – The source of randomness enables each seed to be unpredictable by any individual node until the seed itself is generated and published in a fully decentralized manner. – In Flow, we use the Distributed Random Beacon (DRB) protocol [9] to generate a fully decentralised, reliable source of randomness. • Cryptography Primitives – Flow requires an aggregatable and non-interactive signature scheme, such as BLS [10]. • Network Model – Flow operates on a partially synchronous network with message traverse time bound by ∆t and the relative processing clock of the nodes bound by φt . – Flow uses a Byzantine Fault Tolerant message routing system, that guarantees message delivery with a high probability. • Rewarding and Slashing – Flow requires adequate compensation and slashing mechanics that incentivize nodes to comply with the protocol. • Honest Stake Fraction – Flow requires more than 23 of stake from Collection Nodes, Consensus Nodes, and Verification Nodes to be controlled by honest actors (for each node role separately). We will refer to a group of nodes with more than 32 of stake as a super-majority. A super-majorities of honest nodes probabilistically guarantees the safety of the Flow protocol.  1.4  Flow’s Roles  In Flow, roles are the services that the nodes provide to the system. One may assume each role as a specific set of protocols that a node executes. Accordingly, in Flow, we have four different roles (i.e., set of protocols), which are: Collector Role, Consensus Role, Execution Role, and Verification Role. We refer to the network nodes that execute the set of respective protocols as Collector Node, Consensus Node, Execution Node, and Verification Node. For each role, a minimum stake deposit is required from each of the participating nodes. Hence, single hardware system may host multiple roles in the network by staking for each of them individually. However, Flow treats individual 2  We envision that the length of an epoch will be measured in the number of blocks.  6  roles as if they are independent entities. In other words, each role is staked, unstaked, and slashed independently. The staked nodes associated with the roles are compensated through a combination of block rewards and transaction fees. Any combination of a public key and a role should be unique, and each peer has an independent staked value for each of its roles. We also recommend that multiple roles of a single peer do not share their staking keys for security reasons. We present a detailed description of each node role below. A high-level illustration of the role interactions is shown in Figure 1. Figure 2 shows an overview of the transaction’s lifecycle over considering the different roles of the nodes. 1.4.1  Collector Role  For the sake of load-balancing, redundancy, and Byzantine resilience, the Collector Nodes are staked equally and randomly partitioned into clusters of roughly identical size. At the beginning of an epoch, each Collection Node is randomly assigned to exactly one cluster. Each cluster of Collector Nodes acts as a gateway of Flow with the external world. In the mature Flow, we envision that a cluster will contain somewhere between 20 and 80 Collector Nodes. An external client submits their transaction to a Collector Node. Upon receiving a submitted, well-formed 3 transaction, a Collector Node introduces it to the rest of its cluster. The Collector Nodes of one cluster batch the received transactions into so-called collections. Only a hash reference to a collection is submitted to the Consensus Nodes for inclusion in a block. Each cluster of the collector nodes generates their collections one at a time. Before a new collection is started, the current one is closed and sent to the Consensus Nodes for inclusion in a block. The collections of a cluster are built collaboratively by each Collector Node sharing the transactions submitted to it with the rest of its cluster, and participating in a (light) consensus protocol with the other Collector Nodes of its cluster. The nodes come to consensus on both the end  Figure 1: An overview of the different roles in Flow as well as their interaction. For simplicity, only the message exchange during normal operation is shown. Messages for raising or adjudicating slashing challenges are omitted. 3  As we detail later in this paper, a well-formed transaction has all required fields filled properly and contains valid signature(s) from staked account(s) of Flow  7  of the current collection, to start a new one, and on the transactions included in the new collection. The collection generated as the result of a consensus among the Collector Nodes of a cluster is called a guaranteed collection. 1.4.2  Consensus Role  In Flow, the Consensus Nodes maintain the chain of blocks and are responsible for the chain extension by appending new blocks. They receive hash references to the guaranteed collections that were generated by the Collector Nodes. Furthermore, Consensus Nodes run a Byzantine Fault Tolerant (BFT) consensus algorithm to reach an agreement over the set of collections to be included  Figure 2: The lifecycle of a transaction in Flow. The yellow hexagons indicate the start of the individual stages in the Flow pipeline, e.g., the block sealing (i.e., Sealing Computation Results). The arrows show the inter-role message exchange between the nodes. Green rectangles correspond to the broadcasting events, in which a message is disseminated to all of the staked nodes. White rectangles represent the execution of the operations of the nodes. For the sake of simplicity, we represent the normal flow of the transaction lifecycle and ignored the message exchanges related to adjudicating slashing challenges.  8  in the next block. The block of the ordered collection that has undergone the complete BFT consensus algorithm is called finalized blocks. In Flow, a block specifies4 the included transactions as well as the other inputs (e.g., the random seed), which are required to execute the computation. It is worth noting that a block in Flow does not include the resulting execution state of the block execution. Consensus Nodes are also responsible for sealing a block. A Block Seal is a commitment to the execution result of a block after it is executed and verified (see [5] for more details on Block Sealing). Moreover, Consensus Nodes are responsible for maintaining a part of the state of the system related to the stakes of the nodes, receiving and adjudicating the slashing challenges, and slashing faulty nodes. We elaborate on the notion of protocol state on Flow in Section 1.5. 1.4.3  Execution Role  The Execution Nodes are powerful computational resources that are primarily responsible for scaling the computational power of Flow. Execution Nodes execute the finalized blocks generated by the Consensus Nodes. They publish the resulting execution state as an Execution Receipt. The Execution Nodes also need to provide the required information to the Verification Nodes so they can check the execution result. For this purpose, Execution Nodes break the computations of a block into chunks. Each Execution Node publishes additional information about each chunk in its Execution Receipt for the block executed. We detail the block formation process and chunking in Section 4 of this paper. 1.4.4  Verification Role  Verification Nodes are responsible for collectively verifying the correctness of the Execution Nodes’ published results. With the chunking approach of Flow, each Verification Node only checks a small fraction of chunks. A Verification Node requests the information it needs for re-computing the chunks it is checking from the Execution Nodes. A Verification Node approves the result of a chunk by publishing a Result Approval for that chunk, which means that the Verification Node has verified and agrees with the result of the execution of that chunk. Breaking the verification work into small chunks enables Verification Nodes to check the execution of chunks independently and in parallel. However, as we formally have shown in [5], all Verification Nodes together will check all chunks of the executed blocks with an overwhelming probability.  1.5  States, Staking, and Slashing  State: There are two sorts of states in Flow blockchain known as the execution state and protocol state. Each type of these states is maintained and updated independently. Both states are represented as key-value stores. While the execution state is maintained and updated by the Execution Nodes, the protocol state is maintained and updated by the Consensus Nodes. The execution state contains the register values, which are modified by transaction execution. Although the updates on the execution state of the system are done by the Execution Nodes, the integrity of the updates is governed by Flow’s verification process [5]. 4  A block implicitly specifies its transactions by referencing collections of transactions.  9  Protocol state, on the other hand, keeps track of the system-related features including, all the staked nodes, their roles, public keys, network addresses, and staking amounts. The protocol state is updated when nodes are slashed, join the system via staking, or leave the system via un-staking. Consensus Nodes publish updates on the protocol state directly as part of the blocks they generate. The integrity of the protocol state is guaranteed by the consensus protocol. We elaborate on updates to the protocol state, affected by the consensus protocol, in Section 4 of this paper. Staking: A node in Flow is required to deposit some stake in order to run a role. This requires the node to submit a staking transaction. The staking transactions for the next epoch take place before a specific deadline in the current epoch. Once the staking transaction is processed by the Execution Nodes, the stake is withdrawn from the node’s account balance and is explicitly recorded in the Execution Receipt. Upon Consensus Nodes sealing the block that contains this staking transaction they update the protocol state affected by this staking transaction, and publish the corresponding staking update in the block that holds the seal. Staked nodes are compensated through both block rewards and transaction fees and all roles require a minimum stake to formally participate in that role. Slashing: Any staked node of Flow can detect and attribute misbehavior to another staked node who committed it. Upon detecting and attributing misbehavior, the node issues a slashing challenge against the faulty node. Slashing challenges are submitted to the Consensus Nodes. The slashing challenge is a request for slashing a staked node due to misbehavior and deviation from the protocol. As the sole entity of the system responsible for updating the protocol state, Consensus Nodes adjudicate slashing challenges and adjust the protocol state (i.e., staking balances) of the faulty nodes accordingly. Based on the result of adjudication, the protocol state (i.e., the stake) of a node may be slashed within an epoch. The slashing update is announced in the subsequent blocks and is effective for all honest nodes as soon as they process the block containing the respective stake update.  10  2 2.1  Preliminaries Adversarial Model  We denote the honest nodes (i.e., non-Byzantine nodes) as the ones that follow the description of the Flow protocols. We call a node Byzantine if it deviates from any of Flow’s protocols at any arbitrary point. We also presume the non-responsiveness of a staked node, to messages and queries, to be a Byzantine act. Definition 2.1 Effective Votes During the consensus among a group of nodes, we consider the effective votes of the nodes as the overall staked fraction of the nodes who vote positively in favor of the consensus proposal. The fraction is taken over the entire stakes of the nodes in that group. For the Collector Role, Consensus Role, and Verification Role, we assume that more than 23 of the accumulated stake of each role belong to the honest nodes, and the rest is owned by the Byzantine nodes. For example, more than 32 of the stakes of the Collector Nodes belong to the honest ones and less than 13 of their overall stakes are owned by the Byzantine ones.  2.2  HotStuff  HotStuff [11, 12] is a distributed Byzantine Fault Tolerant (BFT) consensus algorithm. In this subsection, we present a summary of the basic HotStuff protocol. However, we scope out the optimization improvements which are presented in the HotStuff proposal for sake of brevity, and refer the interested readers to [12]. HotStuff assumes the nodes as state machines that hold the same shared replicated data (e.g., the blockchain ledger), and aim to reach consensus over the transition of the next state of the replicated data (e.g., the set of the transactions to be included into the next block). HotStuff is live under partially-synchronous network conditions. It is safe if less than one third of the consensus nodes’ total stake is controlled by Byzantine actors. The protocol progresses in rounds, i.e., each node increments a local counter when reaching consensus, or a timeout. For sake of liveness, the nodes double their timeout interval each time they move to a new round as the result of a time out. In each round, a unique consensus node assumes the role of the leader. Nodes can consistently identify the leader of each round by invoking a deterministic function locally on the round number. The leader advances the consensus via a three-phase commit protocol of prepare, pre-commit, and commit. The consensus at each round starts with a block proposal by the leader, and ends with reaching a consensus over the proposal or a timeout. In Flow, we assume that moving from one phase to another phase of HotStuff requires a minimum effective vote of 32 in favor of progressing with the leader’s proposal. The leader collects and aggregates the votes and broadcasts the aggregated signature to the entire system as a proof of moving to the next phase. Hence, each node can track and verify the correctness of the consensus progress in each phase by confirming that there are at least 23 effective votes. The correctness of the HotStuff protocol is formally proven via the safety and liveness theorems [12]. The safety implies that all honest nodes will eventually commit the same state transitions (i.e., blocks) in the same order. The liveness property of HotStuff guarantees that, after a Global Synchronization Time has passed, there is a bounded interval that enables an honest leader to advance the round towards reaching a consensus. Furthermore, HotStuff provides deterministic finality, 11  i.e., the protocol guarantees that there will be no forks in the chain of committed state transitions. This essentially means that the canonical chain of replicated state transitions (e.g., the ledger of blocks) solely acts as an append-only chain. Hence, in contrast to many existing consensus protocols (including Bitcoin’s Proof-of-Work (PoW)), chain reorganizations do not occur in HotStuff5 . The advantages of using HotStuff compared to the existing BFT leader-based counterparts include • Having n nodes participating in the consensus, the communication complexity of HotStuff is O(n), which makes switching of faulty or non-responsive leaders relatively easy. • The proposal of a failed or timeout leader has the potential to be resumed by the leader of the next round. • Preferring safety over the liveness, i.e., the safety over HotStuff is guaranteed regardless of the underlying synchrony model. However, for the protocol to guarantee its liveness and the progress, the Global Synchronization Time should be passed. • Ability to operate under partially-synchronous network conditions. • It has the so-called responsiveness property, i.e., an honest leader advances the protocol to the consensus in a time that is bounded by the message transmission delay, i.e., ∆t . • It has deterministic finality.  2.3  Distributed Random Beacon  In Flow, we utilize the Distributed Random Beacon (DRB) protocol [9] among a subset of nodes to generate an unbiased and verifiable source of randomness in a fully decentralized manner. By unbiased, we mean that no adversarial party can manipulate the source of randomness towards its desired distribution. By verifiability, we mean that once the source of randomness is determined, it is verifiable against the correct execution of the protocol. As a decentralized protocol, DRB is executed collaboratively among a subset of nodes as an interactive multi-party protocol. In Flow, we consider DRB as a 2-phase protocol consisting of three polynomial time protocols: Setup, RandomGenerator, and Verify. The first phase of DRB is the setup phase that happens only once among the participants. The second phase of DRB is the random generation phase, which is a repetitive operation, i.e., it may happen many times within the lifetime of the system. Each time the participants enter the random generation phase, they generate a random string that is denoted as the source of randomness. In this paper, we call every single execution of the random generation phase of DRB a DRB round. • (VG , ski ) ← Setup(Pi , 1λ ): Setup is a probabilistic decentralized protocol where each participating party i receives its private key, i.e., ski , as well as a public verification vector, i.e., VG . We elaborate on these in the rest of this section. • r ← RandomGenerator(ski , proto(b)): RandomGenerator is a deterministic decentralized protocol where parties collaboratively generate a fresh source of randomness. For each participating party in this protocol, the inputs are the party’s private key (i.e., ski ) and a proto 5  Provided that the stake controlled by Byzantine actors is strictly less than  12  1 . 3  block (i.e., proto(b)). The output of the protocol is the fresh source of randomness, i.e., r. We elaborate on the proto block notion in Section 4 of this paper. • Verify(r, proto(b), VG ) = True/False: Verify is a deterministic protocol that is executed locally by any external or internal party to DRB. Knowing a generated source of randomness (i.e., r), its associated proto block (i.e., proto(b)) and the public verification vector of the DRB (i.e., VG ), one can deterministically verify the correctness of the protocol execution. In addition to being unbiased, DRB provides the following properties: • Unpredictability: Denoting the security parameter of the system by λ, before the execution of a new round of DRB, for each probabilistic polynomial-time predictor A, there exists a negligible function negl(λ), such that: P r[A(r1 , r2 , r3 , ..., rx−1 , proto(b1 ), proto(b2 ), proto(b3 ), ..., proto(bx−1 ), proto(bx )) = rx ] ≤ negl(λ) where ri is the output of the ith execution of DRB, and proto(bi ) is the associated proto block to the ri . In other words, given all the generated entropies as well as their associated proto blocks to the predictor A, it is not able to predict the output of the next round before its execution unless with a negligible probability in the security parameter of the system. • Verifiability: Given the generated source of randomness of a round, its corresponding proto block, as well as some public metadata of the protocol, any external party to the protocol can verify the generated source of randomness. 2.3.1  DRB Random Generation Phase (Threshold Signatures)  The main idea of DRB is based on the existence of a deterministic and unique non-interactive threshold signature protocol. BLS signature primitive [10] provides all these required properties. A threshold signature is a tuple of four polynomial-time algorithms known as (Gen, Sign, Recover, Verify), which is defined over the set G of n parties (i.e., G = {P1 , P2 , ..., Pn }) and is identified by two parameters: t and n. An (t, n)-threshold signature enables the set of n parties to collaboratively generate an aggregated signature over a message m using a distributed secret key in a decentralized manner. The threshold signature enables any entity to efficiently verify a valid signature over m against the distributed secret key. The verification is solely done by verifying the aggregated signature, without requiring the individual party’s signatures. • (VG , sk1 , sk2 , ..., skn ) ← Gen(1λ ): Gen is a probabilistic polynomial-time algorithm that on receiving the security parameter, it implicitly generates a secret key skG and distributes individual key shares over the participating parties i.e., ski for the ith party. skG remains secret and not explicitly computed although it is jointly generated by all parties. Moreover, a public verification vector VG is generated. VG enables parties to recover each others’ public key shares. By the public key of the ith party, we mean the public key pki associated with ski . VG also contains the public key of the group of parties, i.e., pkG associated with skG .  13  • σi ← Sign(ski , m): Sign is a deterministic polynomial-time algorithm that is executed by each party individually. On receiving a message m and the private key of the ith party (i.e., ski ), Sign(ski , m) generates a signature σi on the message m that is attributable to the ith party. σi is also commonly called the threshold signature share of the ith party. Sign is the same signature function defined in the BLS signature scheme. • σ ← Recover(Pi1 , Pi2 , ..., Pit+1 , σi1 , ..., σit+1 ): Recover is a deterministic polynomial-time algorithm that is executed by any entity either internal or external to the group of parties. On receiving the distinct threshold signature shares σik and the list of corresponding distinct parties, Recover combines all the signature shares and recovers the group signature, i.e., σ. Having any combination of more than t signature shares of the parties over the same message m, Recover generates a group signature σ that is attributable to the group secret key, i.e., skG . • Verify(σ, pkG , m) = True/False: Verify is a deterministic polynomial-time algorithm that is executed by any entity either internal or external to the group of parties. On receiving a recovered threshold signature (i.e., σ) on a message m, it returns True if the signature σ was generated using the group secret key (i.e., skG ), and returns False otherwise. Verify is the same signature function defined in the BLS signature scheme. While Sign, Recover and Verify are non-interactive and relatively inexpensive operations, Gen requires to run an interactive and relatively slow protocol by all parties. Keeping the performance factor in mind, Gen is separated from the repetitive DRB random generation phase in Flow. Gen serves as the Setup phase while the tuple (Sign, Recover, Verify) serves as the random generation phase. This is possible because the random generation allows generating repetitive signatures using skG without explicitly reconstructing it, and therefore protecting its confidentiality. The DRB random generation phase is started every time the nodes need to generate a new source of randomness. Protocol 1 shows a DRB random generation round for a specific proto block, i.e., proto(b). To generate a new source of randomness, each node i computes and broadcasts a threshold signature share over proto(b). Upon receiving at least t + 1 distinct threshold signature shares {σb,1 , σb,2 , σb,3 , ..., σb,t+1 } over the proto block, any party can aggregate the signatures into a threshold signature σb , using the function Recover. If the signature σb is correctly formed, it is verifiable against the public key of the group G, i.e., Verify(σr , proto(b), pkG ) = True. The source of randomness of this DRB random generation round is σb . The deterministic digest of σb is used to seed the pseudo-random generators during the current round.  14  Protocol 1 DRB Random Generator Inputs: For each i ∈ [1, n], party Pi holds the inputs of proto(b) and ski . H is a cryptographic hash function. Goal: Parties jointly compute a fresh source of randomness, r, based on proto(b). The protocol: 1. Each party Pi generates σb,i ← Sign(proto(b), ski ), and broadcasts σb,i to all other parties. 2. Any internal or external party then captures the broadcasted σb,j 3. Upon having at least t + 1-many σb,. from t + 1 distinct parties, each party recovers the aggregated threshold signature r = σb ← Recover(Pi1 , ..., Pit+1 , σb,i1 , ..., σb,it+1 ). 4. Having r = σb , any party extracts a fresh seed as H(r) In Flow, we use [13] as the underlying threshold signature scheme for the DRB, which is noninteractive and provides unique signatures. By non-interactive, we mean that the protocol is modeled by a single round of communication. Uniqueness is an inherited property from the BLS scheme, and it means that there exists a unique aggregated threshold signature for each pair of (pkG , m). Hence, the aggregated threshold signature is deterministic and is always the same regardless of the subset of collected threshold signature shares over the same message. The non-interactiveness feature is for sake of improvement on the communication complexity of the DRB protocol over several instances (i.e., rounds) of execution. The uniqueness is vital to guarantee a consistent source of randomness independent of the subset of threshold signature shares, i.e., every valid subset of t + 1-many threshold signature shares results in the same aggregated threshold signature. 2.3.2  DRB Setup Phase (Distributed Key Generation)  Distributed Random Beacon (DRB) setup phase is to generate the keys for the threshold signature protocol. As shown by Protocol 2, the setup phase of DRB in Flow is mainly the Gen function of the threshold signature. During this phase, the parties of the group G collaboratively execute a Distributed Key Generation protocol (DKG) [14] as the decentralized implementation of Gen function. The DKG implicitly generates a group secret key skG . As the result, each party Pi receives the public verification vector VG as well as its individual private key ski . The secret key skG is generated using entropy provided by all participating parties in an unbiased manner. It remains unknown to all participants although it can be reconstructed by any party receiving more than t shares of the secret key. The DKG implemented in Flow is a variant of Pedersen’s protocol called Joint-Feldman [14] protocol. It is a Discrete Logarithm-based protocol and mainly constitutes of n parallel executions of the Feldman verifiable secret sharing (VSS) protocol with each party acting as a leader in exactly a single instance. Although it has been proven that Joint-Feldman protocol does not guarantee a uniform distribution of the secret key skG , the security of the DKG in Flow is based on the hardness of the Discrete Logarithm Problem, which is not weakened by the distribution bias.  15  Protocol 2 DRB Setup Inputs: For each i ∈ [1, n], party Pi holds its index in the set as well as the system’s security parameter, λ. Goal: Parties jointly compute a public verification vector VG as well as their threshold signature private keys, i.e., ski for Pi . The protocol: 1. Each party Pi invokes an instance of DKG protocol as specified in [14], and obtains: (VG , ski ) ← DKG(Pi , 1λ )  2.4  Distributed Random Beacon Setup in Flow  We assume the setup phase of DRB happens once every epoch to prepare the DRB protocol keys. During the setup phase, a protocol-selected subset of ns Consensus Nodes jointly execute the DRB setup protocol. Through executing the setup protocol, each node i generates its DRB private key ski for the threshold signature as well as a public verification vector VG . As explained in Section 2.3, (n, t) are the parameters of the threshold signature scheme. An adversary who corrupts up to t parties is not able to forge a valid threshold signature. It is also required that at least t + 1 parties act honestly to guarantee the liveness of the protocol. In order to satisfy both properties with the high probability, i.e unforgeability and liveness, we compute t as follows. ns − 1 t=b c (1) 2 In Flow, the size of the DRB committee ns is tuned to ensure that unforgeability and liveness are guaranteed with a high probability in the security parameter of the system (i.e., λ). However, ns also is chosen small enough to preserve the efficiency of the DRB execution with respect to the operational complexities.  16  3 3.1  Collection Formation Overview  In Flow, collection formation denotes the process which starts with a user agent submitting a transaction to the Collector Nodes and ends when a guaranteed collection is submitted to the Consensus Nodes. The Collector Nodes are partitioned into clusters. Each transaction is assigned to a certain cluster based on its transaction hash. A user agent submits a transaction to some Collector Nodes in the responsible cluster. On receiving a transaction, the Collector Nodes of the cluster broadcast the transaction among each other. Collector Nodes of a cluster continuously form consensus over the set of transactions contained in the collection under construction. As a result of the consensus, a collection grows over time. Once one of two conditions is met: either the collection size reaches a threshold or a pre-defined time span has passed, the collection is closed and submitted to the Consensus Nodes for inclusion in a block.  3.2  Cluster Formation  In Flow architecture, the stake is a measure of the nodes’ accountability. In specific for the Collector Nodes, the workload accountability of processing the submitted transactions and hence their compensation is a monotonically increasing function of their stake deposit per processed transaction. To make all the Collector Nodes equally accountable, we desire a setup where the stake deposit per processed transaction is similar for all the Collector Nodes. This setup with similarly staked Collector Nodes results in a similar workload distribution among them on processing the submitted transactions. This setup stands against the non-uniform stake distribution of the Collector Nodes, which results in a non-uniform workload of processing the submitted transactions among them. Despite our equally staked Collector Nodes principle, the user agents submitting their transactions to an arbitrary Collector Node of their choice results in a biased distribution of the workload and its associated rewards. It is trivial that over the time the user agents may establish a relationship with Collector Nodes, e.g., running the client software provided by the Collector Nodes, submitting to the Collector Node that provides a better quality of service, etc. This, however, leads to a heterogeneous load distribution on the Collector Nodes and degrades the decentralization of the system. For example, a Collector Node may provide significantly higher utilization than the others, attract more rewards and essentially starve other Collector Nodes of income. To design a system where the stake deposit per processed transaction is comparable for all collectors, Flow introduces the notion of clusters. Several Collector Nodes are grouped into the same cluster to collect and process the same set of transactions. In other words, instead of having a pool of transactions where each Collector Node arbitrarily selects transactions to process, there is a one-way deterministic assignment between each transaction and a cluster of the Collector Nodes using the source of randomness. Clustering of Collector Nodes is done at the beginning of each epoch, e.g., once every week. The number of clusters in Flow is a protocol parameter denoted by c. As the result of clustering, the Collector Nodes are randomly partitioned into c clusters. Clustering is done in a way that the size of each two different clusters varies by at most a single node. The assignment is nevertheless verifiable, i.e., each node is able to run the clustering algorithm offline and reach the same cluster assignment for the Collector Nodes as every other node. As we detail in the rest of this section, this enables the user agents to efficiently map their signed transactions to the cluster of the Collector Nodes that is responsible for processing them.  17  The clustering is done by each Collector Node running the cluster assignment algorithm as illustrated by Algorithm 3.1. The inputs to this algorithm are the list of all Collector Nodes’ public keys (i.e., Nc ), the number of clusters (i.e., c), and the source of randomness (i.e., r), which is generated by the Distributed Random Beacon protocol [9] (see Section 2.3). We denote the cardinality of Nc as nc , which corresponds to the number of Collector Nodes in the system. The output of the algorithm is Cls, which is a map from the public keys of the Collector Nodes to their assigned clusters, i.e., for every pk ∈ Nc , element Cls[pk] is the Collector Node’s assigned cluster id. The clustering is done by deriving a seed s from the source of randomness r solely for the clustering of Collector Nodes (Algorithm 3.1, Line 1). The seed is derived in a deterministic manner and has the same entropy as r. The derived seed is then being utilized by the Fisher-Yates shuffling algorithm [15]. On receiving the seed s and the list of Collector Nodes’ public keys Nc , the Fisher-Yates algorithm turns Nc into a pseudo-random permutation of Collector Nodes’ public keys, denoted by πc . Note that as πc is a permutation of Nc , it is of the same cardinality as Nc , i.e., |Nc | = |πc | = nc (Algorithm 3.1, Line 2). Once the permutation πc is determined, the clustering algorithm partitions the Collector Nodes into c clusters. It does that by clustering the first k Collector Nodes into the cluster number 0, the second k Collector Nodes into the cluster number 1, and similarly the ith sequence of the Collector Nodes of size k into the cluster number i − 1. k is the size of each cluster and is determined as k := b ncc c (Algorithm 3.1, Lines 3-10). In case the number of Collector Nodes is a multiple of the number of clusters, i.e., b ncc c = nc nc c = d c e = k, clustering algorithm stratifies the Collector Nodes based on their position in the permutation πc into c clusters of size k. Otherwise, if the number of Collector Nodes is not a multiple of the number of clusters, clustering based on Lines 3-10, results in some Collector Nodes to be leftover (Specifically, the number of leftover Collector Nodes will be less than c). In that situation, the clustering algorithm does not create an extra cluster to accommodate the leftovers. It rather distributes the leftovers among the existing clusters by adding the ith Collector Node of the leftovers to the ith cluster. Size of the leftovers in this case is nc mod c, i.e., the remainder of nc divided by c. Hence, clustering in this manner results nc mod c clusters of k + 1 Collector Nodes, and c − nc mod c clusters of k Collector Nodes (Algorithm 3.1, Lines 11-15). Having nc Collector Nodes in the system, Algorithm 3.1 has both an asymptotic time and memory complexity of O(nc ), and does not impose a communication overhead to the system.  18  Algorithm 3.1: ClusterAssignment Input: Nc : List; element Nc [i] is public key of ith Collector Node c: System-wide constant unsigned integer; number of the clusters r: byte array; Source of randomness Output: Cls: Map; element Cls[pk] is the cluster id of Collector Node with public key of pk // Deriving a random seed from r for clustering Collector Nodes 1  s := H(‘collector’||‘cluster’||r); // Shuffling the collectors list  2  πc := FisherYates(s, Nc ); // k defines the size of clusters  3  k := b ncc c;  4  i := 0;  // i keeps current cluster’s id // j keeps current Collector Node’s index in πc 5  j := 0; // stratifying Collector Nodes into c clusters of size k  6  while j < c × k do // adding j th Collector Node to cluster i  Cls[πc [j]] := i;  7  // moving to the next Collector Node  j + +; if j mod k = 0 then  8 9  // current cluster has reached size k // moving to the next cluster  i + +;  10 11 12  i := 0; while j < nc do // adding j th Collector Node to cluster i  13  Cls[πc [j]] := i; // moving to the next cluster  14  i + +; // moving to the next Collector Node  15  3.3  j + +;  Transaction submission  Listing 1 represents the structure of the Flow’s transactions that the user agents submit to the Collector Nodes. In this listing, Script corresponds to the content of the transaction. The transaction’s content manipulates the execution state of Flow, and requires a transaction fee to be processed. PayerSignature corresponds to the signature created by the account that is paying the transaction fee (e.g., the gas fee). The ScriptSignature of the transaction corresponds to the signatures of the execution state owners, which grant the transaction permission to manipulate their execution state. A transaction may manipulate several parts of the execution state and hence may require the signatures of several execution state owners. ReferenceBlockHash points to the hash of a previous block, which is used to provide a deadline for the transaction to be processed. Each submitted transaction must be processed within a limited window of blocks, e.g., the height 19  difference between the block that the transaction’s ReferenceBlockHash points to and the block that the transaction appears in should be less than a predefined parameter. For example, assume that the window size for the transactions is 10 blocks and a transaction is pointing to a block at height 1000 in its ReferenceBlockHash. This means that the transaction can only be processed between blocks of height 1001 and 1010. Otherwise, the transaction is invalid. Let h denote hash of the SignedTransaction, interpreted as an unsigned integer. To submit its transaction, the user agent determines the cluster of the Collector Nodes that are responsible for collecting its transaction. The collection determination is done as shown by Equation (2), where c corresponds to the number of clusters in the system. The index of the cluster, to which the transaction with hash h should be submitted to is ch = h mod c .  (2)  Here, mod is the modulus operation that determines the remainder of the integer-division of the left-side operand over the right-side operand. In Flow, the assignment of Collector Nodes to clusters is easily computed in a deterministic fashion by executing Algorithm 3.1. Hence, a user agent can efficiently map its signed transaction to a cluster of the Collector Nodes using Algorithm 3.1 and Equation (2). The transaction submission is done by the user agent sending its signed transaction to the cluster with index ch that is driven by Equation (2). By sending a transaction to a cluster, we mean sending to at least one of the Collector Nodes belonging to that cluster. However, for sake of fault tolerance in the presence of Byzantine Collector Nodes as well as (network) failures, a user agent may send the same transaction to several Collector Nodes of the assigned cluster. In Flow, we leave the number of Collector Nodes that a user agent needs to submit its transaction as a user-dependent decision. A user with a strong set of utilities may prefer to send all or only its important transactions to the entire cluster, while another user may prefer to submit it only to a single node of the assigned cluster. Upon receiving a transaction from a user agent, the Collector Node checks whether the transaction is submitted to the correct cluster according to Equation (2) as well as whether the transaction is well-formed. A transaction is well-formed if it contains all the fields as shown by Listing 1 as well as valid ScriptSignature(s) and PayerSignature by the registered accounts of Flow. If the transaction is well-formed as well as submitted to the right cluster, the Collector Node broadcasts the signed transaction to the entire cluster. Listing 1: User agent broadcasts the transaction to multiple Collector Nodes of the assigned cluster. 1 2 3 4 5 6  message SignedTransaction { bytes Script ; bytes PayerSignature ; repeated Signature ScriptSignature ; bytes Re ferenceBlockHash ; }  3.4  Collection Formation  In Flow, the term collection refers to an ordered list of one or more hashes of signed transactions. Collector Nodes form collections by coming to consensus on the ordered list of transaction hashes. As discussed in Section 2.2, Flow uses HotStuff [11, 12] as the consensus protocol among the 20  Collection Node clusters, for them to form collections. However, any BFT consensus protocol with deterministic finality would apply to the architecture. In each round of consensus, the selected leader either resumes an uncompleted or time-outed proposal of the previous round or makes a new proposal. A proposal is either on appending a list of the signed transactions’ hashes to the current collection or on closing the current collection and starting a new empty collection. For each phase of the consensus to proceed a minimum effective vote of 23 on the leader’s message is required. For a Collector Node to vote in favor of an append proposal, the following conditions must all be satisfied: • The Collector Node has received all the signed transactions that their hashes represent are reflected in the append proposal. • Each of the transaction hashes represented in the append proposal should be well-formed and signed by at least one of the Collector Nodes of the same cluster. • Appending the transactions from the proposal to the collection should not result in duplicate transactions. • There should not be common transactions between the current collection under construction and any other collection that the cluster of the Collector Node has already guaranteed. Once a message has the minimum effective vote of 32 , the leader aggregates the signatures associated with the individual votes which advances the consensus process to the next phase. In this way, every individual Collector Node can verify the correctness of the consensus process in a fully decentralized manner. Initially, a new empty collection is created as a shared replicated state in every Collector Node. Upon reaching a consensus over an append operation, the signed transactions’ hashes are appended to the collection. Once the collection size reaches a protocol’s predefined threshold, a non-Byzantine leader of the subsequent rounds propose closing the collection. Consensus over the closing a collection is done by following the HotStuff protocol. The structure of a Guaranteed Collection is presented in Listing 2, where CollectionHash is the hash of the closed collection. ClusterIndex is the id of the cluster (see Algorithm 3.1) that has generated the collection, reached a consensus over and closed it. AggregatedCollectorSigs holds the aggregated signatures of the Collector Nodes that have guaranteed the collection. For the GuaranteedCollection to be considered as valid, AggregatedCollectorSigs should contain at least 23 effective votes of the Collector cluster. We call each of the Collection Nodes that participate in making a guaranteed collection, a guarantor of that collection. By singing a collection, a guarantor attests to the followings: • All the transactions in the collection are well-formed. • They will store the entire collection including the full script of all transactions. In Flow, we consider a guaranteed collection to be an immutable data structure. The guaranteed collection is broadcasted by the guarantors to all Consensus Nodes for inclusion in a block.  21  Listing 2: Each Collector Node broadcasts the guaranteed collection reference to all the Consensus Nodes. 1 2 3 4 5  message G uaranteedCollection { bytes CollectionHash ; uint32 ClusterIndex ; Signature A gg r eg at e dC o ll ec t or Si g s ; }  3.5  Liveness  We introduce Lemma 3.1 before presenting the safety theorem of the collection formation process, and directly utilize it as part of the proof for the safety of the collection formation process. Lemma 3.1 Given a guaranteed collection that is generated by a set of guarantors in a cluster with the following conditions: • Network Model: partially synchronous network with message traverse time-bound by ∆t and the relative processing clock of the nodes bound by φt • Message Delivery: a fault-tolerant message routing functionality that guarantees the message delivery with high probability • Stake Distribution: all the Collector Nodes are equally staked • Honest Behavior: the honest actors follow the protocols as specified, and maintain their availability and responsiveness in the system over time • Honest Lower-bound: the guaranteed collection has at least one honest guarantor The guaranteed collection is always available and recoverable in the system. Proof of Lemma 3.1: As detailed in Section 3.4, the guarantors of a guaranteed collection maintain the original copy of it, and only broadcast the collection reference that is presented as a GuaranteedCollection to the Consensus Nodes. In other words, a guaranteed collection is supposed to be replicated on all of its guarantors, i.e., the Collector Nodes that signed it. By contradiction, assume that the original content of a guaranteed collection is not recoverable. This would imply the occurrence of at least one of the following: • None of the collection’s guarantors are available or responsive. • The message delivery to and from the collection’s guarantors is compromised by an attack, e.g., Routing Attack. • There is no time-bound on the message delivery to and from the collection’s guarantors. • There is no time-bound on the processing clock of the collection’s guarantors. As following the Honest Lower-bound assumption, there exists at least one honest Collector Node among the guarantors, having none of the collection’s guarantors responsive or available contradicts the Honest Behavior assumption of the lemma stating the necessity on the honest 22  actors to maintain their availability and responsiveness. Having the message delivery concerning the guarantors under attack contradicts the Message Delivery assumption of this lemma on the guaranteed message delivery to and from the guarantors. Having no bound on the processing clock of guarantors or the message delivery to and from them contradicts the Network Model assumption of the lemma. Hence, we conclude that as long as there exists at least one honest guarantor for a guaranteed collection under the assumptions of Lemma 3.1, the collection remains available, accessible, and recoverable.  Theorem 3.2 (Collection Text Availability) Given a system with the following properties: • Network Model: a partially synchronous network with message traverse time bound by ∆t and the relative processing clock of the nodes bound by φt . Each node is assumed to have a proper defence and protection against network attacks (e.g. Denial-of-service attacks). • Message Delivery: a fault-tolerant message routing functionality that guarantees the message delivery with high probability • Stake Distribution: equally staked Collector Nodes • Byzantine Fraction: more than  2 3  of Collector Nodes’ stakes are controlled by honest actors  • Honest Behavior: honest actors follow the protocols as specified, and maintain their availability and responsiveness in the system over time There exists a configuration of Flow where each guaranteed collection is available with a high probability. Proof of Theorem 3.2: In our previous white paper [5], we have shown that by having nc = 1040, 1/3 of these nodes being Byzantine, and our typical cluster size of [50, 80] nodes, the probability of having a Byzantine cluster is very unlikely. Despite Flow’s proactive architectural setup for maintaining the availability of the collections, Flow also introduces a reactive mechanism (i.e., Missing Collection Challenge6 ) to confirm and slash the Byzantine actors attributed to a missing collection [5]. Theorem 3.3 (Liveness of Collection Formation) Given a system with the following properties: • Network Model: a partially synchronous network with message traverse time-bound by ∆t and the relative processing clock of the nodes bound by φt 6  The analysis of the Missing Collection Challenge in [5] (specifically Proof of Theorem 3 therein) is more extensive than the proof presented here. Here, we assume that an honest actor will diligently but unsuccessfully query all guarantors for the desired collection before raising a Missing Collection Challenge. The analysis in [5] also covers the case of a lazy or trolling requester, which might query a subset of the guarantors and raise a challenge if the queried subset does not respond.  23  • Message Delivery: a fault-tolerant message routing functionality that guarantees the message delivery with high probability • Stake distribution: equally staked Collectors Nodes • Byzantine fraction: more than  2 3  of Collector Nodes’ stakes are controlled by honest actors  • Honest Behavior: the honest actors follow the protocols as specified, and maintain their availability and responsiveness in the system over time the collection formation always progresses. Proof of Theorem 3.3: By contradiction, assume that there is an unknown point of time tstop for which the collection formation process as described in this section stops working for all points of time t > tstop . By collection formation being stopped, we mean that there is absolutely no single collection formed by any of the clusters of Collector Nodes once tstop elapses. This implies at least one of the following situations applies to the system by elapsing tstop , which results in none of the clusters being able to reach a consensus over closing their collection: • Less than  2 3  of the Collector Nodes of every cluster are honest.  • The message delivery to a fraction of at least mised by an attack, e.g., Routing Attack.  2 3  of Collector Nodes of each cluster is compro-  • There is no time-bound on the message delivery to and from a subset of at least Collector Nodes in each cluster. • There is no time-bound on the processing clock of at least  2 3  2 3  of the  of Collector Nodes of each cluster.  Following the Honest behavior and Stake distribution assumptions of the theorem, having less than 32 honest Collector Nodes contradicts the Byzantine fraction assumption of the theorem. Likewise, compromising the message delivery to a fraction of at least 23 of the Collector Nodes on each cluster contradicts the Message Delivery assumption of the theorem. Finally, having no bound on either the message delivery time to or the processing time of at least 23 of Collector Nodes in each cluster contradicts the Network Model assumption of the theorem. Hence, we conclude that as long as the assumptions of the theorem hold, the collection formation progresses. It is worth mentioning that, as discussed earlier, we envision it is rare for a Byzantine cluster to exist. Such a cluster may aim at compromising the liveness of the system by ignoring all the submitted transactions to that cluster. Nevertheless, upon detecting an attack on liveness by a user agent, the agent may simply try to switch the corresponding cluster of its transaction by changing some metadata of its transaction, e.g., changing the transaction’s ReferenceBlockHash (see Listing 1). This minor change results in the transaction’s cluster assignment changing and so, it can be routed to a (non-Byzantine) cluster (see Equation (2)) and hence processed towards inclusion in a collection.  24  4  Block Formation  4.1  Overview  Block formation is a continuous process executed by consensus nodes to form new blocks. The block formation process serves multiple purposes: • Including the newly submitted guaranteed collections and reaching agreement over the order of them. • Providing a measure of elapsed time by continuously publishing blocks and determining the length of an epoch. • Publishing result seals for previous blocks of the chain. A block’s result is ready to be sealed, once it is finalized by the Consensus Nodes, executed by the Execution Nodes, and the execution result is approved by the Verification Nodes (see [5] for more details). • Publishing slashing challenges and respective adjudication results. • Publishing protocol state updates, i.e., slashing, staking, and unstaking. Whenever a node’s stake changes, the Consensus Nodes include this update in their next block. • Providing a source of randomness (see Section 2.3 for more details). In an unlikely event of no new guaranteed collection, consensus nodes continue block formation with an empty set of collections. In other terms, block formation process never gets blocked. The block-formation process utilizes the underlying round-base consensus protocol we described in Section 2.2. In Flow, we distinguish between ProtoBlocks and (proper) Blocks, which are formally defined in section 4.2 below. The BFT consensus algorithm generates and finalizes ProtoBlocks. The only difference between a ProtoBlock and a (proper) Block is that a ProtoBlock does not contain any source of randomness. The source of randomness in a Block is required for various processes in Flow, including deterministically generating pseudo-random numbers during transaction execution, assigning collectors to clusters, etc. Hence, ProtoBlocks cannot be processed by nodes other than consensus nodes due to the lack of source of randomness. After a ProtoBlock is generated by the BFT consensus protocol, the Distributed Random Beacon (DRB) nodes sign the ProtoBlock. Their threshold signature over the ProtoBlock is the Block’s source of randomness. The ProtoBlock plus the DRB’s signature together form the (proper) Block. While the Random Beacon is run by a set of consensus nodes, generating the randomness is not part of the consensus process. Instead, the source of randomness is added in a subsequent step to generate a proper Block.  4.2  Data Structures  ProtoBlocks The BFT consensus algorithm works on the level of ProtoBlock. In each round, a consensus node is selected as primary, whose main task is to collect signatures for the last ProtoBlock and propose the next ProtoBlock. Once a ProtoBlock is proposed, the Consensus Nodes vote for its inclusion in the finalized chain. The structure of a well-formed ProtoBlock is specified in Listing 3. We provide definitions for the ProtoBlock’s individual fields below. 25  Listing 3: Flow’s proto block structure 1 2 3 4 5 6 7 8  message ProtoBlock { bytes previousBlockHash ; uint64 height ; repeated GuaranteedCollection gua rante edCo llect ions ; repeated BlockSeal blockSeals ; Sl as hi ngChallenges slashingChallenges ; P r o t o c o lStateUpdates protocolStateUpdates ; }  • previousBlockHash points to the hash of the immediate predecessor of this block on the chain. By pointing to the previous block, blocks form a blockchain. • height corresponds to the current index of the block in the chain. Formally, the block chain forms a tree with the blocks as vertices. The height of a block is the depth of the respective vertex in the tree. • guaranteedCollections is an ordered list of new guaranteed collections that are introduced by this block. As explained in Section 3, a guaranteed collection includes the hash value of the transactions in the collection, but not the transactions themselves. The transaction texts are stored by the Collector Nodes, who guaranteed the collection, and provided by them upon request. • blockSeals is a list of hash commitments to the verified execution results of the previous blocks. Sealing a block happens after the submission of a minimum effective vote of 23 of the block’s result approvals from the Verification Nodes to the Consensus Nodes, with no challenges are pending. Check out our previous paper [5] for more details on execution and verification. • slashingChallenges is the list of new slashing challenges that have been submitted to the Consensus Nodes for adjudication. Any staked node can submit slashing challenges against another node in the Flow network. For example, Verification Nodes may submit slashing challenges against the computation result of the Execution Nodes. • protocolStateUpdates contains a cryptograpic commitment to the updated protocol state of the system as a result of executing this block. In Flow, all the staked nodes follow recently finalized blocks to update their view of the system and progress with their roles. Moreover, publishing this information in the blocks also acts as a public bulletin board of the changes in the system state. • aggregatedConsensusSigs contains the aggregated signatures of the Consensus Nodes that voted in favor of this block.  Blocks The structure of a well-formed Block is presented by Listing 4. It contains commitments to all necessary information for progressing with transaction computation and evolving the protocol state in a deterministic manner. However, from a data perspective, a Block is not self-contained. For example, while cryptographic hashes of the execution state are included, the state data itself is not. This implies that anyone can verify the integrity of any subset of the execution state using the  26  Listing 4: Flow’s block structure 1 2 3 4  message Block { ProtoBlock protoBlock ; Signature sourceOfRandomness ; }  hashes in the Block (and merkle proofs which must be provided alongside the actual data). Essentially, a Block is simply a wrapper around a ProtoBlock, which adds the sourceOfRandomness to the block. Formally, the source of randomness is a reconstructed threshold signature from the DRB Nodes (see Section 2.3 for details.)  4.3  Block formation process  Flow requires that all Consensus Nodes follow the same protocol when responding to events. We describe the block formation process steps from the perspective of a Consensus Node which we denote by this. A Consensus Node can hold two separate secret keys: • skstake this is used for staking. • If the Consensus Node is a member of the DRB, it has a second secret key skDRB this which is used for generating the node’s threshold signature shares. For the node this in the Flow network, its role, public staking key pkstake this , current stake amount, and the node’s network IP address ip addrthis are known to all other nodes. Furthermore, if the node is a member of the DRB committee, also its public key pkDRB this is publically known. 4.3.1  Block Proposal  Following Flow’s underlying consensus protocol, Consensus Nodes are selected as primaries with a probability proportional to their stake using a probabilistic but verifiable protocol. The Consensus Node selected as the primary of the current round has to propose the next ProtoBlock pb. It includes the guaranteed collections, block seals, slashing challenges, protocol state updates, and a commitment to the protocol state after the updates. Subsequently, it broadcasts the proposed ProtoBlock pb to all other Consensus Nodes. As a security measure, the underlying consensus protocol has a timer to detect the primary’s failure. If the primary does not generate a block within this time, the Consensus Nodes progress to the next round and primary. As mentioned earlier, if there are no guaranteed collections available, the primary should produce a ProtoBlock pb with an empty list of guaranteed collections (i.e., b.guaranteedCollections = ∅), but still include the potential block seals, protocol state updates, challenges, adjudications, etc. It is worth noting that the Consensus Nodes only work with and decide upon the order of the collection hashes but not the full collection texts. Hence, they do not need to inspect the collections’ transactions unless an execution result is being challenged [5].  27  4.3.2  Block Proposal Evaluation and Voting  After the primary broadcasts its ProtoBlock, the rest of Consensus Nodes evaluate the proposal and vote. Each Consensus Node follows the consensus steps for this round (see Section 2.2), if it approves the proposed ProtoBlock. Each Consensus Node runs Protocol 3 and only votes in favor of a ProtoBlock if all the listed conditions are satisfied. In Protocol 3, we denote the current Consensus Node as ‘this’. Protocol 3 Flow Consensus Protocol Input A proposed ProtoBlock pb Procedure Check these conditions: 1. pb’s proposer is the selected primary of this round and pb is signed by this node. 2. pb extends the known blockchain and is a descendent of the genesis block (without missing blocks). 3. pb is safe to vote on according to the rules of the consensus protocol. 4. pb.guaranteedCollections contains a set of new guaranteed collections or is empty. 5. this has received all guaranteed collections in pb.guaranteedCollections. 6. All the guaranteed collections in pb.guaranteedCollections are authenticated (as defined in section 3.4). 7. this has received all block seals in pb.blockSeals. 8. All the block seals in pb.blockSeals are valid according to the conditions in [5]. 9. this has received and verified all slashing challenges in pb.slashingChallenges. 10. Apply the protocol state updates listed in pb to the final protocol state of the parent block (referenced by pb.previousBlockHash). Verify that the resulting protocol state matches the commitment in the block pb. If all of these conditions are satisfied, vote in favour of pb. 4.3.3  Finalizing a ProtoBlock  Consensus Nodes sign a ProtoBlock to indicate their approval. In the current architecture of Flow, we implement HotStuff, which is a BFT consensus algorithm with deterministic finality. However, without loss of generality, any other consensus algorithm that is BFT and has deterministic finality is suitable. In HotStuff, a ProtoBlock is finalized when there are three ProtoBlocks built on top of it. In order to build on top of a ProtoBlock, HotStuff requires more than 23 effective votes from Consensus Nodes for it. The safety of the HotStuff protocol guarantees that never two or more competing blocks are finalized. 4.3.4  Source of randomness Attachment  In Flow, a reliable and verifiable randomness is essential for the system’s Byzantine fault tolerance. The sourceOfRandomness field of a Block is used by Flow nodes to seed multiple pseudo-randomnumber generators. To provide a reliable source of randomness, Flow’s DRB closely follows Dfinity’s proposal [9]. Protocol 4 specifies the steps for generating the source of randomness for a ProtoBlock.  28  Participating Consensus Nodes follow a two-step procedure to compute sourceOfRandomness for a given block. First, they sign the hash of the ProtoBlock and share their signatures with the other DRB nodes. Second, a DRB node waits for the signature shares from other DRB members. Upon having more than t distinct threshold signature shares over the hash of the ProtoBlock, each DRB node can recover the full threshold signatures over the hash for the ProtoBlock. The threshold signature is the source of randomness of the block and its digest is used to seed the pseudo-random generators. Once the source of randomness is ready, a DRB node broadcasts the (proper) Block to the entire network. In Protocol 4, we denote the current Consensus Node as ‘this’. Protocol 4 Flow Distributed Random Beacon steps Input A ProtoBlock pb Procedure 1. Step 1: Sign the proto block (a) σthis ← Sign(Hash(pb), skDRB this ) (b) Broadcast σthis to all DRB nodes. 2. Step 2: Wait for (t + 1)-many distinct threshold signature shares σi1 , σi2 , ..., σit+1 (a) σ ← Recover(Pi1 , Pi2 , ..., Pit+1 , σi1 , σi2 , ..., σit+1 ) (see Section 2.3) (b) create Block b with b.sourceOfRandomness = σ and b.protoBlock = pb (c) broadcast b to the entire network  4.4  Protocol State Updates  For each block, one may view the protocol state as a key-value dictionary. A key in the protocol state corresponds to the public staking key pkstake of a node. The corresponding dictionary value stores all of the node’s properties. Relevant properties for a staked node include the node’s role, its staking amount, and its DRB key (if applicable). The way Flow maintains its protocol state is akin to the way Ethereum handles its state: • The protocol state is directly maintained by the Consensus Nodes (which compare to miners in Ethereum). • Each block contains a commitment to the resulting protocol state after all protocol state updates in the block have been applied. Changes in the protocol state of a block propagate to the children of this block. Hence, the protocol state is a relative concept that is evaluated with respect to a base block. For example, having two consecutive blocks A and B where B is the immediate child of A, one must evaluate the correctness of the protocol state updates as a transition from block A to B. If there are forks in the unfinalized part of the chain, the protocol state might be different in the forks. The protocol state of a node may change due to slashing, staking, and unstaking. Any staked node can submit a slashing challenge against another node. The slashing challenges are adjudicated by the Consensus Nodes, and the protocol state of a node may change accordingly. For each epoch,  29  there is a certain block height (i.e., protocol-determined parameter) before which all new staking requests for the next epoch must be submitted as transactions. We envision that in mature Flow this block height is set such that the staking period ends about one day (approximately 80 000 blocks) before the new epoch. To stake, an actor submits a staking transaction which includes its public staking key. Once the staking transactions are included in a block and executed by the Execution Nodes, a notification is embedded into the corresponding Execution Receipt. When sealing the execution result, the Consensus Nodes will update the protocol state of the staking nodes accordingly. For unstaking, a node submits a transaction signed by its staking key. Once an unstaking transaction is included in a block during an epoch, it discharges the associated node’s protocol state as of the following epoch. The discharged stake of an unstaked node is effectively maintained on hold, i.e., it can be slashed but it is not returned to the unstaked node’s account. The stake is returned to the unstaked node after a waiting period of at least one epoch. The reason for doing so is two-fold. First, detecting and adjudicating protocol violations might require some time. Hence, some delay is required to ensure that there is enough time to slash a misbehaving node before its stake is refunded. Second, to prevent a long-range attack wherein a node unstakes, and then retroactively misbehaves, e.g., a Consensus Node signing an alternative blockchain to fork the protocol.  4.5  Adjudicating Slashing Challenges  Consensus Nodes adjudicate slashing challenges and update the protocol state accordingly. On receiving a slashing challenge from a node (the ‘challenger’) against a challenged node, the Consensus Nodes include it into a block. This is done to embed an official record of the slashing challenges in the blockchain. In case the challenge contains a full proof, the Consensus Nodes adjudicate it right away. Otherwise, they wait for the challenged node to reply accordingly. If the challenged node does not reply within a timeout, it is slashed right away. Otherwise, if the challenged party responds with data required to adjudicate the challenge, the Consensus Nodes process the data and slash whomever (i.e., challenger or challenged node) that is at fault. In both cases, the Consensus Nodes publish the adjudication result in the next block together with the protocol state updates. The stakes of the nodes are affected as the result of the updates.  4.6  Correctness Proof  HotStuff [11, 12] is at the heart of Flow’s Consensus Protocol 3. Flow only imposes some additional requirements on the validity of blocks, which the HotStuff protocol is agnostic to. Therefore, the safety of Flow’s consensus protocol follows from HotStuff’s safety. We formalize this insight in the following Corollary 4.0.1. Corollary 4.0.1 (Safety) Given a system with: • Network Model: partially synchronous network with message traverse time-bound by ∆t and the relative processing clock of the nodes bound by φt • Message Delivery: a fault-tolerant message routing functionality that guarantees the message delivery with probability close to 1.  30  • Byzantine Fraction: more than actors  2 3  of Consensus Nodes’ stakes are controlled by honest  • Honest Behavior: the honest actors follow the protocols as specified, and maintain their availability and responsiveness in the system over time • Consensus Protocol: the Consensus Nodes follow HotStuff, which is a BFT consensus protocol with deterministic finality Under these prerequisites, Protocol 3 is safe against arbitrary behaviour of Byzantine nodes. Specifically, Byzantine actors cannot introduce errors by deliberately publishing or voting for faulty blocks. Furthermore, the consensus Protocol 3 has deterministic finality.  HotStuff [11, 12] is a general-purpose BFT consensus protocol which is agnostic to the payload contained in the block. In other words, the detailed structure of the block payload as well as the corresponding validity requirements do not affect liveness of HotStuff as long as an honest Primary has the ability to always propose a block. In Flow’s Consensus Protocol 3, there are no hard requirements on the amount of content a Primary has to put in the block. An honest Primary will only include valid content, or propose an empty block. Therefore, liveness of Flow’s Consensus Protocol 3 is also immediately follows from HotStuff, as stated in the following Corollary 4.0.2. Corollary 4.0.2 (Liveness) Given a system with: • Network Model: partially synchronous network with message traverse time-bound by ∆t and the relative processing clock of the nodes bound by φt • Message Delivery: a fault-tolerant message routing functionality that guarantees the message delivery with probability close to 1 • Byzantine Fraction: more than actors  2 3  of Consensus Nodes’ stakes are controlled by honest  • Honest Behavior: the honest actors follow the protocols as specified, and maintain their availability and responsiveness in the system over time • Consensus Protocol: the Consensus Nodes only vote on ProtoBlocks if they are safe according to the rules of the HotStuff consensus protocol Under these prerequisites, Protocol 3 is live, i.e., the protocol will continue to finalize valid blocks.  31  5  Execution  Execution Nodes follow the new blocks as they are published by the Consensus Nodes. A conservative7 Execution Node only processed finalized blocks to avoid wasting resources on unfinalized blocks, which might be abandoned later. While only Consensus Nodes actively participate in consensus, all other nodes still verify for themselves that a block is finalized according to the criteria of the BFT consensus protocol.  5.1  Overview  As illustrated by Figure 3, in Flow, the execution process is triggered when the Consensus Nodes broadcast evidence that the next block is finalized. It completes when the Execution Nodes broadcast their Execution Receipts. The Execution Nodes are responsible for receiving a finalized block, executing its transactions, and updating the execution state of the system. During this process each Execution Node takes the following steps: • Retrieves each collection of transactions that appeared in the finalized block from the relevant clusters. • Executes the transactions according to their canonical order in the finalized block and updates the execution state of the system.  Figure 3: Lifecycle of a transaction from collection until the execution. The yellow hexagons indicate the start of the individual stages in the Flow pipeline, e.g., the block sealing (i.e., Transactions Collection). The arrows show the inter-role message exchange between the nodes. Green rectangles correspond to the broadcasting events, in which a message is disseminated to all of the staked nodes. For the sake of simplicity, we represent the normal flow of the transaction lifecycle and ignored the message exchanges related to adjudicating slashing challenges. 7  The Flow protocol allows Execution Nodes to compute unfinalized blocks. This might allow the Execution Nodes to gain more revenue in an incentive model that specifically rewards speed of execution. However, such an optimistic Execution Node risks investing resources for computing blocks which are abandoned later. Furthermore, in contrast to finalized blocks, the BFT consensus algorithm does not guarantee the validity of unfinalized block proposals. Hence, an optimistic Execution Node must protect itself against processing invalid blocks.  32  • Builds an Execution Receipt for the block, i.e., a cryptographic commitment on the execution result of the block • Broadcasts the Execution Receipt to Verification and Consensus Nodes. We elaborate on each of these steps in the rest of this section. For the sake of simplicity, we describe the process from the perspective of a single Execution Node. In the live system, the Execution Nodes operate completely independently from each other and execute the protocol in parallel.  5.2  Collection Retrieval  A guaranteed collection, as listed in the block (compare Listing 2), only contains a hash of the set of transactions that it represents but not their full texts. As explained in Section 3, the Collector Nodes that sign a guaranteed collection are also responsible sending its full text to other nodes upon request. Once an Execution Node confirms for itself that a block is finalized (see Section 4), it asks each cluster for their respective collection text so it can begin processing the transactions in order. If the responsible guarantors do not provide a guaranteed collection, a node can issues a Missing Collection Challenge against the guarantors. The challenge is submitted directly to the Consensus Nodes. Missing Collection Challenges are discussed in detail in the third Flow Technical Paper [5]. When an Execution Node retrieves the transactions for a guaranteed collection, it reconstructs the collection’s hash in order to verify that the transaction data is consistent with the guaranteed collection. Provided verification succeeds, a collection is considered successfully recovered.  5.3  Block Execution  An Execution Node computes a finalized block b, when the following conditions are satisfied: • The execution result of the previous block to b (the one referened by b.previousBlockHash) is available from either the Execution Node itself or from another Execution Node. • All guaranteed collections in b.guaranteedCollections have been successfully recovered. Alternatively, the Execution Node has a Missing Collection Attestation, which allows it to skip a lost collection (details are discussed in detail in the third Flow Technical Paper [5]). Once the Execution Node has computed block b, it broadcasts an Execution Receipt to Verification and Consensus Nodes. Listing 5 shows the Execution Receipt’s message format. In addition to the Execution Node’s signature executorSig, an Execution Receipt has two types of data structures: an Execution Result (field executionResult), and a Specialized Proof of Confidential Knowledge (SPoCK) (field Zs). We detail each of these in the following sections 5.3.1 and 5.3.2. Listing 5: Structure of an Execution Receipt in Flow. It consists of two primary data structures: an ExecutionResult and one or more SPoCK(s). 1 2 3 4 5  message ExecutionReceipt { ExecutionResult executionResult ; repeated SPoCK Zs ; bytes executorSig ; }  33  5.3.1  Execution Result  An Execution Result for a block b represents a commitment from an Execution Node to the interim and final states of computing the block b. As shown by Listing 6, an Execution Result for a block contains the block hash (blockHash), hash reference to the previous Execution Result (previousExecutionResultHash), one or more chunks (chunks), and a commitment to the final execution state of the system after executing this block (finalState). The previousExecutionResultHash acts as a commitment to the starting execution state for executing the block. It allows tracing an execution error back to the actor that originally introduced it. Figure 4 shows an example of this case. It is worth noting that this linked sequence of Execution Results could be called ”a chain”, but we avoid the word in order to avoid confusion with the primary ”block chain” maintained by the Consensus Nodes. Also, as shown by Listing 6 each Execution Result has a hash reference to the block that it is associated with. However, we skip the inclusion of the block references in Figure 4 for the sake of simplicity. Once a Verification Node attributes a computation fault to an Execution Node, it issues a Faulty Computation Challenge (FCC) against the faulty Execution Node. Verification of Execution Results and adjudicating the FCCs are covered in detail in our other white paper [5].  Figure 4: The previousExecutionResultHash field of the Execution Results enables identifying and slashing the malicious Execution Nodes that originally injected an error into the system, instead of punishing the honest ones that only propagate the error. In this figure, the gray rectangles represent the correct Execution Results, and the red ones represent a faulty Execution Result. The arrows correspond to previousExecutionResultHash references, i.e., the Execution Result on the right side of an arrow takes its initial execution state from the result of the left one. The faults in the Execution Results that are generated by the honest Execution Node-3 are attributable to the malicious Execution Node-2 that initially injected them into its own Execution Result. In other words, previousExecutionResultHash of the Execution Results provides a chain of attributability that enables the Verification Nodes to trace back the errors, and identify and slash the malicious Execution Node-2 that originally introduced the error instead of punishing honest Execution Node-3 that only propagated the error.  Execution Results consist of one or more chunks. Chunking is the process for dividing a block’s computation so such that the resulting chunks can be verified in a distributed and parallelized manner by many Verification Nodes. The execution of the transactions of a block is broken into a 34  set of chucks. The chunking is done based on the computation consumption of the transactions, and in a way that the overall computation consumption of the transactions of chunks be similar to each other and do not exceed the system-wide threshold of Γchunk . Homogeneity of the chunks computation consumption is a security countermeasure against the injected faults in the computation-heavy chunks. In other words, if the computation consumption of the chunks is heterogeneous, the Verifier Nodes of the computationally heavier chunks may not be able to finish the verification before the sealing of the actual block associated with those chunks [5]. This enables the malicious Execution Nodes to attack the integrity of the system by injecting a computational fault into the computationally heavy chunks, which are likely to be left out of verification. The structure of a chunk is represented by Listing 6, where startStateCommitment denotes a hash commitment to the execution state of the system before the execution of the chunk. startingTransactionCC and startingTransactionIndex are the computation consumption and the index of the chunk’s first transaction. computationConsumption denotes the overall computation consumption of the chunk. The chunking convention is: computationConsumption ≤ Γchunk . Listing 6: Structure of an Execution Result and Chunk in Flow 1 2 3 4 5 6 7 8 9 10 11 12  message ExecutionResult { bytes blockHash ; bytes p r e v i o u s E x e c u t i o n R e s u l t H a s h ; repeated Chunk chunks ; StateCommitment finalState ; } message Chunk { StateCommitment startStateCommitment ; float star tingT rans actio nCC ; uint32 s ta r t i ng T r a ns a c ti o n I nd e x ; float co mp uta ti on Co nsu mp ti on ; }  5.3.2  Specialized Proof of Confidential Knowledge (SPoCK)  The SPoCK is Flow’s countermeasure against Execution Nodes copying Execution Results from each other instead of computing the block on their own. Also, SPoCKs are used to prevent Verification Nodes from blindly approving execution results without doing the verification work. SPoCKs are detailed in [5], but it is sufficient for the purposes of this paper to understand that SPoCKs are cryptographic commitments, which are generated as part of the verification process to indicate that the Verification Node has done its job. Essentially, a SPoCK is a commitment to the execution trace for a single chunk. The field ExecutionRecipt.Zs is a list of SPoCKs, where ExecutionRecipt.Zs[i] is the SPoCK for the ith chunk. It is worth emphasizing that ExecutionRecipt.Zs[i] can be generated by only executing the ith chunk. 5.3.3  Execution Algorithm  Algorithm 5.1 represents the BlockExecution algorithm that an Execution Node invokes individually for executing a block. The inputs to this algorithm are a finalized block b, which is ready for the execution, the hash of the previous Execution Receipt herprev , and the execution state of the system before the execution of the block b, i.e., Λ. The output of the algorithm is an Execution 35  Receipt for block b, which is denoted by erb . The algorithm is invoked as follows: erb = BlockExecution(b, herprev , Λ) The Execution Node processes the transactions referenced by block b in their canonical order. As shown by Figure 5, for the proto block associated with block b, the canonical order of the transactions is achieved by first sorting its guaranteed collections in the same order as they appear in the proto block, and then for each collection, sorting its transactions in the same order as the collection references. The first transaction of the first collection, and the last transaction of the last collection are then the first and last transactions of block b based following the canonical ordering, respectively. To resolve the canonical order of the transactions associated with block b, the Execution Node invokes the function canonical(b). The execution of each transaction ti is done by invoking the execute function, where ti is the ith transaction of the block b following its canonical order. On receiving the current execution state of the system Λ and the transaction text ti , the execute function returns the resulting execution state, the computation consumption of ti ,  Figure 5: The canonical order of the transactions in a proto block with ω transactions is shown in red. The transactions with the canonical orders of 1 and ω are the first and last transactions in the sequential execution order of the proto block, respectively. The tick arrow corresponds to the inclusion relationship, i.e., a proto block includes several guaranteed collections. The dashed arrows correspond to the referential relationship, i.e., each guaranteed collection holds a reference to a collection of several transactions.  36  i.e., τ , and an SPoCK for the execution of ti , i.e., ζ (Algorithm 5.1, Line 9). BlockExecution keeps track of the computation consumption of the current chunk by aggregating the overall computation consumption of the executed transactions since the start of the current chunk (Algorithm 5.1, Line 22). For each transaction ti , the Execution Node checks whether the current chunk’s total computation consumption would exceed the threshold Γchunk when transaction ti is added. If the threshold is not met, the transaction is considered as part of the current chunk, and the SPoCK of the current e is updated by the SPoCK trace of ti ’s execution (Algorithm 5.1, Line 23). chunk (i.e., ζ) Otherwise, if the threshold Γchunk is reached for the inclusion of the transaction ti in the current chunk, the Execution Node closes the current chunk without including ti . Accordingly, the transaction ti is taken as the first transaction of the next chunk. To exclude transactions without the e Closing need for reverting, the BlockExecution algorithm keeps track of the execution state by Λ. the current chunk is done by generating a commitment for the starting state of the chunk, which is tracked by Λstart , and casting the chunk attributes into a Chunk message structure. The commitment for the start state is generated by Flow’s Authenticated State Proof system, which provides three deterministic polynomial-time functions: • StateProofGen • ValueProofGen • ValueProofVrfy StateProofGen(Λstart ) returns a verifiable hash commitment startStateCommitment to a given state snapshot Λstart . Formally, Flow’s execution state Λ is composed of key-value pairs. The key r can be thought of as the memory address and the value v the memory content. As only a small fraction of state values (i.e., key-value pairs) are touched in a chunk, it would be inefficient to transmit the full state to the Verification Node. Instead, Execution Nodes transmit only the key-value pairs that are needed for checking their computation. In addition, an Execution Node also provides a proof to allow the Verification Node to check that the received key-value pairs are consistent with the state commitment in the chunk. The Execution Node generates these proofs by running γx := ValueProofGen(Λstart , rx ). Here, rx denotes the key for an element of the state and vx the corresponding value. The Execution node sends the (rx , vx , γx ) to the Verification Node. The recipient can then verify that the values are indeed from the desired state by executing ValueProofVrfy(rx , vx , γx , startStateCommitment). We leave the details of the Authenticated State Proof scheme as well as its construction to our future publications. Once the commitment to the starting state of the current chunk is ready, BlockExecution creates a new Chunk message (see Listing 6). The generated chunk is appended to the list C, which holds the chunks of the block under execution. The SPoCK ζe for the generated chunk is stored in the list Zs, which holds the SPoCKs for the chunks of the block. Once the current chunk is closed, the algorithm moves to the next chunk by re-initializing the variables that keep the attributes of the current chunk (Algorithm 5.1, Lines 12-21). τ0 keeps track of the computation consumption of the first transaction of the current chunk. Once all transactions of the block b are executed in their canonical order and the corresponding chunks were created, BlockExecution creates an Execution Receipt (erb ) out of the hash of the block b (hb ), the hash of the previous Execution Receipt (herprev ), the list of all the chunks of block b (C), the execution state of the system after executing block b (Λ), and the respective SPoCKs (Zs). The execution of block b ends by each Execution Node individually broadcasting erb to the entire system.  37  Algorithm 5.1: BlockExecution Input: b: finalized block herprev : hash of the previous Execution Receipt Λ: execution state of system prior to execution of b Output: erb : Execution Receipt of block b // initializing the SPoCK list of the Execution Receipt 1  Zs := [ ];  2  C := [ ];  // initializing the list of chunks for block b // initializing start state of the current chunk 3  Λstart := Λ; // initializing starting transaction index of current chunk  4  startTransactionIndex := 0; // initializing computation consumption of the current chunk  5  c := 0; // initializing the SPoCK of the current chunk  6 7  ζe := ∅; for ti ∈ canonical(b) do // caching the state prior to execution of ti for chunk bookkeeping  8 9 10 11 12  e := Λ; Λ Λ, τ, ζ := execute(Λ, ti ); if i = 0 then τ0 := τ if c + τ > Γchunk then // closing the current chunk  13 14 15 16  startStateCommitment := StateProofGen(Λstart ); chunk := new Chunk(startStateCommitment, τ0 , startTransactionIndex, c); C.append(chunk); e Zs.append(ζ); // initializing the variables for the next chunk  17 18 19 20 21 22  e Λstart := Λ; startTransactionIndex := i; τ0 := τ ; ζe := ∅; c := 0; c := c + τ ; // Updating the execution trace of the current chunk  23  e ζ); ζe := TraceUpdate(ζ, // closing the last chunk  24 25 26 27 28  startStateCommitment := StateProofGen(Λstart ); chunk := new Chunk(startStateCommitment, τ0 , startTransactionIndex, c); C.append(chunk); e Zs.append(ζ); erb := new ExecutionReceipt(hb , herprev , C, Λ, Zs)  38  5.4  Correctness Proof  For completeness, we include the following safety Theorem 5.1. For further details and the formal proof, the reader is referred to [5]. Theorem 5.1 (Safety) Given a system with: • Network Model: a partially synchronous network with message traverse time-bound by ∆t and the relative processing clock of the nodes bound by φt • Message Delivery: a fault-tolerant message routing functionality that guarantees the message delivery with high probability • Honest Behavior: honest actors which follow the protocols as specified and maintain their availability and responsiveness in the system over time • Byzantine Fraction: at least one honest Execution Node Even if all but one Execution Nodes are Byzantine and publishing the same faulty result, the faulty result is not sealed with a probability greater than 1 −  with   1. Depending on its system parameters, Flow can be tuned such that  ≤ 10−12 . The system parameters are: the number of chunks in a block and the fraction of chunks of a block that each Verifier Node checks.  Assuming that there is at least one honest Execution Node in the system, a valid execution receipt will be published for each block. As HotStuff guarantees liveness of block formation, liveness of execution receipt generation is also guaranteed. We formalize this statement in the following Corollary 5.1.1. Corollary 5.1.1 (Liveness) Given a system with: • Network Model: a partially synchronous network with message traverse time-bound by ∆t and the relative processing clock of the nodes bound by φt • Message Delivery: a fault-tolerant message routing functionality that guarantees the message delivery with high probability • Honest Behavior: honest actors follow the protocols as specifiedand maintain their availability and responsiveness in the system over time • Byzantine Fraction: at least one honest Execution Node The liveness block execution is guaranteed as the honest Execution Node(s) follow the specified algorithms and eventually, generate an Execution Receipt for the input block.  39  6 6.1  Mitigating Attack Vectors Byzantine Clusters  In mature Flow we envision clusters will be comprised of 20-80 randomly selected Collector nodes. While the probability for sampling a cluster with too many Byzantine nodes is small, it is unfortunately not negligible. A Byzantine cluster may attack the integrity of the system by injecting faulty transactions in their guaranteed collections. It also may attack the liveness of the system by withholding the text of a guaranteed collection, to prevent the Execution Nodes from computing a block. The former attack on the integrity of the system is mitigated following the detectability and attributability design principle of Flow, i.e., the guarantors of a collection containing malformed transactions are slashed. The latter attack on the availability of collections is mitigated by introducing the Missing Collection Challenges (see [5] for details). This challenge is a slashing request against the Collector Nodes that originally guaranteed the availability of the missing collection. The challenge is directly submitted to the Consensus Nodes for adjudication.  6.2  Faulty Computation Result  A malicious Execution Node may publish faulty Execution Receipts. However, we formally prove in [5] that faulty execution results are detected and challenged by the Verification Nodes with overwhelming probability. Upon detection of such faulty execution results, a Verification Node submits a slashing challenge against the faulty Execution Node to the Consensus Nodes (see [5] for more details).  40  Acknowledgments Karim Helmy for his continued support and feedback on our technical papers.  References [1] Kyle Croman, Christian Decker, Ittay Eyal, Adem Efe Gencer, Ari Juels, Ahmed Kosba, Andrew Miller, Prateek Saxena, Elaine Shi, Emin Gün Sirer, Dawn Song, and Roger Wattenhofer. On scaling decentralized blockchains. In Jeremy Clark, Sarah Meiklejohn, Peter Y.A. Ryan, Dan Wallach, Michael Brenner, and Kurt Rohloff, editors, Financial Cryptography and Data Security, pages 106–125, Berlin, Heidelberg, 2016. Springer Berlin Heidelberg. [2] BBC News. Cryptokitties craze slows down transactions on ethereum. 2017. https://www.bbc.com/news/ technology-42237162. [3] Richard Dennis and Jules Pagna Diss. An Analysis into the Scalability of Bitcoin and Ethereum, pages 619–627. 01 2019. [4] Shirley Dieter Hentschel, Alexander and Layne Lafrance. Flow: Separating Consensus and Compute. 2019. [5] Alexander Hentschel, Dieter Shirley, Layne Lafrance, and Maor Zamski. Flow: Separating Consensus and Compute – Execution Verification. arXiv preprint arXiv:1909.05832, 2019. [6] Silvio Micali, Salil Vadhan, and Michael Rabin. Verifiable random functions. In Proceedings of the 40th Annual Symposium on Foundations of Computer Science, FOCS ’99, pages 120–, Washington, DC, USA, 1999. IEEE Computer Society. [7] Leslie Lamport. Proving the correctness of multiprocess programs. IEEE transactions on software engineering, (2):125–143, 1977. [8] Ethan Heilman, Alison Kendler, Aviv Zohar, and Sharon Goldberg. Eclipse attacks on bitcoins peer-to-peer network. In 24th {USENIX} Security Symposium ({USENIX} Security 15), pages 129–144, 2015. [9] Timo Hanke, Mahnush Movahedi, and Dominic Williams. Dfinity technology overview series, consensus system. arXiv preprint arXiv:1805.04548, 2018. [10] Dan Boneh, Ben Lynn, and Hovav Shacham. Short signatures from the weil pairing. Advances in CryptologyASIACRYPT 2001, pages 514–532, 2001. [11] Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham. HotStuff: BFT Consensus with Linearity and Responsiveness. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, PODC ’19, pages 347–356, New York, NY, USA, 2019. ACM. [12] Maofan Yin, Dahlia Malkhi, Michael K Reiter, Guy Golan Gueta, and Ittai Abraham. HotStuff: BFT Consensus in the Lens of Blockchain. arXiv preprint 1803.05069v6, 2019. paper version 6. [13] Alexandra Boldyreva. Threshold signatures, multisignatures and blind signatures based on the gap-diffiehellman-group signature scheme. In International Workshop on Public Key Cryptography, pages 31–46. Springer, 2003. [14] Rosario Gennaro, Stanislaw Jarecki, Hugo Krawczyk, and Tal Rabin. Secure distributed key generation for discrete-log based cryptosystems. In International Conference on the Theory and Applications of Cryptographic Techniques, pages 295–310. Springer, 1999. [15] Ronald A Fisher and Frank Yates. Statistical tables for biological, agricultural and medical research. Oliver and Boyd Ltd, London, 1943.  41  Flow: Separating Consensus and Compute  arXiv:1909.05832v1 [cs.DC] 12 Sep 2019  – Execution Verification –  Dr. Alexander Hentschel alex.hentschel@dapperlabs.com  Maor Zamski  Dieter Shirley  Layne Lafrance  maor@dapperlabs.com  dete@dapperlabs.com  layne@dapperlabs.com  Abstract Throughput limitations of existing blockchain architectures are well documented and are one of the most significant hurdles for their wide-spread adoption. In our previous proof-of-concept work, we have shown that separating computation from consensus can provide a significant throughput increase without compromising security [1]. In our architecture, Consensus Nodes only define the transaction order but do not execute transactions. Instead, computing the block result is delegated to compute-optimized Execution Nodes, and dedicated Verification Nodes check the computation result. During normal operation, Consensus Nodes do not inspect the computation but oversee that participating nodes execute their tasks with due diligence and adjudicate potential result challenges. While the architecture can significantly increase throughput, Verification Nodes still have to duplicate the computation fully. In this paper, we refine the architecture such that result verification is distributed and parallelized across many Verification Nodes. The full architecture significantly increases throughput and delegates the computation work to the specialized Execution Nodes and the onus of checking it to a variety of less powerful Verification Nodes. We provide a full protocol specification of the verification process, including challenges to faulty computation results and the resulting adjudication process. Furthermore, we formally prove liveness and safety of the system.  1  Contents 1 Flow Architecture 1.1 Roles . . . . . . . . . . . . . . . . 1.1.1 Collector Role . . . . . . . 1.1.2 Consensus Role . . . . . . 1.1.3 Execution Role . . . . . . 1.1.4 Verification Role . . . . . 1.1.5 Observer Role . . . . . . . 1.2 Locality of Information . . . . . . 1.3 Computational State vs. Network 1.3.1 Staking and Slashing . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Infrastructure . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . State . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  . . . . . . . . .  3 5 5 6 8 8 9 9 9 9  2 General Techniques 10 2.1 Specialized Proof of Confidential Knowledge . . . . . . . . . . . . . . . . . . . . . . . 10 3 Transaction Processing Flow 3.1 Collector Role: Batching Transactions into Collections 3.2 Consensus Role: Linearizing Computation into Blocks 3.3 Execution Role: Computing Transactions in Block . . 3.3.1 Chunking for Parallelized Verification . . . . . 3.3.2 The Execution Receipt . . . . . . . . . . . . . . 3.4 Verification Role: Checking Execution Result . . . . . 3.4.1 Self-selecting chunks for verification . . . . . . 3.4.2 Verifying chunks . . . . . . . . . . . . . . . . . 3.5 Consensus Role: Sealing Computation Result . . . . . 3.6 Proof of Computational Correctness . . . . . . . . . . 3.7 Computational Load on Verification Nodes . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  12 13 13 14 14 16 17 17 19 21 23 25  4 Mitigation of Attack Vectors 26 4.1 Adjudicating with Faulty Computation Results . . . . . . . . . . . . . . . . . . . . . 26 4.2 Resolving a Missing Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 4.3 Placing errors in chunks checked by colluding Verifier Nodes . . . . . . . . . . . . . . 32 Acknowledgments  34  References  34  2  1  Flow Architecture  In most traditional blockchains, each full node must perform every task associated with running the system. This process is akin to single-cycle microprocessors, where one instruction is executed per step. In contrast, modern CPU design leverages pipelining to achieve higher throughput and scaling. Rather than asking every node to choose the transactions they will include in a block, compute the block’s output, come to a consensus on the output of those transactions with their peers, and finally sign the block, appending it onto the chain, Flow adopts a pipelined architecture. In Flow, different tasks are assigned to specialized node roles: Collection, Consensus, Execution, Verification, and Observation. This design allows high levels of participation in Consensus and Verification by individuals on home internet connections while leveraging large-scale datacenters to do most of the heavy lifting of Execution. Actors participating in Consensus and Verification hold the other nodes accountable with crypto-economic incentives allowing Flow to gain massive throughput improvements without undermining the decentralization or safety of the network. In a system with Consensus and Execution nodes only, Flow achieved a throughput increase by a factor of 56 compared to architectures where consensus nodes also perform block computation [1]. In any design where actors other than the consensus nodes perform tasks, correct task execution is not covered by the safety guarantees of consensus. Therefore, the protocol must include dedicated components to ensure secure system operation, even in the presence of a moderate number of malicious participants. In our first white paper [1], we formally analyzed the security implications of delegating tasks to other actors than Consensus Nodes. The central result of our research was that transaction execution can be transferred to one group of nodes (Execution Nodes), and result verification to an independent group (Verification Nodes). The protocol must include the following components to ensure safety: • The Verifiers must be able to appeal (formally: submit a challenge) to Consensus Nodes if they detect a protocol violation. • Consensus Nodes must have the means to determine whether the challenger or the challenged is correct (formally: adjudicate the challenge). When such mechanisms are included, the pipelined architecture is as secure as a blockchain where all tasks are executed by all consensus nodes [1]. In the present paper, we refine the architecture such that result verification is distributed and parallelized across many Verification Nodes. Furthermore, we specify the details of the different challenges and adjudication protocols for execution verification. Core Architecture Principles Above, we noted that nodes must be able to appeal to Consensus Nodes if they detect a protocol violation. For an appeal system to provide security guarantees that protect from Byzantine attacks, the system must have the following attributes. • Detectable: A single, honest actor in the network can detect deterministic faults, and prove the error to all other honest nodes by asking them to recreate part of the process that was executed incorrectly. • Attributable: All deterministic processes in Flow are assigned to nodes using a verifiable random function (VRF) [2]. Any detected error can be attributed to the nodes responsible for that process. 3  • Punishable: Every node participating in the Flow network must put up a stake, which is slashed in case the node is found to exhibit Byzantine behavior. Reliably punishing errors via slashing is possible because all errors in deterministic processes are detectable1 and attributable. • Recoverable: The Flow protocol contains specific elements for result verification and resolution of potential challenges. These elements serves to deter malicious actors from attempting to induce errors that benefit them more than the slashing penalty, as the probability of their erroneous result being committed is negligible. Assumptions • We solely focus on Proof of Stake blockchains, where all participants are known and each node is authenticatable through its signature. • Nodes commit to (and stake for) participating in the network for a specific time interval, which we refer to as an Epoch. Epochs are system-wide. While nodes can participate over multiple Epochs, the end of an Epoch is a dedicated point in time for nodes to leave or join the system. Epochs are considered to last for about a week. Technical details for determining the length of an Epoch and a mechanism for Epoch changeover are left for future publications. In this paper, we consider the system running only within one epoch. Furthermore, we assume • The existence of a reliable source of randomness that can be used for seeing pseudo-random number generators. We require the random seed to be unpredictable by any individual node until the seed itself is generated and published. Possible solutions include Dfinity’s Random Beacon [3] or proof-of-delay based systems [4]. • An aggregatable, non-interactive signature scheme, such as BLS signatures [5]. • Adequate compensation and slashing mechanics to incentivize nodes to comply with the protocol. • Partially synchronous network conditions with message traversal time bounded by ∆t . Furthermore, we assume that local computation time is negligible compared to message traversal time. • In numerous places throughout this paper, we refer to fractions of nodes. This is a short-form of referring to a set of nodes which hold the respective fraction of stake. Formally, let N (r) be the set of all nodes with role r and sα the stake of node α ∈ N (r) . A fraction of at least κ nodes (with role r) refers to any subset X X e ⊆ N (r) such that N sα ≥ κ sα , (1) e α̃∈N  α∈N (r)  for 0 ≤ κ ≤ 1. For example, stating that “more than 32 of Consensus Nodes have approved of a block” implies that the approving nodes hold more than κ = 23 of the Consensus Nodes’ accumulated stake. 1  In Flow, nodes check protocol-compliant behavior of other nodes by re-executing their work. In most cases, verification is computationally cheap, with the noticeable exception of computing all transactions in a block. We describe in section 3.4 how verifying the block computation is distributed and parallelized such that each Verifier only has to perform a small fraction of the overall block computation.  4  1.1  Roles  Roles are a service a node can provide to the network. These are: Collector Role, Consensus Role, Execution Role, Verification Role, and Observer Role. We refer to a network node that performs the respective role as Collector Node, Consensus Node, etc. From an infrastructure perspective, the same hardware can host multiple roles in the network. However, the network treats individual roles as if they are independent nodes. Specifically, for each role a node stakes, unstakes, and is slashed independently. We furthermore assume that a node has it’s own independent staking key for each of its roles, even if all roles are hosted on the same hardware. Figure 1 illustrates the messages which nodes of the individual roles exchange during normal operation. 1.1.1  Collector Role  The central task of Collector Role is to receive transaction submissions from external clients and introduce them to the network. Staked nodes are compensated through transaction fees and all roles require a minimum stake to formally participate in that role. When receiving a transaction, a Collector Node checks that the transaction is well-formed. By signing the transaction, the node guarantees to store it until the transaction’s result has been sealed. (For details on block sealing, see section 3.5.) Clusters: For the purpose of load-balancing, redundancy, and Byzantine resilience, Collection Nodes are partitioned into clusters. We require that Collection Nodes are staked equally. At the beginning of an Epoch, each Collection Node is assigned to exactly one cluster. Cluster assignment is randomized using the random beacon output. Collections: The central task of Collector Nodes is to collect well-formed transactions from external clients and to batch them into collections. When a Collector Node sees a well-formed transaction, it hashes the text of that transaction and signs the transaction to indicate two things: first, that it is well-formed, and second, that it will commit to storing the transaction text until the Execution Nodes have finished processing it. By signing it, the Collector Node guarantees the transaction’s storage and will subsequently be slashed (along with the rest of the cluster) if it doesn’t produce the transaction. Guraranteed Collections  Transaction  Resu lt A pp ro va l  ck Blo ed liz na Fi  Collectors  Consensus  Veriﬁers  Executors  E xe  User Agent  c u tio n R e cei p ts  Figure 1: Overview of the node roles and messages they exchange. For simplicity, only the messages during normal operation are shown. Messages that are exchanged during the adjudication of slashing requests are omitted.  5  Collector nodes share all well-formed transactions they receive among their cluster and collaborate to form a joint collection of transactions. A cluster forms collections one at a time. Before a new collection is started, the current one is closed and send off to the Consensus Nodes for inclusion in a block. Further details on collections are provided in section 3.1. Collector Nodes in one cluster must agree on the transactions included in the current collection and at what point to close the collection. The determination of when to close a collection is based on a number of factors including token economics, which is out of scope for this paper. This distributed agreement requires the nodes to run a consensus algorithm. Fortunately, the number of nodes in a cluster and the transaction volume to processed by one cluster is moderate2 . Therefore, established BFT consensus algorithms, such as Tendermint [6, 7, 8], SBFT [9], or Hot-Stuff [10], are fully sufficient. Security implication: As there are relatively few collectors in each cluster, a cluster may be compromised by malicious actors. The cluster could withhold the collection content that is referenced in a block but whose execution is still pending. The mitigation strategy (Missing Collection Challenge) for this attack is described in section 4.2. 1.1.2  Consensus Role  The Consensus Node’s central tasks are the following: Block Formation: Consensus Nodes form blocks from the collections. Essentially, Consensus Nodes maintain and extend the core Flow blockchain. In Flow, a block defines the transactions as well as the other inputs (incl. the random seed) required to execute the computation, but not the resulting computational state after block execution. An agreement to accept a proposed block needs to be reached by many nodes which requires a Byzantine-Fault-Tolerant (BFT) consensus algorithm [11, 12]. While the specific design of the Flow’s consensus system is still subject to active research, we restrict the design space to algorithms with the following guarantees. • The algorithm is proven BFT, i.e., it maintains consistency among honest participants, as long as less than one-third of Consensus Nodes are Byzantine. • Safety is guaranteed even in cases of network asynchrony. Per definition of safety [13, 14], Consensus Nodes might declare a block finalized at different points in time, depending on the amount of information they gathered from other Consensus Nodes. Nevertheless, a safe consensus algorithm guarantees that all honest nodes eventually will declare the same block as finalized. • Liveness is guaranteed in partially synchronous network conditions [15]. As the FischerLynch-Paterson (FLP) theorem states, a fault-tolerant, distributed consensus system cannot guarantee safety and liveness at the same time under asynchronous network conditions [16]. For Flow, we prioritize safety over liveness in case of a network split. Consistency of the world-state is more important than forward-progress of the network in extremely rare and adverse circumstances of a large-scale network split.  2  For the mature system, we anticipate on the order of 20 to 50 nodes per cluster.  6  2/3 RA  ER Execution of B2  B1  processes  Veriﬁcation of B1  B2  ﬁnalized B1  B1  blocks  2/3 RA  ER Execution of B1  Veriﬁcation of B2  B2  messages  Bk  Bn  ﬁnalized B k seal B1  ﬁnalized B n seal B2  Bk  Bn  Time (Blocks)  Figure 2: Illustration of the placement of Block Seals within the Flow blockchain. After block B1 is finalized by the Consensus Nodes, the Execution Nodes process their transactions and issue Execution Receipts (see section 3.3), which are subsequently checked by the Verifier Nodes (see section 3.4). Provided that the checked parts of the computation result are found valid, a Verifier sends a Result Approval back to the Consensus Nodes. As soon as conditions for sealing block B1 are satisfied (see section 3.5), the seal for the computation result of block B1 is included in the next block Bk that is generated by the Consensus Nodes.  • A desired core feature of Flow’s consensus algorithm is deterministic finality. Once a block is finalized by any Consensus Node, this commitment will never3 be changed. Deterministic finality provides the significant advantage that dapp developers do not have to deal with the effects of chain reorganizations. • Sustainability: we do not consider proof-of-work consensus systems due to their exorbitant energy consumption. We focus on Proof of Stake systems only, where computational costs are reduced to necessary elements: primarily cryptography and book-keeping. Block Sealing: After a block has been computed and the resulting computational state verified, the Consensus Nodes publish a Block Seal for the respective block. A Block Seal contains a commitment to the resulting computational state after block execution. Furthermore, it proves that the computation has been verified by a super-majority of Verifier Nodes. Consensus Nodes publish the block seals as part of the new blocks they finalize. As executing a block’s transactions follows its finalization, the seal for the block’s computational result cannot be included in the block itself. Instead, the seal is included in a later block. An illustration is shown in Figure 2. Tracking Staked Nodes: Consensus Nodes maintain a table of staked nodes including the nodes’ current stakes and their public keys. It is important to note that staking balances are tracked solely by the Consensus Nodes and are not part of the computational state. Whenever a node’s stake 3  Deterministic finality is guaranteed via BFT consensus, unless the system is under active attack of at least one-third of Byzantine actors.  7  changes, Consensus Nodes publish this update as part of their next finalized block. During normal operations, staking and unstaking can only take place at the switchover from one Epoch to the next. However, involuntary changes of stake through slashing can occur within an epoch and are accounted for by all honest nodes as soon as they process the block containing the respective stake update. Slashing: Consensus Nodes adjudicate slashing challenges and adjust the staking balances accordingly. Security implication: The consensus committee is the central authority in the system. The consensus committee itself must adjudicated challenges against committee members. Safety and liveness are of this process are guaranteed through the BFT consensus algorithm. Flow uses a consensus algorithm with deterministic finality. Therefore, dapp developers do not have to deal with the additional complexity of chain reorganization. Our results hold for any BFT consensus algorithm with deterministic finality. HotStuff [10, 17] is the leading contender. However, we continue to assess other algorithms such as Casper CBC [18, 19, 20] or Fantômette [21]. 1.1.3  Execution Role  Execution Nodes compute the outputs of all finalized blocks they are provided. They then ask the Collector Nodes for the collections containing transaction that are to be executed. With this data they execute the block and publish the resulting computational state as an Execution Receipt. For verification, the computation is broken up into chunks. The Execution Nodes publish additional information (see section 3.3) in the Execution Receipt about each chunk to enable Verification Nodes to check chunks independently and in parallel. The Computation Nodes are primarily responsible for Flow’s improvements in scale and efficiency because only a very small number of these powerful compute resources are required to compute and store the canonical state. Security implication: Malicious Execution Nodes could publish faulty Execution Receipts. The protocol for detecting incorrect execution results is covered in section 3.4 and the adjudication process for the challenges in section 4.1. 1.1.4  Verification Role  Verification Nodes check the computation from the Execution Nodes. While each node only checks a small number of chunks, all Verification Nodes together will check all chunks with overwhelming probability. For each chunk, a Verification Node publishes a Result Approval, provided it agrees with the result. The Execution Receipt and the Result Approval are required for the block to be sealed. Security implication: Like most blockchains, Flow has to address the Verifier’s Dilemma [22]. In a system where workers are producing results and Verifiers are confirming result correctness, there is an incentive for Verifiers to approve results without expending the work of checking. This conflict of incentives is at the heart of the Verifier’s Dilemma. It persists even the worker and Verifiers are not colluding, so additional Verifiers do not help. For Flow, we developed Specialized Proofs of Confidential Knowledge (section 2.1) to overcome the Verifier’s Dilemma (section 3.4.2 for more details).  8  1.1.5  Observer Role  Observer Nodes relay data to protocol-external entities that are not participating directly in the protocol themselves.  1.2  Locality of Information  • Correctness of information is cryptographically verifiable using on-chain information • Flow blockchain is, from a data perspective, not self-contained. For example, cryptographic hashes of the computational state are included in blocks, but the state itself is not. This implies that anyone can verify the integrity of any subset of the computational state using the hashes in the Flow blockchain (and merkle proofs which must be provided alongside the actual data). However, it is impossible to extract the state itself from the core blockchain. • Computational state is local to Execution Nodes • Reference to the information holder is guaranteed by the holder’s signature  1.3  Computational State vs. Network Infrastructure State  An important conceptual difference in Flow is handling information that pertains to the computational state vs. that of the network infrastructure itself. While the computational state is held by the Execution Nodes, the network’s infrastructure state is maintained by the Consensus Nodes. To illustrate the difference, consider the situation where the nodes in the network do not change (nodes never leave, join, or change stake). However, the transactions executed by the system will modify register values, deploy smart contracts, etc. In this setting, only the computational state changes. Its integrity is protected by the verification process (see section 3.4 for details). In contrast, let us now consider a situation where no transactions are submitted to the system. Blocks are still produced but contain no transactions. In this case, the system’s computational state remains constant. However, when nodes leave or join, the state of the network infrastructure changes. The integrity of the network state is protected by the consensus protocol. To modify it, more than 2/3 of Consensus Nodes must approve it. 1.3.1  Staking and Slashing  The network state itself contains primarily a list of all staked nodes which contains the node’s staking amount and its public staking key. Updates to the network state are relevant for all nodes in the network. Hence, Consensus Nodes publish updates directly as part of the blocks they produce. Furthermore, slashing challenges are directly submitted to Consensus Nodes for adjudication. (For example, section 4.1 provides a detailed protocol for challenging execution results.) As Consensus Nodes maintain the network state, including staking balances, they can directly slash the stake of misbehaving nodes, without relying on Execution Nodes to update balances.  9  2  General Techniques  In this section, we describe methods and techniques used across different node roles.  2.1  Specialized Proof of Confidential Knowledge (SPoCK)  A SPoCK allows any number of provers to demonstrate that they have the same confidential knowledge (secret ζ). The cryptographic proof does not leak information about the secret. Each prover’s SPoCK is specialized to them, and can not be copied or forged without possession of the secret. The SPoCK protocol is used in Flow to circumvent the Verifier’s Dilemma [22] (section 3.4.2). The protocol prevents Execution or Verification Nodes from copying Execution Results or Result Approvals from each other. Thereby, actors cannot be compensated for work they didn’t complete in time. In Flow, the secret ζ is derived from the execution trace of the low-level execution environment (e.g., the virtual machine). Executing the entire computation is the cheapest way to create the execution trace, even when the final output of computation is known. Formally, the SPoCK protocol provides two central guarantees: 1. An arbitrary number of parties can prove that they have knowledge of a shared secret ζ without revealing the secret itself. 2. The proofs can be full revealed, in an arbitrary order, without allowing any additional party to pretend knowledge of ζ. The SPoCK protocol works as follows. • Consider a normal blockchain that has a transition function t() where S 0 = t(B, S), with B being a block of transactions that modify the world state, S as the state before processing the block, and S 0 as the state after. We create a new function t̃() that works the same way, but has an additional secret output ζ, such that (S 0 , ζ) = t̃(B, S) for the same S, S 0 , and B as t(). The additional output, ζ, is a value deterministically derived from performing the computation, like a hash of the CPU registers at each execution step, which can’t be derived any more cheaply than by re-executing the entire computation. We can assume the set of possible values for ζ is very large. • An Execution Node, Alice, publishes a signed attestation to S 0 (a merkle root of some sort), and responds to queries about values in S 0 with merkle proofs. Additionally it publishes a SPoCK derived from ζ. • A Verifier Node, Bob verifies that S 0 is an accurate application of t(B, S), and also publishes its own SPoCK of ζ. • An observer can confirm that both SPoCKs are derived from the same ζ, and assume that Bob actually verified the output with high probability4 and didn’t just “rubber stamp” the result. This doesn’t provide any protection in the case where Alice and Bob are actively colluding, but it does prevent a lazy node from “confirming” a result without actually knowing that it is correct. • A SPoCK is created as follows: – Use ζ (or a cryptographic hash of ζ) as the seed for a deterministic key generation process, generating a public/private key pair (pk, sk). 4  We assume that honest nodes will not accept unproved values for ζ, because they would be slashed if the ζ-value was incorrect. Therefore, the observer can assume that statistically more than 23 of the SPoCKs have been obtained by truthful re-computation of the respective chunks.  10  – Use the private key sk to sign your public identity (such as a node ID), and publish the signature along with the deterministically generated public key pk: SPoCK Z = (pk, signsk (ID))  (2)  All observers can verify the signatures to see that both Alice and Bob must have access to the private key sk, but the signatures themselves don’t allow recovery of those private keys. Alice and Bob must both have knowledge of the same underlying secret ζ used to generate the private key. In order t seal a block, several SPoCKs have to be validated for each chunk. Benchmarks of our early BLS implementation indicate that all proofs for sealing a block can be verified on a single CPU core in the order of a second. Parallelization across multiple CPU cores is straight forward. Verification time can be further reduced by utilizing vectorized CPU instructions such as AVX-512 or a cryptographic coprocessor.  11  3  Transaction Processing Flow  In this section, we present a formal definition of the transaction processing flow. Figure 3 provides a high-level overview of the individual stages which we discuss in detail in sections 3.1 – 3.5. For conciseness and clarity, we focus on the core steps and omit protocol optimizations for conservation of bandwidth, run-time, etc. In section 3.6, we formulate Theorem 2 which proves that correctness of a sealed computation result is probabilistically guaranteed even in the presence of Byzantine actors. To formally define the messages that nodes exchange, we use Protobuf-inspired5 pseudo-code [23].  Figure 3: Transaction Processing Flow. The hexagon-shaped boxes indicate the start of the individual stages, which are discussed in sections 3.1 – 3.5. Arrows show message exchange between nodes with specific roles. Green boxes represent broadcast operations where the content is relayed to all staked nodes in the network (independent of their respective roles). White boxes are operations the nodes execute locally. 5  Specifically, we use Protobuf with the exception of omitting the field numbers for the sake of conciseness.  12  3.1  Collector Role: Batching Transactions into Collections  As described in section 1.1.1, there are several clusters of collectors, where each cluster maintains one collection at a time. A cluster may decide to close its collection as long as it is non-empty. As part of closing the collection, it is signed by the cluster’s collector nodes to indicate their agreement with the validity of its content and their commitment to storing it, until the block is sealed. Definition 1 A Guaranteed Collection is a list of transactions that is signed by more than 2/3rds of collectors in its cluster. By signing a collection, a Collector Node attests: • that all transaction in the collection are wellformed; • the collection contains no duplicated transactions; • to storing the collection including the full texts of all contained transactions. A guaranteed collection is considered immutable. Once a collection is guaranteed by the collectors in its cluster, its reference is submitted to the Consensus Nodes for inclusion in a block (see Message 1).  3.2  message GuaranteedCollection { bytes collectionHash; 3: uint32 clusterIndex; 4: Signature aggregatedCollectorSigs; 5: } 1:  2:  Message 1: Collector Nodes send a GuaranteedCollection message to Consensus Nodes to inform them about a guaranteed Collection. For a collection to be guaranteed, more than 23 of the collectors in the cluster (clusterIndex) must have signed it. Instead of storing the signatures individually, aggregatedCollectorSigs is an aggregated signature.  Consensus Role: Linearizing Computation into Blocks  Consensus nodes receive the GuaranteedCollections from Collector clusters and include them in blocks through a BFT consensus algorithm outlined in section 1.1.2. The structure of a finalized block is given in Message 2. The universal fields for forming the core blockchain are Block.height, Block.previousBlockHash. Furthermore, the block contains a Block.entropy as a source of entropy, which is generated by the Consensus Nodes’ Random Beacon. It will be used by nodes that process the block to seed multiple random number generators according to a predefined publicly-known protocol. In Flow, a reliable and verifiable source of randomness is essential for the system’s Byzantine resilience. 1: 2: 3: 4: 5: 6: 7: 8: 9: 10:  message Block { uint64 height; bytes previousBlockHash; bytes entropy; repeated GuaranteedCollection guaranteedCollections; repeated BlockSeal blockSeals; SlashingChallenges slashingChallenges; NetworkStateUpdates networkStateUpdates; Signature aggregatedConsensusSigs; }  Message 2: Consensus Nodes broadcast Blocks to the entire network. Their Block is valid if and only if more than 32 of Consensus Nodes have signed. Instead of storing the signatures individually, we use store an aggregated signature in signedCollectionHashes.  13  The filed Block.guaranteedCollections specifies the new transactions that are to be executed next. During normal operations, Consensus Nodes only require the information provided by the GuaranteedCollection message. In particular, they work only with the collection hash (GuaranteedCollection.collectionHash) but do not need to inspect the collection’s transactions unless an execution result is being challenged. The blockSeals pertain to previous blocks in the chain whose execution results have just been sealed. A Block Seal contains a commitment to the resulting computational state after block execution and proves the computation has been verified by a super-majority of Verifier nodes. Section 3.5 describes in detail how Consensus Nodes seal blocks. The fields Block.slashingChallenges and Block.networkStateUpdates are listed for completeness only. For brevity, we do not formally specify the embedded messages SlashingChallenges and NetworkStateUpdates. Block.slashingChallenges will list any slashing challenges that have been submitted to the Consensus Nodes (but not necessarily adjudicated yet by the Consensus Nodes). Slashing challenges can be submitted by any staked node. Some challenges, such as the Faulty Computation Challenge discussed in section 4.1), require the challenged node to respond and supply supplemental information. The filed networkStateUpdates will contain any updates network infrastructure state (see section 1.3 for details). Most significantly, staking changes due to slashing, staking and unstaking requests are recorded in this field. By including slashingChallenges and networkStateUpdates in a Block, their occurrences are persisted in the chain. As all staked nodes must follow recently finalized blocks to fulfill their respective roles, journaling this information in the block also serves as a way to publicly broadcast it.  3.3  Execution Role: Computing Transactions in Block  When the Execution Nodes receive a finalized Block, as shown in Message 2, they cache it for execution. An Execution Node can compute a block once it has the following information. • The execution result from the previous block must be available, as it serves as the starting state for the next block. In most cases, an Execution Node has also computed the prior block and the resulting output state is known. Alternatively, an Execution Node might request the resulting state from the previous block from a different Execution Node. • The Execution Node must fetch all collections’ text referenced in the block from the Collector Nodes. 3.3.1  Chunking for Parallelized Verification  After completing a block’s computation, an Execution Node broadcasts an Execution Receipt to the Consensus and Verification Nodes. We will discuss the details of the Execution Receipt in more detail in section 3.3.2. At its core, the Execution Receipt is the Execution Node’s commitment to its final result. Furthermore, it includes interim results that make it possible to check the parts of the computation in parallel without re-computing the entire block. The execution result’s correctness must be verified in a dedicated step to ensure an agreed-upon computational state. At the execution phase, the block computation is done by compute-optimised nodes. We therefore assume that a block re-computation by any other node, as part of the independent verification process, will always be slower. To address the bottleneck concern, we take an approach akin to split parallel verification [24]. We define the Execution Receipt to be composed of separate chunks, each constructed to be independently verifiable. A verification process (section 3.4) is defined on a subset of these chunks. 14  By breaking the block computation into separate chunks, we can distribute and parallelize the execution verification across many nodes. Let us consider a block containing Ξ chunks. Furthermore, let η be the fraction of chunks in a block that each Verification Node checks. The parameter-value for η is protocol-generated (see section 3.6 for details). Number of chunks each Verification Node checks = dη Ξe,  (3)  for d·e the ceiling function. A mature system with V Verification Nodes would re-compute in total Vdη Ξe chunks if all Verification Nodes were to fully participate. Hence, on average, each chunk is executed VdηΞ Ξe times. For example, a mature system with V = 1000 Verification Nodes could break up a large block into Ξ = 1000 chunks. With η = 4.2%, each Verification Node would check dη Ξe = 42 chunks and each chunk would be verified by 42 = VdηΞ Ξe different Verification Nodes (on average). The redundancy factor of 42 makes the system resilient against Byzantine actors (see section 3.7). It is important that there is not single chunk that has a significant increased computation consumption compared to others in the block. If chunks had vastly different execution time, Verifiers assigned to check the long-running chunks would likely not be finished before the block is sealed (see section 3.5). Hence, Execution Nodes could attack the network by targeting the long-running chunk to introduce a computational inconsistency that is left unverified. This weakness is mitigated by enforcing chunks with similar computation consumption. Specifically, we introduce the following system-wide parameters. ΓTx : upper limit for the computation consumption of one transaction Γchunk : upper limit for the computation consumption of all transactions in one chunk with ΓTx  Γchunk  (4) (5) (6)  Here, we assume the existence of a measure of computation consumption similar to gas in Ethereum. Since there is a hard limit of computation consumption both on a transaction as well as a chunk, with the chunk’s being significantly higher than the transaction’s, the simple greedy Algorithm 1 will achieve the goal of similar computational consumption. Formally, let Γchunk = n · ΓTx , for n ≥ 1. Then, all chunks, except for the last one, will have a computation consumption c with the following properties. • (n − 1)ΓTx < c. If this was not the case, i.e., c ≤ (n − 1)ΓTx , Algorithm 1 would add more transactions to the current chunk (line 6), as the computation consumption for transactions is upper-bounded by ΓTx . • c ≤ nΓTx , which is guaranteed by Algorithm 1, line 6. Hence, all chunks, except for the last one, will have a computation consumption c (1 − n1 )Γchunk < c ≤ Γchunk .  (7)  Choosing n large enough guarantees that all but the last chunk have similar computation consumption. The last chunk could contain as little as a single transaction. Hence, its computation consumption cLastChunk could take any value 0 < cLastChunk ≤ Γchunk . As opposed to (1 − 1/n)Γchunk < c for any other chunk in the block. The last chunk being significantly smaller than (1 − 1/n)Γchunk does not pose a problem for the following reason. For example, consider a node participating in a network 15  Algorithm 1 Chunking Input: T : List; element T [i] is computation consumption of transaction with index i in current block Output: C: List; element (k, c) ≡ C[i] represents chunk with index i in current block; k is index of first transaction in chunk; c is chunk’s computation consumption 1: Chunking(T ) 2: C←[] . initialize C as empty list 3: c←0 . computation consumption of current chunk 4: k←0 . start index of current chunk 5: for i = 0, 1, . . . , length(T ) − 1: 6: if c + T [i] > Γchunk : . adding transaction with index i would overflow chunk 7: e ← (k, c) . complete current chunk without transaction i 8: C.append(e) 9: k←i . start next chunk at transaction i 10: c ← T [i] 11: else: . current chunk computation consumption of current chunk 12: c ← c + T [i] 13: e ← (k, c) . complete last chunk 14: C.append(e) 15: return C Algorithm 1: separates transactions in a block into chunks. The algorithm presumes that computation consumption T [i] ≤ ΓTx for all i.  that can process blocks with up to Ξ = 1000 chunks. For η = 4.2%, a node would be required to have the capacity to process up to 42 chunks per block (as outlined above). Hence, for each block with Ξ0 ≤ 42, an honest Verifier would simply check the entire block. For blocks with more chunks, the workload is failry uniform across all Verifiers, even though each Verifier samples a subset of chunks for verification. This is because the large majority of all chunks have fairly comparable computation consumption as derived in equation (7). 3.3.2  The Execution Receipt  As Message 3 shows, the Execution Receipt is a signed ExecutionResult, which provides authentication of the sender and guarantees integrity of the message. The ExecutionResult encapsulates the Execution Node’s commitments to both its interim and final results. Specifically, it contains a reference, ExecutionResult.blockHash, to the block whose result it contains and ExecutionResult.finalState as a commitment6 to the resulting state. Consider the situation where an Execution Node truthfully computed a block but used the computational state from a faulty Execution Receipt as input. While the node’s output will likely propagate the error, it is important to attribute the error to the malicious node that originally introduced it and slash the malicious node. To ensure attributability of errors, ExecutionResults specify 6  We don’t specify details of the StateCommitment here. It could either be the full state (which is likely much too large for a mature system). Alternatively, StateCommitment could be the output of a hashing algorithm, such as Merkle Patricia Trie used by Etherum [25].  16  the computation result they built on top of as ExecutionResult.previousExecutionResultHash. As we will show later in section 3.6, a faulty computation result will be rejected with overwhelming probability. Lastly, the ExecutionResult.chunks contains the Execution Node’s result of running the Chunking 1: message Chunk { 2: StateCommitment startState; algorithm (Chunk.startingTransactionIndex and Chunk.computationConsumption). Also, a Chunk 3: float τ0 ; states the starting state (Chunk.startState) for the 4: uint32 startingTransactionIndex; computation and the computation consumption for 5: float computationConsumption; 6: } the first transaction in the chunk (Chunk.τ0 ) 1:  Solving the Verifier’s Dilemma  2:  The field ExecutionReceipt.Zs is a list of SPoCKs which is generated based on interim states from computing the chunks. Formally, for the i th chunk VerificationProof.Zs[i] holds the SPoCK demonstrating knowledge of the secret derived from executing the chunk. As explained in section 3.4.2, the SPoCK is required to resolve the Verifier’s Dilemma in Flow. Furthermore, it prevents Execution Nodes from copying ExecutionResult from each other, pretending their computation is faster than it is. Given that there are several Execution Nodes, it is likely that multiple Execution Receipts are issued for the same block. We define consistency of Execution Receipts as follows.  3: 4: 5: 6: 1: 2: 3: 4: 5:  message ExecutionResult { bytes blockHash; bytes previousExecutionResultHash; repeated Chunk chunks; StateCommitment finalState; } message ExecutionReceipt { ExecutionResult executionResult; repeated SPoCK Zs; bytes executorSig; }  Message 3: Execution Nodes broadcast an ExecutionReceipt to the entire network after computing a block.  Definition 2 Consistency Property of Execution Receipts Execution Receipts are consistent if and only if 1. their ExecutionResults are identical and 2. their SPoCKs attest to the same confidential knowledge.  3.4  Verification Role: Checking Execution Result  The verification process is designed to probabilistically guarantee safety of the computation result (see page 23, Theorem 2). A crucial aspect for the computational safety is that Verification Nodes verifiably self-select the chunks they check independently from each other. This process is described in the section below. In section 3.4.2, we describe how the selected chunks are verified. 3.4.1  Self-selecting chunks for verification  The protocol by which Verification Nodes self-select their chunks is given in Algorithm 2. While inspired by Algorand’s random sortition [26], it has significantly reduced complexity.  17  Algorithm 2 ChunkSelfSelection Input: η: fraction of chunks to be checked by a Verification Node er: Execution Receipt sk: Verifier Node’s secret key Output: L: List of chunk indices that are assigned to the Verifier p: proof of protocol-compliant selection 1: ChunkSelfSelection(η, er, sk) 2: chunks ← er.executionResult.chunks . list of chunks from Execution Receipt 3: Ξ ← length(chunks) . number of chunks in the block 4: p ← signsk (er.executionResult) . sign Execution Receipt’s ExecutionResult 5: seed ← hash(p) . use signature’s hash as random number generator seed 6: n ← dη · Ξe . number of chunks for verification 7: L ← FisherYatesShuffle(chunks, seed, n) . generate random permutation of chunks 8: return L, p Algorithm 2: randomly selects chunks for subsequent verification. In line 6, d·e is the ceiling operator. The function FisherYatesShuffle(list, seed, n) draws a simple random sample without replacement from the input list. The seed is used for initializing the pseudo-random-number generator.  ChunkSelfSelection has the following crucial properties. (1) Verifiability. The seed for the random selection of the chunks is generated in lines 4 – 5. Given the ExecutionReceipt, the Verifier’s public key, and the proof p, any other party can confirm that p was generated according to protocol. Moreover, given p, anyone can re-compute the Verifier’s expected chunk assignment. Thereby, the chunk-selection protocol is verifiable and deviating from the protocol is detectable, attributable, and punishable. (2) Independence. Each Verifier samples its chunks locally and independently from all the other Verifiers. (3) Unpredictability. Without knowledge of the Verifier’s secret key sk, it is computationally infeasible to predict the sample. Formally, the computation of seed can be considered a verifiable random function [2]. (4) Uniformity. A Verifier uses Fisher-Yates shuffling [27, 28] to self-select the chunks it checks. The Fisher-Yates algorithm’s pseudo-random number generator is seeded with the output of a cryptographic hash function. The uniformity of the seed and the uniformity of the shuffling algorithm together guarantee the uniformity of the generated chunk selection. Corollary 1 Algorithm 2 samples a subset of dη · Ξe chunks, for Ξ the number of chunks in the block and η the fraction of chunks to be checked by each Verification Node. The selection probability uniform and independently and identically distributed (i.i.d.) for all Verifiers. The sample is unpredictable without the Verification Node’s secret key. Independence and unpredictability are crucial for the system’s security. A selection protocol without these properties might be abused by a malicious Execution Node (see section 4.3 for a detailed discussion). 18  Algorithm 3 ChunkVerification Input: Λ: starting state for computing the chunk Txs: List of transactions in chunk τ0 : computation consumption of leading transaction in this chunk Λ0 : resulting state after computing the chunk as stated in the Execution Receipt τ00 : computation consumption of leading transaction in next chunk; or ∞ if this is the last chunk in the block c: chunk’s computation consumption as stated in the Execution Receipt Output: true: if and only if chunk passes verification 1: ChunkVerification(Λ, Txs, Λ0 , ntx, c) 2: γ←0 . accumulated computation consumption 3: for t in Txs: . for each transaction in chunk 4: Λ, τ ← execute(Λ, t) . execute transaction 5: if t is first transaction in chunk: 6: assert that τ0 == τ . computation consumption for first transaction in chunk is correct 7: γ ←γ+τ . add transaction’s computation consumption to γ 8: assert that γ == c . computation consumption for entire chunk is correct 9: assert that c ≤ Γchunk . computation consumption does not exceed limit 0 10: assert that c + τ0 > Γchunk . chunk is full: no more translations can be appended to chunk 11: assert that Λ == Λ0 . verify Execution Node’s resulting state 12: return true Algorithm 3: verifies a chunk. The function execute(Λ, t) applies the transaction t to the computational state Λ and returns a pair of values: the resulting state (first return value) and transaction’s computation consumption (second return value). The assert statement raises an exception if the condition is false.  Our approach to independent verification of chunks has similarities with traditional acceptance sampling theory [24, 29, 30], yet differs as our model assumptions are different. In contrast to traditional acceptance sampling, where physical items are tested, identical copies of our digital chunks can be checked in parallel by multiple Verifiers. Part of the novelty of our approach is that we elevate an acceptance sampling model with parallel sampling. 3.4.2  Verifying chunks  The verification protocol is designed to be self-contained, meaning any Execution Receipt can be verified in isolation. All required data is specified through the hashes in the execution receipt. Checking the correctness of a chunk requires re-computing the transactions in the chunk. The details of the verification protocol are given in Algorithm 3. The inputs τ0 and τ00 for the ChunkVerification algorithm are taken directly from the Execution Receipt. Λ, Txs, and Λ0 , have to be fetched from the Execution Nodes and checked to match the Execution Receipt (specifically: Λ, Λ0 ) or the original block (specifically: Txs) via hashing7 . Therefore, errors uncovered by the verification process can be attributed to the data provider and slashed. The verification process is also given enforcement power, as we enable it to request slashing 7  We use the term ‘hashing’ here to refer to the one-time application of a conventional hash function as well as iterative hashing schemes such as Merkle trees or Merkle Patricia trie.  19  1: 2: 3: 4: 1: 2: 3: 4: 5: 1: 2: 3: 4: 5:  message CorrectnessAttestation { bytes executionResultHash; bytes attestationSig; } message VerificationProof { repeated uint32 L; bytes p; repeated SPoCK Zs; } message ResultApproval { CorrectnessAttestation attestation; VerificationProof verificationProof; bytes verifierSig; }  . Hash of approved ExecutionResult . Signature over executionResultHash  . list of chunk indices assigned to verifer . proof to verify correctness of chunk assignment . for each assigned chunk: proof of re-computation  . signature over all the above fields  Message 4: Verification Nodes broadcast a ResultApproval to all Consensus Nodes if all their assigned chunks pass verification.  against an Execution Node. A successful verification process results in a ResultApproval (Message 4) being broadcast by the Verifier to all Consensus Nodes. It is important to note that a ResultApproval (specifically ResultApproval.attestation) attests to the correctness of an ExecutionResult. Specifically, in CorrectnessAttestation, the Verifier signs the ExecutionResult, not the Execution Receipt. Per definition 2, multiple consistent Execution Receipts have identical ExecutionResults. Hence, their correctness is simultaneously attested to by a single ResultApproval message. This saves communication bandwidth, as each Verifier Node issues only one ResultApproval for the common case that several Execution Nodes publish the same results by issuing consistent Execution Receipts. The second important component is the ResultApproval.verificationProof, which proves that the Verification Node completed the assigned verification tasks. We designed this protocol component to address the Verifier’s Dilemma [22]. It prevents the Verifier from just approving ExecutionReceipts, betting on honesty of the Execution Nodes, and thereby being compensated for work the Vertifier didn’t do. The field VerificationProof.L specifies the chunk indices the Verifier has selected by running ChunkSelfSelection (Algorithm 2). As detailed in section 3.4.1 (property (1) Verifiability), protocol-compliant chunk selection is proven by VerificationProof.p, which holds the value for p returned by ChunkSelfSelection. The list VerificationProof.Zs contains for each assigned chunk a proof of verification. Formally, for each i ∈ VerificationProof.L, VerificationProof.Zs[i] holds a SPoCK. Each Verifier samples the chunks it checks independently (property (2)). Furthermore, statistically each chunk is checked by a larger number of nodes (e.g., on the order of 40 as suggested in section 3.7) Therefore, with overwhelming probability, all chunks are checked. (We formalize this argument in Theorem 2).  20  3.5  Consensus Role: Sealing Computation Result  Sealing a block’s computation result happens after the block itself is already finalized. Once the computation results have been broadcast as Execution Receipts, Consensus Nodes wait for the ExecutionResults to accumulate matching Results Approvals. Only after a super-majority of Verifier Nodes approved the result and no FaultyComputationChallenge has been submitted (see section 4.1 for details), the ExecutionResult is considered for sealing by the Consensus Nodes. The content of a BlockSeal is given in Message 5. Algorithm 4 specifies the full set of conditions a BlockSeal must satisfy. Algorithm 4 enforces a specific structure in our blockchain which is illustrated in Figure 4. Once a Consensus Node finds that all conditions are valid, it incorporates the BlockSeal as an element of Block.blockSeals into the next Block it proposes. All honest Consensus Nodes will check the validity of the BlockSeal as part of verifying the Block, before voting for it. Thereby the validity of a BlockSeal is guaranteed by the BFT consensus algorithm. Furthermore, condition (8) guarantees that any FaultyComputationChallenge has been received before the block is sealed. This gives rise to the following corollary: Corollary 2 Given a system with partially synchronous network conditions with message traversal time bounded by ∆t . A Block is only sealed if more than 23 of Verification Nodes approved the ExecutionResult and no FaultyComputationChallenge was submitted and adjudicated with the result that the ExecutionResult is faulty.  The 3 conditions for a new BlockSeal  Latest sealed block B  1. Link to previous ExecutionResult  Next block ^ to seal B ^ 𝛽. blockHas  h  𝛽.block H  ash  2. Blocks referenced by ExecutionResults are chained Candidate seal 𝛽^  Seal 𝛽 of highest sealed block  3. B’s computational output state must ^ match starting state for B  Figure 4: Validity conditions for a new BlockSeal according to Algorithm 4.  21  1: 2: 3: 4: 5: 6: 7:  message BlockSeal { bytes blockHash; bytes executionResultHash; bytes executorSigs; bytes attestationSigs; bytes proofOfWaiting; }  . Signatures from Execution Nodes over ExecutionResult . Signatures from Verification Nodes approving the ExecutionResult . output of VDF to prove waiting for a FaultyComputationChallenge  Message 5: In order to seal a block (with hash blockHash), Consensus Nodes add a BlockSeal to the next block (field Block.blockSeals) they finalize. The field executorSigs is the aggregated signature of at least one Execution Node that published an Execution Receipt with a compatible ExecutionResult. Their BlockSeal is valid only if more than 23 of Verifier Nodes have signed. Instead of storing the signatures individually, we use store an aggregated signature in attestationSigs.  Algorithm 4 Validity of Block Seal Let β be the (accepted) BlockSeal for the highest sealed block, i.e., β is contained in a finalized block. A candidate BlockSeal β̂ must satisfy the following conditions to be valid. (1) β̂.executionResult.previousExecutionResultHash must be equal to β.executionResultHash (2) Let • B be the block whose result is sealed by β, i.e., B is referenced by β.blockHash; • B̂ be the block whose result is sealed by β̂, i.e., B̂ is referenced by β̂.blockHash. B̂.previousBlockHash must reference B. (3) Let • E be the ExecutionResult that referenced (sealed) by β.executionResultHash; • Ê be the ExecutionResult that is referenced by β̂.executionResultHash. The starting state for computing B̂ must match the B computational output state, i.e., Ê.chunks[0].startState == E.finalState (4) β̂.attestationSigs must contain more than 32 of Verifier Nodes’ signatures (5) For each Verifier who contributed to β̂.attestationSigs, the VerificationProof has been validated. (6) No FaultyComputationChallenge against the ExecutionResult is pending (i.e., not yet adjudicated). (7) No FaultyComputationChallenge against the ExecutionResult was adjudicated with the result that the ExecutionResult is faulty. (8) β.proofOfWaiting proves a sufficiently long wait time ∆t Per axiom, we consider the genesis block as sealed. Algorithm 4: protocol for determining validity of a BlockSeal. Figure 4 illustrates conditions (1) – (4).  22  3.6  Proof of Computational Correctness  Below, we prove in Theorem 1 that Flow’s computation infrastructure has guaranteed liveness, even in the presence of a moderate number of Byzantine actors. Furthermore, Theorem 2 proves that block computation is safe, i.e., the resulting states in sealed blocks are correct with overwhelming probability. While safety is unconditional on the network conditions, liveness requires a partially synchronous network. Theorem 1 Computational Liveness Given a system with • more than 23 of the Consensus Nodes’ stake is controlled by honest actors; • and at least one honest Execution Node; • and more than 23 of the Verification Nodes’ stake is controlled by honest actors; • and partially synchronous network conditions with message traversal time bounded by ∆t . The computation and sealing of finalized blocks always progresses. Proof of Theorem 1 • Assuming liveness of the consensus algorithm, finalization of new blocks always progresses. • For a system with one honest Execution Node, there is at least one Execution Receipt with a correct ExecutionResult. • Every honest Validator will approve a correct ExecutionResult. Hence, there will be Result Approvals by at least 23 of the Verification Nodes. • Malicious Verifiers might temporarily delay block sealing by raising a FaultyComputationChallenge, which triggers condition (6) in Algorithm 4. However, the resolution process (see section 4.1 for details) guarantees that the FaultyComputationChallenge is eventually adjudicated and malicious Verifiers are slashed (Corollary 3). Therefore, malicious Verifiers cannot indefinitely suppress block sealing via condition (6) or even reach condition (7). • Consequently, all honest Consensus nodes will eventually agree on the validity of the Block Seal. • Assuming a consensus algorithm with guaranteed chain quality8 , an honest Consensus Node will eventually propose a block and include the Block Seal as prescribed by the protocol. • Given that there are more than 23 of honest Consensus Nodes, the block containing the seal will eventually be finalized (by liveness of consensus).  Theorem 2 Computational Safety Given a system with • partially synchronous network conditions with message traversal time bounded by ∆t ; • more than 23 of the Consensus Nodes’ stake is controlled by honest actors; • all Verification Nodes are equally staked and more than 23 of them are honest. e denote the number of honest Verification Nodes and η the fraction of chunks each Verifier checks. Let N The probability Perror for a computational error in a sealed block is bounded by Perror . Ξ · 1 − η  Ne  (8)  e . Here, Ξ denotes the number of chunks in the Execution Receipt. for large N 8  Formally, chain quality of a blockchain is the ratio  number of blocks contributed by honest nodes number of blocks contributed by malicious nodes  23  [31].  Theorem 2 states that the probability of a computational error decreases exponentially with the number of Verifiers. Proof of Theorem 2 Consider an Execution Receipt. • For brevity, we denote the Execution Receipt’s chunks as L , i.e., L = ExecutionReceipt.executionResult.chunks. Without loss of generality, we treat L as a set (instead of an ordered list), as this proof does not depend on the order of L’s elements. • Let Ξ be the total number of chunks in the Execution Receipt, i.e Ξ = |L|, where | · | denotes the cardinality operator. • Let ξ ∈ L denote a chunk. As formalized in Corollary 1, each honest Verifier randomly selects a subset S ⊂ L with |S| = dη · Ξe by execution ChunkSelfSelection (Algorithm 2). As each Verifier selects the chunks by FisherYates shuffle, it follows that chunk ξ not being selected as the first element is Ξ−1 Ξ and that ξ is not Ξ−2 selected as the second element is Ξ−1 , etc. Hence, the probability that chunk ξ is not checked by one specific verifer is Ξ−n Ξ−n n Ξ−1 Ξ−2 · · ... · = =1− , for n := |S| = dη · Ξe. (9) p̄ξ = Ξ Ξ−1 Ξ − (n − 1) Ξ Ξ Let P̄ξ be the probability that ξ is checked by none of the honest Verifiers. e e n N P̄ξ = p̄N ξ = 1− Ξ Ne ≤ 1−η  (10) (11)  as n = dη · Ξe ≥ η · Ξ. Consequently, the probability of the specific chunk ξ not being checked e . Figure 5 visualizes the exact probability decreases exponentially with the number of Verifiers N P̄ξ as a function of η as given in equation (10). The probability that all chunks are checked by at least one honest Verifier is (1 − P̄ξ )Ξ . Consequently, the probability an error in any chunk in the block remaining undetected is Perror = 1 − (1 − P̄ξ )Ξ .  (12)  We assume that the system parameter η is chosen such that P̄ξ ≈ 0 to ensure computational safety. Hence, we can approximate eq. (12) by its first order Taylor series in P̄ξ . (1 − P̄ξ )Ξ ' 1 − Ξ · P̄ξ  (13)  Inserting equations (13) and (11) into (12) yields Perror . Ξ · 1 − η  Ne  ,  (14)  which proves equation (8) from the theorem. We have now shown that the probability of a faulty chunk in an Execution Receipt not being checked by an honest Verifier is bounded by (14). Furthermore, every honest Verifier will challenge any faulty chunk it is assigned to check by raising a FaultyComputationChallenge (see section 4.1 for details). Corollary 2 guarantees that a block is only sealed if no correct FaultyComputationChallenge was raised. Hence, the only way a block can be sealed with a faulty ExecutionResult is if the faulty chunks are not checked by honest Verifers. Consequently, eq. (14) also bounds the probability of a faulty ExecutionResult being sealed.  24  Figure 5: Probability P̄ξ that a specific chunk is checked by none of the honest Verifiers. η is the fraction of chunks e = 667 honest Verification Nodes verifying each Verifier selects for checking. The graph illustrates probabilities for N an Execution Receipt with Ξ = 1000 chunks. The blue curve shows the dependency when Verifiers sample their chunks based on Fisher-Yates as specified in Algorithm 2, i.e., sample chunks from the Execution Receipt without replacement. For comparison, we show sampling with replacement.  3.7  Computational Load on Verification Nodes  Using equation (8), we can compute the required fraction η of chunks that each Verifier has to check to achieve a specific Perror . For the mature system under full load, we expect there to be 1000 Verification Nodes and each block to contain up to Ξ = 1000 chunks. Furthermore, we make the e = 667. conservative assumption that only 23 of the Verification Nodes are honest, i.e., N Let the probability for a malicious Execution Node to succeed with an attempt to introduce a compromised state into the blockchain be Perror ≤ 10−6 . To achieve this, every Verification Node would need to check 32 chunks, i.e., execute η = 3.2% of the work of an Execution Node. To lower the probability even further to Perror ≤ 10−9 , Verifiers only need to execute η = 4.2% of the work of an Execution Node. This shows that distributing and parallelizing the verification of computation results is efficient. Furthermore, note that checking the chunks can be trivially executed in parallel.  25  4  Mitigation of Attack Vectors  In the following, we will discuss the most severe attacks on Flow. In this context, we would like to re-iterate that the staking balances are maintained by the Consensus Nodes as part of the network state (compare section 1.3). Hence, Consensus Nodes can adjudicate and slash misbehaving nodes directly. The resulting updates to the network state are published in the blocks (field slashingChallenges in message 2) without needing to involve Execution Nodes.  4.1  Adjudicating with Faulty Computation Results  In section 3.6, Theorem 2, we have shown that a faulty computation state will be challenged by a Verification Node with near certainty. Formally, a Verification Node submits a Faulty Computation Challenge (FCC), to the Consensus Nodes for adjudication. We start by introducing the necessary notation and then proceed with specifying the details of an FCC and the protocol for processing them. Nomenclature (illustrated in Figure 6): For an 1: message FaultyComputationChallenge { Execution Receipt with Ξ chunks, the field ExecutionReceipt.executionResult.chunks 2: bytes executionReceiptHash; holds the StateCommitments [Λ0 , Λ1 , . . . , ΛΞ ]. 3: uint32 chunkIndex; For i ∈ {0, 1, . . . , Ξ − 1}, Λi denotes the start- 4: ProofOfAssignment proofOfAssignment; ing state for the computation of the chunk with 5: StateCommitment stateCommitments; index i. ΛΞ is the final state at the end of the 6: Signature verifierSig; 7: } block (after all transactions are computed). To denote the (interim) states between inMessage 6: Verification Nodes send this message to Condividual transactions, we extend the notation sensus Nodes to challenge a specific Execution Receipt accordingly. Let the chunk at index i contain (executionReceiptHash). The FaultyComputationChallenge χi transactions. For k ∈ {0, 1, . . . , χi − 1}, is specific to a computational output state for one of the chunks, where chunkIndex is a zero-based inΛi,k denotes the input state for computing the dex into ExecutionReceipt.executionResult.chunks (compare transaction with index k within the chunk. Ac- Message 3). cordingly, Λi,χi −1 is the state at the end of the chunk. Note that Λi,χi −1 simultaneously serves as the starting state for the next chunk at index i + 1. Hence, Λi,χi as well as Λi+1 and Λi+1,0 refer to the same StateCommitment. The different nomenclatures are introduced for ease of notation only. Definition 3 A well-formed Faulty Computation Challenge (FCC), specified in Message 6, challenges one specific chunk at index chunkIndex of an Execution Receipt (referenced by executionReceiptHash). It provides the list stateCommitments = [ΛchunkIndex,0 , ΛchunkIndex,1 , . . . , ΛchunkIndex,χchunkIndex ].  26  (15)  Published in Execution Receipt States between chunks  chunk 0: transaction 0 chunk 0: transaction 1  chunk 0: transaction chunk 1: transaction 0  chunk : transaction 0 chunk : transaction 1  Published in FCC against chunk i: interim states between transactions for chunk with index  chunk : transaction  Figure 6: Illustration of nomenclature State commitments (e.g., hashes) are represented as vertical lines and denoted by Λ. The bold lines visualize the starting states Λ0 , Λ1 , . . . , ΛΞ−1 for the chunks, as well as the final state ΛΞ . Furthermore, Λi,k is the input state for computing the transaction with index k within the chunk.  Definition 4 Protocol for Adjudicating a Faulty Computation Result Let there be a Verifier Node V that is checking the chunk at index i and disagrees with the resulting e and the Execution state Λi+1 ≡ Λi,χi . In the following, we denote the Verifier’s StateCommitments as Λ Node’s with Λ. 1. Verifier V broadcasts a FaultyComputationChallenge to the Consensus Nodes with • FaultyComputationChallenge.chunkIndex ← i e i,1 , . . . , Λ e i,χ ] • FaultyComputationChallenge.stateCommitments ← [Λ i 2. Consensus Nodes publish the FCC in their next finalized block (field slashingChallenges in message 2) 3. The challenged Execution Node has a limited time to broadcast a FaultyComputationResponse (Message 7) to the Consensus Nodes. Time is measured using a verifiable delay function [32].  1: 2: 3: 4: 5:  message FaultyComputationResponse { bytes FaultyComputationChallengeHash; StateCommitment stateCommitments; Signature executerSig; }  Message 7: A challanged Execution Node broadcast a FaultyComputationResponse to the Consensus Nodes.  27  (a) Should the Execution Node not respond, it is slashed. Consensus Nodes will include a corresponding notification in the next block (field networkStateUpdates in Message 2) that also includes the output of the VDF as proof of waiting. In this case, adjudication ends with the Execution Node being slashed. (b) To prevent being slashed, the Execution Node must disclose all interim states in the chunk by submitting a FaultyComputationResponse with • FaultyComputationResponse.stateCommitments ← [Λi,1 , . . . , Λi,χi ] to the Consensus Nodes. In case the Execution Node sends a FaultyComputationResponse, the protocol continues with step 4 below. 4. Consensus Nodes now compare the stateCommitments from both parties element-wise and find the first mismatch. Let the first mismatch occur at index `, i.e. e i,` Λi,` 6= Λ e i,l . for all l ∈ [0, 1, . . . , ` − 1] : Λi,l = Λ  (16) (17)  Essentially, both parties agree that, starting from state Λi ≡ Λi,0 , the computation should lead to Λi,`−1 as the input state for the next transaction. However, they disagree on the resulting state after computing this next transaction. 5. Consensus Nodes request state Λi,`−1 from either party. Furthermore, by resolving the texts of collections in the block, Consensus Nodes obtain the transaction with index ` in the chunk, whose output is disputed. 6. Consensus Nodes use Λi,`−1 as input state for computing transaction with index ` in the chunk. e i,` . Consensus Nodes now compare their locally computed output state with Λi,` and Λ 7. Any party who published a computation result that does not match the values computed by the Consensus Nodes is slashed. Consensus Nodes will include a corresponding notification in the next block (field networkStateUpdates in message 2). Informally, Definition 4 describes a protocol by which a Verifier Node can appeal to the committee of Consensus Nodes to re-compute a specific transaction whose output the Verifier does not agree with. To avoid being slashed, the challenged Execution node must provide all information that is required for the Consensus Nodes to re-compute the transaction in question. Nevertheless, there is no leeway for the Execution Node to provide wrong information as honest Consensus Nodes will verify the correctness based on previously published hash commitments: • Consensus Nodes request transaction texts of collections. The collection hashes are stated in blocks, which allow verification of the collection texts. • Consensus Nodes request the last interim state Λi,`−1 in the chunk that both parties agree on. A hash of this state was published by both the Verifier and the challenged Execution Node. This allows the Consensus Nodes verify state variables (e.g., via Merkle proofs). Furthermore, the described protocol is executed by all honest Consensus Nodes. The resulting slashing is included in a block and hence secured by BFT consensus. Assuming a super-majority of honest Consensus Nodes, it is guaranteed that the misbehaving node is slashed. The following Corollary 3 formalizes this insight.  28  Corollary 3 Given a system with • more than 23 of the Consensus Nodes’ stake is controlled by honest actors; • and partially synchronous network conditions with message traversal time bounded by ∆t . The following holds. • If an honest Verifier node is assigned to verify a chunk that has a faulty computation result, the Execution Node who issues the corresponding Execution Receipt will be slashed. • If a dishonest Verifier Node challenges a correct computation result, the Verifier will be slashed.  4.2  Resolving a Missing Collection  As message 2 and 1 show, a block references its collections by hash but does not contain the individual transactions texts. The transactions texts are stored by the Collector Node cluster which build the collection and is only required when Execution Nodes want to compute the blocks’ transactions. Hence, a cluster of Collector nodes could withhold the transaction texts for a guaranteed collection. While block production is not impacted by this attack, block execution halts without access to the needed transaction texts. When an Execution Node is unable to resolve a guaranteed collection, it issues a Missing Collection Challenge (MCC). An MCC is a request to slash the cluster of Collector Nodes (Message 1, line 4) who have guaranteed the missing collection. MCCs are directly submitted to Consensus Nodes. Definition 5 Protocol for Resolving a Missing Collection 1. An Execution Node determines that the transaction texts for a GuaranteedCollection from the current block are not available as expected. The protocol does not dictate how this determination is reached, but the obvious implementation is assumed (ask the Guarantors, wait for a response, ask other Execution Nodes, wait for a response). 2. The Execution Node broadcasts an MCC to all Collector and Consensus Nodes. The Consensus Nodes record the challenge in the next block, but do not otherwise adjudicate the challenge at this stage. 3. Any honest Collector Node, which is not a member of the challenged cluster, sends a request to κ randomly selected Guarantors to provide the missing Collection. If the request is answered, the requesting Collector Node forwards the result to the Execution Nodes. 4. If the Guarantor does not respond within a reasonable time period ∆t , the requesting Collector Node will sign a Missing Collection Attestation (MCA), including the hash of the collection in question. Time is measured using a verifiable delay function [32] and the MCA contains the VDF’s output as proof of waiting. The MCA is broadcast to all Consensus and Execution Nodes. 5. An honest challenged Guarantor will respond with the complete Collection text to any such requests. 6. If the Execution Nodes receive the collection, they process the block as normal. Otherwise, they wait for more than 32 of the Collector Nodes to provide MCAs. 7. Appropriate MCAs must be included in all Execution Receipts that skip one or more Collections from the block. 8. Every MCC will result in a small slashing penalty for each Execution Node and each challenged Guarantor. Even if the MCC is resolved by finding the Collection, each of these actors must pay the fine, including the Execution Nodes that did not initiate the MCC. This is designed to prevent the following edge cases:  29  • Lazy Guarantors that only respond when challenged: without a fine for challenged Guarantors, even in the case where the collection is resolved, there is no incentive for Guarantors to respond without being challenged. • Spurious MCCs coming from Byzantine Execution Nodes: without a fine for Execution Nodes, there is a zero-cost to generating system load through unjustified MCCs. • Don’t punish the messenger: all Execution Nodes must be fined equally so that there is no disincentive to be the first Execution Node to report a problem. 9. If an Execution Receipt containing an MCC is sealed, ALL guarantors for the missing Collection are subject to a large slashing penalty (equal to the minimum staking requirement for running a Collector Node). Discussion • The slashing penalty for a resolved MCC should be small enough that it doesn’t automatically eject the Collector Nodes from the network (by dropping them below the minimum staking threshold), but must be significant enough to account for the fact that resolving an MCC is very expensive. • Resolving an MCC is very expensive. Each Collector Node will request the Collection from one Guarantor, so each Guarantor will have to respond to many requests or risk being slashed. Each Execution Node will be flooded with copies of the Collection if it is available. We are operating under the theory that if MCCs have a very high probability of being resolved correctly (Lemma 3), spurious MCCs should be very rare specifically because of the described penalties. • If most Execution Nodes are Byzantine and raise spurious MCCs, but at least one Execution Node is honest and generates complete Execution Receipts, a correct Execution Receipt will be sealed (assuming an honest super-majority of Collector Nodes and Consensus Nodes). Furthermore, the Execution Nodes who raised the spurious MCC will be slashed. • If most Guarantors of a particular Collection are Byzantine, and refuse to provide Collection data, but at least one Guarantor is honest, the Collection will be provided to an honest Execution Node and executed properly. • A cabal of 100% of Execution Nodes acting maliciously can halt the network by not executing new blocks. Nevertheless, no faulty state can be introduced into the network by such a denial of service attack. • In order for an attacker to obtain the ability to introduce an error into the computation state (with non-negligible probability), the attacker would need to control a Byzantine fraction of 100% of Execution Nodes and more than 32 of Verifier Nodes. Theorem 3 Liveness of Collection Text Resolution Given a system with • more than 2/3 of the Consensus Nodes’ stake is controlled by honest actors; • more than 2/3 of the Collector Nodes’ stake is controlled by honest actors; • and partially synchronous network conditions with message traversal time bounded by ∆t . The system can be configured such that any guaranteed collection is available with probability close to 1. Proof of Theorem 3 For this proof, we assume that Byzantine nodes collaborate to the maximum extent possible to 30  prevent a collection from being resolved. Unless there are protocol-level guarantees, we consider the most favorable conditions for the Byzantine nodes. Let us assume that the Byzantine nodes successfully prevented a collection from being resolved, i.e., more than 2/3 of the collectors issued an MCA. Let e the number of honest collectors, and N̄ the number • N the total number of collector nodes, N of Byzantine collectors; • ncluster the size of the collector cluster that produced the missing collection, ñcluster the number of honest collectors in the cluster, and n̄cluster the number of Byzantine collectors in the cluster; • ng the number of guarantors of the missing collection, ñg the number of honest guarantors, and n̄g the number of Byzantine guarantors. e + N̄ = 1000 collector nodes where N e = 2bN/3c+1. We consider a system configuration with N = N In Flow, the clusters are created by randomly partitioning the collectors into clusters of size ncluster via Fisher-Yates shuffling. Hence, the probability of drawing a cluster with n̄cluster Byzantine actors is given by the hypergeometric distribution   N −N̄ N̄ Pn  cluster ,N,N̄  (n̄cluster ) =  n̄cluster  ncluster −n̄cluster  N ncluster  .  (18)  For a collection, at least ng = 2bncluster /3c + 1 = n̄g + ñg guarantors are required. The number of Byzantine guarantors n̄g could take any value in 0, 1, . . . , n̄cluster . There could be more Byzantine nodes in the cluster than required to guarantee the collection, i.e., ng < n̄cluster . In this case, we assume that only the minimally required number of ng Byzantine nodes would guarantee the collection to minimize slashing. n o n̄g ∈ 0, 1, . . . , min ng , n̄cluster . (19) As each honest guarantor increases the probability of a collection being successfully retrieved, we assume that the Byzantine nodes only involve the absolute minimum number of honest nodes to get the collection guaranteed: ñg = ng − n̄g = 2bncluster /3c + 1 − n̄g  (20)  When an honest collector that is not a guarantor receives a MCC, it selects κ guarantors and requests the collection from them. We assume that only honest guarantors would answer such a request. The probability for a correct node to receive no answer when inquiring about the missing collection, i.e., to issue in MCA, is  ng −ñg κ Pκ,ng ,ñg (0) = . (21) ng κ  Furthermore, every Byzantine node that is not a guarantor of the collection would issue an MCA to increase the chances that the Missing Collection Challenge is accepted. Hence, there are (N̄ − ng ) MCAs from Byzantine nodes. For a collection to be considered missing, the protocol require 23 N collectors to send MCAs. Consequently, the minimally required number of MCAs from honest nodes is e − (N̄ − n̄g ). r=N 31  (22)  Figure 7: Worst-case probability that a collection cannot be resolved. The graph shows numerical values for e = 667 and N̄ = 333. P (MCC accepted), equation (25), for N = 1000, N  As honest nodes independently contact guarantors, each honest node conducts a Bernoulli trial and issues a MCAs with probability Pκ,ng ,ñg (0). Consequently, the probability that r honest nodes issue MCAs follows a Binomial distribution  e r  Ne −r N  B(r) = Pκ,ng ,ñg (0) 1 − Pκ,ng ,ñg (0) . (23) r Given the number n̄cluster of byzantine actors in the cluster, the worst-case probability that the MCC is accepted by the system is P (MCC accepted | n̄cluster ) =  max  n̄g ∈ {0,1,...,min(ng , n̄cluster )}  e N X  The overall probability of an MCC being accepted is, therefore, X P (MCC accepted) = P (MCC accepted | n̄cluster ) · Pn n̄cluster ∈ {0,...,ncluster }  B(r) .  (24)  r= e −N̄ +n̄g N  cluster ,N,N̄  (n̄cluster )  (25)  Figure 7 illustrates the worst-case probability of a chunk missing, i.e., that a MCC shows the probabilities according to equation (25).   4.3  Placing errors in chunks checked by colluding Verifier Nodes  If an Execution Node and several Verifier Nodes are colluding, they have the ability to secretly determine which chunks would be checked by the colluding Verifiers before even publishing an Execution Receipt. However, the self-assignment scheme defined in section 3.4 is independent for 32  each Verifier and in addition non-predictable for anyone without the Verifier’s private key. Therefore, honest Verifiers will still check each chunk with uniform probability, independently of the colluding Verifiers. Consequently, if a malicious Execution Node wants to introduce a computation error, there is no advantage in placing the error in chunks that are checked by colluding Verifiers. This insight is formalized as Corollary 4. Corollary 4 Given a system with partially synchronous network conditions with message traversal time bounded by ∆t . Let there be a malicious Execution Node that tries to introduce a computation error into one of the chunks of a block. The success probability cannot be increased by the chunk selection of Byzantine Verifiers.  33  Acknowledgments We thank Dan Boneh for many insightful discussions, and Alex Bulkin, Karim Helmy, Chris Dixon, Jesse Walden, Ali Yahya, Ash Egan, Joey Krug, Arianna Simpson, as well as Lydia Hentschel for comments on earlier drafts.  References [1] Alexander Hentschel, Dieter Shirley, Layne Lafrance, and Ross Nicoll. Flow: Separating consensus and compute, 2019. [2] Silvio Micali, Salil Vadhan, and Michael Rabin. Verifiable random functions. In Proceedings of the 40th Annual Symposium on Foundations of Computer Science, FOCS ’99, pages 120–, Washington, DC, USA, 1999. IEEE Computer Society. [3] Timo Hanke, Mahnush Movahedi, and Dominic Williams. DFINITY technology overview series, consensus system. CoRR, abs/1805.04548, 2018. [4] Benedikt Bünz, Steven Goldfeder, and Joseph Bonneau. Proofs-of-delay and randomness beacons in ethereum. 2017. [5] Dan Boneh, Manu Drijvers, and Gregory Neven. Compact multi-signatures for smaller blockchains. In ASIACRYPT, 2018. [6] Jae Kyun Kwon. Tendermint : Consensus without mining. 2014. https://github.com/cosmos/cosmos/tree/ master/tendermint. [7] Ethan Buchman. Tendermint: Byzantine Fault Tolerance in the Age of Blockchains, Jun 2016. [8] Jae Kwon and Ethan Buchman. Cosmos - A Network of Distributed Ledgers. 2016. https://cosmos.network/ cosmos-whitepaper.pdf. [9] Guy Golan-Gueta, Ittai Abraham, Shelly Grossman, Dahlia Malkhi, Benny Pinkas, Michael K. Reiter, DragosAdrian Seredinschi, Orr Tamir, and Alin Tomescu. SBFT: a scalable decentralized trust infrastructure for blockchains. CoRR, abs/1804.01626, 2018. [10] Ittai Abraham, Guy Gueta, and Dahlia Malkhi. Hot-stuff the linear, optimal-resilience, one-message BFT devil. CoRR, abs/1803.05069, 2018. [11] J. H. Wensley, L. Lamport, J. Goldberg, M. W. Green, K. N. Levitt, P. M. Milliar-Smith, R. E. Shostak, and C. B. Weinstock. Tutorial: Hard real-time systems. chapter SIFT: Design and Analysis of a Fault-tolerant Computer for Aircraft Control, pages 560–575. IEEE Computer Society Press, Los Alamitos, CA, USA, 1989. [12] M. Pease, R. Shostak, and L. Lamport. Reaching agreement in the presence of faults. J. ACM, 27(2):228–234, April 1980. [13] L. Lamport. The Weak Byzantine Generals Problem. J. ACM, 30(3):668–676, July 1983. [14] Heidi Howard. Distributed consensus revised. PhD thesis, 2018. https://www.cl.cam.ac.uk/techreports/ UCAM-CL-TR-935.pdf. [15] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer. Consensus in the presence of partial synchrony. J. ACM, 35(2):288–323, April 1988. [16] Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. Impossibility of distributed consensus with one faulty process. J. ACM, 32(2):374–382, April 1985. [17] Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham. HotStuff: BFT Consensus with Linearity and Responsiveness. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, PODC ’19, pages 347–356, New York, NY, USA, 2019. ACM. [18] Vlad Zamfir. A Template for Correct-By-Construction Consensus Protocols. 2017. https://github.com/ ethereum/research/tree/master/papers/cbc-consensus. [19] Vlad Zamfir. Casper the Friendly Ghost: A ‘Correct By Construction’ Blockchain Consensus Protocol. 2017. https://github.com/ethereum/research/blob/master/papers/CasperTFG. [20] Vlad Zamfir, Nate Rush, Aditya Asgaonkar, and Georgios Piliouras. Introducing the “Minimal CBC Casper” Family of Consensus Protocols. 2018. https://github.com/cbc-casper/cbc-casper-paper.  34  [21] Sarah Azouvi, Patrick McCorry, and Sarah Meiklejohn. Betting on blockchain consensus with fantomette. CoRR, abs/1805.06786, 2018. [22] Loi Luu, Jason Teutsch, Raghav Kulkarni, and Prateek Saxena. Demystifying incentives in the consensus computer. In Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS ’15, pages 706–719, New York, NY, USA, 2015. ACM. [23] Google. Protocol buffers. https://developers.google.com/protocol-buffers/docs/proto. [24] Håkan L. S. Younes and Reid G. Simmons. Probabilistic verification of discrete event systems using acceptance sampling. In Ed Brinksma and Kim Guldstrand Larsen, editors, Computer Aided Verification, pages 223–235, Berlin, Heidelberg, 2002. Springer Berlin Heidelberg. [25] Ethereum Foundation. Patricia tree. https://github.com/ethereum/wiki/wiki/Patricia-Tree. [26] Yossi Gilad, Rotem Hemo, Silvio Micali, Georgios Vlachos, and Nickolai Zeldovich. Algorand: Scaling Byzantine Agreements for Cryptocurrencies. In Proceedings of the 26th Symposium on Operating Systems Principles, SOSP ’17, pages 51–68, New York, NY, USA, 2017. ACM. [27] Ronald A. Fisher and Frank. Yates. Statistical tables for biological, agricultural and medical research / by the late Sir Ronald A. Fisher and Frank Yates. Longman [Harlow], 6th ed. revised and enlarged. edition, 1974. [28] Donald E. Knuth. The Art of Computer Programming, Volume 2: Seminumerical Algorithms. Addison-Wesley, Boston, third edition, 1997. [29] G. Barrie Wetherill. Acceptance sampling: basic ideas, pages 12–43. Springer US, Boston, MA, 1977. [30] E.G. Schilling and Dean Neubauer. Acceptance Sampling in Quality Control. CRC Press, 2017. [31] Juan Garay, Aggelos Kiayias, and Nikos Leonardos. The bitcoin backbone protocol: Analysis and applications. In Elisabeth Oswald and Marc Fischlin, editors, Advances in Cryptology - EUROCRYPT 2015, pages 281–310, Berlin, Heidelberg, 2015. Springer Berlin Heidelberg. [32] Dan Boneh, Joseph Bonneau, Benedikt Bünz, and Ben Fisch. Verifiable delay functions. In Advances in Cryptology – CRYPTO 2018 - 38th Annual International Cryptology Conference, 2018, Proceedings, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), pages 757–788. Springer-Verlag, 1 2018.  35  The Golem Project Crowdfunding Whitepaper  final version  November 2016  Table of contents Overview of the Golem Project Grand vision and core features Golem as an Ecosystem Supply of Infrastructure Demand for Computing Resources Software & Microservices The first use case: CGI rendering Long term vision: Golem as a building block of Web 3.0 Golem Network Token (GNT) Application Registry Transaction Framework Resilience Roadmap Brass Golem Clay Golem Stone Golem Iron Golem Future integrations Crowdfunding Crowdfunding summary Budget and levels of funding Golem Team Managers Key developers Developers Business development and communication  Overview of the Golem Project Grand vision and core features   Golem is the first truly ​decentralized supercomputer, creating a global market for computing power. Combined with flexible tools to aid developers in securely distributing and monetizing their software, Golem altogether changes the way compute tasks are organized and executed. By powering decentralized microservices and asynchronous task execution, Golem is set to become a key building block for future Internet service providers and software development. And, by substantially lowering the price of computations, complex applications such as CGI rendering, scientific calculation, and machine learning become more accessible to everyone.    Golem connects computers in a peer-to-peer network, enabling both application owners and individual users ("requestors") to rent resources of other users’ ("providers") machines. These resources can be used to complete tasks requiring any amount of computation time and capacity. Today, such resources are supplied by centralized cloud providers which, are constrained by closed networks, proprietary payment systems, and hard-coded provisioning operations. Also core to Golem’s built-in feature set is a dedicated Ethereum-based transaction system, which enables direct payments between requestors, providers, and software developers.    The function of Golem as the backbone of a decentralized market for computing power can be considered both Infrastructure-as-a-Service (IaaS), as well as Platform-as-a-Service (PaaS). However, Golem reveals its true potential by adding dedicated software integrations to the equation. Any interested party is free to create and deploy software to the Golem network by publishing it to the Application Registry. Together with the Transaction Framework, developers can also extend and customize the payment mechanism resulting in unique mechanisms for monetizing software.  Golem as an Ecosystem Golem's business case boils down to the fact that, due to relatively recent technological advances, the market for computing resources can be organized according to entirely new principles. In contrast, the compute market today is dominated by heavyweight players such as Amazon, Google, Microsoft and IBM, who leverage their market power and assets to ensure hefty margins, resulting in inefficiently priced compute services. Luckily, the market is not doomed to function this way forever. With Golem the supply of computing resources is based on contributions of individual and professional providers, combined with an array of dedicated software solutions via Golem’s Application Registry – itself operating on a single and competitive market with nearly ​complete information​ market. Scaling the compute market enabled by Golem requires onboarding three groups: suppliers of computing resources ("providers"), task creators ("requestors") who submit their tasks to be computed by the network, and of course, software developers. These three groups comprise Golem’s unique, interdependent ecosystem.  Group  Golem features  Incentive to participate  Requestors  Golem offers tools to execute compute-intensive tasks.  Requestors get access to affordable and scalable solutions, which combine hardware and software.  Providers  Golem combines and utilizes (almost) any kind of existing computing hardware.  Hardware providers get paid for renting out their hardware.  Software Developers  Golem is a flexible platform to deploy and monetize software.  Software developers use Golem as a distribution channel, associated with access to hardware.  Supply of Infrastructure The supply of computing power to the network comes from providers. This could be anyone, from an individual renting out idle CPU cycles of a gaming PC, to a large datacenter contributing their entire capacity. Providers have the incentive to join Golem because they receive payments from requestors for the completed tasks. Of course, Golem’s user interface will be easy to use, giving providers a clear way to set prices and decide what fraction of their own idle resources they are willing to rent out.  Demand for Computing Resources In order to reward providers for contributing their resources, Golem needs to attract requestors seeking additional computing resources. The market Golem creates will be highly competitive due to nearly complete information, and the ease of deploying tasks on any hardware. This will not only make using Golem simple, which will attract requestors - highly competitive setup will also increase efficiency of the market, very likely resulting in much more comprehensive and advantageous pricing compared to the existing cloud computing platforms. One important feature is that Golem's marketplace will enable requestors to become providers: because most of them will need additional resources only occasionally. They will be able to rent out their own hardware and earn extra fees. In addition, financial aspects are not going to be the sole incentive to use Golem: one of its key features will be the availability of diverse software running on the Golem network, accessible from the Application Registry.  Software & Microservices Although some initial use cases (such as CGI rendering) are going to be developed and implemented by the Golem team, it is essential to engage other software developers to develop their own ideas for Golem applications. The number and quality of such applications is one of the key factors in Golem’s future success. For this reason, the Application Registry and Transaction Framework are among the most important features of the entire ecosystem,  and will be front and central to the development process. Once introduced, they will provide developers with flexible and efficient tools to deploy, distribute, and monetize software running on Golem. This is going to be a perfect solution for microservices and some of the forthcoming decentralized applications (DApps), but could also become an interesting way to distribute existing proprietary and open source software.  The first use case: CGI rendering  Golem Alpha release: CGI rendering using Blender It is public, follow ​the link​ to test Golem.  CGI rendering is the first and very illustrative case of real Golem usage. Rather than using costly cloud-based services or waiting ages for one's own machine to complete the task, CGI artists can now rent compute resources from other users to render an image quickly. The payment from a requestor (in this case, a CGI artist) is sent directly to providers who made their resources available. In addition, when the artist's machine is idle, it can itself accept tasks from other users.  Long term vision: Golem as a building block of Web 3.0 We believe the future Internet will be a truly decentralized network, enabling users to securely and directly exchange content, without sharing it with corporations or other middlemen. Accordingly, Golem will be useful not only to compute specific tasks, but also to bulk-rent machines in order to perform operations within a self-organizing network. Of course, this will require the simultaneous development of other technologies, many of which have gained significant traction in recent years. Better data-sharing technologies are necessary, but taking into account the ongoing development of IPFS/Filecoin and Swarm, the appropriate solutions seem to be within reach. Eventually, the Ethereum network will become more scalable, more efficient, and include a fully functional network of micropayment channels. Once these technologies become available, it is easy to imagine Golem primarily as a platform for microservices, allowing users to run both small (e.g. a note-taking app) and large (e.g. a streaming service) applications in a completely decentralized way. Although ambitious, this vision seems to be the ultimate argument for Golem’s long-term potential.  Golem Network Token (GNT) The Golem Network Token ("GNT") account is a core component of Golem and is designed to ensure flexibility and control over the future evolution of the project. GNT is created during the crowdfunding period (described in this whitepaper) and, following the first major release of Golem, GNT will be attributed a variety of functions in the Golem network.      Payments from requestors to providers for resource usage, and remuneration for software developers is going to be exclusively conducted in GNT. Once the Application Registry and Transaction Framework are implemented, GNT will be necessary for other interactions with Golem, such as submitting deposits by providers and software developers or participation in the process of software validation and certification (as described in the Application Registry section). The general conditions for using GNT will be set in the Transaction Framework, but specific parameters of these interactions will be possible to define within each software integration.  The supply of GNT will be limited to the pool of tokens created during crowdfunding period. Creation of the GNT and initial GNT account functionalities         The Golem Network Token is a token on Ethereum platform. Its design follows widely adopted token implementation standards. This makes it easy to manage using existing solutions including Ethereum Wallet. Maximum number of tokens created during crowdfunding period: ○ Total: 1 000 000 000 (100%) ○ Crowdfunding participants: 820 000 000 (82%) ○ Golem team 60 000 000 (6%) ○ Golem Factory GmbH 120 000 000 (12%) Sending 1 ether to the GNT account will create 1 000 GNT No token creation, minting or mining after the crowdfunding period. Tokens will be transferable once the crowdfunding is successfully completed. Go to the crowdfunding section to learn the details and see how to support the Golem Project via crowdfunding.  Application Registry The Application Registry is an Ethereum smart contract, to which anyone can publish their own applications that are ready to run on Golem network. The goal of the Application Registry is to:     Give developers a way to publish their integrations and reach out to users in a decentralized manner; Give requestors a place to look for specific tools fitting their needs; Give providers full control over the code they run because of the security concerns.  Since the Golem network is fully decentralized, we also want the Application Registry to be driven by the community. Golem allows requestors to execute the code of an application on someone else's computer. This code is sandboxed and executed with the minimal required privileges. But software bugs are everywhere, and once in a while people defeat sandboxes, manage to execute malicious code on a host machine, and sometimes even take it over. That’s why we can't rely only on sandboxing. We could try to automatically evaluate whether or not the code is safe, but this is literally impossible (​vide halting problem). The process of code review and validation cannot be fully automated and left to the autonomous network. On the other hand, it is impossible to assume that no one will ever publish malicious software to run on top of Golem network. We solve these problems by splitting Application Registry users into three categories: authors, validators and providers. Authors publish applications, validators review and certify applications as safe and trustworthy by adding them to their own whitelist. Validators may also mark applications as malicious by adding them to their own blacklists. Providers are also given the right to choose whom to trust by selecting validators whose lists are used by the particular instance of Golem running on their nodes. Apart from that, providers may maintain their own whitelists or blacklists. This gives each provider a lot of flexibility in deciding exactly what software to run, and what amount of work to put into software curation. What is more, this system does not exclude any party, and there is always room for new validators to emerge. By default Golem runs using a whitelist of trusted applications. Since an empty whitelist is a problem for someone just trying Golem out for the first time, we will add a number of verified entries to the whitelist as a part of the initial distribution. A provider can take advantage of this mechanism, managing her own whitelist, or simply using whitelists of validators she trusts. On the other hand, a provider running a computing farm may wish to rely entirely on blacklists. This is an option tailored for administrators of dedicated machines, who want to maximize their profits and who are willing to ride the bleeding-edge. In this scenario, a blacklist is used to banish any known troublemaking applications. Again, the provider can maintain her own blacklist, or use the blacklists of validators she trusts.  Transaction Framework When creating something new and exciting, it’s hard if not impossible to predict all the opportunities which the new artifact will suddenly make possible. Golem is a generalized global supercomputer, and as such, it will no doubt find its niche with vastly varied applications. They might need very diverse remuneration models. We are not able to design a one-size-fits-all payment system for Golem, nor will we attempt to force one upon application authors. When a developer integrates her application with Golem, she has the freedom to decide which transaction model she implements, as long as it is compliant with Golem's Transaction Framework. The Transaction Framework will take the form of a set of requirements to follow; basic requirements may include:      Entry in the Application Registry; Use of open source and/or deterministic environment, such as EVM; Community approval or rating of transaction model; Use of GNT for remunerating software and resource providers.  We are building the transaction framework on top of Ethereum. Ethereum gives us expressive power, which is much-needed when implementing advanced, trustless schemes. Example transaction framework components:  Diverse payout schemes such as ​nanopayments​, ​batching  Off-chain payment channels  Custom receipts  Payment to software developer  Per-unit use of software (per-node, per-hour, etc.) In the future, this might evolve into community-reviewed template code to be used in the implementation of custom transaction models. It is also possible to introduce more sophisticated components into the transaction model design, to meet specific goals not related to payments. For example:       Requestor escrow for tasks where a higher level of commitment is required (high price because of specialized hardware or long running subtasks); the requestor may create a two-party escrow account and require providers to take part in it. Provider deposit: the requestor may require to be in control of some amount of timelocked GNT. Requestor deposit: the provider may accept tasks ​only from requestors who are in control of some amount of timelocked GNT. Registration of a task as an anchor for a TrueBit-style conflict resolution fallback mechanism.  Resilience Golem is a fully decentralized, open source P2P network which is resilient to censorship, and free from a single point-of-failure. Consensus is important for resilience and is another reason why Ethereum is utilized for transactions, as well as replicating some shared state and metadata. Golem tasks rely on integrity and consensus mechanisms for deployment, task execution, validation, and transactions. Naturally, Golem inherits Byzantine fault tolerance properties from Ethereum. For task execution and validation, Golem will initially rely on redundant validation and Ethereum state. As research and development progresses, optimizations will be implemented to reduce costs, increase throughput, and improve resilience. Optimisations will result in improvements of integrity and consensus validation schemes, P2P network formation, asynchronous transactions, and off-chain state transitions. The Golem P2P network will be built on a continuation of the devp2p protocol series, which was spearheaded at the Ethereum Foundation, and which will see improvements in confidentiality, robustness, latency, modularity, and the inclusion of pertinent libp2p and IPFS standards. Additional means to achieve resilience:           Signed and encrypted messages inside the Golem network ensure authenticity, which protects against man-in-the-middle attacks and passive data collection. Computation takes place in isolated environments with minimal privileges and lack of external network connectivity. Whitelist and blacklist mechanisms allow providers to build trust networks and run only applications prepared by trusted developers. Reputation system helps to detect malicious node and mitigate them and additionally evaluates metrics to assist in secure, efficient and correct task routing. Together, the application registry and transaction framework mitigates Sybil and whitewashing attacks by providing an incentive to participate, introducing an economic and computational cost to participation, and providing a metric for reputation in order to maintain optimal connectivity. The Ethereum integration and ​transaction framework makes possible custom payment-based security mechanisms, eg. escrows, deposits, insurance and audit proofs. Security audits will be conducted for every release, performed by external contractors.  Roadmap In this section we present planned milestones for Golem development. You will find a nontechnical description of Golem’s architecture ​in this post on our blog, some thoughts about challenges ahead ​here​ and of course you can examine the code on ​GitHub​. The successive versions of Golem’s software are split into milestones. This plan should be considered preliminary, as Golem is using bleeding-edge technologies, and is itself a very complex project. Each milestone should be preceded with research, with results described in technical whitepapers. At every stage new functionalities are added, but the scope of deliverables at every milestone depends on the level of funding raised. In the description of milestones presented below, these functionalities have been assigned to four indicative funding scenarios: (1) are implemented regardless of the level of funding, while with (2), (3) and (4) will be implemented if the adequate​ level of funding​ is reached. We have codenamed concurrent versions of Golem using descriptions from ​the long list of D&D’s golems​. The analogs might not be perfect, but these are e-golems after all.  Go-to-Market Strategy For ​every milestone there is a ​corresponding Go-to-Market Strategy (GMS). Each of them is focused on value propositions developed in respective milestones and specific needs of Golem’s ecosystem: software developers, requestors and providers.   In the long run, software developers are pivotal elements of the GMS, especially starting from the Stone release onwards​. With Golem architecture being mature enough to allow developers to integrate their own solutions, further growth of the entire ecosystem will depend on innovative and widely-adopted software running on Golem. Apart from creating state-of-the art technical infrastructure for such third-party integrations, the GMS include appropriate marketing activities and incentives.    Requestors are the main focus of the GMS in Brass and Clay releases​. The efforts will include facilitating integrations of widely-needed and commercially viable use cases, combined with dedicated marketing activities and eradicating barriers for potential requestors to join the network.    Providers are most likely to respond to economic incentives offered by Golem and simply join the network in order to earn tokens in exchange for their computing resources​. Nevertheless, the GMS assume communication with this target group, so that at every stage the demand is matched with sufficient supply.  Note: ​Each of the GMS assumes the full level of funding (i.e. the crowdfunding cap is reached). In other scenarios, the GMS will be adjusted to deliverables of a particular funding level.  Brass Golem ...these are created to fulfill one goal, set at the time of their creation, and wait with absolute patience until activated to perform this task. Brass Golem is where we are at the moment with our proof-of-concept, in alpha testing now. This current version of Golem is only focused on rendering in Blender and LuxRender, and although it will be useful to CGI artists, we consider CGI rendering to be a proof of concept primarily, and also a training ground. Brass Golem should be frozen within 6 months after end of crowdfunding period and a full battery of tests. Even though we do not expect that Blender CGI rendering will create enough turnover to justify all the work we have put into the project, this will be the first decentralised compute market. List of proposed functionalities:  (1) Basic Task Definition Scheme that allows to prepare first task definition;  (1) Basic Application Registry - first version of Ethereum-based Application Registry which allows to save tasks defined with basic task definition scheme;  (1) ​IPFS integration for coordinating task data and content delivery, e.g. deliver files needed to compute a task, deliver the results back to the requester;  (1) Docker environment with Golem-provided images for sandboxing the computations;  (1) Local verification: a probabilistic verification system based on the calculation of a fraction of the task on the requestor’s machine;  (1) Basic UI and CLI;  (1) Basic reputation system;  (1) Implementation of ​Blender​ and ​LuxRender​ tasks. Brass Golem: Go-to-Market Strategy Objectives​: 1. Create user-base of requestors​ by attracting customers interested in applying Golem in CGI rendering using supported open-source software. [target group: ​requestors​] 2. Attract a matching supply of computing power​. [target group: ​providers​] Value proposition​:  CGI rendering solution based on LuxRenderer/Blender, which is cheaper, more reliable and offer user experience superior to existing cloud-based solutions, notably render farms. requestors CGI artists and other users of open source rendering software:  Targeted marketing: artists’ portals, social media etc.  Incentives for initial purchases of computing resources in Golem (subsidies, vouchers, etc.).  providers  software developers  First pool of providers: Golem community, alpha/beta testers and first requestors (prosumer model). Targeted marketing: owners of data centers, mining rigs etc. (if needed) Broader marketing aimed  Software developers not targeted in this release.    Referral schemes.  at users of high-end PCs.  Clay Golem There is a chance (...) that a Clay Golem will be possessed by a chaotic evil spirit. If this happens control is lost and the Golem attacks the closest living creature. Clay Golem is a big leap from the Brass milestone. Clay milestone introduces the Task API and the Application Registry, which together are going to make Golem a multi-purpose, generalized distributed computation solution. Developers now have the means to integrate with Golem. This advance, however, may come at the cost of compromised stability and security, so this version should be considered an experiment for early adopters and tech enthusiasts. Prototype your new ideas and solutions on Clay. Clay Golem should be delivered within 15 months after end of crowdfunding period. List of proposed functionalities:            (1) Basic Task API: an interface that allows a user to define simple tasks; (1) Initial ​Transaction Framework Model​ with hard-coded payments schemes; (1) Redundant verification: a verification scheme based on the comparison of redundant computation results; (1) Basic subtask delegation: ​a mechanism for more fined grained subtasks distribution (e.g. can be used to help with creation of an ad-hoc proxy delegating tasks in a more efficient manner)​; (1) Basic tutorials for software developers; (2) Support for virtual machines as a sandbox for computation; (2) Set of extended tutorials for developers explaining how to implement their own tasks for Golem network; (2) Example computational chemistry use case implementation; (3) Example machine learning use case implementation.  Clay Golem: Go-to-Market Strategy Objectives​: 1. Broaden the user-base of requestors​ by extending the rendering use case (integration of commercial rendering engines) and implementing a new use cases (computational chemistry, machine learning). [target group: ​requestors​] 2. Stimulate first experimental independent integrations with Golem​. [target group: ​developers​] 3. Attract and retain a matching supply of computing power​. [target group: ​providers​] Value proposition​:  CGI rendering use case extended to popular commercial software solutions.  Additional use cases implemented by Golem team (computational chemistry, machine learning).    A basic rudimentary platform for software developers to implement first experimental integrations with Golem. requestors  CGI artists and other users of open source and commercial rendering software:  Targeted marketing: artists’ portals, social media etc.  Economic incentives for initial purchases of computing resources in Golem (subsidies, vouchers, ect.).  Referral schemes.  providers  software developers  Targeted marketing: owners of data centers, mining rigs etc.  Tutorials for software developers (experimental integrations with Golem API).  Broader marketing aimed at users of high-end PCs.  Small-scale communication with different communities of developers.  Similar means applied to attracting users for the computational chemistry and machine learning use cases.  Stone Golem Stone Golems do not revoke their creators control like (...) Clay Golems. Stone Golem will add more security and stability, but also enhance the functionalities implemented in Clay. An advanced version of the Task API will be introduced. The Application Registry will be complemented by the Certification Mechanism that will create a community-driven trust network for applications. Also, the Transaction Framework will create an environment that will allow Golem to be used in a SaaS model. Clay Golem should be delivered within 24 months after end of crowdfunding period. List of proposed functionalities:         (1) Full Task API: an interface that allows users to define tasks; (1) ​Application Registry​: where developers publish applications ready to run on Golem; (1) ​Transaction Framework that allows a choice of remuneration models for task templates; (1) Basic Certification support for Software: A mechanism that allows users to whitelist and blacklist applications, building a decentralized trust network; (1) Support for SaaS: the possibility to add support for proprietary software which can be used in tasks. Payments for task creators should also be implemented in the application; (1) Application Registry and Transaction Framework tutorials for developers;    (2) SaaS tasks examples - example use cases that shows developers how to create tasks available in SaaS model;  Stone Golem: Go-to-Market Strategy Objectives​: 1. Stimulate first wide-scale independent integrations with Golem​. [target group: ​developers​] 2. Retain and further broaden the user-base of requestors​ by facilitating marketing efforts of independent requestors and continuing promotion of existing use cases (rendering, computational chemistry, machine learning). [target group: ​requestors​] 3. Attract and retain a matching supply of computing power​. [target group: ​providers​] Value proposition​:  A much more advanced platform for developers, including a fully functional API, Application Registry and Transaction Framework as well as a basic version of the Certification mechanism and support for SaaS. requestors Customer retention and further acquisition: continuous development and marketing of rendering, computational chemistry, machine learning use cases. First wide-scale third-party integrations: independent marketing attracting additional groups of requestors, however Golem team will facilitate and coordinate these marketing efforts.  providers  Targeted marketing: owners of data centers, mining rigs etc. Broader marketing aimed at users of high-end PCs.  software developers Tutorials for software developers (Application Registry, Transaction Framework). Demonstration of SaaS implementations in Golem. Economic incentives for the most successful and innovative third-party integrations.  Iron Golem Iron Golems are made of iron and are among the strongest type of Golem. They never revoke the control of the wizard that created them. Iron is a deeply tested Golem that gives more freedom to developers, allowing them to create applications that use an Internet connection or applications that run outside the sandbox. Of course, the decision to accept higher-risk applications will still belong to the providers renting their compute power. Iron Golem should be robust, highly resistant to attacks, stable and scalable. Iron will also introduce various tools for developers that will make application creation far easier. Finally, the Golem Standard Library will be implemented. Assuming that maximum financing will be reached, Iron Golem should be delivered within 48 months after end of crowdfunding period. List of proposed functionalities:                    (2) External data link: enables Golem to use resources and interface with software outside of the Golem network; (2) Host-direct mode: a trusted mode for explicitly whitelisted applications or invulnerable environments, where Golem runs computation outside the Docker/VM; (2) Certification support for Environments; (2) Network Status Dashboard - public website displaying basic stats about Golem network; (2) Additional security mechanism - tasks that uses public data link or host-direct mode are particularly challenging for security. Additional means may be necessary to make running those tasks safer for providers (eg. central audit oracles, agreements contracts or code-execution observers may be implemented); (2) Golem web client: a web interface for Golem nodes as an alternative to the native GUI / console interface; (3) Golem Developer Toolkit: a set of diagnostic and test tools to make creation process of applications for Golem even easier; (3) Reputation-system: reputation protocol that allows the node to effectively supervise network behaviour; (3) Advanced transaction system: a system that automatically tries to match requestors with providers in a way that is most profitable to all participants; (3) Golem Developer Toolkit tutorial (3) Provider dashboard - providing stats, graphs and more advance settings management for providers; (4) devp2p integration - changes in p2p and network protocols using new version of devp2p; (4) MapReduce and topological sorting of tasks: add the next abstraction layer, allowing users to define more generic tasks that are interdependent; (4) Golem Standard Library (Golem STD): language agnostic functionality providing access to the low level core components required to interact with Golem from within a programming language. Special attention will be paid to I/O functions exposed to tasks and subtasks related functionalities. Each supported programming language will have bindings to Golem STD. These bindings will serve as a means of extending the default standard library of the language in question (custom extensions provided by developers of programming languages will also be possible). With Golem STD an automatic task definition, independent from the operating system, will be possible. Golem STD will allow users to create Golem applications using different programming languages, which shall significantly increase the number of potential use cases and simplify task creation process. (4) Golem STD tutorial for developers  Iron Golem: Go-to-Market Strategy Objectives​: 1. Stimulate successful and innovative integrations with Golem​. [target group: ​developers​] 2. Manage and facilitate the user-base of requestor​. [​providers​] 3. Attract and retain a matching supply of computing power​. [target group: ​providers​] Value proposition​:  A fully-functional for developers, including the API, Application Registry, Transaction Framework, support for SaaS, Golem Development Toolkit, devp2p as well as an array of functionalities to interact with Golem from with a programming language (Golem Standard Library). requestors  Customer retention and further acquisition: continuous development and marketing of existing use cases. Marketing of the entire Golem network, coordination of marketing efforts of independent software developers.  providers  Targeted marketing: owners of data centers, mining rigs etc. Broader marketing aimed at users of high-end PCs.  software developers Tutorials for software developers (Golem Development Toolkit, devp2p, Golem Standard Library). Targeted marketing: software developers from outside of the crypto-world (in order to attract ideas with the largest potential for wide-scale adoption).  Future integrations There are numerous Ethereum dapps and future platforms currently under development or in alpha release. We see great opportunities in this environment, not to mention solutions that could potentially be used as a part of Golem's ecosystem, either directly or as extensions. The following systems will be considered for integration and their implementation will be dependent upon the release of production code and complexity of integration:      Payment channel solutions based on P2P routing and transactions, eg. ​Raiden or multi-party payment channels​; External decentralized identity services, e.g. ​uPort​; External solutions for task verification or reputation, eg. ​TrueBit​; External solutions for storage, eg. FileCoin, Swarm​.  Crowdfunding The crowdfunding of Golem and the corresponding token creation process are organised around smart contracts running on Ethereum. Participants willing to support development of the Golem Project can do so by sending ether to the designated address. By doing so they create Golem Network Tokens (GNT) at the rate of 1 000 GNT per 1 ETH. A participant must send ether to the account after the start of the crowdfunding period (specified as the block number). Crowdfunding ends when the end block is created, or when the amount of ether sent to the account reaches the maximum.  Crowdfunding summary GNT created per 1 ether  1 000 GNT  Minimum ether  150 000 ETH  Maximum ether  820 000 ETH  % of tokens generated to Golem team  6%  % of tokens generated to Golem Factory GmbH Approximate date of start (StartBlock)  12% 11th November 2016, 3:00 pm GMT  Approximate date of end (EndBlock)  2nd December 2016, 3:00 pm GMT  Maximum number of GNT generated  1 000 000 000 GNT  of which crowdfunding participants  820 000 000 GNT  of which Golem team and Golem Factory GmbH  180 000 000 GNT  The crowdfunding address will be announced at the crowdfunding start through the following channels:       Project webpage: ​golem.network Official Twitter: ​twitter.com/golemproject Official Slack: ​golemproject.slack.com​ (you can join ​here​) Official Blog: ​blog.golemproject.net Reddit: ​reddit.com/r/golemproject  Please, double-check the address before sending ETH. For security reasons, we advise to confirm the address using at least two different sources above.  On the project webpage, you will also find a detailed guide on how to participate in the crowdfunding using popular Ethereum wallets. Crowdfunding is implemented as a smart contract with a few simple parameters:  Golem Factory GmbH: controls the contract and the address to which gathered ether will be sent (implemented as a multisig address);  Percent of pre-allocated tokens is 18% (6% - Golem team, 12% - Golem Factory GmbH);  StartBlock, EndBlock: these block numbers indicate the start and the end of the crowdfunding process;  minTarget: minimum cap for this crowdfunding, crowdfunding fails if not reached;  maxCap: maximum cap for this crowdfunding, denominated in GNT;  GNT creation rate, denominated in ETH. The crowdfunding contract conforms to a few important rules:  Before the crowdfunding starts, no ether can be sent to the crowdfunding contract;  After the crowdfunding (either maxCap was reached or the crowdfunding deadline passed), no ether can be sent to the contract;  During the crowdfunding, participants simply send ether to the crowdfunding contract which results in GNT creation;  All created tokens are ​non-transferable​ during the crowdfunding;  Only after the crowdfunding period has ended: ○ Any user can initiate the transfer of ether to the specified address of Golem Factory GmbH; ○ The crowdfunding contract creates an 18% endowment of tokens such that crowdfunding participants’ tokens constitute 82% of supply, regardless of the level of funding; ○ The crowdfunding contract finalizes funding which results in an allocation of founders’ tokens and unlocking the created GNT. If minimum financing is not reached, then after the crowdfunding period, participants can claim their ethers back from the contract. Crowdfunding process leads to creation of GNT, a backbone token for the Golem network. GNT implementation follows widely adopted token implementation standards with two additional functionalities core to the crowdfunding process and future upgrades, namely, token creation and token migration:     Create token - during the crowdfunding process, the crowdfunding contract can issue new GNT (based on the amount of sent ETH). ○ By default, created GNT is locked (nontransferable). Only when crowdfunding is finalized are tokens unlocked and participants able to transfer them. ○ The creation function is enabled only during the crowdfunding phase; it does not allow the creation of tokens after the crowdfunding phase is over (token supply is constant ever after). Migrate token - a function which implements GNT migration to another contract.  ○  ○  ○  Does nothing by default, but if at some point a GNT upgrade is required, a separate migration contract can be specified and recommended by Golem Factory GmbH to be used to transfer tokens to the new contract. Technically speaking, if a GNT upgrade is required, a contract implementing the ​MigrationAgent interface is created and set by Golem Factory GmbH in the GNT contract (for security reasons this can be done only once). Following that, each GNT holder can decide whether to call ​MigrationAgent.migrateTokens to transfer GNT to the new contract, or not. MigrationAgent can be implemented only after the new token is implemented and deployed. That’s why only an interface is provided right now.  Migration is to be used if it turns out at some point that token upgrade is needed for whatever reason (e.g. changes in Ethereum, or changes in Golem’s design). The upgrade will need action from token holders and cannot be imposed by Golem Factory GmbH.  Budget and levels of funding The ether raised during crowdfunding will be used by Golem Factory GmbH in accordance with the roadmap ​presented above​. Crowdfunding code implies that level of project financing might be anything between minimum financing and the maximum financing (cap). The roadmap is a full vision to be completed if the cap is reached. Golem should be considered an R&D project involving bleeding-edge technologies. The progress we have already made while working on the Brass Golem alpha proves the validity of our general assumptions presented in this whitepaper, but we are also well aware of the huge amount of work ahead. The commitment of Golem team with respect to the technologies presented in this whitepaper is full, but still ultimately depends on the level of success of the crowdfunding. In the 'minimum financing' scenario, the ultimate deliverable is a working Stone Golem with functionality enabling the creation of a decentralized market for computing power, as well as a rudimentary toolbox for developers to integrate their own software with Golem. In particular, the minimum financing will be sufficient to introduce a basic version of both the Application Registry and the Transaction Framework. In the 'maximum financing' scenario, we are making a commitment to deliver Iron Golem, which is not only gunning for total disruption of the market for computing power, but also dives head-first into the development of some important components of Web 3.0. In particular, this level of financing will make it possible to create a flexible platform to distribute and monetize innovative software solutions, notably dapps and microservices. Assuming higher thresholds of funding are reached, the Golem team commits to create integrations useful to the entire community in earlier releases as well to provide software developers with more support in integrating their solutions with Golem and bringing them into the market. The extended marketing effort outlined in the Go-to-Market Strategy is one of the crucial factors behind Golem’s ultimate commercial success, but it will not be possible to put it into action without sufficient financial resources.  Functionality vs funding Min financing (1)    Core Functionality          [B] Basic Task Definition Scheme [B] Basic Application Registry [B] Local verification [C] Basic Task API [C] Redundant verification [C] Subtask delegation [S] Full Task API [S] Application Registry [S] Certification support for Software  additional features (2, 3, 4)           [I] Certification support for Environments (2) [I] External data-link (2) [I] Host-direct mode (2) [I] Additional security mechanism (2) [I] MapReduce (4) [I] Topological sorting (4)    [I] Additional security mechanism for host-direct mode (2) [I] Reputation system (3)    [I] Advanced transaction system (3)    [C] Initial Transaction Framework Model [S] Transaction Framework  Integrations     [B] IPFS [B] Docker as sandbox     [C] Virtual machine as sandbox (2) [I] devp2p (4)  UX/UI    [B] Basic GUI and CLI      [I] Web client (2) [I] Provider dashboard (3) [I] Golem Developer Toolkit IDE (3)    [B] CGI Rendering (Blender, LuxRender)      [C] Computational chemistry use case (2) [C] Machine-learning use case (3) [S] Examples of SaaS integration (2)    [C] Enhance tutorials for task developers (2) [I] Golem Developer Toolkit (3) [I] Golem Developer Toolkit tutorial (3) [I] Golem Standard Library (Golem STD) (4) ○ [I] Golem STD integration with GDT (4) ○ [I] Golem STD support for programming languages (4) [I] Golem STD tutorial (4)  Reputation & security  Transaction system  Use cases      Tools for developers    [B] Basic reputation  [C] Basic tutorial for task developers [S] Application registry and Transaction Framework tutoria​l       Go-to-Market Strategy    [B, C, S] Essential elements of the Go-to-Market Strategy  Levels of funding (indicative thresholds):  (1) 150k ETH  (2) 320k ETH  (3) 530k ETH  (4) 820k ETH    [B, C, S, I] All elements of the Go-To-Market Strategy focused on value propositions of additional features and broader expansion.  Milestones:  [B] Brass Golem  [C] Clay Golem  [S] Stone Golem  [I] Iron Golem  Budget structure for maximum and minimum financing Golem team consists solely of employment costs. We assume that with maximum financing we will be able to finance team of 20 people (most of them developers) for a period of 4 years. Office and indirect costs includes costs of offices in both Zug and Warszawa, as well as other indirect, employment-related costs. Contractors ​covers all third parties we are willing to work with. The number here is high largely because of security audits (four in case of maximum financing: one for every release). Legal and accounting services are also included in this category. Marketing, community animation and expansion activities are strictly related to Golem’s Go-to-Market Strategy This includes both communication and marketing efforts to get new communities on board, as well as supporting (financing or co-financing) third party integrations with Golem. Activities here will be mostly requestor-oriented, especially in Brass and Clay releases, to ensure that there are a growing number of use cases integrated, with users actively using them on the Golem network. The ​Complementary technologies category covers expenditures on external technologies Golem is dependent on. This will most likely take the form of financing original efforts to introduce modifications needed by Golem. Contingency fund ​is calculated as 10% of the total budget (5% for minimum financing).  Golem Team Julian (CEO) and Andrzej (COO) have worked together on a variety of consulting projects and business endeavours since 2008. At the time, Julian was developing a consulting branch at IBS​, a Polish economic think-tank, where he served as vice president of the board between 2006 and 2012. In 2013, both Julian and Andrzej left IBS to create ​imapp​, a consulting and software development company. As a team, they complement each others’ competences very well: Julian might be described as an extraverted leader: passionate , ready to enlist in a Mars no-return-trip at the drop of a hat (if not for his beloved family and children). Andrzej on the other hand is more of the introverted perfectionist, paying attention to every detail and voicing criticisms loud-and-clear if he considers the business to be heading in the wrong direction. Together as imapp, they have proven over the last 10 years that they are able to run a profitable consulting and software development company that delivers a high-quality product. A brief history of imapp is described in this ​blog post​. Among the numerous consulting and software development projects we have completed over the last years, the following deserve special mention:        BlackVision is a real-time TV broadcasting rendering engine, used internally by BlackBurst, one of the leading Polish broadcasting production companies. You can see an example of its use ​here​. Real-time rendering proficiency was a good starting point for working with the photorealistic CGI rendering that we are implementing in Brass Golem. BlackVision is a work in progress and will become standalone product. We work as an IT contractor for GSI on ​Chematica​, which gives us great insight into how computing chemistry is done, why this is relevant to business, and how to perform tremendously demanding computing in a cloud environment. Since September 2014, we work for the Ethereum Foundation - now this is mostly Pawel’s work on the EVM. We are also a contractor to Omise and its Blockchain Lab, which gave us the opportunity to work with (and in some cases, contribute to) many exciting blockchain technologies, including ​Factom​, ​Hydrachain​, and ​Raiden​.  We are not a random collection of people, but a strong team with a proven track record of delivering. Most of us have worked together on many projects over the last couple of years (Andrzej, Julian, Piotr ‘Viggith’, Aleksandra, Paweł ‘chfast’, Radek). Alex and Wendell heard about the project long time ago, and were always ready to support us, but only recently have they begun working on the project as part of the team. And, to be able to ensure our delivery, we have enlisted over the last couple of months Marek, Pawel ‘pepesza’, Magdalena, and Adam. They have become Golem believers as well, fascinated as we are by the work ahead of us.  Managers Julian Zawistowski​, CEO, founder MA economics (Warsaw School of Economics) Leader and entrepreneur, willing to change the world for the better. His first attempt to do so was as an economist running a think-tank and policy analysts for government, but he at some point realized that technology is what can really make the difference. Since then, Julian has been running imapp, with an agenda to build software projects which will advance all of us beyond 140 characters. With this catchy agenda, he was able to gather top-class team around him, to complement skills he is definitely missing (such as software development).  Piotr ‘Viggith’ Janiuk​, CTO, co-founder MSc mathematics, MSc computer science​ (University of Warsaw) Piotr is an experienced computer programmer interested in bleeding edge technologies, and keen on bringing innovative, ground breaking (in the field) projects which bring these technologies to life. He worked on a secure P2P communicator when P2P was used primarily for file sharing, implemented highly optimized CUDA code just after the technology appeared, adding it to production-ready compositing software (the first in the world), implemented the fastest-at-the-time software DCP encoder, including a fast software jpeg2000 codec, assisted in optimization of particle renderer for The Witcher 2, and the father of Black Vision (see above). Piotr implemented core components of Golem prototype and leads the design of Golem protocol and its preliminary use cases.  Andrzej Regulski​, COO, co-founder MA economics (Warsaw School of Economics) Andrzej is an experienced manager and consultant. He devoted his professional life to running a successful consulting company focused on advising public authorities in their attempts to support innovative enterprises. With this experience, he is aware of most pitfalls, and the “dos” and “don’ts” which tech startups are often facing. He loves managing people who are smarter than him, and is obsessed with increasing the efficiency of organizations beyond the scope of imagination.  Key developers Aleksandra Skrzypczak​, Lead Software Engineer, co-founder MSc mathematics, BSc computer science ​(University of Warsaw) Hungry for new challenges, Aleksandra decided to join imapp more than two years ago, and was the first to work full-time on the Golem Project, doing R&D and generally planning the project. She's not easily disheartened by difficulties, and still knows every line of code in the Golem repository. Prior to joining imapp, Aleksandra was an intern at ICM, building training algorithms for automatic terrain type detection, and for detecting aptamers in DNA and RNA code. She was also an employee of Red Ocean, a software development and deployments group operating under the Ministry of Finance and Custom Service.  Alex Leverington​, P2P Network, Adviser Alex has been involved with Ethereum from a very early stage. He famously worked at ETHDEV, where he architected and coded devp2p, Ethereum's underlying P2P protocol layer. Alex also made key contributions in Ethereum encryption and security, primarily touching the various communications protocols. He remains active in the protocol steering group, and is presently interested in developing a decentralized, secure, and authenticated P2P messaging system, which will be one of essential technologies in Golem.  Paweł ‘chfast’ Bylica​, Lead Ethereum Engineer MSc computer science Technology)  (Wroclaw University  of Science  and  Paweł is an experienced programmer who specializes in C++, but uses other languages as well, sometimes even designing new ones. Currently he is focused on ​Ethereum and its Ethereum Virtual Machine (EVM). Pawel is the creator of ​EVMJIT ​project, and a still active member of the Ethereum C++ Team. He is also a contributor to the ​LLVM​ and ​C++ Guidelines Support Library​.  Developers Marek Franciszkiewicz​, developer MSc Electronics and telecommunication (Warsaw University of Technology) Marek touches all of Golem's internals, but mostly works on networking. He was previously the lead back-end developer of a few commercial web services, where he built and handled daemons, and he also left his mark on Temptonik's real-time vocal assessment software. During his university years, Marek worked on orchestration protocols for wireless sensor networks. For his Engineer's thesis, he developed an IP packet mangling engine with a focus on network steganography.  Paweł ‘pepesza’ Peregud​, developer An experienced programmer, crypto and security enthusiast, and console warrior, Paweł’s largest deployment so far was a system handling 1.5 millions of concurrent websockets with 99.997% uptime, since 2012. A gentleman who sometimes can’t help to brag about property-testing distributed protocols.  Adam Banasiak​, developer MSc cryptology (Military University of Technology) This bonafide Linux and security enthusiast joined the Golem team to find new challenges and to work with newest technologies. In the past, he was a developer in Samsung R&D, responsible for the Tizen platform (mobile, wearable and TV profiles). Adam is also the winner of the Polish finals of the Imagine Cup 2013, game category.  Magdalena Stasiewicz,​ developer As a student of computer science, astronomy and physics at the University of Warsaw, Magda fit right in, and now has almost one and a half years of experience with imapp during which she was primarily working on Golem. Besides programming, Magda also helps our testers with technical problems of various severity. She loves to learn new things, and one of the main reasons she is excited about Golem, is because there is ​tons of research to be done.  Radosław Zagórowicz​, developer MSc mathematics​ ​ (University of Warsaw) With 9 years of an experience as a C++/Java/Python programmer, Radek is head of imapp’s IT section, and led development of the Chematica project. Along with Julian and Piotr, Radek is one of the three who came up with the original idea of Golem more than two years ago. After long day and night of discussions he started implementing the first prototype of a Golem client.  Business development and communication Wendell Davis, BDM As a technology visionary and founder/entrepreneur both in and out of the blockchain space, Wendell serves an important role in Golem as connective tissue between Ethereum and the larger movement of decentralization. Some of his past endeavors include Hive, one of the earliest easy-to-use Bitcoin wallets, Vizor, an open source WebVR development platform, Splice, an online music-making/mashup community, and the now-sadly-defunct Kitchensurfing, which connected mobile chefs with buyers of their trade.  Hashcash - A Denial of Service Counter-Measure Adam Back e-mail: adam@cypherspace.org 1st August 2002 Abstract Hashcash was originally proposed as a mechanism to throttle systematic abuse of un-metered internet resources such as email, and anonymous remailers in May 1997. Five years on, this paper captures in one place the various applications, improvements suggested and related subsequent publications, and describes initial experience from experiments using hashcash. The hashcash CPU cost-function computes a token which can be used as a proof-of-work. Interactive and noninteractive variants of cost-functions can be constructed which can be used in situations where the server can issue a challenge (connection oriented interactive protocol), and where it can not (where the communication is store–and– forward, or packet oriented) respectively.  Key Words: hashcash, cost-functions  1 Introduction Hashcash [1] was originally proposed as a mechanism to throttle systematic abuse of un-metered internet resources such as email, and anonymous remailers in May 1997. Five years on, this paper captures in one place the various applications, improvements suggested and related subsequent publications, and describes initial experience from experiments using hashcash. The hashcash CPU cost-function computes a token which can be used as a proof-of-work. Interactive and noninteractive variants of cost-functions can be constructed which can be used in situations where the server can issue a challenge (connection oriented interactive protocol), and where it can not (where the communication is store–and– forward, or packet oriented) respectively. At the time of publication of [1] the author was not aware of the prior work by Dwork and Naor in [2] who proposed a CPU pricing function for the application of combatting junk email. Subsequently applications for costfunctions have been further discussed by Juels and Brainard in [3]. Jakobsson and Juels propose a dual purpose for the work spent in a cost-function: to in addition perform an otherwise useful computation in [4].  2 Cost-Functions A cost-function should be efficiently verifiable, but parameterisably expensive to compute. We use the following notation to define a cost-function. In the context of cost-functions we use client to refer to the user who must compute a token (denoted ) using a cost-function MINT() which is used to create tokens to participate in a protocol with a server. We use the term mint for the cost-function because of the analogy between creating cost tokens and minting physical money. The server will check the value of the token using an evaluation function VALUE(), and only proceed with the protocol if the token has the required value. The functions are parameterised by the amount of work  that the user will have to expend on average to mint a token. With interactive cost-functions, the server issues a challenge  to the client – the server uses the CHAL  function to compute the challenge. (The challenge function is also parameterised by the work factor.) 1    CHAL   MINT  VALUE          server challenge function mint token based on challenge token evaluation function  With non-interactive cost-functions the client choses it’s own challenge or random start value in the MINT() function, and there is no CHAL() function.    MINT  VALUE    mint token token evaluation function  Clearly a non-interactive cost-function can be used in an interactive setting, whereas the converse is not possible.  2.1 Publicly Auditable, Probabilistic Cost     A publicly auditable cost-function can be efficiently verified by any third party without access to any trapdoor or secret information. (When we say publicly auditable we mean implicitly that the cost-function is efficiently publicly auditable compared to the cost of minting the token, rather than auditable in the weaker sense that the auditor could repeat the work done by the client.)    A fixed cost cost-function takes a fixed amount of resources to compute. The fastest algorithm to mint a fixed cost token is a deterministic algorithm. A probabilistic cost cost-function is one where the cost to the client of minting a token has a predictable expected time, but a random actual time as the client can most efficiently compute the cost-function by starting at a random start value. Sometimes the client will get lucky and start close to the solution. There are two types of probabilistic cost bounded probabilistic cost and unbounded probabilistic cost. – An unbounded probabilistic cost cost-function, can in theory take forever to compute, though the probablity of taking significantly longer than expected decreases rapidly towards zero. (An example would be the cost-function of being required to throw a head with a fair coin; in theory the user could be unlucky and end up throwing many tails, but in practice the probability of not throwing a head for  throws tends towards  rapidly as  "!#%&$  ('  .) – With a bounded probabilistic cost cost-function there is a limit to how unlucky the client can be in it’s search for the solution; for example where the client is expected to search some key space for a known solution; the size of the key space imposes an upper bound on the cost of finding the solution.  2.2 Trapdoor-free A disadvantage of known solution cost-functions is that the challenger can cheaply create tokens of arbitrary value. This precludes public audit where the server may have a conflict of interests, for example in web hit metering, where the server may have an interest to inflate the number of hits on it’s page where it is being paid per hit by an advertiser.   A trapdoor-free cost-function is one where the server has no advantage in minting tokens.  An example of a trapdoor-free cost-function is the Hashcash [1] cost-function. Juels and Brainard’s client-puzzle cost-function is an example of a known-solution cost-function where the server has an advantage in minting tokens. Client-puzzles as specified in the paper are in addition not publicly auditable, though this is due to a storage optimization and not inherent to their design.  2  3 The Hashcash cost-function Hashcash is a non-interactive, publicly auditable, trapdoor-free cost function with unbounded probabilistic cost. First we introduce some notation: consider bitstring '*) +-,/.10 , we define 2 354 to means the bit at offset i, where 2 3 is the left-most bit, and 2 376 896 is the right-most bit. 2 -34;:<:<: = means the bit-wise substring between and including bits > $ ' ' 2 -3 4A@BCBBD@ 2 -3 = . So 2 -3 :<:<: 6 896 . and ? , 2 -3 45:<:<: = $  We define a binary infix comparison operator M bit-strings. M  'LK EH F GJI  M  'PORQ EH F GNI '  E FHGNI  where b is the length of the common left-substring from the two  K Q  2  ' 3  M Q  2  $TS K U C4 V :<:<: $  3 2  $ 34  ' 2  Q  34  Hashcash is computed relative to a service-name , to prevent tokens minted for one server being used on another (servers only accept tokens minted using their own service-name). The service-name can be any bit-string which uniquely identifies the service (eg. host name, email address, etc). The hashcash function is defined as (note this is an improved simplifed variant since initial publication see note in section 5: WW WW  PUBLIC: WW  hash M function size XPY  with output []\ M  W  M    MINT Z WW WW WW W    VALUE    find M return X_ @  )   '_b  E FHGNI a  +-,/.^0  st X_ @  '_` F GNI  EH   bits         return c The hashcash cost-function is based on finding partial hash collisions on the all 0 bits  -bit string   . The fastest algorithm for computing partial collisions is brute force. There is no challenge as the client can safely choose his own random M challenge, and so the hashcash cost-function is a trapdoor-free and non-interactive cost-function. In addition the Hashcash cost-function is publicly auditable, because anyone can efficiently verify any published tokens. M (In practice d d should be chosen to be large enough to make the probability that clients reuse a previously used start ' ,%e/f bits should be enough even for a busy server.) value negligible; d d The server needs to keep a double spending database of spent tokens, to detect and reject attempts to spend the same token again. To prevent the database growing indefinitely, the service string can include the time at which it was minted. This allows the server to discard entries from the spent database after they have expired. Some reasonable expiry period should be chosen to take account of clock inaccuracy, computation time, and transmission delays. Hashcash was originally proposed as a counter-measure against email spam, and against systematic abuse of anonymous remailers. It is necessary to use non-interactive cost-functions for these scenarios as there is no channel for the server to send a challenge over. However one advantage of interactive cost-functions is that it is possible to prevent pre-computation attacks. For example, if there is a cost associated with sending each email this may be sufficient to limit the scale of email abuse perpetrated by spammers; however for a pure DoS-motivated attack a determined adversary may spend a year pre-computing tokens to all be valid on the same day, and on that day be able to temporarily overload the system. It would be possible to reduce the scope for such pre-computation attacks by using a slowly changing beacon (unpredictable broadcast authenticated value changing over time) such as say this weeks winning lottery numbers. In this event the current beacon value is included in the start string, limiting pre-computation attacks to being conducted within the time period between beacon value changes.  4 Interactive Hashcash With the interactive form of hashcash, for use in interactive settings such as TCP, TLS, SSH, IPSEC etc connection establishment a challenge is chosen by the server. The aim of interactive hashcash is to defend server resources from premature depletion, and provide graceful degradation of service with fair allocation across users in the face of a DoS attack where one user attempts to deny service to the other users by consuming as many server resources as he can. In 3  the case of security protocols such as TLS, SSH and IPSEC with computationally expensive connection establishment phases involving public key crypto the server resource being defended is the servers available CPU time. [i\ The interactive hashcash cost-function is defined as follows: WW WW  g WW  CHAL   choose h \ +-,j. M [] return  " 7h  M  k(  find return  W   MINT  WW WW WW  W  )    VALUE ;m  XP  )  M    +-,j.%0  M  st XP  @ h @  '_` lE H F GNI      '  @ h @    aE FHGJI  b     return c  4.1 Dynamic throttling With interactive hashcash it becomes possible to dynamically adjust the work factor required for the client based on server CPU load. The approach also admits the possibility that interactive hashcash challenge-response would only be used during periods of high load. This makes it possible to phase-in DoS resistent protocols without breaking backwards compatibility with old client software. Under periods of high load non-hashcash aware clients would be unable to connect, or would be placed in a limited connection pool subject to older less effective DoS counter-measures such as random connection dropping.  4.2 hashcash-cookies With connection-slot depletion attacks such as the syn-flood attack, and straight-forward TCP connection-slot depletion the server resource that is being consumed is space available to the TCP stack to store per-connection state. In this scenario it may be desirable to avoid keeping per connection state, until the client has computed a token with the interactive hashcash cost-function. This defense is similar to the syn-cookie defense to the syn-flood attack, but here we propose to additionally impose a CPU cost on the connecting machine to reserve a TCP connection-slot. To avoid storing the challenge in the connection state (which itself consumes space) the server may choose to compute a keyed MAC of the information it would otherwise store and sent it to the client as part of the challenge so it can verify the authenticity of the challenge and token when the client returns them. (This general technique – of sending a record you would otherwise store together with a MAC to the entity the information is about – is referred to as a symmetric key certificate.) This approach is analogous to the technique used in syn-cookies, and Juels and Brainard proposed a related approach but at the application protocol level in their client-puzzles paper. For example with MAC function n keyed by server key o the challenge MAC could be computed as: WW WW  PUBLIC: WW  g  MAC function [ \   WW  CHAL ;Z  choose h compute rs return ;t9w/ u  )  npYC^Y  q-,/.    np5o#t @  @vul@   @ h-  "hxrg  The client must send the MAC r , and the challenge h and challenge parameters u with the response token so that the server can verify the challenge and the response. The server should also include in the MAC the connection parameters, at minimum enough to identify the connection-slot and some time measurement or increasing counter t so that old challenge responses can not be collected and re-used after the connection-slots are free. The challenge and MAC would be sent in the TCP SYN-ACK response message, and the client would include the interactive hashcash token (challenge-response) in the TCP ACK message. As with syn-cookies, the server would not need to keep any state per connection prior to receiving the TCP ACK. For backwards compatibility with syn-cookie aware TCP stacks, a hashcash-cookie aware TCP stack would only turn on hashcash-cookies when it detected that it was subject to a TCP connection-depletion attack. Similar arguments as given by Dan Bernstein in [5] can be used to show that backwards compatibility is retained, namely under syn-flood attacks Bernstein’s arguments show how to provide backwards compatibility with non syn-cookie aware implementations; similarly under connection-depletion attack hashcash-cookies are only turned on at a point where service would anyway otherwise be unavailable to a non-hashcash-cookie aware TCP stack. 4  As the flood increases in severity the hashcash-cookie algorithm would increase the collision size required to be in the TCP ACK message. The hashcash-cookie aware client can still connect (albeit increasinly slowly) with a more fair chance against the DoS attacker presuming the DoSer has limited CPU resources. The DoS attacker will effectively be pitting his CPU against all the other (hashcash-cookie aware) clients also trying to connect. Without the hashcashcookie defense the DoSer can flood the server with connection establishments and can more easily tie up all it’s slots by completing n connections per idle connection time-out where n is the size of the connection table, or pinging the connections once per idle connection time-out to convince the server they are alive. Connections will be handed out to users collectively in rough proportion to their CPU resources, and so fairness is CPU resource based (presuming each user is trying to open as many connections as he can) so the result will be biased in favor of clients with fast processors as they can compute more interactive-hashcash challenge-response tokens per second.  5 Hashcash improvements In the initially published hashcash scheme, the target string to find a hash collision on was chosen fairly by using the hash of the service-name (and respectively the service-name and challenge in the interactive setting). A subsequent improvement suggested independently by Hal Finney [6] and Thomas Boschloo [7] for hashcash is to find a collision against a fixed output string. Their observation is that a fixed collision target is also fair, simpler and reduces verification cost by a factor of 2. A fixed target string which is convenient to compare trial collisions against is the k-bit string  where  is the hash output size.   6 Low Variance Ideally cost-function tokens should take a predictable amount of computing resources to compute. Juels and Brainard’s client-puzzle construction provides a probabilistic bounded-cost by issuing challenges with known-solutions, however while this limits the theoretical worst case running time, it makes limited practical difference to the variance and typical experienced running time. The technique of using known solutions is also not applicable to the non-interactive setting. It is an open question as to whether there exist probabilistic bounded-cost, or fixed-cost non-interactive cost-functions with the same order of magnitude of verification cost as hashcash. The other more significant incremental improvement due to Juels and Brainard is the suggestion to use multiple sub-puzzles with the same expected cost, but lower variance in cost. This technique should be applicable to both the non-interactive and interactive variants of hashcash.  6.1 Non-Parallelizability and Distributed DoS Roger Dingledine, Michael Freedman and David Molnar put forward the argument that non-parallelizable costfunctions are less vulnerable to Distributed DoS (DDoS) in chapter 16 of [8]. Their argument is that non-parallelizable cost-functions frustrate DDoS because the attacker is then unable sub-divide and farm out the work of computing an individual token. The author described a fixed-cost cost-function in [9] using Rivest, Shamir and Wagner’s time-lock puzzle [10] which also happens to be non-parallelizable. The time-lock puzzle cost-function can be used in either an interactive or non-interactive setting as it is safe for the user to chose their own challenge. The applicability of Rivest et al’s time-lock puzzle as a cost-function was also subsequently observed by Dingledine et al in [8]. For completeness we present the time-lock puzzle based fixed-cost and non-parallelizable cost-function from [9] here:  5  WW WW  PUBLIC: PRIVATE: WW WW WW  y  '  u{z  primes u and [ \ z  A|l5y  '   u~}  ,  z}  ,%  WW WW WW  g WW    CHAL  MINT   WW WW WW WW WW  WW WW WWW W  VALUE    2 qy choose h M return hx M{% compute XP @ h- compute QM ; 1y return hx"M Q  compute XP M{ `  @ h- compute T 1|l;y 'Q if 5 y return  else return   The client does not know |a5y , and so the most efficient method for the client to calculate MINT() is repeated exponentiation, which requires  exponentiations. The challenger knows |l5y which allows a more efficient computation by reducing the exponent  1{|l5y , so the challenger can execute VALUE() with 2 modular exponentiations. The challenger as a side-effect has a trapdoor in computing the cost-function as he can compute MINT() efficiently using the same algorithm. We argue however that the added DDoS protection provided by non-parallelizable cost-functions is marginal: unless the server restricts the number of challenges it hands out to a recognizably unique client the DDoS attacker can farm out multiple challenges as easily as farm out a sub-divided single challenge, and consume resources on the server at the same rate as before. Further it is not that hard for a single client to masquerade as multiple clients to a server. Consider also: the DDoS attacker has generally due to the nature of his method of commandeering nodes an equal number of network connected nodes at his disposal as processors. He can therefore in any case have each attack node directly participate in the normal protocol indistinguisably from any legitimate user. This attack strategy is also otherwise optimal anyway as the attack nodes will present a varied set of source addresses which will foil attempts at per-connection fairness throttling strategies and router based DDoS counter-measures based on volume of traffic across IP address ranges. Therefore for the natural attack node marshalling patterns non-parallelizable cost-functions offer limited added resistance. As well as the arguments against the practical efficacy and value of non-parallelizable cost-functions, to date non-parallelizable cost functions have had orders of magnitude slower verification functions than non-parallelizable cost-functions. This is because the non-parallelizable cost-functions so far discussed in the literature are related to trapdoor public key cryptography constructs which are inherently less efficient. It is an open question as to whether there exist non-parallelizable cost-functions based on symmetric-key (or public-key) constructs with verification functions of the same order of magnitude as those of symmetric-crypto based cost-functions. While for the application of time-lock puzzles to cost-functions, a reduced public key size could be used to speed up the verification function, this approach introduces risk that the modulus will be factored with the result that the attacker gains a big advantage in minting tokens. (Note: factoring is itself a largely parallelizable computation.) To combat this the server should change the public parameters periodically. However in the particular case of the public parameters used by time-lock puzzles (which are the same as the RSA modulus used in RSA encryption), this operation is itself moderately expensive, so this operation would not be performed too frequently. It would probably not be wise to deploy software based on key sizes below 768 bits for this aplication, in addition it would help to change keys periodically, say every hour or so. (RSA modulii of 512 bits have recently been factored by a closed group as discussed in [11] and more recently have been demonstrated by Nicko van Someren et al to be factorizable using standard equipment in an office as reported in [12]; DDoS attackers are known be able to muster significant resources, probably easily exceeding those used in this demonstration.) The time-lock puzzle cost-function also is necessarily trap-door as the server needs a private verification-key to allow it to efficiently verify tokens. The existance of a verification-key presents the added risk of key compromise allowing the attacker to by-pass the cost-function protection. (The interactive hashcash cost-function by comparison is trap-door-free, so there is no key which would allow an attacker a short-cut in computing tokens). In fact if the verification-key were compromised, it could be replaced, but this need adds complexity and administrative overhead as this event needs to be detected and manual intervention or some automated detection triggering key-replacement implemented. 6  The time-lock puzzle cost-function also will tend to have larger messages as there is a need to communicate planned and emergency re-keyed public parameters. For some applications, for example the syn-cookie and hashcashcookie protocols, space is at a premium due to backwards compatibility and packet size constraints imposed by the network infrastructure. So in summary we argue that non-parallelizable cost-functions are of questionable practical value in protecting against DDoS attacks, have more expensive verification functions, incur the risk of verification key compromise and attendant key management complexities, have larger messages, and are significantly more complex to implement. We therefore recommend instead the simpler hashcash protocol (or if the public-auditability and non-interactive options are not required Juels and Brainard’s client-puzzles are roughly equivalent).  7 Applications Apart from the initially proposed applications for hashcash of throttling DoS against remailer networks and detering email spam, since publication the following applications have been discussed, explored and in some cases implemented and deployed:     hashcash-cookies, a potential extension of the syn-cookie as discussed in section 4.2 for allowing more graceful service degradation in the face of connection-depletion attacks.    interactive-hashcash as discussed in section 4 for DoS throttling and graceful service degradation under CPU overload attacks on security protocols with computationally expensive connection establishment phases. No deployment but the analogous client-puzzle system was implemented with TLS in [13]    hashcash throttling of DoS publication floods in anonymous publication systems such as Freenet [14], Publius [15], Tangler [16],    hashcash throttling of service requests in the cryptographic Self-certifying File System [17]    hashcash throttling of USENET flooding via mail2news networks [18] hashcash as a minting mechanism for Wei Dai’s b-money electronic cash proposal, an electronic cash scheme without a banking interface [19]  8 Cost-function classification scheme We list here a classification of characteristics of cost-functions. We use the following notation to denote the properties of a cost-function: 2   ')  ,/ & $ 1.^3 %2   ')  q & $ -,/.-3H^2  '  ) >  vH.^3 %2  )     .-3H^2  )   .^3 %2 ) u u .^3 t9 t7  Where  is the efficiency: value  , means efficiently-verifiable – verifiable with cost comparable to or lower than the cost of verifying symmetric key constructs such as hashcash which consume just a single compression round of an iterative compression function based hash function such as SHA1 or MD5. Value  ' &$ means practically-verifiable we mean less efficiently than efficienty-verifiable, but still efficient enough to be practical for some applications, for example the author considers the time-lock puzzle based cost-function with it’s two modular exponentiations to fall into this category. Value  '  means verifiable but impractical, that the cost-function is verifiable but the verification function is impractically slow such that the existance of the cost-function serves only as a proof of concept to be improved upon for practical use. ' ' & $ means bounded  means fixed-cost,  And  is a characterization of the standard-deviation, value  ' , means unbounded probabilistic cost. Note by bounded probabilistic-cost we mean probabilistic cost and  usefully bounded – a bound in the work factor in excess of a work-factor that an otherwise functionally similar unbounded cost-function would only reach with negligible probability would not be useful. > And denotes that the cost-function is interactive, and  that the cost-function is non-interactive.   And denotes that the cost-function is publicly auditable,  denotes that the cost-function is not publicly auditable, which means in practice that it is only verifiable by the service using a private key material. Note by public-auditability 7  we mean efficiently publicly-auditable, and would not consider repeating the work of the token minter as adequate efficiency to classify. And t denotes that the server has a trapdoor in computing the cost-function, conversely t  denotes that server has no trapdoor in computing the cost-function. And u denotes that the cost-function is parallelizable, u  deontes that the cost-function is non-parallelizable.  interactive 5  '  trapdoor-free hashcash ,/7  '  ,/  >     u  tw  5  '  hashcash ,  '  ,vv    '  5 5  non-interactive  trapdoor client-puzzles ,  '  >  &$    t9 u   '  time-lock & $   '  q  >   - t9/u    u  t9   '  time-lock & $ 7  '   qv- t9u   8.1 Open Problems   '    existance of efficiently-verifiable non-interactive fixed-cost cost-functions 5 ' ,/7 weaker problem: existance of same with probabilistic bounded-cost 5 '  & $    existance of efficiently-verifiable non-interactive non-parallelizable cost-functions > ' ,/ u  ) weaker problem: existance of same in interactive setting 5   existance of publicly-auditable non-interactive fixed-cost cost-functions 5 problem: existance of same with bounded probabilistic-cost  ' &$ vv   )  8  '  '  ,  '  5  +vv    +vH  (and the related  ,/vu   (and the related  )    (and the related weaker  References [1] Adam Back. Hashcash, May 1997. Published at http://www.cypherspace.org/hashcash/. [2] Cynthia Dwork and Moni Naor. Pricing via processing or combatting junk mail. In Proceedings of Crypto, 1992. Also available as http://www.wisdom.weizmann.ac.il:81/Dienst/UI/2.0/Describe/ ncstrl.weizmann_il/CS95-20. [3] Ari Juels and John Brainard. Client puzzles: A cryptographic countermeasure against connection depletion attacks. In Network and Distributed System Security Symposium, 1999. Also available as http://www. rsasecurity.com/rsalabs/staff/bios/ajuels/publications/client-puzzles/. [4] Markus Jakobsson and Ari Juels. Proofs of work and bread pudding protocols. In Proceedings of the IFIP TC6 and TC11 Joint Working Conference on Communications and Multimedia Security (CMS ’99), Leuven, Belgium, September 1999. Also available as http://citeseer.nj.nec.com/238810.html. [5] Dan Bernstein. Syn cookies. Published at http://cr.yp.to/syncookies.html. [6] Hal Finney. Personal communication, Mar 2002. [7] Thomas Boschloo. Personal communication, Mar 2002. [8] Andy Oram, editor. Peer-to-Peer: Harnessing the Power of Disruptive Technologies. O’Reilly and Associates, 2001. Chapter 16 also available as http://freehaven.net/doc/oreilly/ accountability-ch16.html. [9] Adam Back. Hashcash - amortizable publicly auditable cost functions. Early draft of paper, 2000. [10] Ronald L Rivest, Adi Shamir, and David A Wagner. Time-lock puzzles and timed-release crypto. Technical Report MIT/LCS/TR-684, 1996. Also available as http://theory.lcs.mit.edu/˜rivest/ publications.html. [11] Herman te Riele. Security of e-commerce threatened by 512-bit number factorization. Published at http: //www.cwi.nl/˜kik/persb-UK.html, Aug 1999. [12] Dennis Fisher. Experts debate risks to crypto, Mar 2002. Also available as http://www.eweek.com/ article/0,3658,s=720&a=24663,00.asp. [13] Drew Dean and Adam Stubblefield. Using cleint puzzles to protect tls. In Proceedings of the 10th USENIX Security Symposium, Aug 2001. Also available as http://www.cs.rice.edu/˜astubble/papers. html. [14] Ian Clarke, Oskar Sandberg, Brandon Wiley, and Theodore Hong. Freenet: A distributed anonymous information storage and retrieval system. In Hannes Federrath, editor, Proceedings of the International Workshop on Design Issues in Anonymity and Unobservability. Springer, 2001. Also available as http://freenetproject. org/cgi-bin/twiki/view/Main/Papers. [15] Marc Waldman, Aviel D Rubin, and Lorrie Faith Cranor. Publius: A robust, tamper-evident, censorshipresistant web publishing system. In Proceedings of the 9th USENIX Security Symposium, Aug 2000. Also available as http://www.usenix.org/publications/library/proceedings/sec2000/ waldman/waldman_html/v2.html. [16] Marc Waldman and David Mazieres. Tangler: A censorship resistant publishing system based on document entanglement. In Proceedings of the 8th ACM Conference on Computer and Communication Security, Nov 2001. Also available as http://www.cs.nyu.edu/˜waldman/. [17] David Mazieres. Self-certifying File System. PhD thesis, Massachusetts Institute of Technology, May 2000. Also available as http://scs.cs.nyu.edu/˜dm/.  9  [18] Alex de Joode. Hashcash support at dizum mail2news gateway. Published at https://ssl.dizum.com/ hashcash/, 2002. [19] Wei Dai. b-money. Published at http://www.eskimo.com/˜weidai/bmoney.txt, Nov 1998.  10  HotStuff: BFT Consensus with Linearity and Responsiveness Maofan Yin  Dahlia Malkhi  Michael K. Reiter  Guy Golan Gueta  Ittai Abraham  Cornell University VMware Research  VMware Research  UNC-Chapel Hill VMware Research  VMware Research  VMware Research  ABSTRACT We present HotStuff, a leader-based Byzantine fault-tolerant replication protocol for the partially synchronous model. Once network communication becomes synchronous, HotStuff enables a correct leader to drive the protocol to consensus at the pace of actual (vs. maximum) network delay—a property called responsiveness—and with communication complexity that is linear in the number of replicas. To our knowledge, HotStuff is the first partially synchronous BFT replication protocol exhibiting these combined properties. Its simplicity enables it to be further pipelined and simplified into a practical, concise protocol for building large-scale replication services.  CCS CONCEPTS • Software and its engineering → Software fault tolerance; • Security and privacy → Distributed systems security.  KEYWORDS Byzantine fault tolerance; consensus; responsiveness; scalability; blockchain ACM Reference Format: Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham. 2019. HotStuff: BFT Consensus with Linearity and Responsiveness. In 2019 ACM Symposium on Principles of Distributed Computing (PODC ’19), July 29-August 2, 2019, Toronto, ON, Canada. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3293611.3331591  1  INTRODUCTION  Byzantine fault tolerance (BFT) refers to the ability of a computing system to endure arbitrary (i.e., Byzantine) failures of its components while taking actions critical to the system’s operation. In the context of state machine replication (SMR) [35, 47], the system as a whole provides a replicated service whose state is mirrored across n deterministic replicas. A BFT SMR protocol is used to ensure that non-faulty replicas agree on an order of execution for client-initiated service commands, despite the efforts of f Byzantine replicas. This, in turn, ensures that the n − f non-faulty replicas will run commands identically and so produce the same response for each command. As is common, we are concerned here with the partially synchronous communication model [25], whereby a known bound ∆ on message transmission holds after some unknown global Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6217-7/19/07. https://doi.org/10.1145/3293611.3331591  stabilization time (GST). In this model, n ≥ 3f + 1 is required for non-faulty replicas to agree on the same commands in the same order (e.g., [12]) and progress can be ensured deterministically only after GST [27]. When BFT SMR protocols were originally conceived, a typical target system size was n = 4 or n = 7, deployed on a local-area network. However, the renewed interest in Byzantine fault-tolerance brought about by its application to blockchains now demands solutions that can scale to much larger n. In contrast to permissionless blockchains such as the one that supports Bitcoin, for example, so-called permissioned blockchains involve a fixed set of replicas that collectively maintain an ordered ledger of commands or, in other words, that support SMR. Despite their permissioned nature, numbers of replicas in the hundreds or even thousands are envisioned (e.g., [30, 42]). Additionally, their deployment to wide-area networks requires setting ∆ to accommodate higher variability in communication delays. The scaling challenge. Since the introduction of PBFT [20], the first practical BFT replication solution in the partial synchrony model, numerous BFT solutions were built around its core twophase paradigm. The practical aspect is that a stable leader can drive a consensus decision in just two rounds of message exchanges. The first phase guarantees proposal uniqueness through the formation of a quorum certificate (QC) consisting of (n − f ) votes. The second phase guarantees that the next leader can convince replicas to vote for a safe proposal. The algorithm for a new leader to collect information and propose it to replicas—called a view-change—is the epicenter of replication. Unfortunately, view-change based on the two-phase paradigm is far from simple [38], is bug-prone [4], and incurs a significant communication penalty for even moderate system sizes. It requires the new leader to relay information from (n − f ) replicas, each reporting its own highest known QC. Even counting just authenticators (digital signatures or message authentication codes), conveying a new proposal has a communication footprint of O(n 3 ) authenticators in PBFT, and variants that combine multiple authenticators into one via threshold digital signatures (e.g., [18, 30]) still send O(n2 ) authenticators. The total number of authenticators transmitted if O(n) view-changes occur before a single consensus decision is reached is O(n 4 ) in PBFT, and even with threshold signatures is O(n3 ). This scaling challenge plagues not only PBFT, but many other protocols developed since then, e.g., Prime [9], Zyzzyva [34], Upright [22], BFT-SMaRt [13], 700BFT [11], and SBFT [30]. HotStuff revolves around a three-phase core, allowing a new leader to simply pick the highest QC it knows of. It introduces a second phase that allows replicas to “change their mind” after voting in the phase, without requiring a leader proof at all. This alleviates the above complexity, and at the same time considerably simplifies the leader replacement protocol. Last, having (almost)  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham  canonized all the phases, it is very easy to pipeline HotStuff, and to frequently rotate leaders. To our knowledge, only BFT protocols in the blockchain arena like Tendermint [15, 16] and Casper [17] follow such a simple leader regime. However, these systems are built around a synchronous core, wherein proposals are made in pre-determined intervals that must accommodate the worst-case time it takes to propagate messages over a wide-area peer-to-peer gossip network. In doing so, they forego a hallmark of most practical BFT SMR solutions (including those listed above), namely optimistic responsiveness [42]. Informally, responsiveness requires that a non-faulty leader, once designated, can drive the protocol to consensus in time depending only on the actual message delays, independent of any known upper bound on message transmission delays [10]. More appropriate for our model is optimistic responsiveness, which requires responsiveness only in beneficial (and hopefully common) circumstances— here, after GST is reached. Optimistic or not, responsiveness is precluded with designs such as Tendermint/Casper. The crux of the difficulty is that there may exist an honest replica that has the highest QC, but the leader does not know about it. One can build scenarios where this prevents progress ad infinitum (see Section 4.4 for a detailed liveless scenario). Indeed, failing to incorporate necessary delays at crucial protocol steps can result in losing liveness outright, as has been reported in several existing deployments, e.g., see [2, 3, 19].  a Pacemaker, cleanly separated from the mechanisms needed for safety (Section 6).  Our contributions. To our knowledge, we present the first BFT SMR protocol, called HotStuff, to achieve the following two properties: • Linear View Change: After GST, any correct leader, once designated, sends only O(n) authenticators to drive a consensus decision. This includes the case where a leader is replaced. Consequently, communication costs to reach consensus after GST is O(n2 ) authenticators in the worst case of cascading leader failures. • Optimistic Responsiveness: After GST, any correct leader, once designated, needs to wait just for the first n − f responses to guarantee that it can create a proposal that will make progress. This includes the case where a leader is replaced. Another feature of HotStuff is that the costs for a new leader to drive the protocol to consensus is no greater than that for the current leader. As such, HotStuff supports frequent succession of leaders, which has been argued is useful in blockchain contexts for ensuring chain quality [28]. HotStuff achieves these properties by adding another phase to each view, a small price to latency in return for considerably simplifying the leader replacement protocol. This exchange incurs only the actual network delays, which are typically far smaller than ∆ in practice. As such, we expect this added latency to be much smaller than that incurred by previous protocols that forgo responsiveness to achieve linear view-change. Furthermore, throughput is not affected due to the efficient pipeline we introduce in Section 5. HotStuff has the additional benefit of being remarkably simple. Safety is specified via voting and commit rules over graphs of nodes. The mechanisms needed to achieve liveness are encapsulated within  2  RELATED WORK  Reaching consensus in face of Byzantine failures was formulated as the Byzantine Generals Problem by Lamport et al. [37], who also coined the term “Byzantine failures”. The first synchronous solution was given by Pease et al. [43], and later improved by Dolev and Strong [24]. The improved protocol has O(n 3 ) communication complexity, which was shown optimal by Dolev and Reischuk [23]. A leader-based synchronous protocol that uses randomness was given by Katz and Koo [32], showing an expected constant-round solution with (n − 1)/2 resilience. Meanwhile, in the asynchronous settings, Fischer et al. [27] showed that the problem is unsolvable deterministically in asynchronous setting in face of a single failure. Furthermore, an (n−1)/3 resilience bound for any asynchronous solution was proven by BenOr [12]. Two approaches were devised to circumvent the impossibility. One relies on randomness, initially shown by Ben-Or [12], using independently random coin flips by processes until they happen to converge to consensus. Later works used cryptographic methods to share an unpredictable coin and drive complexities down to constant expected rounds, and O(n 3 ) communication [18]. The second approach relies on partial synchrony, first shown by Dwork, Lynch, and Stockmeyer (DLS) [25]. This protocol preserves safety during asynchronous periods, and after the system becomes synchronous, DLS guarantees termination. Once synchrony is maintained, DLS incurs O(n 4 ) total communication and O(n) rounds per decision. State machine replication relies on consensus at its core to order client requests so that correct replicas execute them in this order. The recurring need for consensus in SMR led Lamport to devise Paxos [36], a protocol that operates an efficient pipeline in which a stable leader drives decisions with linear communication and one round-trip. A similar emphasis led Castro and Liskov [20, 21] to develop an efficient leader-based Byzantine SMR protocol named PBFT, whose stable leader requires O(n 2 ) communication and two round-trips per decision, and the leader replacement protocol incurs O(n3 ) communication. PBFT has been deployed in several systems, including BFT-SMaRt [13]. Kotla et al. introduced an optimistic linear path into PBFT in a protocol named Zyzzyva [34], which was utilized in several systems, e.g., Upright [22] and Byzcoin [33]. The optimistic path has linear complexity, while the leader replacement protocol remains O(n 3 ). Abraham et al. [4] later exposed a safety violation in Zyzzyva, and presented fixes [5, 30]. On the other hand, to also reduce the complexity of the protocol itself, Song et al. proposed Bosco [49], a simple one-step protocol with low latency on the optimistic path, requiring 5f + 1 replicas. SBFT [30] introduces an O(n 2 ) communication view-change protocol that supports a stable leader protocol with optimistically linear, one round-trip decisions. It reduces the communication complexity by harnessing two methods: a collector-based communication paradigm by Reiter [45], and signature combining via threshold cryptography on protocol votes by Cachin et al. [18]. A leader-based Byzantine SMR protocol that employs randomization was presented by Ramasamy et al. [44], and a leaderless  HotStuff: BFT Consensus with Linearity and Responsiveness  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  Correct leader  Authenticator complexity Leader failure (view-change)  f leader failures  O (n 4 ) O (n 2 ) O (n) O (n 2 ) O (n) O (n)  O (n 4 ) O (n 3 ) O (n 2 ) O (n 2 ) O (n) O (n)  O (n 4 ) O (f n 3 ) O (f n 2 ) O (f n 2 ) O (f n) O (f n)  Protocol DLS [25] PBFT [20] SBFT [30] Tendermint [15] / Casper [17] Tendermint* / Casper* HotStuff * Signatures  Responsiveness  ✓ ✓  !  can be combined using threshold signatures, though this optimization is not mentioned in their original works.  Table 1: Performance of selected protocols after GST. variant named HoneyBadgerBFT was developed by Miller et al. [39]. key and the function tverify. We require that if ρ i ← tsign i (m) At their core, these randomized Byzantine solutions employ ranfor each i ∈ I , |I | = k, and if σ ← tcombine(m, {ρ i }i ∈I ), then domized asynchronous Byzantine consensus, whose best known tverify(m, σ ) returns true. However, given oracle access to oracles communication complexity was O(n 3 ) (see above), amortizing the {tsign i (·)}i ∈[n]\F , an adversary who queries tsign i (m) on strictly cost via batching. However, most recently, based on the idea in fewer than k − f of these oracles has negligible probability of producthis HotStuff paper, a parallel submission to PODC’19 [8] further ing a signature σ for the message m (i.e., such that tverify(m, σ ) reimproves the communication complexity to O(n2 ). turns true). Throughout this paper, we use a threshold of k = 2f +1. Bitcoin’s core is a protocol known as Nakamoto Consensus [40], Again, we will typically leave invocations of tverify implicit in our a synchronous protocol with only probabilistic safety guarantee and protocol descriptions. no finality (see analysis in [6, 28, 41]). It operates in a permissionless We also require a cryptographic hash function h (also called a model where participants are unknown, and resilience is kept via message digest function), which maps an arbitrary-length input Proof-of-Work. As described above, recent blockchain solutions to a fixed-length output. The hash function must be collision rehybridize Proof-of-Work solutions with classical BFT solutions in sistant [46], which informally requires that the probability of an various ways [7, 17, 26, 29, 31, 33, 42]. The need to address rotating adversary producing inputs m and m ′ such that h(m) = h(m ′ ) is leaders in these hybrid solutions and others provide the motivation negligible. As such, h(m) can serve as an identifier for a unique behind HotStuff. input m in the protocol.  3  MODEL  We consider a system consisting of a fixed set of n = 3f + 1 replicas, indexed by i ∈ [n] where [n] = {1, . . . , n}. A set F ⊂ [n] of up to f = |F | replicas are Byzantine faulty, and the remaining ones are correct. We will often refer to the Byzantine replicas as being coordinated by an adversary, which learns all internal state held by these replicas (including their cryptographic keys, see below). Network communication is point-to-point, authenticated and reliable: one correct replica receives a message from another correct replica if and only if the latter sent that message to the former. When we refer to a “broadcast”, it involves the broadcaster, if correct, sending the same point-to-point messages to all replicas, including itself. We adopt the partial synchrony model of Dwork et al. [25], where there is a known bound ∆ and an unknown Global Stabilization Time (GST), such that after GST, all transmissions between two correct replicas arrive within time ∆. Our protocol will ensure safety always, and will guarantee progress within a bounded duration after GST. (Guaranteeing progress before GST is impossible [27].) In practice, our protocol will guarantee progress if the system remains stable (i.e., if messages arrive within ∆ time) for sufficiently long after GST, though assuming that it does so forever simplifies discussion. Cryptographic primitives. HotStuff makes use of threshold signatures [14, 18, 48]. In a (k, n)-threshold signature scheme, there is a single public key held by all replicas, and each of the n replicas holds a distinct private key. The i-th replica can use its private key to contribute a partial signature ρ i ← tsign i (m) on message m. Partial signatures {ρ i }i ∈I , where |I | = k and each ρ i ← tsign i (m), can be used to produce a digital signature σ ← tcombine(m, {ρ i }i ∈I ) on m. Any other replica can verify the signature using the public  Complexity measure. The complexity measure we care about is authenticator complexity, which specifically is the sum, over all replicas i ∈ [n], of the number of authenticators received by replica i in the protocol to reach a consensus decision after GST. (Again, before GST, a consensus decision might not be reached at all in the worst case [27].) Here, an authenticator is either a partial signature or a signature. Authenticator complexity is a useful measure of communication complexity for several reasons. First, like bit complexity and unlike message complexity, it hides unnecessary details about the transmission topology. For example, n messages carrying one authenticator count the same as one message carrying n authenticators. Second, authenticator complexity is better suited than bit complexity for capturing costs in protocols like ours that reach consensus repeatedly, where each consensus decision (or each view proposed on the way to that consensus decision) is identified by a monotonically increasing counter. That is, because such a counter increases indefinitely, the bit complexity of a protocol that sends such a counter cannot be bounded. Third, since in practice, cryptographic operations to produce or verify digital signatures and to produce or combine partial signatures are typically the most computationally intensive operations in protocols that use them, the authenticator complexity provides insight into the computational burden of the protocol, as well.  4  BASIC HOTSTUFF  HotStuff solves the State Machine Replication (SMR) problem. At the core of SMR is a protocol for deciding on a growing log of command requests by clients. A group of state-machine replicas apply commands in sequence order consistently. A client sends a command request to all replicas, and waits for responses from (f + 1) of them. For the most part, we omit the client from the  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham  discussion, and defer to the standard literature for issues regarding numbering and de-duplication of client requests. The Basic HotStuff solution is presented in Algorithm 2. The protocol works in a succession of views numbered with monotonically increasing view numbers. Each viewNumber has a unique dedicated leader known to all. Each replica stores a tree of pending commands as its local data structure. Each tree node contains a proposed command (or a batch of them), metadata associated with the protocol, and a parent link. The branch led by a given node is the path from the node all the way to the tree root by visiting parent links. During the protocol, a monotonically growing branch becomes committed. To become committed, the leader of a particular view proposing the branch must collect votes from a quorum of (n − f ) replicas in three phases, prepare, pre-commit, and commit. A key ingredient in the protocol is a collection of (n − f ) votes over a leader proposal, referred to as a quorum certificate (or “QC” in short). The QC is associated with a particular node and a view number. The tcombine utility employs a threshold signature scheme to generate a representation of (n − f ) signed votes as a single authenticator. Below we give an operational description of the protocol logic by phases, followed by a precise specification in Algorithm 2, and conclude the section with safety, liveness, and complexity arguments.  safeNode predicate. The safeNode predicate is a core ingredient of the protocol. It examines a proposal message m carrying a QC justification m.justify, and determines whether m.node is safe to accept. The safety rule to accept a proposal is the branch of m.node extends from the currently locked node locked QC .node. On the other hand, the liveness rule is the replica will accept m if m.justify has a higher view than the current locked QC . The predicate is true as long as either one of two rules holds.  4.1  Phases  prepare phase. The protocol for a new leader starts by collecting new-view messages from (n − f ) replicas. The new-view message is sent by a replica as it transitions into viewNumber (including the first view) and carries the highest prepareQC that the replica received (⊥ if none), as described below. The leader processes these messages in order to select a branch that has the highest preceding view in which a prepareQC was formed. The leader selects the prepareQC with the highest view, denoted highQC , among the new-view messages. Because highQC is the highest among (n − f ) replicas, no higher view could have reached a commit decision. The branch led by highQC .node is therefore safe. Collecting new-view messages to select a safe branch may be omitted by an incumbent leader, who may simply select its own highest prepareQC as highQC . We defer this optimization to Section 6 and only describe a single, unified leader protocol in this section. Note that, different from PBFT-like protocols, including this step in the leader protocol is straightforward, and it incurs the same, linear overhead as all the other phases of the protocol, regardless of the situation. The leader uses the createLeaf method to extend the tail of highQC .node with a new proposal. The method creates a new leaf node as a child and embeds a digest of the parent in the child node. The leader then sends the new node in a prepare message to all other replicas. The proposal carries highQC for safety justification. Upon receiving the prepare message for the current view from the leader, replica r uses the safeNode predicate to determine whether to accept it. If it is accepted, the replica sends a prepare vote with a partial signature (produced by tsign r ) for the proposal to the leader.  pre-commit phase. When the leader receives (n − f ) prepare votes for the current proposal curProposal , it combines them into a prepareQC . The leader broadcasts prepareQC in pre-commit messages. A replica responds to the leader with pre-commit vote having a signed digest of the proposal. commit phase. The commit phase is similar to pre-commit phase. When the leader receives (n − f ) pre-commit votes, it combines them into a precommitQC and broadcasts it in commit messages; replicas respond to it with a commit vote. Importantly, a replica becomes locked on the precommitQC at this point by setting its locked QC to precommitQC (Line 25 of Algorithm 2). This is crucial to guard the safety of the proposal in case it becomes a consensus decision. decide phase. When the leader receives (n − f ) commit votes, it combines them into a commitQC . Once the leader has assembled a commitQC , it sends it in a decide message to all other replicas. Upon receiving a decide message, a replica considers the proposal embodied in the commitQC a committed decision, and executes the commands in the committed branch. The replica increments viewNumber and starts the next view. nextView interrupt. In all phases, a replica waits for a message at view viewNumber for a timeout period, determined by an auxiliary nextView(viewNumber ) utility. If nextView(viewNumber ) interrupts waiting, the replica also increments viewNumber and starts the next view.  4.2  Data Structures  Messages. A message m in the protocol has a fixed set of fields that are populated using the Msg() utility shown in Algorithm 1. m is automatically stamped with curView , the sender’s current view number. Each message has a type m.type ∈ {new-view, prepare, pre-commit, commit, decide}. m.node contains a proposed node (the leaf node of a proposed branch). There is an optional field m.justify. The leader always uses this field to carry the QC for the different phases. Replicas use it in new-view messages to carry the highest prepareQC . Each message sent in a replica role contains a partial signature m.partialSig by the sender over the tuple ⟨m.type, m.viewNumber , m.node⟩, which is added in the voteMsg() utility. Quorum certificates. A Quorum Certificate (QC) over a tuple ⟨type, viewNumber, node⟩ is a data type that combines a collection of signatures for the same tuple signed by (n − f ) replicas. Given a QC qc, we use qc.type, qc.viewNumber , qc.node to refer to the matching fields of the original tuple. Tree and branches. Each command is wrapped in a node that additionally contains a parent link which could be a hash digest of the parent node. We omit the implementation details from the  HotStuff: BFT Consensus with Linearity and Responsiveness  pseudocode. During the protocol, a replica delivers a message only after the branch led by the node is already in its local tree. In practice, a recipient who falls behind can catch up by fetching missing nodes from other replicas. For brevity, these details are also omitted from the pseudocode. Two branches are conflicting if neither one is an extension of the other. Two nodes are conflicting if the branches led by them are conflicting.  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  as a leader // r = leader(curView ) // we assume special new-view messages from view 0 wait for (n − f ) new-view messages: M ← {m | matchingMsg(m, new-view, curView −1)}   2: 3:  highQC ← arg max {m.justify .viewNumber } .justify  4:  m∈M  curProposal ← createLeaf(highQC .node, client’s command) broadcast Msg(prepare, curProposal, highQC ) as a replica wait for message m from leader(curView ) m : matchingMsg(m, prepare, curView ) if m.node extends from m.justify .node ∧ safeNode(m.node, m.justify) then send voteMsg(prepare, m.node, ⊥) to leader(curView )  5: 6:  Bookkeeping variables. A replica uses additional local variables for bookkeeping the protocol state: (i) a viewNumber , initially 1 and incremented either by finishing a decision or by a nextView interrupt; (ii) a locked quorum certificate locked QC , initially ⊥, storing the highest QC for which a replica voted commit; and (iii) a prepareQC , initially ⊥, storing the highest QC for which a replica voted pre-commit. Additionally, in order to incrementally execute a committed log of commands, the replica maintains the highest node whose branch has been executed. This is omitted below for brevity.  4.3  Protocol Specification  The protocol given in Algorithm 2 is described as an iterated viewby-view loop. In each view, a replica performs phases in succession based on its role, described as a succession of “as” blocks. A replica can have more than one role. For example, a leader is also a (normal) replica. Execution of as blocks across roles can be proceeded concurrently. The execution of each as block is atomic. A nextView interrupt aborts all operations in any as block, and jumps to the “Finally” block. Algorithm 1 Utilities (for replica r ). 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27:  function Msg(type, node, qc) m.type ← type m.viewNumber ← curView m.node ← node m.justify ← qc return m function voteMsg(type, node, qc) m ← Msg(type, node, qc) m.partialSig ← tsign r ( ⟨m.type, m.viewNumber, m.node ⟩) return m procedure createLeaf(parent, cmd) b .parent ← parent b .cmd ← cmd return b function QC(V ) qc.type ← m.type : m ∈ V qc.viewNumber ← m.viewNumber : m ∈ V qc.node ← m.node : m ∈ V qc.sig ← tcombine( ⟨qc.type, qc.viewNumber, qc.node ⟩, {m.partialSig | m ∈ V }) return qc function matchingMsg(m, t, v) return (m.type = t ) ∧ (m.viewNumber = v) function matchingQC(qc, t, v) return (qc.type = t ) ∧ (qc.viewNumber = v) function safeNode(node, qc) return (node extends from lockedQC .node) ∨ // safety rule (qc.viewNumber > lockedQC .viewNumber ) // liveness rule  Algorithm 2 Basic HotStuff protocol (for replica r ). 1: for curView ← 1, 2, 3, . . . do  ▷ prepare phase  7: 8: 9: 10:  11: 12: 13: 14: 15: 16: 17: 18:  19: 20: 21: 22: 23: 24: 25: 26:  27: 28: 29: 30: 31: 32: 33:  34: 35:  ▷ pre-commit phase as a leader wait for (n − f ) votes: V ← {v | matchingMsg(v, prepare, curView )} prepareQC ← QC(V ) broadcast Msg(pre-commit, ⊥, prepareQC ) as a replica wait for message m from leader(curView ) m : matchingQC(m.justify, prepare, curView ) prepareQC ← m.justify send to leader(curView ) voteMsg(pre-commit, m.justify .node, ⊥) ▷ commit phase as a leader wait for (n − f ) votes: V ← {v | matchingMsg(v, pre-commit, curView )} precommitQC ← QC(V ) broadcast Msg(commit, ⊥, precommitQC ) as a replica wait for message m from leader(curView ) m : matchingQC(m.justify, pre-commit, curView ) lockedQC ← m.justify send to leader(curView ) voteMsg(commit, m.justify .node, ⊥) ▷ decide phase as a leader wait for (n − f ) votes: V ← {v | matchingMsg(v, commit, curView )} commitQC ← QC(V ) broadcast Msg(decide, ⊥, commitQC ) as a replica wait for message m from leader(curView ) m : matchingQC(m.justify, commit, curView ) execute new commands through m.justify .node, respond to clients ▷ Finally nextView interrupt: goto this line if nextView(curView ) is called during “wait for” in any phase send Msg(new-view, ⊥, prepareQC ) to leader(curView + 1)  4.4  Safety, Liveness, and Complexity  Safety. We first define a quorum certificate qc to be valid if tverify(⟨qc.type, qc.viewNumber , qc.node⟩, qc.sig) is true. Lemma 1. For any valid qc 1 , qc 2 in which qc 1 .type = qc 2 .type and qc 1 .node conflicts with qc 2 .node, we have qc 1 .viewNumber , qc 2 .viewNumber .  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham  Proof. To show a contradiction, suppose qc 1 .viewNumber = qc 2 .viewNumber = v. Because a valid QC can be formed only with n − f = 2f + 1 votes (i.e., partial signatures) for it, there must be a correct replica who voted twice in the same phase of v. This is impossible because the pseudocode allows voting only once for each phase in each view. □  Lemma 3. If a correct replica is locked such that locked QC = precommitQC , then at least f + 1 correct replicas voted for some prepareQC matching locked QC .  Theorem 2. If w and b are conflicting nodes, then they cannot be both committed, each by a correct replica. Proof. We prove this important theorem by contradiction. Let qc 1 denote a valid commitQC (i.e., qc 1 .type = commit) such that qc 1 .node = w, and qc 2 denote a valid commitQC such that qc 2 .node = b. Denote v 1 = qc 1 .viewNumber and v 2 = qc 2 .viewNumber . By Lemma 1, v 1 , v 2 . W.l.o.g. assume v 1 < v 2 . We will now denote by vs the lowest view higher than v 1 for which there is a valid prepareQC , qc s (i.e., qc s .type = prepare) where qc s .viewNumber = vs , and qc s .node conflicts with w. Formally, we define the following predicate for any prepareQC : E(prepareQC ) B(v 1 < prepareQC .viewNumber ≤ v 2 ) ∧ (prepareQC .node conflicts with w). We can now set the first switching point qc s :   prepareQC .viewNumber | qc s B arg min . prepareQC is valid ∧ E(prepareQC ) prepareQC Note that, by assumption such a qc s must exist; for example, qc s could be the prepareQC formed in view v 2 . Of the correct replicas that sent a partial result tsign r (⟨qc 1 .type, qc 1 .viewNumber , qc 1 .node⟩), let r be the first that contributed tsign r (⟨qc s .type, qc s .viewNumber , qc s .node⟩); such an r must exist since otherwise, one of qc 1 .sig and qc s .sig could not have been created. During view v 1 , replica r updates its lock locked QC to a precommitQC on w at Line 25 of Algorithm 2. Due to the minimality of vs , the lock that replica r has on w is not changed before qc s is formed. Otherwise r must have seen some other prepareQC with lower view because Line 17 comes before Line 25, contradicting to the minimality. Now consider the invocation of safeNode in the prepare phase of view vs by replica r , with a message m carrying m.node = qc s .node. By assumption, m.node conflicts with locked QC .node, and so the disjunct at Line 26 of Algorithm 1 is false. Moreover, m.justify .viewNumber > v 1 would violate the minimality of vs , and so the disjunct in Line 27 of Algorithm 1 is also false. Thus, safeNode must return false and r cannot cast a prepare vote on the conflicting branch in view vs , a contradiction. □ Liveness. There are two functions left undefined in the previous section: leader and nextView. Their definition will not affect safety of the protocol, though they do matter to liveness. Before giving candidate definitions for them, we first show that after GST, there is a bounded durationTf such that if all correct replicas remain in view v during Tf and the leader for view v is correct, then a decision is reached. Below, we say that qc 1 and qc 2 match if qc 1 and qc 2 are valid, qc 1 .node = qc 2 .node, and qc 1 .viewNumber = qc 2 .viewNumber .  Proof. Suppose replica r is locked on precommitQC . Then, (n − f ) votes were cast for the matching prepareQC in the prepare phase (Line 10 of Algorithm 2), out of which at least f + 1 were from correct replicas. □ Theorem 4. After GST, there exists a bounded time period Tf such that if all correct replicas remain in view v during Tf and the leader for view v is correct, then a decision is reached. Proof. Starting in a new view, the leader collects (n − f ) newview messages and calculates its highQC before broadcasting a prepare messsage. Suppose among all replicas (including the leader itself), the highest kept lock is locked QC = precommitQC ∗ . By Lemma 3, we know there are at least f +1 correct replicas that voted for a prepareQC ∗ matching precommitQC ∗ , and have already sent them to the leader in their new-view messages. Thus, the leader must learn a matching prepareQC ∗ in at least one of these new-view messages and use it as highQC in its prepare message. By the assumption, all correct replicas are synchronized in their view and the leader is non-faulty. Therefore, all correct replicas will vote in the prepare phase, since in safeNode, the condition on Line 27 of Algorithm 1 is satisfied (even if the node in the message conflicts with a replica’s stale locked QC .node, and so Line 26 is not). Then, after the leader assembles a valid prepareQC for this view, all replicas will vote in all the following phases, leading to a new decision. After GST, the duration Tf for these phases to complete is of bounded length. The protocol is Optimistically Responsive because there is no explicit “wait-for-∆” step, and the logical disjunction in safeNode is used to override a stale lock with the help of the three-phase paradigm. □ We now provide simple constructions for leader and nextView that suffice to ensure that after GST, eventually a view will be reached in which the leader is correct and all correct replicas remain in this view for Tf time. It suffices for leader to return some deterministic mapping from view number to a replica, eventually rotating through all replicas. A possible solution for nextView is to utilize an exponential back-off mechanism that maintains a timeout interval. Then a timer is set upon entering each view. When the timer goes off without making any decision, the replica doubles the interval and calls nextView to advance the view. Since the interval is doubled at each time, the waiting intervals of all correct replicas will eventually have at least Tf overlap in common, during which the leader could drive a decision. Livelessness with two-phases. We now briefly demonstrate an infinite non-deciding scenario for a “two-phase” HotStuff. This explains the necessity for introducing a synchronous delay in Casper and Tendermint, and hence for abandoning (Optimistic) Responsiveness. In the two-phase HotStuff variant, we omit the pre-commit phase and proceed directly to commit. A replica becomes locked when it votes on a prepareQC . Suppose, in view v, a leader proposes b. It completes the prepare phase, and some replica rv votes  HotStuff: BFT Consensus with Linearity and Responsiveness  for the prepareQC , say qc, such that qc.node = b. Hence, rv becomes locked on qc. An asynchronous network scheduling causes the rest of the replicas to move to view v + 1 without receiving qc. We now repeat ad infinitum the following single-view transcript. We start view v + 1 with only rv holding the highest prepareQC (i.e. qc) in the system. The new leader l collects new-view messages from 2f + 1 replicas excluding rv . The highest prepareQC among these, qc ′ , has view v − 1 and b ′ = qc ′ .node conflicts with b. l then proposes b ′′ which extends b ′ , to which 2f honest replicas respond with a vote, but rv rejects it because it is locked on qc, b ′′ conflicts with b and qc ′ is lower than qc. Eventaully, 2f replicas give up and move to the next view. Just then, a faulty replica responds to l’s proposal, l then puts together a prepareQC (v + 1, b ′′ ) and one replica, say rv+1 votes for it and becomes locked on it. Complexity. In each phase of HotStuff, only the leader broadcasts to all replicas while the replicas respond to the sender once with a partial signature to certify the vote. In the leader’s message, the QC consists of a proof of (n − f ) votes collected previously, which can be encoded by a single threshold signature. In a replica’s response, the partial signature from that replica is the only authenticator. Therefore, in each phase, there are O(n) authenticators received in total. As there is a constant number of phases, the overall complexity per view is O(n).  5  CHAINED HOTSTUFF  It takes three phases for a Basic HotStuff leader to commit a proposal. These phases are not doing “useful” work except collecting votes from replicas, and they are all very similar. In Chained HotStuff, we improve the Basic HotStuff protocol utility while at the same time considerably simplifying it. The idea is to change the view on every prepare phase, so each proposal has its own view. This reduces the number of message types and allows for pipelining of decisions. A similar approach for message type reduction was suggested in Casper [1]. More specifically, in Chained HotStuff the votes over a prepare phase are collected in a view by the leader into a genericQC . Then the genericQC is relayed to the leader of the next view, essentially delegating responsibility for the next phase, which would have been pre-commit, to the next leader. However, the next leader does not actually carry a pre-commit phase, but instead initiates a new prepare phase and adds its own proposal. This prepare phase for view v + 1 simultaneously serves as the pre-commit phase for view v. The prepare phase for view v + 2 simultaneously serves as the pre-commit phase for view v + 1 and as the commit phase for view v. This is possible because all the phases have identical structure. The pipeline of Basic HotStuff protocol phases embedded in a chain of Chained HotStuff proposals is depicted in Figure 1. Views v 1 , v 2 , v 3 of Chained HotStuff serve as the prepare, pre-commit, and commit Basic HotStuff phases for cmd 1 proposed in v 1 . This command becomes committed by the end of v 4 . Views v 2 , v 3 , v 4 serve as the three Basic HotStuff phases for cmd 2 proposed in v 2 , and it becomes committed by the end of v 5 . Additional proposals generated in these phases continue the pipeline similarly, and are denoted by dashed boxes. In Figure 1, a single arrow denotes the b.parent field for a node b, and a double arrow denotes b.justify .node.  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  Hence, there are only two types of messages in Chained HotStuff, a new-view message and generic-phase generic message. The generic QC functions in all logically pipelined phases. We next explain the mechanisms in the pipeline to take care of locking and committing, which occur only in the commit and decide phases of Basic HotStuff. Dummy nodes. The genericQC used by a leader in some view viewNumber may not directly reference the proposal of the preceding view (viewNumber − 1). The reason is that the leader of a preceding view fails to obtain a QC, either because there are conflicting proposals, or due to a benign crash. To simplify the tree structure, createLeaf extends genericQC .node with blank nodes up to the height (the number of parent links on a node’s branch) of the proposing view, so view-numbers are equated with node heights. As a result, the QC embedded in a node b may not refer to its parent, i.e., b.justify .node may not equal b.parent (the last node in Figure 2). One-Chain, Two-Chain, and Three-Chain. When a node b ∗ carries a QC that refers to a direct parent, i.e., b ∗ .justify .node = b ∗ .parent, we say that it forms a One-Chain. Denote by b ′′ = b ∗ .justify .node. Node b ∗ forms a Two-Chain, if in addition to forming a One-Chain, b ′′ .justify .node = b ′′ .parent. It forms a ThreeChain, if b ′′ forms a Two-Chain. Looking at chain b = b ′ .justify .node, b ′ = b ′′ .justify .node, ′′ b = b ∗ .justify .node, ancestry gaps might occur at any one of the nodes. These situations are similar to a leader of Basic HotStuff failing to complete any one of three phases, and getting interrupted to the next view by nextView. If b ∗ forms a One-Chain, the prepare phase of b ′′ has succeeded. Hence, when a replica votes for b ∗ , it should remember genericQC ← b ∗ .justify. We remark that it is safe to update genericQC even when a One-Chain is not direct, so long as it is higher than the current genericQC . In the implementation code described in Section 6, we indeed update genericQC in this case. If b ∗ forms a Two-Chain, then the pre-commit phase of b ′ has succeeded. The replica should therefore update locked QC ← b ′′ .justify. Again, we remark that the lock can be updated even when a Two-Chain is not direct—safety will not break—and indeed, this is given in the implementation code in Section 6. Finally, if b ∗ forms a Three-Chain, the commit phase of b has succeeded, and b becomes a committed decision. Algorithm 3 shows the pseudocode for Chained HotStuff. The proof of safety given by [50] is similar to the one for Basic HotStuff. We require the QC in a valid node refers to its ancestor. For brevity, we assume the constraint always holds and omit checking in the code. Algorithm 3 Chained HotStuff protocol. 1: procedure createLeaf(parent, cmd, qc) 2: b .parent ← branch extending with blanks from parent to height  curView ; b .cmd ← cmd; b .justify ← qc; return b  3: for curView ← 1, 2, 3, . . . do 4: 5:  ▷ generic phase as a leader // r = leader(curView ) wait for (n − f ) new-view messages: M ← {m | matchingMsg(m, new-view, curView −1)} // M includes the previous leader new-view message, if received  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  ··· cmd 1  Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham  v1  v2  v3  v4  v5  QC cmd 1  QC v1 cmd 2  QC v2 cmd 3  QC v3 cmd 4  QC v4 cmd 5  pre-commit  commit  decide  prepare  pre-commit  commit  decide  prepare  pre-commit  commit  prepare  pre-commit  prepare decide  cmd 2  commit  decide  pre-commit  commit  cmd 3  decide  cmd 4  cmd 5  ···  prepare  Figure 1: Chained HotStuff is a pipelined Basic HotStuff where a QC can serve in different phases simultaneously.  b : v3 ···  cmd  b ′ : v4  b ′′ : v 5  b ∗ : v6  v8  QC cmd  QC cmd  QC cmd  · ·QC · cmd  Figure 2: The nodes at views v 4 , v 5 , v 6 form a Three-Chain. The node at view v 8 does not make a valid One-Chain in Chained HotStuff (but it is a valid One-Chain after relaxation in the algorithm of Section 6).  // while waiting in  previous view 6:    genericQC ← arg max {m.justify .viewNumber } .justify m∈M  7:  8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24:  25: 26:  6  curProposal ← createLeaf(genericQC .node, client’s command, genericQC ) // prepare phase (leader-half) broadcast Msg(generic, curProposal, ⊥) as a replica wait for message m from leader(curView ) m : matchingMsg(m, generic, curView ) b ∗ ← m.node; b ′′ ← b ∗ .justify .node; b ′ ← b ′′ .justify .node; b ← b ′ .justify .node if safeNode(b ∗, b ∗ .justify) then send voteMsg(generic, b ∗, ⊥) to leader(curView ) // start pre-commit phase on b ∗ ’s parent if b ∗ .parent = b ′′ then genericQC ← b ∗ .justify // start commit phase on b ∗ ’s grandparent if (b ∗ .parent = b ′′ ) ∧ (b ′′ .parent = b ′ ) then lockedQC ← b ′′ .justify // start decide phase on b ∗ ’s great-grandparent if (b ∗ .parent = b ′′ ) ∧ (b ′′ .parent = b ′ ) ∧ (b ′ .parent = b) then execute new commands through b, respond to clients as a leader // pre-commit phase (leader-half) wait for (n − f ) votes: V ← {v | matchingMsg(v, generic, curView )} genericQC ← QC(V ) // for liveness, the message here counts as (n − f ) at Line 5 as the next leader wait for message m from leader(curView ) m : matchingMsg(m, new-view, curView ) ▷ Finally nextView interrupt: goto this line if nextView(curView ) is called during “wait for” in any phase send Msg(new-view, ⊥, genericQC ) to leader(curView + 1)  IMPLEMENTATION  HotStuff is a practical protocol for building efficient SMR systems. Because of its simplicity, we can easily turn Algorithm 3 into an  event-driven-style specification that is almost like the code skeleton for a prototype implementation. As shown in Algorithm 4, the code is further simplified and generalized by extracting the liveness mechanism from the body into a module named Pacemaker. Instead of the next leader always waiting for a genericQC at the end of the generic phase before starting its reign, this logic is delegated to the Pacemaker. A stable leader can skip this step and streamline proposals across multiple heights. Additionally, we relax the direct parent constraint for maintaining the highest genericQC and locked QC , while still preserving the requirement that the QC in a valid node always refers to its ancestor. The proof of correctness is similar to Chained HotStuff and we also defer it to the appendix of [50]. Data structures. Each replica u keeps track of the following main state variables: V [·] vheight block bexec qc high bleaf  mapping from a node to its votes. height of last voted node. locked node (similar to locked QC ). last executed node. highest known QC (similar to genericQC ) kept by a Pacemaker. leaf node kept by a Pacemaker.  It also keeps a constant b0 , the same genesis node known by all correct replicas. To bootstrap, b0 contains a hard-coded QC for itself, block , bexec , bleaf are all initialized to b0 , and qc high contains the QC for b0 . Pacemaker. A Pacemaker is a mechanism that guarantees progress after GST. It achieves this through two ingredients. The first one is “synchronization”, bringing all correct replicas, and a unique leader, into a common height for a sufficiently long period. The usual synchronization mechanism in the literature [15, 20, 25] is for replicas to increase the count of ∆’s they spend at larger heights, until progress is being made. A common way to deterministically elect a leader is to use a rotating leader scheme  HotStuff: BFT Consensus with Linearity and Responsiveness  in which all correct replicas keep a predefined leader schedule and rotate to the next one when the leader is demoted. Second, a Pacemaker needs to provide the leader with a way to choose a proposal that will be supported by correct replicas. As shown in Algorithm 5, after a view change, in onReceiveNewView, the new leader collects new-view messages sent by replicas through onNextSyncView to discover the highest QC to satisfy the second part of the condition in onReceiveProposal for liveness (Line 18 of Algorithm 4). During the same view, however, the incumbent leader will chain the new node to the end of the leaf last proposed by itself, where no new-view message is needed. Based on some application-specific heuristics (to wait until the previously proposed node gets a QC, for example), the current leader invokes onBeat to propose a new node carrying the command to be executed. It is worth noting that even if a bad Pacemaker invokes onPropose arbitrarily, or selects a parent and a QC capriciously, and against any scheduling delays, safety is always guaranteed. Therefore, safety guaranteed by Algorithm 4 alone is entirely decoupled from liveness by any potential instantiation of Algorithm 5. Algorithm 4 Event-driven HotStuff (for replica u). 1: procedure createLeaf(parent, cmd, qc, height) 2: b .parent ← parent; b .cmd ← cmd; 3: b .justify ← qc; b .height ← height; return b 4: procedure update(b ∗ ) 5: b ′′ ← b ∗ .justify .node; b ′ ← b ′′ .justify .node 6: b ← b ′ .justify .node 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31:  // pre-commit phase on b ′′ updateQCHigh(b ∗ .justify) if b ′ .height > block .height then block ← b ′ // commit phase on b ′ if (b ′′ .parent = b ′ ) ∧ (b ′ .parent = b) then onCommit(b) bexec ← b // decide phase on b procedure onCommit(b) if bexec .height < b .height then onCommit(b .parent); execute(b .cmd) procedure onReceiveProposal(Msgv (generic, bnew , ⊥)) if bnew .height > vheight ∧ (bnew extends block ∨ bnew .justify .node .height > block .height) then vheight ← bnew .height send(getLeader(), voteMsgu (generic, bnew , ⊥)) update(bnew ) procedure onReceiveVote(m = voteMsgv (generic, b, ⊥)) if ∃ ⟨v, σ ′ ⟩ ∈ V [b] then return // avoid duplicates V [b] ← V [b] ∪ { ⟨v, m.partialSig ⟩ } // collect votes if |V [b] | ≥ n − f then qc ← QC({σ | ⟨v ′, σ ⟩ ∈ V [b]}) updateQCHigh(qc) function onPropose(bleaf , cmd, qc high ) bnew ← createLeaf(bleaf , cmd, qc high , bleaf .height + 1) // send to all replicas, including u itself broadcast(Msgu (generic, bnew , ⊥)) return bnew  Algorithm 5 Code skeleton for a Pacemaker (for replica u). // We assume Pacemaker in all correct replicas will have synchronized leadership after GST.  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  1: function getLeader // . . . specified by the application 2: procedure updateQCHigh(qc ′high ) 3: 4: 5:  if qc ′high .node .height > qc high .node .height then qc high ← qc ′high bleaf ← qc high .node  6: procedure onBeat(cmd) 7: if u = getLeader() then 8: bleaf ← onPropose(bleaf , cmd, qc high ) 9: procedure onNextSyncView 10: send Msg(new-view, ⊥, qc high ) to getLeader() 11: procedure onReceiveNewView(Msg(new-view, ⊥, qc ′high )) 12:  updateQCHigh(qc ′high )  Algorithm 6 update replacement for two-phase HotStuff. 1: procedure update(b ∗ ) 2: b ′ ← b ∗ .justify .node ; b ← b ′ .justify .node 3: updateQCHigh(b ∗ .justify) 4: if b ′ .height > block .height then block ← b ′ 5:  if (b ′ .parent = b) then onCommit(b); bexec ← b  Two-phase HotStuff variant. To further demonstrate the flexibility of the HotStuff framework, Algorithm 6 shows the two-phase variant of HotStuff. Only the update procedure is affected, a TwoChain is required for reaching a commit decision, and a One-Chain determines the lock. As discussed above (Section 4.4), this twophase variant loses Optimistic Responsiveness, and is similar to Tendermint/Casper. The benefit is fewer phases, while liveness may be addressed by incorporating in Pacemaker a wait based on maximum network delay. Evaluation. Due to the space limitation, we defer our evaluation results to the longer paper [50]. There, we compare our implementation to BFT-SMaRt [13], a state-of-the-art implementation based on a two-phase PBFT variant. We show that even though threephase HotStuff has an additional phase for its responsiveness and uses digital signatures universally (where BFT-SMaRt only uses MACs for votes), it still achieves similar latency, while being able to outperform BFT-SMaRt in throughput. It also scales better than BFT-SMaRt.  ACKNOWLEDGMENTS We are thankful to Mathieu Baudet, Avery Ching, George Danezis, François Garillot, Zekun Li, Ben Maurer, Kartik Nayak, Dmitri Perelman, and Ling Ren, for many deep discussions of HotStuff, and to Mathieu Baudet for exposing a subtle error in a previous version posted to the ArXiv of this manuscript.  REFERENCES [1] 2017. Casper FFG with One Message Type, and Simpler Fork Choice Rule. https://ethresear.ch/t/casper-ffg-with-one-message-type-and-simplerfork-choice-rule/103. (2017). [2] 2018. Istanbul BFT’s Design Cannot Successfully Tolerate Fail-Stop Failures. https://github.com/jpmorganchase/quorum/issues/305. (2018). [3] 2018. A Livelock Bug in the Presence of Byzantine Validator. https://github.com/ tendermint/tendermint/issues/1047. (2018). [4] Ittai Abraham, Guy Gueta, Dahlia Malkhi, Lorenzo Alvisi, Ramakrishna Kotla, and Jean-Philippe Martin. 2017. Revisiting Fast Practical Byzantine Fault Tolerance. CoRR abs/1712.01367 (2017). arXiv:1712.01367 [5] Ittai Abraham, Guy Gueta, Dahlia Malkhi, and Jean-Philippe Martin. 2018. Revisiting Fast Practical Byzantine Fault Tolerance: Thelma, Velma, and Zelma. CoRR abs/1801.10022 (2018). arXiv:1801.10022  PODC ’19, July 29-August 2, 2019, Toronto, ON, Canada  Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham  [6] Ittai Abraham and Dahlia Malkhi. 2017. The Blockchain Consensus Layer and BFT. Bulletin of the EATCS 123 (2017). [7] Ittai Abraham, Dahlia Malkhi, Kartik Nayak, Ling Ren, and Alexander Spiegelman. 2017. Solida: A Blockchain Protocol Based on Reconfigurable Byzantine Consensus. In 21st International Conference on Principles of Distributed Systems, OPODIS 2017, Lisbon, Portugal, December 18-20, 2017. 25:1–25:19. https: //doi.org/10.4230/LIPIcs.OPODIS.2017.25 [8] Ittai Abraham, Dahlia Malkhi, and Alexander Spiegelman. 2019. Validated Asynchronous Byzantine Agreement with Optimal Resilience and Asymptotically Optimal Time and Word Communication. In Proceedings of the 2019 ACM Symposium on Principles of Distributed Computing, PODC 2019, Toronto, ON, Canada, July 29-August 2, 2019. [9] Yair Amir, Brian A. Coan, Jonathan Kirsch, and John Lane. 2011. Prime: Byzantine Replication under Attack. IEEE Trans. Dependable Sec. Comput. 8, 4 (2011), 564– 577. https://doi.org/10.1109/TDSC.2010.70 [10] Hagit Attiya, Cynthia Dwork, Nancy A. Lynch, and Larry J. Stockmeyer. 1994. Bounds on the Time to Reach Agreement in the Presence of Timing Uncertainty. J. ACM 41, 1 (1994), 122–152. https://doi.org/10.1145/174644.174649 [11] Pierre-Louis Aublin, Rachid Guerraoui, Nikola Knezevic, Vivien Quéma, and Marko Vukolic. 2015. The Next 700 BFT Protocols. ACM Trans. Comput. Syst. 32, 4 (2015), 12:1–12:45. https://doi.org/10.1145/2658994 [12] Michael Ben-Or. 1983. Another Advantage of Free Choice: Completely Asynchronous Agreement Protocols (Extended Abstract). In Proceedings of the Second Annual ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing, Montreal, Quebec, Canada, August 17-19, 1983. 27–30. https://doi.org/10.1145/ 800221.806707 [13] Alysson Neves Bessani, João Sousa, and Eduardo Adílio Pelinson Alchieri. 2014. State Machine Replication for the Masses with BFT-SMART. In 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2014, Atlanta, GA, USA, June 23-26, 2014. 355–362. https://doi.org/10.1109/DSN.2014.43 [14] Dan Boneh, Ben Lynn, and Hovav Shacham. 2004. Short Signatures from the Weil Pairing. J. Cryptology 17, 4 (2004), 297–319. https://doi.org/10.1007/s00145004-0314-9 [15] Ethan Buchman. 2016. Tendermint: Byzantine fault tolerance in the age of blockchains. Ph.D. Dissertation. [16] Ethan Buchman, Jae Kwon, and Zarko Milosevic. 2018. The Latest Gossip on BFT Consensus. CoRR abs/1807.04938 (2018). arXiv:1807.04938 [17] Vitalik Buterin and Virgil Griffith. 2017. Casper the Friendly Finality Gadget. CoRR abs/1710.09437 (2017). arXiv:1710.09437 [18] Christian Cachin, Klaus Kursawe, and Victor Shoup. 2005. Random Oracles in Constantinople: Practical Asynchronous Byzantine Agreement Using Cryptography. J. Cryptology 18, 3 (2005), 219–246. https://doi.org/10.1007/s00145-0050318-0 [19] Christian Cachin and Marko Vukolic. 2017. Blockchain Consensus Protocols in the Wild. CoRR abs/1707.01873 (2017). arXiv:1707.01873 [20] Miguel Castro and Barbara Liskov. 1999. Practical Byzantine Fault Tolerance. In Proceedings of the Third USENIX Symposium on Operating Systems Design and Implementation (OSDI), New Orleans, Louisiana, USA, February 22-25, 1999. 173–186. https://dl.acm.org/citation.cfm?id=296824 [21] Miguel Castro and Barbara Liskov. 2002. Practical Byzantine Fault Tolerance and Proactive Recovery. ACM Trans. Comput. Syst. 20, 4 (2002), 398–461. https: //doi.org/10.1145/571637.571640 [22] Allen Clement, Manos Kapritsos, Sangmin Lee, Yang Wang, Lorenzo Alvisi, Michael Dahlin, and Taylor Riche. 2009. Upright cluster services. In Proceedings of the 22nd ACM Symposium on Operating Systems Principles 2009, SOSP 2009, Big Sky, Montana, USA, October 11-14, 2009. 277–290. https://doi.org/10.1145/1629575. 1629602 [23] Danny Dolev and Rüdiger Reischuk. 1985. Bounds on Information Exchange for Byzantine Agreement. J. ACM 32, 1 (1985), 191–204. https://doi.org/10.1145/2455. 214112 [24] Danny Dolev and H. Raymond Strong. 1982. Polynomial Algorithms for Multiple Processor Agreement. In Proceedings of the 14th Annual ACM Symposium on Theory of Computing, May 5-7, 1982, San Francisco, California, USA. 401–407. https://doi.org/10.1145/800070.802215 [25] Cynthia Dwork, Nancy A. Lynch, and Larry J. Stockmeyer. 1988. Consensus in the Presence of Partial Synchrony. J. ACM 35, 2 (1988), 288–323. https: //doi.org/10.1145/42282.42283 [26] Ittay Eyal, Adem Efe Gencer, Emin Gün Sirer, and Robbert van Renesse. 2016. Bitcoin-NG: A Scalable Blockchain Protocol. In 13th USENIX Symposium on Networked Systems Design and Implementation, NSDI 2016, Santa Clara, CA, USA, March 16-18, 2016. 45–59. [27] Michael J. Fischer, Nancy A. Lynch, and Mike Paterson. 1985. Impossibility of Distributed Consensus with One Faulty Process. J. ACM 32, 2 (1985), 374–382. https://doi.org/10.1145/3149.214121 [28] Juan A. Garay, Aggelos Kiayias, and Nikos Leonardos. 2015. The Bitcoin Backbone Protocol: Analysis and Applications. In Advances in Cryptology - EUROCRYPT 2015 - 34th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Sofia, Bulgaria, April 26-30, 2015, Proceedings, Part II.  281–310. https://doi.org/10.1007/978-3-662-46803-6_10 [29] Yossi Gilad, Rotem Hemo, Silvio Micali, Georgios Vlachos, and Nickolai Zeldovich. 2017. Algorand: Scaling Byzantine Agreements for Cryptocurrencies. In Proceedings of the 26th Symposium on Operating Systems Principles, Shanghai, China, October 28-31, 2017. 51–68. https://doi.org/10.1145/3132747.3132757 [30] Guy Golan-Gueta, Ittai Abraham, Shelly Grossman, Dahlia Malkhi, Benny Pinkas, Michael K. Reiter, Dragos-Adrian Seredinschi, Orr Tamir, and Alin Tomescu. 2018. SBFT: a Scalable Decentralized Trust Infrastructure for Blockchains. CoRR abs/1804.01626 (2018). arXiv:1804.01626 [31] Timo Hanke, Mahnush Movahedi, and Dominic Williams. 2018. DFINITY Technology Overview Series, Consensus System. CoRR abs/1805.04548 (2018). arXiv:1805.04548 [32] Jonathan Katz and Chiu-Yuen Koo. 2009. On Expected Constant-Round Protocols for Byzantine Agreement. J. Comput. Syst. Sci. 75, 2 (2009), 91–112. https: //doi.org/10.1016/j.jcss.2008.08.001 [33] Eleftherios Kokoris-Kogias, Philipp Jovanovic, Nicolas Gailly, Ismail Khoffi, Linus Gasser, and Bryan Ford. 2016. Enhancing Bitcoin Security and Performance with Strong Consistency via Collective Signing. CoRR abs/1602.06997 (2016). [34] Ramakrishna Kotla, Lorenzo Alvisi, Michael Dahlin, Allen Clement, and Edmund L. Wong. 2009. Zyzzyva: Speculative Byzantine Fault Tolerance. ACM Trans. Comput. Syst. 27, 4 (2009), 7:1–7:39. https://doi.org/10.1145/1658357.1658358 [35] Leslie Lamport. 1978. Time, Clocks, and the Ordering of Events in a Distributed System. Commun. ACM 21, 7 (1978), 558–565. https://doi.org/10.1145/359545. 359563 [36] Leslie Lamport. 1998. The Part-Time Parliament. ACM Trans. Comput. Syst. 16, 2 (1998), 133–169. https://doi.org/10.1145/279227.279229 [37] Leslie Lamport, Robert E. Shostak, and Marshall C. Pease. 1982. The Byzantine Generals Problem. ACM Trans. Program. Lang. Syst. 4, 3 (1982), 382–401. https: //doi.org/10.1145/357172.357176 [38] James Mickens. 2014. The Saddest Moment. ;login: 39, 3 (2014). https://www. usenix.org/publications/login/june14/mickens [39] Andrew Miller, Yu Xia, Kyle Croman, Elaine Shi, and Dawn Song. 2016. The Honey Badger of BFT Protocols. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Vienna, Austria, October 24-28, 2016. 31–42. https://doi.org/10.1145/2976749.2978399 [40] Satoshi Nakamoto. 2008. Bitcoin: A Peer-to-Peer Electronic Cash System. https: //bitcoin.org/bitcoin.pdf. (2008). [41] Rafael Pass, Lior Seeman, and Abhi Shelat. 2017. Analysis of the Blockchain Protocol in Asynchronous Networks. In Advances in Cryptology - EUROCRYPT 2017 - 36th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Paris, France, April 30 - May 4, 2017, Proceedings, Part II. 643–673. https://doi.org/10.1007/978-3-319-56614-6_22 [42] Rafael Pass and Elaine Shi. 2018. Thunderella: Blockchains with Optimistic Instant Confirmation. In Advances in Cryptology - EUROCRYPT 2018 - 37th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Tel Aviv, Israel, April 29 - May 3, 2018 Proceedings, Part II. 3–33. https://doi.org/10.1007/978-3-319-78375-8_1 [43] Marshall C. Pease, Robert E. Shostak, and Leslie Lamport. 1980. Reaching Agreement in the Presence of Faults. J. ACM 27, 2 (1980), 228–234. https: //doi.org/10.1145/322186.322188 [44] HariGovind V. Ramasamy and Christian Cachin. 2005. Parsimonious Asynchronous Byzantine-Fault-Tolerant Atomic Broadcast. In Principles of Distributed Systems, 9th International Conference, OPODIS 2005, Pisa, Italy, December 12-14, 2005, Revised Selected Papers. 88–102. https://doi.org/10.1007/11795490_9 [45] Michael K. Reiter. 1994. The Rampart Toolkit for Building High-Integrity Services. In Theory and Practice in Distributed Systems, International Workshop, Dagstuhl Castle, Germany, September 5-9, 1994, Selected Papers. 99–110. https://doi.org/10. 1007/3-540-60042-6_7 [46] Phillip Rogaway and Thomas Shrimpton. 2004. Cryptographic Hash-Function Basics: Definitions, Implications, and Separations for Preimage Resistance, SecondPreimage Resistance, and Collision Resistance. In Fast Software Encryption, 11th International Workshop, FSE 2004, Delhi, India, February 5-7, 2004, Revised Papers (Lecture Notes in Computer Science), Bimal K. Roy and Willi Meier (Eds.), Vol. 3017. Springer, 371–388. https://doi.org/10.1007/978-3-540-25937-4_24 [47] Fred B. Schneider. 1990. Implementing Fault-Tolerant Services Using the State Machine Approach: A Tutorial. ACM Comput. Surv. 22, 4 (1990), 299–319. https: //doi.org/10.1145/98163.98167 [48] Victor Shoup. 2000. Practical Threshold Signatures. In Advances in Cryptology - EUROCRYPT 2000, International Conference on the Theory and Application of Cryptographic Techniques, Bruges, Belgium, May 14-18, 2000, Proceeding. 207–220. https://doi.org/10.1007/3-540-45539-6_15 [49] Yee Jiun Song and Robbert van Renesse. 2008. Bosco: One-Step Byzantine Asynchronous Consensus. In Distributed Computing, 22nd International Symposium, DISC 2008, Arcachon, France, September 22-24, 2008. Proceedings. 438–450. https://doi.org/10.1007/978-3-540-87779-0_30 [50] Maofan Yin, Dahlia Malkhi, Michael K. Reiter, Guy Golan Gueta, and Ittai Abraham. 2018. HotStuff: BFT Consensus in the Lens of Blockchain. CoRR abs/1803.05069 (2018). arXiv:1803.05069  white paper  May 2018  white paper  Abstract The Internet is a global network connecting individuals and entities around the world. In the 30 years since widespread use, the Internet has broken down barriers and distances in terms of information and commerce. In terms of information, where you were once confined to your local newspaper, your local broadcast news and your local library, you can now find information from every corner of the world. In terms of commerce, transaction costs have come down where, for example, once all your shopping was done locally, you can now buy from a number of online websites advertising products from all over the world at incredible discounts to physical stores. However, one area that has not reaped all the efficiency gains and reduced costs from the Internet is the travel industry. Yes, you can find hotel rooms and airfares more easily than before with the proliferation of comparison travel websites but the travel industry is still underpinned by a legacy cost structure that supports itself at your expense. From currency translation fees, credit card fees, service surcharges to the inherent costs of fiat money, the travel industry is rife with transaction inefficiencies that end up costing you money. Travel is expensive, but we are at the cusp of a revolution that will democratize travel and leisure for everyone. The Internet was the first part of the revolution. The other part is blockchain technology and cryptocurrencies. This white paper will detail how the introduction and adoption of HoweyCoins as a universal medium of exchange for travel will drive down costs in travel by:   Capturing the value and the inherent efficiencies of cryptocurrencies compared to the current fiat money structure that was created more in service of central governments and the banks that control the international financial system;    Eliminating the extra fees and expenses that underpin the current legacy cost structure in travel; and    Creating synergies between travel services providers—the airlines, hotels and restaurants—and forecasted returns on HoweyCoins valuations to drive initial discounts in the 30%-40% range (and the discounts will exponentially increase as the adoption and usage rate of HoweyCoins increase over time).  Getting in on the initial coin offering (ICO) for HoweyCoins is the only way to guarantee these massive cost savings. If you buy HoweyCoins later, the discounts will get smaller as:   More travel service providers and travelers adopt HoweyCoins as a medium of exchange; while    The finite amount of HoweyCoins limits supply in the face of ever increasing demand by travel service providers and travelers; and    The travel industry experiences exponential growth in the number of, and spending by, tourists because of a growing worldwide middle class, including in China and India.  www.HoweyCoins.com  1  white paper  Cryptocurrencies changing the monetary world The world is in the midst of a digital payments revolution that will cause greater disruption than that caused by the Internet over the last 30 years. Cryptocurrencies and other blockchain-driven digital tokens like HoweyCoins will piggyback on the ubiquitous communications of the Internet to transform how we pay for goods and services. Instead of currencies controlled by national governments, different types of cryptocurrencies and digital tokens associated with the types of services or goods you are looking to purchase will bridge the efficiency of the digital world with the traditional friction of a physical economy. HoweyCoins will be the cryptocurrency standard for the travel industry. By undercutting the monopoly role of the national governments in creating currency, savings that you may not realize are being kept by the elite and the government will flow to you—the consumer. Cryptocurrencies will democratize money. The current system is underpinned by a national government and a central bank legally requiring your use of legal tender to pay for goods and services. However, with the advent of Bitcoin and other cryptocurrencies over the last few years, fiat money is no longer the only universal medium of exchange for services and goods. Fiat money is the U.S. dollar, the British pound and any other currency created and declared by a central government as legal tender. Fiat money is highly controlled by the state and between how much is printed to the various fees and interest rates charged by the banks that control each step of the dissemination process, the government and the banks capture the vast majority of the value with the creation of fiat money. You are left with a small piece of the money pie. Take the financial system in the United States as an example. When the U.S. Treasury prints a dollar bill, it only costs the government fractions of a penny to actually make a dollar bill or even a US$20 bill. But when the U.S. Treasury then transfers that US$1 bill to the Federal Reserve, who statutorily controls the money supply, it books it at US$1. This is known as seigniorage and the U.S. government gets almost a whole US$1 profit for a piece of paper—the US$1 bill. Now that the US$1 is at the Federal Reserve, it charges a discount rate to banks who want to borrow that US$1 to get it out into the greater economy. The banks will then use that US$1 and loan it out to private individuals and companies charging interest at the prime rate or higher depending on the credit worthiness of the borrower. This can range from the current prime rate of 4.5% to the 20%+ charged if you are using a credit card. At every step, the U.S. government, the central banks and the private banks all get a profit off of just getting a US$1 bill into the private economy. When that US$1 bill gets to us, the private individuals and companies, we end up bearing all the cost for the dissemination of that US$1. All other fiat currencies in other countries get disseminated similarly to this example for the U.S. dollar. These costs continually increase as national governments continue printing money leading to inflation. Inflation is a defect of this system and is solely borne by us.  www.HoweyCoins.com  2  white paper  Cryptocurrencies and initial coin offerings are upending this monetary system and democratizing money for you and me. All the costs of fiat money and the associated inflation of prices will no longer underscore the price of everything we pay for. Instead, cryptocurrencies will stop inflation by being based only on a finite supply. There will be no costs to you and hidden profits to banks and governments. Blockchain is the key driver of cryptocurrencies. This technology leverages the Internet as a registry for all cryptocurrencies thereby undermining the central control that governments and banks have over the monetary system. HoweyCoins will spearhead this digital revolution into the travel industry by passing along the savings of cryptocurrencies to you. Because of its finite supply and dedicated use in the travel industry it will be the coin of the realm for travel. These savings will be passed on to you and the earlier you get in on the HoweyCoins ICO the greater the discounts are going to be.  www.HoweyCoins.com  3  white paper  HoweyCoins to deliver unsurpassed travel savings We have entered into a number of, and close to finalizing negotiations on just a few remaining, exclusive HoweyCoins partner agreements with a number of large, well-known travel service providers—international hotel and resort brands, airlines, travel industry-related comparison websites, car rental companies and ondemand online car services, large chain restaurants located in travel hotspots, and travel activity providers such as tour companies and equipment rentals (e.g., diving, bicycles, kayaks, etc.), among others. These negotiations and agreements are all currently subject to nondisclosure agreements in order for us to maintain our bargaining position with our negotiating partners and to prime a large media campaign upon a successful ICO. However, to provide some level of information, our hotel and resort partners encompass thousands of properties in every continent save Antarctica. At launch, we will have two of the world’s largest airline alliances and a possible third signed on as HoweyCoins partners. The top three airline alliances accounted for over 60% revenue passenger miles in 2015. In addition to the airline alliances, we have a top discount airline in each major travel market as a travel partner. Discount airlines do not typically participate in airline alliances. Upon a successful ICO, the nondisclosure agreements terminate and we will be able to disclose the largest network of travel partners assembled where you can use your HoweyCoins. We are planning to publicize HoweyCoins and our large network of travel partners with a worldwide media campaign in broadcast, print and physical display ads in airports and other travel hotspots. Buyers in our ICO will possess the first batch of HoweyCoins and a successful ICO along with the publicity of HoweyCoins and the travel network we have built will provide immediate returns in the valuation of HoweyCoins as more and more people learn of its great value. A large part of the proceeds from the ICO will fund this advertising and publicity campaign. Our travel partners are eager and excited at the tremendous opportunity being presented by HoweyCoin. Our HoweyCoins partner agreements lock in an average initial discount of 30% for airfare and 42% for hotel room rates for all HoweyCoin-denominated transactions. These discounts are available because of:   the savings being unleashed from bypassing the traditional fiat money system;    the savings generated from moving to a transaction-cost-free payment system in a high-transactionvolume business;    the guaranteed and dedicated customer base represented by HoweyCoins users;    the projected explosion in worldwide travelers coinciding with the introduction, adoption and usage of a finite supply of HoweyCoins; and    the guaranteed valuation increases for HoweyCoins that are being forecasted by the sophisticated finance teams of our various partners.  www.HoweyCoins.com  4  white paper  Not only will our travel partners realize savings by bypassing the current fiat money system, they will save all the fees and charges typically incurred when having to deal with banks in all their transactions with customers. Compared to some other businesses, the travel industry sees a very large number of credit card transactions on a daily basis with its customers—from the millions that fly daily and pay for WiFi, meals, or drinks on the flight to the millions that daily rent rooms for travel and order room service. Each of these transactions entails credit card surcharges, foreign exchange fees and regular bank transfer charges, among others. This adds up and can represent up to 10% of the value of each transaction. HoweyCoins free the travel partner from having to constantly incur all these fees and charges. Customers that use HoweyCoins mean additional savings to our travel partners making HoweyCoins very attractive to them. We have negotiated for the bulk of these discounts to be passed on to HoweyCoins users. In addition, as with other cryptocurrencies, the increasing adoption and usage of HoweyCoins is guaranteed to lead to higher valuations. This means it will become more valuable for participants in our ICO and our travel partners. A part of the generous discounts negotiated with our travel partners in the HoweyCoins partner agreements takes into account the guaranteed increase in valuation of the HoweyCoins that they will receive as payment for services and products. The sophisticated finance teams of our partners—including those at world leading airlines and hotel brands who have enormous experience negotiating multimillion dollar arrangements—have forecasted these increasing valuations when determining the exact discounts to HoweyCoins users for their products and services. The guaranteed increasing valuation in HoweyCoins is designed to incentivize our travel partners to offer generous discounts in order to attract HoweyCoins for payment and investment for themselves. In order to generate greater value in HoweyCoins over time, we have also built into each HoweyCoins partner agreement a contractual mechanism that increases the discount on services and products for users of HoweyCoins as the valuation of HoweyCoins increase. This incentivizes the travel partner to further seek HoweyCoins as payment in order to increase their holdings of HoweyCoins as its value increase. In exchange, some of this value to the travel partner gets translated to you as higher discounts.  www.HoweyCoins.com  5  white paper  Increasingly global world of travelers We are introducing HoweyCoins at a time of rapid growth in the global middle class who have more disposable income for travel. In addition, a global cultural change away from material goods to seeking out novel experiences is shifting money being spent on luxury goods to the travel industry. The introduction of HoweyCoins and the ICO is perfectly positioned to capture this explosive growth in travel. According to the United Nations World Tourism Organization, the current volume of international trips has grown exponentially since 1950. The organization estimates international tourist arrivals were 25 million in 1950. In 2016 this number was up to 1.24 billion and is expected to grow to 1.8 billion by 2030. International tourism receipts in 1950 were US$2.1 billion, had increased to US$919 billion by 2010 and reached US$1,260 billion in 2015.  China has far surpassed others in the number of tourists it sends, and the amount of money its tourists spend, in other countries. Spending by Chinese travelers increased by 26% in 2015 over just the prior year to reach US$292 billion, as the total number of outbound travelers rose by 10% to 128 million. China has registered double digit growth in tourism expenditure every year since 2004.  www.HoweyCoins.com  6  white paper  Little noticed is that India with a population soon to exceed that of China is not currently in the top 10 of countries sending tourists internationally. Over the next decade, with rising living standards and a growing middle class, as with the growth of Chinese tourism over the last decade, we expect India to soon join China in the top 10 of countries in terms of its tourists traveling and spending internationally. Coincidentally, cryptocurrencies have found a source of their greatest growth and adoption in China and India because of less entrenched traditional payment systems (other than cash). We expect tremendous opportunity and potential from travelers in China and India—the two fastest growing populations and economies in the world. This phenomenal growth in travel over the next decade guarantees an increasing valuation in HoweyCoins. HoweyCoins are of finite supply and getting in first on the ICO gets you a piece of this value that can only increase as demand increases for HoweyCoins. With more and more tourists from the growing middle classes in China and India, for example, and more Americans traveling globally each year using their disposable income to gain experiences rather than buying goods (hence the slowing growth in the retail sector), the discounts in travel afforded by HoweyCoins will only make it more attractive to more and more people.  www.HoweyCoins.com  7  white paper  The business traveler Business travel is an ever growing expense for large global Fortune 500 corporations in an increasingly globally connected world. Executives and employees traveling globally to offices, manufacturing plants and client locations results in travel expenditures that can reach into the millions of U.S. dollars. Consider the example of a world-leading petroleum company with worldwide drilling and exploration sites, refineries and office locations, and executives and engineers traveling back and forth. HoweyCoins can represent savings of millions of U.S. dollars. The HoweyCoins ICO will be limited to only individuals. However, following a six-month publicity and advertising campaign after the ICO letting the world know of the worldwide network of HoweyCoins travel partners we have created and the enormous discounts for using HoweyCoin, we plan on a second offering of HoweyCoins at an increased valuation to corporate travel departments as well as individuals. Instead of the preferential terms offered in the ICO, however, we expect to conduct this second offering as an auction. Because of the inclusion of corporate travel departments, we expect a bidding war to increase HoweyCoins valuations by severalfold. We are planning on offering 10x the number of HoweyCoins at this second offering compared to the ICO, but this second offering will be at a much higher valuation. The recent tax reforms in the United States have resulted in more investment capital available for U.S.-taxed corporations. Deployment of some of this additional capital to invest in guaranteed returns on HoweyCoins as well as to generate savings on the travel expenditures of its employees is a proposition no chief financial officer can pass up. Discussions with the finance teams of our travel partners analyzing HoweyCoins for partnering and discount arrangements have confirmed this.  Conclusion HoweyCoins are the cryptocurrency for the travel industry at exactly the right time. Between the confluence of:   technological advances in blockchain and cryptocurrencies that is revolutionizing the current legacy fiat money system;    to being on the cusp of exponential growth in the travel industry;    coupled with savings realized by our travel partners and passed along to HoweyCoins users; and    forecasted returns by the finance teams of our travel partners and expected severalfold increase in valuation upon a second offering to corporations;  your HoweyCoin’s ICO is a can’t miss investment opportunity.  www.HoweyCoins.com  8  The Libra Blockchain Zachary Amsden, Ramnik Arora, Shehar Bano, Mathieu Baudet, Sam Blackshear, Abhay Bothra, George Cabrera, Christian Catalini, Konstantinos Chalkias, Evan Cheng, Avery Ching, Andrey Chursin, George Danezis, Gerardo Di Giacomo, David L. Dill, Hui Ding, Nick Doudchenko, Victor Gao, Zhenhuan Gao, François Garillot, Michael Gorven, Philip Hayes, J. Mark Hou, Yuxuan Hu, Kevin Hurley, Kevin Lewi, Chunqi Li, Zekun Li, Dahlia Malkhi, Sonia Margulis, Ben Maurer, Payman Mohassel, Ladi de Naurois, Valeria Nikolaenko, Todd Nowacki, Oleksandr Orlov, Dmitri Perelman, Alistair Pott, Brett Proctor, Shaz Qadeer, Rain, Dario Russi, Bryan Schwab, Stephane Sezer, Alberto Sonnino, Herman Venter, Lei Wei, Nils Wernerfelt, Brandon Williams, Qinfan Wu, Xifan Yan, Tim Zakian, Runtian Zhou*  Abstract. The Libra Blockchain is a decentralized, programmable database designed to support a low-volatility cryptocurrency that will have the ability to serve as an efficient medium of exchange for billions of people around the world. We present a proposal for the Libra protocol, which implements the Libra Blockchain and aims to create a financial infrastructure that can foster innovation, lower barriers to entry, and improve access to financial services. To validate the design of the Libra protocol, we have built an open-source prototype implementation — Libra Core — in anticipation of a global collaborative effort to advance this new ecosystem. The Libra protocol allows a set of replicas — referred to as validators — from different authorities to jointly maintain a database of programmable resources. These resources are owned by different user accounts authenticated by public key cryptography and adhere to custom rules specified by the developers of these resources. Validators process transactions and interact with each other to reach consensus on the state of the database. Transactions are based on predefined and, in future versions, user-defined smart contracts in a new programming language called Move. We use Move to define the core mechanisms of the blockchain, such as the currency and validator membership. These core mechanisms enable the creation of a unique governance mechanism that builds on the stability and reputation of existing institutions in the early days but transitions to a fully open system over time.  1 Introduction The spread of the internet and resulting digitization of products and services have increased efficiency, lowered barriers to entry, and reduced costs across most industries. This connectivity has driven economic empowerment by enabling more people to access the financial ecosystem. Despite this progress, access to financial services is still limited for those who need it most — impacted by cost, reliability, and the ability to seamlessly send money. This paper presents a proposal for the Libra protocol, which supports the newly formed Libra ecosystem that seeks to address these challenges, expand access to capital, and serve as a platform for innovative financial services. This ecosystem will offer a new global currency — the Libra coin — which will be fully backed with a basket of bank deposits and treasuries from high-quality central ∗  The authors work at Calibra, a subsidiary of Facebook, Inc., and contribute this paper to the Libra Association under a Creative Commons Attribution 4.0 International License. For more information on the Libra ecosystem, please refer to the Libra white paper [1].  1  banks. All of these currencies experience relatively low inflation, and thus the coin mechanically inherits this property as well as the advantages of a geographically diversified portfolio of assets. The Libra protocol must scale to support the transaction volume necessary for this currency to grow into a global financial infrastructure and provide the flexibility to implement the economic and governance policies that support its operations. The Libra protocol is designed from the ground up to holistically address these requirements and build on the learnings from existing projects and research — a combination of novel approaches and well-understood techniques. A key prerequisite for healthy competition and innovation in financial services is the ability to rely on common infrastructure for processing transactions, maintaining accounts, and ensuring interoperability across services and organizations. By lowering barriers to entry and switching costs, the Libra protocol will enable startups and incumbents to compete on a level playing field, and experiment with new types of business models and financial applications. Blockchain technology lends itself well to address these issues because it can be used to ensure that no single entity has control over the ecosystem or can unilaterally shape its evolution to its advantage [2]. The Libra Blockchain will be decentralized, consisting of a collection of validators that work together to process transactions and maintain the state of the blockchain. These validators also form the membership of the Libra Association, which will provide a framework for the governance of the network and the reserve that backs the coin. Initially, the association (and validators) will consist of a geographically distributed and diverse set of Founding Members. These members are organizations chosen according to objective participation criteria, including that they have a stake in bootstrapping the Libra ecosystem and investing resources toward its success. Over time, membership eligibility will shift to become completely open and based only on the member’s holdings of Libra. The association has published reports outlining its vision [1], its proposed structure [3], the coin’s economics [4], and the roadmap for the shift toward a permissionless system [5]. This paper is the first step toward building a technical infrastructure to support the Libra ecosystem. We are publishing this early report to seek feedback from the community on the initial design, the plans for evolving the system, and the currently unresolved research challenges discussed in the proposal. Thus, the association has established an open-source community [6] for the discussion and development of the project. The Libra protocol. The Libra Blockchain is a cryptographically authenticated database [7, 8, 9] maintained using the Libra protocol. The database stores a ledger of programmable resources, such as Libra coins. A resource adheres to custom rules specified by its declaring module, which is also stored in the database. A resource is owned by an account that is authenticated using public key cryptography. An account could represent direct end users of the system as well as entities, such as custodial wallets, that act on behalf of their users. An account’s owner can sign transactions that operate on the resources held within the account. Figure 1 shows the two types of entities that interact using the Libra protocol: (1) validators, which maintain the database and (2) clients, which perform queries on the database and submit transactions to modify it. validators 1  client  leader  2  3 execution 5  3 execution 4  Figure 1: Overview of the Libra protocol.  2  other validators  Validators maintain the database and process transactions submitted by clients for inclusion in the database ( 1 ). The validators use a distributed consensus protocol to agree on an ever-growing list of transactions that have been committed to the database as well as the results of executing those transactions. This consensus protocol must be reliable even in the presence of malicious or erroneous behavior by a minority of validators. Validators take turns driving the process of accepting transactions. When a validator acts as a leader, it proposes transactions, both those directly submitted to it by clients and those indirectly submitted through other validators, to the other validators ( 2 ). All validators execute the transactions ( 3 ) and form an authenticated data structure that contains the new ledger history. The validators vote on the authenticator for this data structure as part of the consensus protocol ( 4 ). As part of committing a transaction Ti at version i, the consensus protocol outputs a signature on the full state of the database at version i — including its entire history — to authenticate responses to queries from clients ( 5 ). Clients can issue queries to a validator to read data from the database. Since the database is authenticated, clients can be assured of the accuracy of the response to their query. As part of the response to a read query, a validator returns a signed authenticator for the latest version i of the database known to the validator. In addition, a client can optionally create a replica of the entire database by synchronizing the transaction history from the validators. While creating a replica, a client can verify that validators executed transactions correctly, which increases accountability and transparency in the system. Other clients can read from a client that holds a replica in the same way they would read from a validator to verify the authenticity of the response. For the sake of simplicity, the rest of this paper assumes that clients query a validator directly rather than a replica. Organization. This paper discusses the components of the Libra protocol: • Logical Data Model (Section 2) describes the logical data model that organizes the decentralized database visible to validators and clients. • Executing Transactions (Section 3) describes the use of Move [10], a new programming language that is used to define and execute database transactions. • Authenticated Data Structures and Storage (Section 4) describes the mapping of the logical model into authenticated data structures based on Merkle trees [11]. • Byzantine Fault Tolerant Consensus (Section 5) describes the LibraBFT [12] variant of the HotStuff protocol [13], which allows a network with potentially malicious validators to maintain a single, consistent database by executing transactions with Move and coming to agreement on their execution using the authenticated data structures. • Networking (Section 6) describes the protocol that enables validators to communicate with each other securely, as required for consensus. Subsequently, we present the open-source Libra Core prototype [6]. Section 7 discusses how Libra Core combines the components of the Libra protocol to process a transaction. Section 8 discusses performance considerations. Finally, we explain how the protocol is being used to support the economic stability and governance policies of the Libra ecosystem. Section 9 shows how we use the Move language to implement the lowvolatility, reserve-backed Libra coin and a validator management system that mirrors the governance of the Libra Association. Section 10 concludes the paper with a discussion of future plans and ongoing challenges for the Libra ecosystem.  3  2 Logical Data Model All data in the Libra Blockchain is stored in a single versioned database [14, 15]. A version number is an unsigned 64-bit integer that corresponds to the number of transactions the system has executed. At each version i, the database contains a tuple (Ti , Oi , Si ) representing the transaction (Ti ), transaction output (Oi ), and ledger state (Si ). Given a deterministic execution function Apply, the meaning of the tuple is: executing transaction Ti against ledger state Si−1 produces output Oi and a new ledger state Si ; that is, Apply(Si−1 , Ti ) → ⟨Oi , Si ⟩. The Libra protocol uses the Move language to implement the deterministic execution function (see Section 3). In this section, we focus on the versioned database, which allows validators to: 1. Execute a transaction against the ledger state at the latest version. 2. Respond to client queries about the ledger history at both current and previous versions. We first explain the structure of the ledger state stored in a single version and then discuss the purpose of the versioned ledger history view.  2.1 Ledger State The ledger state represents the ground truth about the Libra ecosystem, including the quantity of Libra held by each user at a given version. Each validator must know the ledger state at the latest version in order to execute new transactions. The Libra protocol uses an account-based data model [16] to encode the ledger state. The state is structured as a key-value store, which maps account address keys to account values. An account value in the ledger state is a collection of published Move resources and modules. The Move resources store data values and modules store code. The initial set of accounts and their state are specified in the genesis ledger state (see Section 3.1). Account addresses. An account address is a 256-bit value. To create a new account, a user first generates a fresh verification/signature key-pair (vk, sk) for a signature scheme and uses the cryptographic hash of the public verification key vk as an account address a = H(vk).1 The new account is created in the ledger state when a transaction sent from an existing account invokes the create_account(a) Move instruction. This typically happens when a transaction attempts to send Libra to an account at address a that has not yet been created. Once the new account is created at a, the user can sign transactions to be sent from that account using the private signing key sk. The user can also rotate the key used to sign transactions from the account without changing its address, e.g., to proactively change the key or to respond to a possible compromise of the key. The Libra protocol does not link accounts to a real-world identity. A user is free to create multiple accounts by generating multiple key-pairs. Accounts controlled by the same user have no inherent link to each other. This scheme follows the example of Bitcoin and Ethereum in that it provides pseudonymity [19] for users. Resource values. A resource value, or resource, is a record that binds named fields to simple values — such as integers — or complex values — such as other resources embedded inside this resource.  1  Concretely we instantiate hash functions with SHA3-256 [17] and digital signatures with EdDSA using the edwards25519 curve [18].  4  Address  Account Content  0x12…  Currency.T  0x34…  Currency.T  StateChannel.T  0x56…  Currency  Currency.T  0x78…  StateChannel  Figure 2: An example ledger state with four accounts. In this diagram, ovals represent resources and rectangles represent modules. A directed edge from a resource to a module means that the type of the resource was declared by that module. The account at address 0x12 contains a Currency.T resource declared by the Currency module. The code for the Currency module is stored at address 0x56. The account at address 0x34 contains both a Currency.T resource and a StateChannel.T resource, which is declared by the module stored at address 0x78.  Every resource has a type declared by a module. Resource types are nominal types [20] that consist of the name of the type and the name and address of the resource’s declaring module. For example, the type of the Currency.T resource in Figure 2 is 0x56.Currency.T. Here, 0x56 is the address where the Currency module is stored, Currency is the name of the module, and Currency.T is the name of the resource. To retrieve the resource 0x56.Currency.T under account address 0x12, a client would request 0x12/resources/0x56.Currency.T. The purpose of this design is to let modules define a predictable schema for top-level account values — that is, every account stores its 0x56.Currency.T resource under the same path. As such, each account can store at most one resource of a given type. However, this limitation is not restrictive, since programmers can define wrapper resources that organize resources in a custom way (e.g., resource TwoCoin { c1: 0x56.Currency.T, c2: 0x56.Currency.T }). The rules for mutating, deleting, and publishing a resource are encoded in the module that created the resource and declared its type. Move’s safety and verification rules prevent other code from making modifications to the resource. Module values. A module value, or module, contains Move bytecode that declares resource types and procedures (see Section 3.4 for more details). Like a resource type, a module is identified by the address of the account where the module is declared. For example, the identifier for the Currency module in Figure 2 is 0x56.Currency. A module must be uniquely named within an account — each account can declare at most one module with a given name. For example, the account at address 0x56 in Figure 2 could not declare another module named Currency. On the other hand, the account at address 0x34 could declare a module named Currency. The identifier of this module would be 0x34.Currency. Note that 0x56.Currency.T and 0x34.Currency.T are distinct types and cannot be used interchangeably.  5  In the current version of the Libra protocol, modules are immutable. Once a module has been declared under an account address, it cannot be modified or deleted, except via a hard fork. We are researching options for a scheme to enable safe module updates in future versions.  2.2 Transactions Clients of the Libra Blockchain update the ledger state by submitting transactions. At a high level, a transaction consists of a transaction script (written in Move bytecode) and arguments to the transaction script (e.g., a recipient account address or the number of Libra to send). A validator executes the transaction by running the script with its arguments and the current ledger state as input to produce a completely deterministic transaction output. The ledger state is not changed until the transaction is committed during consensus (Section 5) by agreeing on a binding commitment (Section 4) to the output of the transaction. We discuss the structure and execution of transactions in more detail in Section 3. Transaction output. Executing a transaction Ti produces a new ledger state Si as well as the execution status code, gas usage, and event list (aggregated in output Oi ). The execution status code records the result of executing a transaction (e.g., success, exited with an error e, ran out of gas, etc.), and the gas usage records the number of gas units used in executing the transaction (see Section 3.1 for the details of how gas is used to manage the fee associated with transaction processing). Events. The event list is a set of side effects produced by executing the transaction.2 Move code can trigger an event emission through an event structure. Each event is associated with a unique key, which identifies the structure through which the event was emitted, and a payload, which provides detailed information about the event. Once a transaction has been committed by the consensus protocol, events generated by the transaction are added to the agreed ledger history and provide evidence that the successful execution of a transaction resulted in a specific effect. For example, a payment transaction emits an event that allows the recipient to confirm that a payment has been received and confirm the amount of the payment. At first glance, events might seem redundant. Instead of querying for the events emitted by a transaction Ti , a client could ask whether the transaction Ti has been included in the blockchain. However, this is error-prone because the inclusion of Ti does not imply successful execution (e.g., it might be interrupted after running out of gas). In a system where transactions can fail, an event provides evidence not only that a particular transaction has executed but also that it had successfully completed with the intended effect. Transactions can only generate events — they cannot read events. This design allows transaction execution to be a function only of the current state, not historical information, such as previously generated events.  2.3 Ledger History The ledger history stores the sequence of committed and executed transactions as well as the associated events they emitted. The purpose of the ledger history is to keep a record of how the latest ledger state was computed. There is no concept of a block of transactions in the ledger history.3 The consensus protocol batches transactions into blocks as an optimization and to drive the consensus 2  Events serve a purpose similar to the concept of logs and events in Ethereum [16]. However, the mechanics of events in the Libra protocol are quite different. 3 This is in contrast with Bitcoin and Ethereum, in which the concept of a block and the maximum size of a block play important roles.  6  protocol (see Section 5 for details on consensus). However, in the logical data model, the transactions occur in sequence without distinction as to which block contained each transaction. Although a validator does not need to know the ledger history to execute new transactions, the client can perform authenticated queries against the ledger history and use the ledger history for auditing the transaction execution. Responding to client queries. Validators can use the ledger history to answer client queries about previous ledger states, transactions, and outputs. For example, a client might ask about the ledger state at a specific version (e.g., “What was the balance in the account at address x at version 30?”) or the history of events of a certain type (e.g., “What payments did the account at address y receive in the last 20 minutes?”). Auditing transaction execution. A client can check that the ledger state is correct by re-executing each transaction Ti in the history and comparing the computed ledger state to the corresponding ledger state Si and transaction output Oi in the versioned database. This mechanism allows clients to audit the validators to ensure that transactions are being executed correctly.  3 Executing Transactions In the Libra protocol, the only way to change the blockchain state is by executing a transaction. This section outlines the requirements for transaction execution, defines the structure of transactions, explains how the Move virtual machine (VM) executes a transaction, and describes the key concepts of the Move language. In the initial version of the Libra protocol, only a limited subset of Move’s functionality is available to users. While Move is used to define core system concepts, such as the Libra currency, users are unable to publish custom modules that declare their own resource types. This approach allows the Move language and toolchain to mature — informed by the experience in implementing the core system components — before being exposed to users. The approach also defers scalability challenges in transaction execution and data storage that are inherent to a general-purpose smart contract platform. As we discuss in Section 10, we are committed to exposing the full programmability supported by the Move language.  3.1 Execution Requirements Known initial state. All validators must agree on the initial, or genesis, ledger state of the system. Because the core components of the blockchain — such as the logic of accounts, transaction validation, validator selection, and Libra coins — are defined as Move modules, the genesis state must define these modules. The genesis state must also have sufficient instantiations of these core components so that transactions can be processed (e.g., at least one account must be able to pay fees for the first transaction; a validator set must be defined so a quorum of the set can sign the authenticator committing to the first transaction). To simplify the design of the system, the initial state of the system is represented as an empty state. The genesis state is then created through a special transaction T0 that defines specific modules and resources to be created, rather than going through the normal transaction process. Clients and validators are configured to accept only ledger histories beginning with a specific T0 , which is  7  identified by its cryptographic hash. These special transactions cannot be added to the ledger history through the consensus protocol, only through configuration.4 Deterministic. Transaction execution must be deterministic and hermetic. This means that the output of transaction execution is completely predictable and based only on the information contained within the transaction and current ledger state. Transaction execution does not have external effects (e.g., printing to the console or interacting with the network). Deterministic and hermetic execution ensures that multiple validators can agree on the state resulting from the same sequence of transactions even though transactions are executed independently by each validator. It also means that the transaction history of the blockchain can be re-executed from the genesis block onwards to produce the current ledger state (see Section 2.3). Metered. In order to manage demand for compute capacity, the Libra protocol charges transaction fees, denominated in Libra coins.5 This follows the gas model popularized by Ethereum [16]. We take the approach of selecting validators with sufficient capacity to meet the needs of the Libra ecosystem (see Section 8). The only intention of this fee is to reduce demand when the system is under a higher load than it was provisioned for (e.g., due to a denial-of-service attack). The system is designed to have low fees during normal operation, when sufficient capacity is available. This approach differs from some existing blockchains, which target validators with lower capacity and thus at times have more demand to process transactions than throughput. In these systems, fees spike during periods of high demand — representing a revenue source for the validators but a cost for the users. The size of the fee is determined by two factors: gas price and gas cost. Each transaction specifies a price in Libra per unit of gas that the submitter is willing to pay. The execution of a transaction dynamically accounts for the computational power it expends in the form of a gas cost. Validators prioritize executing transactions with higher gas prices and may drop transactions with low prices when the system is congested. This reduces the demand for transactions under high load. In addition, a transaction includes a maximum gas amount, which specifies the maximum number of gas units that the submitter is willing to pay for at the specified price. During execution, the VM tracks the number of gas units used. If the maximum gas limit is reached before execution completes, the VM halts immediately. None of the partial changes resulting from such a transaction are committed to the state. However, the transaction still appears in the transaction history, and the sender account is charged for the gas used. As we discuss in this section, many parts of the core logic of the blockchain are defined using Move, including the deduction of gas fees. To avoid circularity, the VM disables the metering of gas during the execution of these core components. These core components must be defined in the genesis state and must be written defensively to prevent malicious transactions from triggering the execution of computationally expensive code. The cost of executing this logic can be included in the base fee charged for the transaction. Asset semantics. As we explain in Section 2.1, the ledger state directly encodes digital assets with real-world value. Transaction execution must ensure that assets such as Libra coins are not duplicated, lost, or transferred without authorization. The Libra protocol uses the Move virtual machine (Section 3.4) to implement transactions and custom assets with these properties safely.  4  We are exploring the idea of using special transactions of this form to define hard forks cleanly. Clients could specify a preferred configuration, stating that a similar special transaction — which makes specific changes to modules and resources outside the standard transaction processing rules — should be appended to the ledger history at version i. Since many parts of the Libra protocol are expressed using Move, updates of this form can be expressive yet would only require a configuration change to client software. 5 In Section 4.4, we discuss approaches to managing demand for storage capacity.  8  3.2 Transaction Structure A transaction is a signed message containing the following data: • Sender address: The account address of the transaction sender. The VM reads the sequence number, authentication key, and balance from the LibraAccount.T resource stored under this address. • Sender public key: The public key that corresponds to the private key used to sign the transaction. The hash of this public key must match the authentication key stored under the sender’s LibraAccount.T resource. • Program: A Move bytecode transaction script to execute, an optional list of inputs to the script, and an optional list of Move bytecode modules to publish. • Gas price: The number of Libra coins that the sender is willing to pay per unit of gas in order to execute this transaction. • Maximum gas amount: The maximum number of gas units that the transaction is allowed to consume before halting. • Sequence number: An unsigned integer that must be equal to the sequence number from the sender’s LibraAccount.T resource. After this transaction executes, the sequence number is incremented by one. Since only one transaction can be committed for a given sequence number, transactions cannot be replayed.  3.3 Executing Transactions Executing a transaction proceeds through a sequence of six steps inside the VM. Execution is separate from the update of the ledger state. First, a transaction is executed as part of an attempt to reach agreement on its sequencing. Since the execution is hermetic, this can be done without causing external side effects. Subsequently, if agreement is reached, its output is written to the ledger history. Executing a transaction performs the following six steps: (1) Check signature. The signature on the transaction must match the sender’s public key and the transaction data. This step is a function only of the transaction itself — it does not require reading any data from the sender’s account. (2) Run prologue. The prologue authenticates the transaction sender, ensures that the sender has sufficient Libra coin to pay for the maximum number of gas units specified in the transaction, and checks that the transaction is not a replay of a previous transaction. All of these checks are implemented in Move via the prologue procedure of the LibraAccount module. Gas metering is disabled during the execution of the prologue. Specifically, the prologue does the following: • Checks that the hash of the sender’s public key is equal to the authentication key stored under the sender’s account. Without this check, the VM would erroneously accept a transaction with a cryptographically valid signature even though there is no correspondence to the key that is associated with the account. • Checks that gas_price * max_gas_amount <= sender_account_balance. Without this check, the VM would execute transactions that may fail in the epilogue because they would be unable to pay for gas. • Ensure that the transaction sequence number is equal to the sequence number stored under the user’s account. Without this check, an adversary could replay old transactions (e.g., Bob could replay a transaction from Alice that paid him ten Libra coins).  9  (3) Verify transaction script and modules. Once the transaction prologue has completed successfully, the VM performs well-formedness checks on the transaction script and modules using the Move bytecode verifier. Before actually running or publishing any Move code, the bytecode verifier checks crucial properties like type-safety, reference-safety (i.e., no dangling references), and resource-safety (i.e., resources are not duplicated, reused, or inadvertently destroyed). (4) Publish modules. Each module in the program field of the transaction is published under the transaction sender’s account. Duplicate module names are prohibited — for example, if the transaction attempts to publish a module named M to an account that already contains a module named M, the step will fail. (5) Run transaction script. The VM binds the transaction arguments to the formal parameters of the transaction script and executes it. If this script execution completes successfully, the write operations performed by the script and the events emitted by the script are committed to the global state. If the script execution fails (e.g., due to having run out of gas or a runtime execution failure), no changes from the script are committed to the global state. (6) Run epilogue. Finally, the VM runs the transaction epilogue to charge the user for the gas used and increment the sender’s account sequence number. Like the prologue, the transaction epilogue is a procedure of the Move LibraAccount module and runs with gas metering disabled. The epilogue is always run if execution advances beyond step (2), including when steps (3), (4), or (5) fail. The prologue and the epilogue work together to ensure that all transactions accepted in the ledger history are charged for gas. Transactions that do not proceed beyond step (2) are not appended to the ledger history. The fact that these transactions were considered for execution is never recorded. If a transaction advances past step (2), the prologue has ensured that the account has enough Libra coins to pay for the maximum number of gas units allowed for the transaction. Even if the transaction runs out of gas, the epilogue is able to charge it for this maximum amount.  3.4 The Move Programming Language As we have seen, a transaction is an authenticated wrapper around a Move bytecode program. Move [10] is a new programming language created during the design of the Libra protocol. Move has three important roles in the system: 1. To enable flexible transactions via transaction scripts. 2. To allow user-defined code and datatypes, including “smart contracts” via modules. 3. To support configuration and extensibility of the Libra protocol (see Section 9). The key feature of Move is the ability to define custom resource types, which have semantics inspired by linear logic [21]. Resource types are used to encode programmable assets that behave like ordinary program values: resources can be stored in data structures, passed as arguments to procedures, and so on. However, the Move type system provides special safety guarantees for resources. A resource can never be copied, only moved. In addition, a resource type can only be created or destroyed by the module that declares the type. These guarantees are enforced statically by the Move VM. This allows us to represent Libra coins as a resource type in the Move language (in contrast to Ether and Bitcoin, which have a special status in their respective languages). We have published a separate, more detailed report on the design of Move [10]. In the remainder of this section, we give a brief overview of the key Move concepts for transaction execution: developing Move transaction scripts and modules and executing Move bytecode with the virtual machine.  10  Writing Move programs. There are three different representations of a Move program: source code, intermediate representation (IR), and bytecode. We are currently in the process of designing the Move source language, which will be an ergonomic language designed to make it easy to write and verify safe code. In the meantime, programmers can develop modules and transaction scripts in Move IR. Move IR is high level enough to write human-readable code, yet low level enough to directly translate to Move bytecode. Both the future Move source language and the Move IR are compiled into Move bytecode, which is the format used by the Libra protocol. We use a verifiable bytecode as the executable representation of Move for two reasons: • The safety guarantees described above must apply to all Move programs. Enforcing these guarantees in the compiler is not enough. An adversary could always choose to bypass the compiler by writing malicious code directly using the bytecode (running a compiler as part of transaction execution would make execution slower and more complex and would require validators trusting the correctness of the full compiler code base). Thus, the protocol avoids trusting the compiler by enforcing all of Move’s safety guarantees via bytecode verification: type-safety, reference-safety, and resource-safety. • Move’s stack-based bytecode has fewer instructions than a higher-level source language would. In addition, each instruction has simple semantics that can be expressed via an even smaller number of atomic steps. This reduces the specification footprint of the Libra protocol and makes it easier to spot implementation mistakes. Transaction scripts. A transaction script is the main procedure of the Libra protocol. Transaction scripts enable flexible transactions because a script is an arbitrary Move bytecode program. A script can invoke multiple procedures of modules published in the ledger state, use conditional logic, and perform local computation. This means that scripts can perform expressive one-off actions, such as paying a specific set of recipients. We expect that most transaction scripts will perform a single procedure call that wraps generic functionality. For example, the LibraAccount.pay_from_sender(recipient_address, amount) procedure encapsulates the logic for performing a peer-to-peer payment. A transaction script that takes recipient_address and amount as arguments and invokes this procedure is the same as an Ether transfer transaction in Ethereum [16]. Modules. A module is a code unit published in the ledger state. A module declares both struct types and procedures. A struct value contains data fields that may hold primitive values, such as integers or other struct values. Each struct must be tagged as either resource or unrestricted (i.e., non-resource). Unrestricted structs are not subject to the restrictions on copying and destruction described above. However, unrestricted structs cannot contain resource structs (either directly or transitively) and cannot be published under an account in the ledger state. At a high level, the module/struct/procedure relationship is similar to the class/object/method relationship in object-oriented programming. However, there are important differences. A module may declare multiple struct types (or zero struct types). No data field can be accessed outside of its declaring module (i.e., no public fields). The procedures of a module are static procedures rather than instance methods; there is no concept of this or self in a procedure. Move modules are similar to a limited version of ML-style modules [22] without higher-order functions. Move modules are related to, but not the same as, the concept of “smart contracts” in Ethereum and other blockchain platforms. An Ethereum smart contract contains both code and data published in the ledger state. In Libra, modules contain code values, and resources contain data values. In object-oriented terms, an Ethereum smart contract is like a singleton object published under a single  11  account address. A module is a recipe for creating resources, but it can create an arbitrary number of resources that can be published under different account addresses. The Move virtual machine. The Move virtual machine implements a verifier and an interpreter for the Move bytecode. The bytecode targets a stack-based virtual machine with a procedure-local operand stack and registers. Unstructured control-flow is encoded via gotos and labels. A developer writes a transaction script or module in Move IR that is then compiled into the Move bytecode. Compilation converts structured control-flow constructs (e.g., conditionals, loops) into unstructured control-flow and converts complex expressions into a small number of bytecode instructions that manipulate an operand stack. The Move VM executes a transaction by verifying then running this bytecode. The Move VM supports a small number of types and values: booleans, unsigned 64-bit integers, 256-bit addresses, fixed-size byte arrays, structs (including resources), and references. Struct fields cannot be reference types, which prevents the storage of references in the ledger state. The Move VM does not have a heap — local data is allocated on the stack and freed when the allocating procedure returns. All persistent data must be stored in the ledger state.  4 Authenticated Data Structures and Storage After executing a transaction, a validator translates the changes to the logical data model into a new version of an authenticated data structure [7, 8, 9] used to represent the database. The short authenticator of this data structure is a binding commitment to a ledger history, which includes the newly executed transaction. Like transaction execution, the generation of this data structure is deterministic. The consensus protocol uses this authenticator to agree on an ordering of transactions and their resulting execution (we discuss consensus in detail in Section 5). As part of committing a block of transactions, validators collectively sign the short authenticator to the new version of the resulting database. Using this collective signature, clients can trust that a database version represents the full, valid, and irreversible state of the database’s ledger history. Clients can query any validator (or a third-party replica of the database) to read a specific database value and verify the result using the authenticator and a short proof. Therefore, clients do not need to trust the party that executes the query for the correctness of the resulting read. Data structures in the Libra protocol are based on Merkle trees and inspired by those of other blockchains; however, in several cases, we have made slightly different decisions which we highlight below. First, we briefly discuss the Merkle tree approach to creating an authenticator. We then describe the authenticated data structure, starting from the root of the data structure, then we discuss the substructures within it. Figure 3 depicts the data structure and provides a visual guide to this section.  4.1 Background on Authenticated Data Structures An authenticated data structure allows a verifier V to hold a short authenticator a, which forms a binding commitment to a larger data structure D. An untrusted prover P , which holds D, computes f (D) → r and returns both r — the result of the computation of some function f on D — as well as π — a proof of the correct computation of the result — to the verifier. V can run Verify(a, f, r, π), which  12  2 Validator Signatures 1  Ledger History: Merkle Tree Accumulator  TransactionInfoi  3  Ledger State ( Si ): Sparse Merkle Tree  4  Signed Transaction ( Ti )  5  Event Tree ( Ei ): Merkle Tree  6 Account Blob  7  /accesspath/1: value1 /accesspath/2: value2  Figure 3: ( 1 ) The root hash of the ledger history structure is the authenticator to the full state of the system that is ( 2 ) signed by a quorum of validators. As transactions are added to the database, the authenticator (Section 4.2) committing to the ledger history grows (depicted with dashed arrows). Each leaf of the ledger history commits to a ( 3 ) TransactionInfo structure. This structure commits to the ( 4 ) signed transaction (Ti ), ( 5 ) the list of events generated during that transaction (Ei , Section 4.5), and the ( 6 ) state after the execution of that transaction (Si , Section 4.3). The state is a sparse Merkle tree with an ( 7 ) account blob at each leaf.  returns true if and only if f (D) = r. In the context of the Libra Blockchain, provers are generally validators and verifiers are clients executing read queries. However, clients — even those with only a partial copy of the database — can also serve as a prover and perform authenticated read queries for other clients. a=H(h4||h5) h4=H(h0||h1) h0=H(0||s0)  h5=H(h2||h3)  h1=H(1||s1)  h2=H(2||s2)  h3=H(3||s3)  Figure 4: A Merkle tree storing D = {0 : s0, . . .}. If f is a function that gets the third item (shown with a dashed line) then r = s2 and π = [h3, h4] (these nodes are shown with a dotted line). Verify(a, f, r, π) ?  verifies that a = H(h4∥H(H(2∥r)∥h3)).  Merkle trees [11] are a common form of authenticated data structure, used to store maps between integers and string values. In a Merkle tree of size 2k , the structure D maps every integer key i ∈ [0, 2k ) to a string value si . The authenticator is formed from the root of a full binary tree created from the strings, labeling leaves as H(i∥si ) and internal nodes as H(left∥right), where H is a cryptographic hash function (which we will refer to as a hash).6 The function f , which the prover wishes to authenticate, is an inclusion proof that a key-value pair (k, v) is within the map D. P authenticates lookups for an item i in D by returning a proof π that consists of the labels of the sibling of each of the ancestors of node i. Figure 4 shows a lookup for item three in a Merkle tree of size four. Item three’s node is shown with a dotted line, and the nodes included in π are shown with a dashed line. 6  Secure Merkle trees must use different hash functions to hash the leaves and internal nodes to avoid confusion between the two types of nodes. While we have omitted this detail in the example for simplicity, the Libra protocol uses a unique hash function to distinguish between different hash function types to avoid attacks based on type confusion [23].  13  4.2 Ledger History Most blockchains, starting with Bitcoin [24], maintain a linked list of each block of transactions agreed on by the consensus protocol with a block containing the hash of a single ancestor. This structure leads to inefficiencies for clients. For example, a client that trusts some block B and wants to verify information in an ancestor block B ′ needs to fetch and process all intermediate ancestors. The Libra protocol uses a single Merkle tree to provide an authenticated data structure for the ledger history. Figure 3 illustrates the full data structure that underpins the Libra database, including the ledger history that contains TransactionInfoi records, which, in turn, contain information about database states, events, and accounts. The ledger history is represented as a Merkle tree, mapping a sequential database version number i to a TransactionInfoi structure. Each TransactionInfoi structure contains a signed transaction (Ti ), the authenticator for the state after the execution of Ti (Si , discussed in Section 4.3), and the authenticator for the events generated by Ti (Ei , discussed in Section 4.5). Like other Merkle trees, the ledger history supports O(log n)-sized proofs — where n is the total number of transactions processed — to authenticate the lookup of a specific TransactionInfoi . When a client wishes to query the state of version i or lookup an event generated in version i, it performs an authenticated lookup of TransactionInfoi along with an authenticated lookup using the contained state or event list authenticator. Specifically, the ledger history uses the Merkle tree accumulator [25, 26] approach to form Merkle trees, which also provides efficient append operations. This operation is useful, as a ledger history can be incrementally computed by appending new transactions to an old ledger history. Merkle tree accumulators can also generate an efficient proof that, given an authenticator a committing to the ledger info up to transaction i, an authenticator a′ committing to the ledger info up to j > i has an identical history up to transaction i — in other words, proving that a is a prefix of a′ . These proofs allow efficient verification that one ledger history commitment is a continuation of another. Pruning Storage. The root hash of the ledger history Merkle accumulator is an authenticator for the full state of the system. It commits to the state at every existing version of the system, every transaction ever sent, and every event ever generated. While the state storage described in Section 4.3 allows efficient storage of multiple versions of the ledger state, validators may wish to reduce the space consumed by past versions. Validators are free to prune old states as they are not necessary for the processing of new transactions. Merkle tree accumulators support the pruning of historical data, requiring only O(log n) storage to append new records [25]. Any replica of the system is free to retain the entire state and can trace the authenticity of their data to the root hash signed by the consensus protocol. Replicas are more amenable to scaling than validators. Replicas do not need to be secured as they do not participate in consensus, and multiple replicas can be created to handle read queries in parallel.  4.3 Ledger State A ledger state Si represents the state of all accounts at version i as a map of key-value pairs. Keys are based on the 256-bit account addresses, and their corresponding value is the authenticator of the account (discussed in Section 4.4).7 Using a representation similar to Figure 4, the map is represented as a Merkle tree of size 2256 . 7  The hash of the address is used as the key. Even though addresses are hashes of the public key, an adversary could create a new account at an address that is a near collision with a real address. While no transactions can be sent from this account because the private key corresponding to its address is unknown, the existence of this near-collision in the tree increases the proof length for the nearby address. Hashing the address before using it as a key reduces the impact of this type of attack.  14  1 0  0  0  1  0  1  0  A  1  0  1  0  1  0  1  1  B  0  0  1  0  1  0  0  1  C  3  A  1  0  1  1  0  1  0 0  1  0  1  1  0  2  1  0  1  1  0  0  B  1  1  0  1  C  1  0  A  0 0  B  1  1  C  Figure 5: Three versions of a sparse Merkle tree storing D = {01002 : A, 10002 : B, 10112 : C}.  While a tree of size 2256 is an intractable representation, optimizations can be applied to form a sparse Merkle tree. Figure 5 shows two optimizations that are applied to transform a naive implementation ( 1 ) into an efficient one. First, subtrees that consist entirely of empty nodes are replaced with a placeholder value ( 2 ) as used in the certificate transparency system [27]. This optimization creates a representation of a tractable size without substantially changing the proof generation of the Merkle tree. However, leaves are always stored at the bottom level of the tree, meaning that the structure requires 256 hashes to be computed on every leaf modification. A second optimization replaces subtrees consisting of exactly one leaf with a single node ( 3 ). In expectation, the depth of any given item is O(log n), where n is the number of items in the tree. This optimization reduces the number of hashes to be computed when performing operations on the map. Efficient implementations, such as Libra Core, can optimize their disk layout and batch layers of nodes to avoid seeks during lookup.8 When the sparse Merkle tree is updated after the execution of a transaction, the new tree reuses unchanged portions of the previous version, forming a persistent data structure [28, 29]. If a transaction modifies m accounts out of n accounts in the system, on average O(m · log n) new branches and leaves are created in the new ledger state tree that differ from the previous version. This approach allows validators to store multiple versions of the ledger state efficiently. This feature also allows the efficient recomputation of the ledger state authenticator after processing a transaction. We considered AVL-based trees, which provide more optimal worst-case proof length than sparse Merkle trees [30]. However, the sparse Merkle tree approach allows implementers to make optimizations, such as sharding storage across servers and parallelizing root hash computation.  4.4 Accounts At the logical level, an account is a collection of resources and modules stored under the account address. At the physical level, an account is treated as an ordered map of access paths to byte array values. An access path is a delimited string similar to a path in a file system. In the first iteration of the protocol, we serialize an account as a list of access paths and values sorted by access path. The authenticator of an account is the hash of this serialized representation. Note that this representation requires recomputing the authenticator over the full account after any 8  This optimization gives similar on-disk performance to the 16-ary Patricia Merkle tree approach used in Ethereum but with shorter proofs.  15  modification to the account. The cost of this operation is O(n), where n is the length of the byte representation of the full account. Furthermore, reads from clients require the full account information to authenticate any specific value within it. Our strategy for storing data within accounts differs from other systems such as Ethereum.9 Our approach allows the Move language to provide better programming abstractions by representing accounts as ordered maps of access paths to values. This representation allows Move to efficiently manage the conservation of resources through the VM. Move encourages each user to hold resources in their own account. In the initial release of Libra, we optimize for small accounts. In future releases, we may consider supporting more efficient structures to represent larger accounts, such as Merkle AVL trees [30]. Account Eviction and Recaching. We anticipate that as the system is used, eventually storage growth associated with accounts may become a problem. Just as gas encourages responsible use of computation resources (see Section 3.1), we expect that a similar rent-based mechanism may be needed for storage. We are assessing a wide range of approaches for a rent-based mechanism that best suits the ecosystem. We discuss one option that can be applied to any policy that determines an expiration time after which data can be evicted. After each transaction execution affecting an account, the VM computes the logical expiration time at which the storage allocated for an account can be freed. The VM is free to apply any deterministic approach to determine the expiration time. One example of such a policy would be to charge a fee denominated in Libra coins that is based on the amount of time the account is stored and its size. The account’s authenticator is represented as H(H(AccountBlob)∥ExpirationTime), which encodes the expiration time for the account. After expiration, the VM denies access to the account, raising an error. Since the AccountBlob is inaccessible, a validator is free to prune this data after ExpirationTime. However, a validator allows a transaction to reactivate the account by recaching its contents after the expiration time. The transaction must contain the preimage of the AccountBlob and have an associated transaction fee that covers the cost of further storage. The authenticity of the recached contents is ensured through recomputing and checking the hash value associated with the account, which is not deleted. This method to implement rent is an improvement on existing account eviction schemes, which require a third party to send a transaction to delete the storage of the account.  4.5 Events Ei is the list of events emitted during the execution of Ti . Each event is stored as a leaf in a Merkle tree that forms an authenticator for Ei . An event is serialized as a tuple of the form (A, p, c), which represents the access path of the event structure that emitted the event (A), the data payload (p), and the counter value (c). These tuples are indexed by the order j in which the events were emitted during the execution of Ti . By convention, we denote an event j → (A, p, c) being included in Ei as (j, (A, p, c)) ∈ Ei . The authenticator for Ei is included in TransactionInfoi . Thus, a validator can construct an inclusion proof that within the ith transaction the j th event was (A, p, c). The counter c, included in each event, plays a special role in allowing clients to retrieve a list of events for a given access path A. Clients can also be assured that the list is complete. An event structure representing events with access path A within the Move language maintains a counter C for the total number of events it has emitted. This counter is stored within the ledger state and incremented after each transaction execution emits an event on access path A. 9  In contrast, Ethereum [16] stores data as a sparse memory map using a Merkle tree that represents an unordered map of 256-bit keys to 256-bit values. This approach allows Ethereum to handle large accounts efficiently. This design matches the standard practice within that ecosystem, which has accounts that manage assets on behalf of many users.  16  A client can obtain an authenticator for a recent ledger state, query the counter associated with the event structure for access path A, and retrieve the total number of events C. Then the client can query an untrusted service for the list of events on access path A. The query response consists of a set of tuples (i, j, A, p, c) — one corresponding to each event — where i is the transaction sequence number emitting the event and j is the order of the event emission within this transaction. Associated proofs of inclusion for (j, (A, p, c)) ∈ Ei can be provided for each event. Since the client knows the total number of events emitted by access path A, they can ensure that the untrusted service has provided this number of distinct events and also order them by their sequence number 0 ≤ c < C. This convinces the client that the list of events returned for A is complete. This scheme allows the client to hold an authenticated subscription to events on access path A. The client can periodically poll the total counter C for event access path A to determine if the subscription is up-to-date. For example, a client can use this to maintain a subscription to incoming payment transactions on an address it is watching. Untrusted third-party services can provide feeds for events indexed by access path, and clients can efficiently verify the results they return.  5 Byzantine Fault Tolerant Consensus The consensus protocol allows a set of validators to create the logical appearance of a single database. The consensus protocol replicates submitted transactions among the validators, executes potential transactions against the current database, and then agrees on a binding commitment to the ordering of transactions and resulting execution. As a result, all validators can maintain an identical database for a given version number following the state machine replication paradigm [31]. The Libra Blockchain uses a variant of the HotStuff [13] consensus protocol called LibraBFT. It provides safety and liveness in the partial synchrony model in the tradition of DLS [32] and PBFT [33] as well as the newer Casper [34] and Tendermint [35]. This section outlines the key decisions in LibraBFT. A more detailed analysis, including proofs of safety and liveness, is covered in the full report on LibraBFT [12]. Agreement on the database state must be reached between validators, even if there are Byzantine faults [36]. The Byzantine failures model allows some validators to arbitrarily deviate from the protocol without constraint, except for being computationally bounded (and thus not able to break cryptographic assumptions). Byzantine faults are worst-case errors where validators collude and behave maliciously to try to sabotage system behavior. A consensus protocol that tolerates Byzantine faults caused by malicious or hacked validators can also mitigate arbitrary hardware and software failures. LibraBFT assumes that a set of 3f + 1 votes is distributed among a set of validators that may be honest, or Byzantine. LibraBFT remains safe, preventing attacks such as double spends and forks when at most f votes are controlled by Byzantine validators. LibraBFT remains live — committing transactions from clients — as long as there exists a global stabilization time (GST), after which all messages between honest validators are delivered to other honest validators within a maximal network delay δ (this is the partial synchrony model introduced in [32]). In addition to traditional guarantees, LibraBFT maintains safety when validators crash and restart — even if all validators restart at the same time. Overview of LibraBFT. Validators receive transactions from clients and share them with each other through a shared mempool protocol. The LibraBFT protocol then proceeds in a sequence of rounds. In each round, a validator takes the role of leader and proposes a block of transactions to extend a certified sequence of blocks (see quorum certificates below) that contain the full previous transaction history. A validator receives the proposed block and checks their voting rules to determine if it should vote for certifying this block. These simple rules ensure the safety of LibraBFT — and  17  their implementation can be cleanly separated and audited.10 If the validator intends to vote for this block, it executes the block’s transactions speculatively and without external effect. This results in the computation of an authenticator for the database that results from the execution of the block. The validator then sends a signed vote for the block and the database authenticator to the leader. The leader gathers these votes to form a quorum certificate (QC), which provides evidence of ≥ 2f +1 votes for this block and broadcasts the QC to all validators. A block is committed when a contiguous 3-chain commit rule is met. A block at round k is committed if it has a QC and is confirmed by two more blocks and QCs at rounds k + 1 and k + 2. The commit rule eventually allows honest validators to commit a block. LibraBFT guarantees that all honest validators will eventually commit the block (and proceeding sequence of blocks linked from it). Once a sequence of blocks has committed, the state that results from executing their transactions can be persisted and forms a replicated database. Advantages of the HotStuff paradigm. We evaluated several BFT-based protocols against the dimensions of performance, reliability, security, ease of robust implementation, and operational overhead for validators. Our goal was to choose a protocol that would initially support at least 100 validators and would be able to evolve over time to support 500–1,000 validators. We had three reasons for selecting the HotStuff protocol as the basis for LibraBFT: (1) simplicity and modularity of the safety argument; (2) ability to easily integrate consensus with execution; and (3) promising performance in early experiments. The HotStuff protocol decomposes into modules for safety (voting and commit rules) and liveness (pacemaker). This decoupling provides the ability to develop and experiment independently and on different modules in parallel. Due to the simple voting and commit rules, protocol safety is easy to implement and verify. It is straightforward to integrate execution as a part of consensus to avoid forking issues that arise from non-deterministic execution in a leader-based protocol. Finally, our early prototypes confirmed high throughput and low transaction latency as independently measured in HotStuff [13]. We did not consider proof-of-work based protocols due to their poor performance and high energy (and environmental) costs [24]. HotStuff extensions and modifications. In LibraBFT, to better support the goals of the Libra ecosystem, we extend and adapt the core HotStuff protocol and implementation in several ways. Importantly, we reformulate the safety conditions and provide extended proofs of safety, liveness, and optimistic responsiveness. We also implement a number of additional features. First, we make the protocol more resistant to non-determinism bugs by having validators collectively sign the resulting state of a block rather than just the sequence of transactions. This also allows clients to use QCs to authenticate reads from the database. Second, we design a pacemaker that emits explicit timeouts, and validators rely on a quorum of those to move to the next round — without requiring synchronized clocks. Third, we design an unpredictable leader election mechanism in which the leader of a round is determined by the proposer of the latest committed block using a verifiable random function [37]. This mechanism limits the window of time in which an adversary can launch an effective denial-ofservice attack against a leader. Fourth, we use aggregate signatures [38] that preserve the identity validators who sign QCs. This allows us to provide incentives to validators that contribute to QCs (discussed in Section 9.3). It also does not require a complex threshold key setup [39]. Validator management. The Libra protocol manages the set of validators using a Move module. This creates a clean separation between the consensus system and the cryptoeconomic system that defines a trusted set of validators. We discuss the Libra Blockchain’s implementation of this contract 10 Validators  only vote for increasing rounds of proposals and only vote if the proposed block links to a previous block with an equal or higher preferred round number. The full details and proofs of safety are presented in the separate report on LibraBFT.  18  in Section 9.2. Abstracting the validator management in this way is an example of the flexibility granted by defining core blockchain primitives in Move. Each change to the group of validators defines a new epoch. If a transaction causes the validator management module to alter the validator set, that transaction will be the last transaction committed by the current epoch — any subsequent transactions in that block or future blocks from that epoch will be ignored. Once the transaction has been committed, the new set of validators can start the next epoch of the consensus protocol. Within an epoch, clients do not need to synchronize every QC. Since a committed QC contains a binding commitment to all previous states, a client only needs to synchronize to the latest available QC in its current epoch. If this QC is the last in its epoch, the client can see the new set of validators, update its epoch, and again synchronize to the latest QC. If a validator chooses to prune history as described in Section 4.2, it needs to retain at least enough data to provide proof of the validator set change to clients. The validator management contract must provide a validator set that satisfies the security properties required by the consensus protocol. No more than f voting power can be controlled by Byzantine validators. The voting power must remain honest both during the epoch as well as for a period time after the epoch in order to allow clients to synchronize to the new configuration. A client that is offline for longer than this period needs to resynchronize using some external source of truth to acquire a checkpoint that they trust (e.g., from the source it uses to receive updated software). Furthermore, the validator set must not rotate so frequently that the rotation disrupts the performance of the system or causes clients to download QCs for an excessive number of epochs. We plan to research the optimal epoch length but anticipate it to be less than a day.  6 Networking The Libra protocol, like other decentralized systems, needs a networking substrate to enable communication between its members. Both the consensus and shared mempool protocols between validators require communication over the internet, as described in Section 5 and Section 7, respectively. The network layer is designed to be general-purpose and draws inspiration from the libp2p [40] project. It currently provides two primary interfaces: (1) Remote Procedure Calls (RPC) and (2) DirectSend, which implements fire-and-forget-style message delivery to a single receiver. The inter-validator network is implemented as a peer-to-peer system using Multiaddr [41] scheme for peer addressing, TCP for reliable transport, Noise [42] for authentication and full end-to-end encryption, Yamux [43] for multiplexing substreams over a single connection, and push-style gossip for peer discovery. Each new substream is assigned a protocol supported by both the sender and the receiver. Each RPC and DirectSend type corresponds to one such protocol. The networking system uses the same validator set management smart contract as the consensus system as a source of truth for the current validator set. This contract holds the network public key and consensus public key of each validator. A validator detects changes in the validator set by watching for changes in this smart contract. To join the inter-validator network, a validator must authenticate using a network public key in the most recent validator set defined by the contract. Bootstrapping a validator requires a list of seed peers, which first authenticate the joining validator as an eligible member of the inter-validator network and then share their state with the new peer. Each validator in the Libra protocol maintains a full membership view of the system and connects directly to any validator it needs to communicate with. A validator that cannot be connected to directly is assumed to fall within the quota of Byzantine faults tolerated by the system. Validator  19  other validators  1 client  4  3 admission control  6  5  mempool  2  consensus 10  VM  storage  7  9  execution  8 validator  Figure 6: The flow of a write transaction through the internal components of Libra Core.  health information, determined using periodic liveness probes, is not shared between validators; instead, each validator directly monitors its peers for liveness. We expect this approach to scale up to a few hundred validators before requiring partial membership views, sophisticated failure detectors, or communication relays.  7 Libra Core Implementation To validate the Libra protocol, we have built an open-source prototype implementation, Libra Core [6]. The implementation is written in Rust. We chose Rust due to its focus on enabling safe coding practices, support for systems programming, and high performance. We have split the internal components of the system as gRPC [44] services. Modularity allows for better security; for instance, the consensus safety component can be run in a separate process or even on a different machine. The security of the Libra Blockchain rests on the correct implementation of validators, Move programs, and the Move VM. Addressing these issues in Libra Core is a work in progress. It involves isolating the parts of the code that contribute to a validator signing a block of transactions during consensus and applying measures to increase assurance in the correctness of these components (e.g., extensive testing, formal specification, and formal verification). Developing high assurance also involves ensuring the security of the dependencies of the code (e.g., the code review process, source control, open-source library dependencies, build systems, and release management).  7.1 Write Request Lifecycle Figure 6 shows the lifecycle of transactions within Libra Core that support write operations to the decentralized database. This section takes an in-depth look at how a transaction flows through the internal components of the validators. A request begins when a client wishes to submit a transaction to the database. We currently assume clients have an out-of-band mechanism to find the addresses of validators to submit transactions to — the final version of this mechanism is yet to be designed. Admission control. Upon receiving a transaction, a validator’s admission control component performs initial syntactic checks ( 1 ) to discard malformed transactions that will never be executed. Doing these checks as early as possible avoids spending resources on transactions that spam the system. Admission control may access the VM ( 2 ), which uses the storage component to perform  20  checks, such as ensuring the account has a sufficient balance to pay the gas for the transaction. The admission control component is designed so that a validator can run multiple instances of the component. This design allows the validator to scale the processing of incoming transactions and mitigate denial-of-service attacks. Mempool. Transactions that pass the checks of the admission control component are sent to the validator’s mempool, which holds transactions waiting to be executed in an in-memory buffer ( 3 ). The mempool may hold multiple transactions sent from the same address. Because the mempool processes multiple transactions at a time, it can perform checks such as validating that a sequence of operations on the same address can all pay for gas that the admission control system cannot. Using the shared-mempool protocol ( 4 ), a validator shares the transactions in its mempool with other validators and places transactions received from other validators in its own mempool. Consensus. Validators create blocks by selecting a sequence of transactions from their mempool. When a validator acts as the leader of the consensus protocol, it forms a block of transactions from its mempool ( 5 ). It sends this block of transactions as a proposal to other validators ( 6 ). The consensus component is responsible for coordinating agreement among all validators on the sequence of these blocks of transactions and their resulting execution using the LibraBFT protocol (Section 5). Transaction execution. As part of reaching agreement, the block of transactions is passed to the executor component ( 7 ), which manages the execution of transactions (Section 3) in the VM ( 8 ). This execution happens speculatively before the transaction has been agreed on. This early execution is safe since transactions are deterministic and have no external effects. After executing the transactions in the block, the execution component builds a ledger history (Section 4.2) with these transactions appended. The ledger history authenticator is returned to the consensus component ( 9 ). The leader then attempts to reach consensus on this authenticator by forming a chain of quorum certificates (Section 5), each of which is signed by a set of validators with at least 2f + 1 votes. Committing a block. Once the consensus algorithm reaches agreement, any honest validator can be sure that all other honest validators will eventually commit a consistent ledger history. The validator reads the result of the block execution from the cache in the execution component and updates its local database storage ( 10 ).  7.2 Read Request Lifecycle Clients may also submit read requests to query validators for the content of an account in the decentralized database. Read requests do not mutate state and can be processed locally without going through consensus. Reads are submitted to the validator’s admission control component. The admission control component performs preliminary checks, reads the data from storage, and the result is sent back to the client. The response comes with a quorum certificate (described in Section 5) that contains a root hash (described in Section 4). The QC allows the client to authenticate the response to the query. The client uses the same technique as the VM to interpret the raw bytes of an account using the logical data model (Section 2). For example, to read the balance of the account at address a, the client decodes the LibraCoin.T resource embedded inside the account’s raw bytes.  21  8 Performance The mission of the Libra protocol is to support a global financial infrastructure. Performance is an integral part of meeting that need. We discuss three components of blockchain performance: 1. Throughput: The number of transactions that the blockchain can process per second. 2. Latency: The time between a client submitting a transaction to the blockchain and another party seeing that the transaction was committed. 3. Capacity: The ability of the blockchain to store a large number of accounts. While the Libra protocol is still at a prototype stage and we do not have concrete performance metrics yet to report, we anticipate the initial launch of Libra protocol to support 1,000 payment transactions per second with a 10-second finality time between a transaction being submitted and committed. Over time, we expect to be able to increase the system’s throughput to meet the needs of the network. We anticipate that many payment transactions will occur off-chain, for example, within a custodial wallet or by using payment channels [45]. Therefore, we believe that supporting 1,000 transactions per second on the blockchain will meet the initial needs of the ecosystem. The Libra protocol is designed to achieve these goals in several ways: Protocol design. Many elements of the Libra protocol are chosen partly based on performance. For example, the LibraBFT algorithm achieves consensus in three rounds of network communication and does not require any real-time delay to propose or vote on blocks. This allows the commit latency to be limited only by the network latency between validators. We also select elements of the protocol with parallelization and sharding in mind. The sparse Merkle tree approach to computing authenticators allows sharding the database across multiple machines (which increases capacity) or processing updates in parallel (which increases throughput). Initial transaction validation, which includes computationally expensive signature verification, can also be parallelized. Validator selection. Like most services, the performance of the Libra Blockchain depends on the performance of the underlying validators that operate it. There is a tradeoff between decentralization and performance. Requiring extremely well-resourced validators limits the number of entities that could perform that role. However, the presence of extremely under-resourced validators would limit the performance of the whole system. We favor a balance of these approaches by targeting nodes that can run on commodity hardware that many entities can purchase. However, we do assume that nodes run on server-class hardware and within well-connected data centers. We use an approximate analysis to show that the system is likely able to meet the demand of 1,000 transactions per second. • Bandwidth: If we assume that each transaction requires 5 KB of traffic — including the cost of receiving the transaction via the mempool, rebroadcasting it, receiving blocks from the leader, and replicating to clients — then validators require a 40 Mbps internet connection to support 1,000 transactions per second. Access to such bandwidth is widely available. • CPU: Signature verification is a significant computational cost associated with a payment transaction. We have designed the protocol to allow parallel verification of transaction signatures. Modern signature schemes support over 1,000 verifications per second over a commodity CPU. • Disk: Servers with 16 TB of SSD storage are available from major server vendors. Since the current state is the only piece of information the validator needs to use to process a transaction, we estimate that if accounts are approximately 4 KB (inclusive of all forms of overhead), then this allows validators to store 4 billion accounts. We anticipate that developments in disk  22  storage, scaling validators to multiple shards, and economic incentives will allow the system to remain accessible to commodity systems. Historical data may grow beyond the amount that can be handled by an individual server. Validators are free to discard historical data not needed to process new transactions (see Section 4.2); however, this data may be of interest to clients who wish to query events from past transactions. Since the validators sign a binding commitment to this data, clients are free to use any system to access data without having to trust the system that delivers it. We expect this type of read traffic to be easy to scale through parallelism.  9 Implementing Libra Ecosystem Policies with Move The Libra Blockchain is a unique system that balances the stability of traditional financial networks with the openness offered by systems governed by cryptoeconomic means. As we discuss in Section 1, the Libra protocol is designed to support the Libra ecosystem in implementing novel economic [4] and governance [3] policies. The protocol specifies a flexible framework that is parametric in key system components such as the native currency, validator management, and transaction validation. In this section, we discuss how the Libra Blockchain uses the Move programming language to customize these components. Our discussion focuses on both the challenges of aligning network participant and validator incentives as well as the challenges of supporting the operations, governance, and evolution of the Libra ecosystem.  9.1 Libra Coin Many cryptocurrencies are not backed by real-world assets. As a result, investment and speculation have been primary use cases. Investors often acquire these currencies under the assumption that they will substantially appreciate and can later be sold at a higher price. Fluctuations in the beliefs about the long-term value of these currencies have caused corresponding fluctuations in price, which sometimes yield massive swings in value. To drive widespread adoption, Libra is designed to be a currency where any user will know that the value of a Libra today will be close to its value tomorrow and in the future. The reserve is the key mechanism for achieving value preservation. Through the reserve, each coin is fully backed with a set of stable and liquid assets. With the presence of a competitive group of liquidity providers that interface with the reserve, users can have confidence that any coin they hold can be sold for fiat currency at a narrow spread above or below the value of the underlying assets. This gives the coin intrinsic value from the start and helps protect against the speculative swings that are experienced by existing cryptocurrencies. The reserve assets are a collection of low-volatility assets, including cash and government securities from stable and reputable central banks. As the value of Libra is effectively linked to a basket of fiat currencies, from the point of view of any specific currency, there will be fluctuations in the value of Libra. The makeup of the reserve is designed to mitigate the likelihood and severity of these fluctuations, particularly in the negative direction (e.g., even in economic crises). To that end, the basket has been structured with capital preservation and liquidity in mind. The reserve is managed by the Libra Association (see Section 9.2), which has published a detailed report on the reserve’s operations [4]. Users do not directly interface with the reserve. Instead, to support higher efficiency, there are authorized resellers who are the only entities authorized by the association to transact large amounts of fiat and Libra in and out of the reserve. These authorized  23  resellers integrate into exchanges and other institutions that buy and sell cryptocurrencies and provide these entities with liquidity for users who wish to convert from cash to Libra and back again. To implement this scheme, the Libra coin contract allows the association to mint new coins when demand increases and destroy them when the demand contracts. The association does not set a monetary policy. It can only mint and burn coins in response to demand from authorized resellers. Users do not need to worry about the association introducing inflation into the system or debasing the currency: for new coins to be minted, there must be a commensurate fiat deposit in the reserve. The ability to customize the Libra coin contract using Move allows the definition of this scheme without any modifications to the underlying protocol or the software that implements it. Additional functionality can be created, such as requiring multiple signatures to mint currency and creating limited-quantity keys to increase security.  9.2 Validator Management and Governance The consensus algorithm relies on the validator-set management Move module to maintain the current set of validators and manage the allocation of votes among the validators. This contract is responsible for maintaining a validator set in which at most f votes out of 3f + 1 total votes are controlled by Byzantine validators. Initially, the Libra Blockchain only grants votes to Founding Members, entities that: (1) meet a set of predefined Founding Member eligibility criteria [46] and (2) own Libra Investment Tokens purchased in exchange for their investment in the ecosystem. These rules help to ensure the security requirements of having a safe and live validator set. Using the Founding Member eligibility criteria ensures that the Founding Members are organizations with established reputations, making it unlikely that they would act maliciously, and suggesting that they will apply diligence in defending their validator against outside attacks. Libra Investment Tokens represent an expectation of returns from interest on the reserve, further incentivizing the validators to keep the system operational. Because the assets in the reserve are low-risk and low-yield, excess returns for early investors will only materialize if the network is successful and the reserve grows substantially in size. While this method of assessing validator eligibility is an improvement on traditional permissioned blockchains, which usually form a set of closed business relationships, we aspire to make the Libra Blockchain fully permissionless. To do this, we plan to gradually transition to a proof-of-stake [47] system where validators are assigned voting rights proportional to the number of Libra coins they hold. This transitions the governance of the ecosystem to its users while preventing Sybil attacks [48] by requiring validators to hold a scarce resource and align their incentives with healthy system operations. This transition requires (1) the ecosystem to be sufficiently large to prevent a single bad actor from causing disruption; (2) the existence of a competitive and reliable market for delegation for users that do not wish to become validators; and (3) addressing technological and usability challenges in the staking of Libra coins. Another unique aspect of governance in the Libra ecosystem is that the validators form a real-world entity, the not-for-profit Libra Association, which is managed by a council of validators. A member in the association council represents each validator. A member’s voting weight in the council is the same as the validator’s voting weight in the consensus protocol. The association performs tasks such as managing the reserve, assessing the validator eligibility criteria, and guiding the open-source development of the Libra protocol. Move makes it possible to encode the rules for validator management and governance as a module. Libra Investment Tokens can be implemented using Move resources. Move allows the staking of either investment tokens or coins by wrapping them in a resource that prevents access to the underlying  24  asset. The staked resources can be used to compute the voting rights of the validators. The contract can configure the interval at which changes take effect, to reduce churn of the validator set. The Libra Association’s operation is also aided by Move. Since the association is the operator of the reserve, it can create Move modules that delegate the authority to mint and burn coins to an operational arm that interacts with authorized resellers. This operational arm can also assess if potential Founding Members meet the eligibility criteria. Move allows flexible governance mechanisms such as allowing the council to assert its authority and take back its delegated authority through a vote. The association has published a detailed document outlining its proposed structure [3]. All governance in the association stems from the council of validators — this council has the ultimate right to assert any authority provided to the association. Thus, the governance of the entire Libra ecosystem evolves as the validator set changes from the initial set of Founding Members to a set based on proof of stake.  9.3 Validator Security and Incentives In the initial setting, using Founding Members as validators, we believe that the institutional reputation and financial incentives of each validator are sufficient to ensure that Byzantine validators control no more than f votes. In the future, however, an open system where representation is based on coin ownership will require a substantially different market design. We have started to understand the governance and equilibrium structure of a blockchain system based on stakeholdings and consumer confidence in wallets and other delegates. In the process, we have identified new market design trade-offs between the Libra approach and more established approaches, such as proof of work [49]. However, more research is needed to determine how best to maintain long-run competition in the ecosystem while ensuring the security and efficiency of the network. Furthermore, as stake-based governance introduces path dependence in influence, it is essential to explore mechanisms for protecting smaller stakeholders and service providers. Move allows flexibility in the definition of the relevant incentive schemes such as gas pricing or staking. For example, stake slashing, a commonly discussed mechanism, could be implemented in Move by locking stake for a period of time and automatically punishing validators if they violate the rules of the LibraBFT algorithm in a way that affects safety. Similarly, when a validator votes in the LibraBFT algorithm, those votes can be recorded in the database. This record allows a Move module to distribute incentives based on participation in the algorithm, thereby incentivizing validators to be live. Interest generated on the Libra Reserve and gas payments can also be used as sources of incentives. Both sources are managed by Move modules, which add flexibility in their allocation. While more research is required to design an approach that will support the evolution of the Libra ecosystem, the flexibility of Move ensures that the desired approach can be implemented with few, if any, changes to the Libra protocol.  10 What's Next for Libra? We have presented a proposal for the Libra protocol, which allows a set of validators to provide a decentralized database for tracking programmable resources. We have discussed an open-source prototype — Libra Core — of the Libra protocol and shown how the introduction of the Move programming language for smart contracts allows the protocol to implement the unique design of the Libra ecosystem.  25  Both the protocol and the implementation described in this paper are currently at the prototype stage. We hope to gather feedback from the newly formed Libra Association, as well as the broader community, to turn these ideas into an open financial infrastructure. We are currently running a testnet to allow the community to experiment with this system. We are working toward an initial launch of the system, and to keep it within a manageable scope, we plan to make several simplifications in the first version. In the early days of the system, using an externally recognized set of Founding Members reduces the demands on the consensus incentive system and allows for a faster pace of updates. We anticipate using Move only for system-defined modules, as opposed to letting users define their own modules, which allows for the Libra ecosystem to launch before the Move language is fully formed. This also allows for breaking changes to be made without compromising the flexibility that comes from defining core system behavior using Move. However, we intend for future versions of the Libra protocol to provide open access to the Move language. Finally, we are currently working within the framework of the Libra Association to launch the technical infrastructure behind this new ecosystem. We have published a roadmap [50] of work that we plan to contribute to support this launch. One of the top goals of the Libra Association is to migrate the Libra ecosystem to a permissionless system. We have documented the technical challenges [5] involved in making this migration. The association’s open-source community [6] provides information about how to start using the Libra testnet, try out the Move language, and contribute to the Libra ecosystem.  Acknowledgments We would like to thank the following people for helpful discussions and feedback on this paper: Adrien Auclert, Morgan Beller, Tarun Chitra, James Everingham, Maurice Herlihy, Ravi Jagadeesan, Archana Jayaswal, Scott Duke Kominers, John Mitchell, Rajesh Nishtala, Roberto Rigobon, Jared Saia, Christina Smedley, Catherine Tucker, Kevin Weil, David Wong, Howard Wu, Nina Yiamsamatha, and Kevin Zhang.  26  References [1] The Libra Association, “An Introduction to Libra,” https://libra.org/en-us/whitepaper. [2] C. Catalini and J. S. Gans, “Some simple economics of the blockchain,” WP No. 22952, National Bureau of Economic Research, 2016. [3] The Libra Association, “The Libra Association,” https://libra.org/en-us/association-council-pr inciples. [4] C. Catalini et al., “The Libra reserve,” https://libra.org/about-currency-reserve. [5] S. Bano et al., “Moving toward permissionless consensus,” https://libra.org/permissionless-bloc kchain. [6] The Libra Association, “Libra developers,” https://developers.libra.org/. [7] P. T. Devanbu et al., “Authentic third-party data publication,” in Data and Application Security, Development and Directions, IFIP TC11/ WG11.3 Fourteenth Annual Working Conference on Database Security, Schoorl, The Netherlands, August 21-23, 2000, 2000, pp. 101–112. [8] M. Naor and K. Nissim, “Certificate revocation and certificate update,” IEEE Journal on Selected Areas in Communications, vol. 18, no. 4, pp. 561–570, 2000. [9] R. Tamassia, “Authenticated data structures,” in Algorithms - ESA 2003, 11th Annual European Symposium, Budapest, Hungary, September 16-19, 2003, Proceedings, 2003, pp. 2–5. [10] S. Blackshear et al., “Move: A language with programmable resources,” https://developers.lib ra.org/docs/move-paper. [11] R. C. Merkle, “A digital signature based on a conventional encryption function,” in Advances in Cryptology - CRYPTO ’87, A Conference on the Theory and Applications of Cryptographic Techniques, Santa Barbara, California, USA, August 16-20, 1987, Proceedings, 1987, pp. 369– 378. [12] M. Baudet et al., “State machine replication in the Libra Blockchain,” https://developers.libra .org/docs/state-machine-replication-paper. [13] M. Yin et al., “Hotstuff: BFT consensus in the lens of blockchain,” 2018. [Online]. Available: https://arxiv.org/abs/1803.05069 [14] P. A. Bernstein, V. Hadzilacos, and N. Goodman, “Concurrency control and recovery in database systems.” Addison-Wesley, 1987. [15] D. P. Reed, “Naming and synchronization in a decentralized computer system,” Ph.D. dissertation, Massachusetts Institute of Technology, Cambridge, MA, USA, 1978. [16] G. Wood, “Ethereum: A Secure Decentralised Generalised Transaction Ledger,” http://gavwoo d.com/paper.pdf, 2016. [17] G. Bertoni et al., “Keccak,” in Advances in Cryptology - EUROCRYPT 2013, 32nd Annual International Conference on the Theory and Applications of Cryptographic Techniques, Athens, Greece, May 26-30, 2013. Proceedings, 2013, pp. 313–314. [18] S. Josefsson and I. Liusvaara, “Edwards-curve digital signature algorithm (EdDSA),” RFC, vol. 8032, pp. 1–60, 2017. [19] A. Pfitzmann and M. Köhntopp, “Anonymity, unobservability, and pseudonymity—a proposal for terminology,” in Designing privacy enhancing technologies, 2001, pp. 1–9.  27  [20] B. C. Pierce, “Types and programming languages.” MIT Press, 2002, ch. 19. [21] J. Girard, “Light linear logic,” Inf. Comput., vol. 143, no. 2, pp. 175–204, 1998. [22] R. Milner, M. Tofte, and R. Harper, “Definition of standard ML.” MIT Press, 1990. [23] “Leaf-node weakness in Bitcoin merkle tree design,” https://bitslog.com/2018/06/09/leaf-nodeweakness-in-bitcoin-merkle-tree-design/. [24] S. Nakamoto, “Bitcoin: A Peer-to-Peer Electronic Cash System,” https://bitcoin.org/bitcoin.pdf, 2008. [25] S. A. Crosby and D. S. Wallach, “Efficient data structures for tamper-evident logging,” in 18th USENIX Security Symposium, Montreal, Canada, August 10-14, 2009, Proceedings, 2009, pp. 317–334. [26] L. Reyzin and S. Yakoubov, “Efficient asynchronous accumulators for distributed PKI,” in Security and Cryptography for Networks - 10th International Conference, SCN 2016, Amalfi, Italy, August 31 - September 2, 2016, Proceedings, 2016, pp. 292–309. [27] B. Laurie, A. Langley, and E. Käsper, “Certificate transparency,” RFC, vol. 6962, pp. 1–27, 2013. [28] J. R. Driscoll et al., “Making data structures persistent,” J. Comput. Syst. Sci., vol. 38, no. 1, pp. 86–124, 1989. [29] C. Okasaki, “Purely functional data structures.” Cambridge University Press, 1999. [30] L. Reyzin et al., “Improving authenticated dynamic dictionaries, with applications to cryptocurrencies,” in Financial Cryptography and Data Security - 21st International Conference, FC 2017, Sliema, Malta, April 3-7, 2017, Revised Selected Papers, 2017, pp. 376–392. [31] F. B. Schneider, “Implementing fault-tolerant services using the state machine approach: A tutorial,” ACM Computing Surveys (CSUR), pp. 299–319, 1990. [32] C. Dwork, N. Lynch, and L. Stockmeyer, “Consensus in the presence of partial synchrony,” Journal of the ACM (JACM), vol. 35, no. 2, pp. 288–323, 1988. [33] M. Castro and B. Liskov, “Practical Byzantine fault tolerance,” in USENIX Symposium on Operating Systems Design and Implementation (OSDI), 1999, pp. 173–186. [34] V. Buterin and V. Griffith, “Casper the friendly finality gadget,” CoRR, vol. abs/1710.09437, 2017. [35] E. Buchman, J. Kwon, and Z. Milosevic, “The latest gossip on BFT consensus,” CoRR, vol. abs/1807.04938, 2018. [36] L. Lamport, R. Shostak, and M. Pease, “The Byzantine generals problem,” ACM Transactions on Programming Languages and Systems (TOPLAS’82), vol. 4, no. 3, pp. 382–401, 1982. [37] S. Micali, M. Rabin, and S. Vadhan, “Verifiable random functions,” in 40th Annual Symposium on Foundations of Computer Science (FOCS’99). IEEE, 1999, pp. 120–130. [38] D. Boneh, B. Lynn, and H. Shacham, “Short signatures from the Weil pairing,” in Advances in Cryptology (ASIACRYPT 2001). Springer-Verlag, 2001, pp. 514–532. [39] A. Kate and I. Goldberg, “Distributed key generation for the internet,” in IEEE International Conference on Distributed Computing Systems (ICDCS’09), 2009, pp. 119–128. [40] “The libp2p project,” https://libp2p.io/.  28  [41] “The multiaddr project,” https://multiformats.io/multiaddr/. [42] T. Perrin, “The noise project,” https://noiseprotocol.org/noise.html, 2018. [43] “The yamux project,” https://github.com/hashicorp/yamux/blob/master/spec.md, 2016. [44] “The gRPC project,” https://grpc.io/. [45] L. Gudgeon et al., “SoK: Off the chain transactions,” IACR Cryptology ePrint Archive, vol. 2019, p. 360, 2019. [46] The Libra Association, “Becoming a Founding Member,” https://libra.org/becoming-foundingmember. [47] I. Bentov, A. Gabizon, and A. Mizrahi, “Cryptocurrencies without proof of work,” in Financial Cryptography and Data Security - FC 2016 International Workshops, BITCOIN, VOTING, and WAHC, Christ Church, Barbados, February 26, 2016, Revised Selected Papers, 2016, pp. 142–157. [48] J. R. Douceur, “The sybil attack,” in Peer-to-Peer Systems, First International Workshop, IPTPS 2002, Cambridge, MA, USA, March 7-8, 2002, Revised Papers, 2002, pp. 251–260. [49] C. Catalini, R. Jagadeesan, and S. D. Kominers, “Market design for a blockchain-based financial system,” SSRN Working Paper No. 3396834, 2019. [50] The Libra Association, “The path forward,” https://developers.libra.org/blog/2019/06/18/thepath-forward.  29  State Machine Replication in the Libra Blockchain Mathieu Baudet, Avery Ching, Andrey Chursin, George Danezis, François Garillot, Zekun Li, Dahlia Malkhi, Oded Naor, Dmitri Perelman, Alberto Sonnino*  Abstract. This report presents LibraBFT, a robust and efficient state machine replication system designed for the Libra Blockchain. LibraBFT is based on HotStuff, a recent protocol that leverages several decades of scientific advances in Byzantine fault tolerance (BFT) and achieves the strong scalability and security properties required by internet settings. LibraBFT further refines the HotStuff protocol to introduce explicit liveness mechanisms and provides a concrete latency analysis. To drive the integration with the Libra Blockchain, this document provides specifications extracted from a fully-functional simulator. These specifications include state replication interfaces and a communication framework for data transfer and state synchronization among participants. Finally, this report provides a formal safety proof that induces criteria to detect misbehavior of BFT nodes, coupled with a simple reward and punishment mechanism.  1. Introduction The advent of the internet and mobile broadband has connected billions of people globally, providing access to knowledge, free communications, and a wide range of lower-cost, more convenient services. This connectivity has also enabled more people to access the financial ecosystem. Yet, despite this progress, access to financial services is still limited for those who need it most. Blockchains and cryptocurrencies have shown that the latest advances in computer science, cryptography, and economics have the potential to create innovation in financial infrastructure, but existing systems have not yet reached mainstream adoption. As the next step toward this goal, we have designed the Libra Blockchain [1], [2] with the mission to enable a simple global currency and financial infrastructure that empowers billions of people. At the heart of this new blockchain is a consensus protocol called LibraBFT — the focus of this report — by which blockchain transactions are ordered and finalized. LibraBFT decentralizes trust among a set of validators that participate in the consensus protocol. LibraBFT guarantees consensus on the history of transactions among honest validators and remains safe even if a threshold of participants are Byzantine (i.e., faulty or corrupt [3]). By embracing the classical approach to Byzantine fault tolerance, LibraBFT builds on solid and rigorously proven foundations in distributed computing. Furthermore, the scientific community has made steady progress, which LibraBFT builds on, in scaling consensus technology and making it robust for internet settings.  ∗  The authors work at Calibra, a subsidiary of Facebook, Inc., and contribute this paper to the Libra Association under a Creative Commons Attribution 4.0 International License. For more information on the Libra ecosystem, please refer to the Libra white paper [1].  1  Initially, the participating validators will be permitted into the consensus network by an association consisting of a geographically distributed and diverse set of Founding Members, which are organizations chosen according to objective membership criteria with a vested interest in bootstrapping the Libra ecosystem [2]. Over time, membership eligibility will shift to become open and based only on an organization’s holdings of Libra [4]. The LibraBFT consensus is based on a cutting-edge technique called HotStuff [5], [6] that bridges between the world of BFT consensus and blockchain. This choice reflects vast expert knowledge and exploration of various alternatives and provides LibraBFT with the following key properties that are crucial for decentralizing trust: • Safety: LibraBFT maintains consistency among honest validators, even if up to one-third of the validators are corrupt. • Asynchrony: Consistency is guaranteed even in cases of network asynchrony (i.e., during periods of unbounded communication delays or network disruptions). This reflects our belief that building internet-scale consensus protocol whose safety relies on synchrony would be inherently both complex and vulnerable to Denial-of-Service (DoS) attacks on the network. • Finality: LibraBFT supports a notion of finality, whereby a transaction becomes irreversibly committed. It provides concise commitments that authenticate the result of ledger queries to an end user. • Linearity and Responsiveness: LibraBFT has two desirable properties that BFT consensus protocols preceding HotStuff were not able to simultaneously support — linearity and responsiveness. These two technical concepts are linked with the notion of leaders, a key approach for driving progress against partial synchrony. Informally, linearity guarantees that driving transaction commits incurs only linear communication (this is optimal) even when leaders rotate; responsiveness means that the leader has no built-in delay steps and advances as soon as it collects responses from validators. • Simplicity and Modularity: The core logic of LibraBFT allows simple and robust implementation, paralleling that of public blockchains based on Nakamoto consensus [7]. Notably, the protocol is organized around a single communication phase and allows a concise safety argument. • Sustainability: Current public blockchains, where trust is based on computational power, have been reported to consume vast amounts of energy [8] and may be subject to centralization [9]. LibraBFT is designed as a proof-of-stake system, where participation privileges are granted to known members based on their financial involvement. LibraBFT can support economic incentives to reward good behaviors and/or punish wrongdoings from stakeholders. Computational costs in LibraBFT consist primarily of cryptographic signatures, a standard concept with efficient implementations. Key technical approach. LibraBFT is a consensus protocol that progresses in rounds, where in each round a leader is chosen amongst the validators. As mentioned above, this key approach is needed for driving progress against partial synchrony. The leader proposes a new block consisting of transactions and sends it to the rest of the validators, who approve the new block if it consists of valid transactions. Once the leader collects a majority of votes, she sends it to the rest of the validators. If a leader fails to propose a valid block or does not aggregate enough votes, a timeout mechanism will force a new round, and a new leader will be chosen from the validators. This way, new blocks extend the blockchain. Eventually, a block will meet the commit rule of LibraBFT, and once this happens, this block and any prior block is committed. Related work. A comprehensive survey is beyond the scope of this manuscript (see for example [10]– [12]). Here we mention key concepts and mechanisms that influenced our work.  2  Consensus algorithms in classical setting. The Byzantine consensus problem was pioneered by Lamport et al. [3], who also coined the term Byzantine to model arbitrary, possibly maliciously corrupt behavior. The safety of the solution introduced by Lamport et al. relied on synchrony, a dependency that practical systems wish to avoid both due to complexity and because it exposes the system to DoS attacks on safety. In lieu of synchrony assumptions, randomized algorithms, pioneered by Ben-Or [13], guarantee progress with high probability. A line of research gradually improved the scalability of such algorithms, including [14]–[17]. However, most practical systems did not yet incorporate randomization. In the future, LibraBFT may incorporate certain randomization to thwart adaptive attacks. A different approach for asynchronous settings, introduced by Dwork et al. [18], separated safety (at all times) from liveness (during periods of synchrony). Dwork et al. introduced a round-by-round paradigm where each round is driven by a designated leader. Progress is guaranteed during periods of synchrony as soon as an honest leader emerges, and until then, rounds are retired by timeouts. Dwork et al.’s approach (DLS) underlies most practical BFT works to date, with steady improvements to its performance. Specifically, it underlies the first practical solution introduced by Castro and Liskov [19] called PBFT. In PBFT, an honest leader reaches a decision in two all-to-all communication rounds. In addition to the original open-source implementation of PBFT, the protocol has been integrated into BFT-SMaRt [20] and, recently, into FaB [21]. Zyzzyva [22] adds an optimistically fast track to PBFT that can reach a decision in one round when there are no failures. An open-source implementation of Zyzzyva was built in Upright [23]. Response aggregation using threshold cryptography was utilized in consensus protocols by Cachin [24] and Reiter [25] to replace all-to-all communication with an all-tocollector and collector-to-all pattern that incurs only linear communication costs. Threshold signature aggregation has been incorporated into several PBFT-based systems, including Byzcoin [26] and SBFT [27]. Similarly, LibraBFT incorporates message collection and fast signature aggregation [28]. Compared with threshold signature, signature aggregation in LibraBFT does not require distributed setup and enables economic incentives for voters at the price of one additional bit per node per signature. Two blockchain systems, Tendermint [29] and Casper [30], presented a new variant of PBFT that simplifies the leader-replacement protocol of PBFT such that it has only linear communication cost (linearity). These variants forego a hallmark property of practical solutions called responsiveness. Informally, (optimistic) responsiveness holds when leaders can propose new blocks as soon as they receive a fixed number of messages, as opposed to waiting for a fixed delay. Thus, Tendermint and Casper introduced into the field a trade-off in practical BFT solutions — either they have linearity or responsiveness, but not both. The HotStuff solution, which LibraBFT is based on (as well as other recent blockchains, notably ThunderCore with a variant named PaLa [31]) resolved this trade-off and presented the first BFT consensus protocol that has both. Consensus in a permissionless setting. All the works mentioned above assume a permissioned setting, i.e., the participating players are known in advance. Differently, in a permissionless setting, any party can join and participate in the protocol — which is what Nakamoto Consensus (NC) [7] aims to solve — resulting in an entirely different protocol structure. In NC, transactions (gathered in blocks) are chained and simply disseminated to the network with a proof of work. Finality is defined probabilistically — the probability that a block remains in the history is proportional to the computational cost of succeeding blocks in the blockchain. The reward mechanism for extending the current chain suffices to incentivize miners to accept the current chain de facto and rapidly converge on a single, longest fork. Casper and HotStuff exhibit similar simplicity of protocol structure. They embed the protocol rounds into a (possibly branching) chain and deduce commit decisions by simple offline analysis of the chains.  3  Several blockchains are similarly based on graphs of blocks in the form of direct acyclic graphs (DAG) allowing greater concurrency in posting blocks into the graph, e.g., GHOST [32], Conflux [33], Blockmania [34] and Hashgraph [35]. Our experience with some of these paradigms indicates that recovering graph information and verifying it after a participant loses connection temporarily can be challenging. In LibraBFT, only leaders can extend chains; hence, disseminating, recovering, and verifying graph information is simple and essentially linear. Revisiting LibraBFT. LibraBFT leverages HotStuff (ArXiv version [5], to appear in PODC’19 [6]) and possesses many of the benefits achieved in four decades of works presented above. Specifically, LibraBFT adopts the DLS and PBFT round-based approach, has signature aggregation, and has both linearity and responsiveness. We also found that the chain structure of Casper and HotStuff leads to robust implementation and concise safety arguments. Compared with HotStuff itself, LibraBFT makes a number of enhancements. LibraBFT provides a detailed specification and implementation of the pacemaker mechanism by which participants synchronize rounds. This is coupled with a liveness analysis that consists of concrete bounds to transaction commitment. LibraBFT includes a reconfiguration mechanism of the validator voting rights (epochs). It also describes mechanisms to reward proposers and voters. The specification allows deriving safe and complete criteria to detect validators that attempt to break safety, enabling punishment to be incorporated into the protocol in the future. We also elaborate on the protocol for data dissemination among validators to synchronize their state. Organization. The remainder of this report is structured as follows: we start by introducing important concepts and definitions (Section 2) and how LibraBFT is used in the Libra Blockchain (Section 3). We then describe the core data types of LibraBFT and its network communication layer (Section 4). Next, we present the protocol itself (Section 5) with enough detail to prepare the proof of safety (Section 6). We then describe the pacemaker module (Section 7) and prove liveness (Section 8). Finally, we discuss the economic incentives of LibraBFT (Section 9) and conclude in Section 10. In this initial report, we have chosen to use a minimal subset of Rust as a specification language for the protocol, whenever code was needed. We provide code fragments directly extracted from our reference implementation in a discrete-event simulated environment. We intend to share the code for this simulator and provide experimental results in a subsequent version of the report.  2. Overview and Deﬁnitions We start by describing the desired properties of LibraBFT and how our state machine replication protocol is meant to be integrated into the Libra Blockchain.  2.1. State Machine Replication State Machine Replication (SMR) protocols [36] are meant to provide an abstract state machine distributed over the network and replicated between many processes, also called nodes. Specifically, a SMR protocol is started with some initial execution state. Every process can submit commands and observe a sequence of commits. Each commit contains the execution state that is the result of executing a particular command on top of the previous commit. Commands may be rejected during execution: in this case, they are called invalid commands. Assuming that command execution is deterministic, we wish to guarantee the following properties: (safety) All honest nodes observe the same sequence of commits.  4  (liveness) New commits are produced as long as valid commands are submitted. Note that nodes should observe commits in the same order but not necessarily at the same time. The notion of an honest node is made precise below in Section 2.3.  2.2. Epochs For practical applications, the set of nodes participating in the protocol can evolve over time. In LibraBFT, this is addressed by supporting a notion of epoch: • Each epoch begins using the last execution state of the previous epoch — or using system-wide initial parameters for the first epoch. • We assume that every execution state contains a value epoch_id that identifies the current epoch. • When a command that increments epoch_id is committed, the current epoch stops after this commit, and the next epoch is started.  2.3. Byzantine Fault Tolerance Historically, fault-tolerant protocols were meant to address common failures, such as crashes. In the context of a blockchain, the SMR consensus protocol is used to limit the power of individual nodes in the system. To do so, we must guarantee safety and liveness even when certain nodes deviate arbitrarily from the protocol. In the rest of this report, we assume a fixed, unknown subset of malicious nodes for every epoch, called Byzantine nodes [3]. All the other nodes, called honest nodes, are assumed to follow protocol specifications scrupulously. During a given epoch, we assume that every SMR node 𝛼 has a fixed voting power, denoted 𝑉 (𝛼) ≥ 0. We write 𝑁 for the total voting power of all nodes and assume a security threshold 𝑓 as a function of 𝑁 such that 𝑁 > 3𝑓. For example, we may define 𝑓 = ⌊ 𝑁−1 3 ⌋. For notational simplicity in this report, we refer to a voting power of 𝑥 as consisting of 𝑥 nodes. We analyze all consensus properties in the context of the following BFT assumption: (bft-assumption) The combined voting power of Byzantine nodes during any epoch must not exceed the security threshold 𝑓. A subset of nodes whose combined voting power 𝑀 satisfies 𝑀 ≥ 𝑁 − 𝑓 is called a quorum. The notion of quorum is justified by the following classic lemma [37]: Lemma B1: Under BFT assumption, for every two quorums of nodes in the same epoch, there exists an honest node that belongs to both quorums. We recall the proof of Lemma B1 in Section 6.1.  2.4. Cryptographic Assumptions We assume a hash function and a digital signature scheme that are secure against computationallybounded adversaries and require that every honest node keeps its private signature key(s) secret. Since our protocol only hashes and signs public values, we may assume all digital signatures to be unforgeable in a strong non-probabilistic sense, meaning that any valid signature must originate from the owner of the private key. Similarly, we may assume that collisions in the hash function hash will never happen, therefore hash(𝑚1 ) = hash(𝑚2 ) implies 𝑚1 = 𝑚2 .  5  2.5. Networking Assumptions and Honest Crashes While the safety of LibraBFT is guaranteed under the BFT assumption alone, liveness requires additional assumptions on the network and the processes. Specifically, we will assume that the network alternates between periods of bad and good connectivity, known as periods of asynchrony and synchrony, respectively. Liveness can only be guaranteed during long-enough periods of synchrony. During periods of asynchrony, we allow messages to be lost or to take an arbitrarily long time. We also allow honest nodes to crash and restart. During periods of synchrony, we assume that there exists an upper bound 𝛿𝑀 to the transmission delay taken by any message between honest nodes; besides, honest processes must be responsive and cannot crash. We must stress that the adversary controls malicious nodes and the scheduling of networking messages even during periods of synchrony, subject to the maximal delay 𝛿𝑀 . The parameters of this model — such as the value of 𝛿𝑀 or whether the network is currently synchronous or not — are not available to the participants within the system. To simplify the analysis, it is usual in the literature to consider only two periods: before some unknown global stabilization time, called GST, and after GST. Our proof of liveness (Section 8) will give concrete upper bounds on the time needed by the system to produce a commit after GST. More formally, the assumptions on network and crashes are written as follows: (eventually-synchronous-network) After GST, the network delivers all messages between honest nodes under some (unknown) time delay 𝛿𝑀 > 0. (eventually-no-crash) After GST, honest nodes are perfectly responsive and never crash. We remark that the GST model does not take into account CPU time associated with message processing. Besides, message sizes in LibraBFT are not bounded, and, therefore, a fixed 𝛿𝑀 is arguably over-simplified. In future work, we may enforce strict bounds on message sizes or see the networking delay for each message as the composition of a fixed latency and a delay of transmission proportional to the message size.  2.6. Leaders, Votes, Quorum Certiﬁcates LibraBFT belongs to the family of leader-based consensus protocols. In leader-based protocols, validators make progress in rounds, each having a designated validator called a leader. Leaders are responsible for proposing new blocks and obtaining signed votes from the validators on their proposals. LibraBFT follows the chained variant of HotStuff [5], where a round is a communication phase with a single designated leader, and leader proposals are organized into a chain using cryptographic hashes. During a round, the leader proposes a block that extends the longest chain it knows. If the proposal is valid and timely, each honest node will sign it and send a vote back to the leader. After the leader has received enough votes to reach a quorum, it aggregates the votes into a Quorum Certificate (QC) that extends the same chain again. The QC is broadcast to every node. If the leader fails to assemble a QC, participants will timeout and move to the next round. Eventually, enough blocks and QCs will extend the chain in a timely manner, and a block will match the commit rule of the protocol. When this happens, the chain of uncommitted blocks up to the matching block become committed.  6  validator  Libra transaction  validator  SMR  Exec  SMR  Exec  SMR  Exec  SMR  Exec  validator  validator  Figure 1: Integration of the SMR module into the Libra Blockchain  3. Integration with the Libra Blockchain 3.1. Consensus Protocol We expect LibraBFT to be used in the Libra Blockchain [2] as follows: • The validators of Libra participate in the LibraBFT protocol in order to securely replicate the state of the Libra Blockchain. We call SMR module the software implementation of a LibraBFT node run by each validator. From here on, we refer to participants of LibraBFT as validator nodes, or simply nodes. • Commands sent to the SMR module are sequences of Libra transactions. From the point of view of the SMR module, commands and execution states are opaque data structures. The SMR module delegates the execution of commands entirely to the execution module of Libra (see possible APIs in Appendix A.1). We read the epoch_id from the execution state. (Recall that the current epoch stops when a change to epoch_id is committed.) • Importantly, the SMR module also delegates to the rest of the system the computation of voting rights within a given epoch. This is done using the same callbacks (Appendix A.1) to the execution layer as the ones managing epochs. For better flexibility and transparency, we expect this logic to be written in Move [38], the language for programmable transactions in Libra. • Every time a command needs to be executed, the execution engine is given a time value meant for Move smart contracts. This value is guaranteed to be consistent across every SMR node that executes the same command. • Execution states seen by the SMR module need not be the actual blockchain data. In practice, what we call “execution state” in this report is a lightweight data structure (e.g., a hash value) that refers to a concrete execution state stored in the local storage of a validator. Every command that is committed must be executed locally at least once by every validator. In the future, LibraBFT may include additional mechanisms for a validator to synchronize with the local storage of another validator corresponding to a recent execution state.  3.2. Libra Clients The design of LibraBFT is mostly independent of how validator nodes interact with the clients of the Libra system. However, we can make the following observations:  7  • Transactions submitted by the clients of Libra are first shared between validator nodes using a mempool protocol. Consensus leaders pull transactions from the mempool when they need to make a proposal. • To authenticate the state of blockchain with respect to Libra clients, during the protocol, LibraBFT nodes sign short commitments to vouch that a particular execution state is being committed. This results in cryptographic commit certificates that are verifiable independently from the details of the consensus protocol of LibraBFT, provided that Libra clients know the set of validator keys for the corresponding epoch. We describe how commitments are created together with consensus data in section (Section 4.1). • Regarding this last assumption, for now, we will consider that Libra clients can learn the set of validator keys from conventional interactions with one or several trusted validators. In the future, we will provide a security protocol for this purpose.  3.3. Security Blockchain applications also require additional security considerations: • Participants to the protocol should be able to cap the amount of resources (e.g., CPU, memory, storage, etc.) that they allocate to other nodes to ensure practical liveness despite Byzantine behaviors. We will see in the next section (Section 4) that our data-communication layer provides mechanisms to let receivers control how much data they consume (aka back pressure). A more thorough analysis is left for future versions of this report. • The economic incentives of rational nodes should be aligned with the security and the performance of the SMR protocol. We sketch low-level mechanisms enabling reward and punishment in section (Section 9). • Leaders can be subject to targeted denial-of-service attacks. Our pacemaker specifications (Section 7) sketches how to introduce a verifiable random function (VRF) [39] to assign leaders to round numbers in a less predictable way. In the future, we may also influence leader selection in order to select robust leaders more often. Protection of nodes at the system level is out of the scope of this report. Note on Fairness. Besides safety and liveness, another abstract property often discussed in SMR systems is fairness. This notion is traditionally defined as the fact that every valid command submitted by an honest node is eventually committed. Yet, this classic definition is less relevant to a blockchain application such as Libra, where transactions go through a shared mempool first and are subject to auctions on transaction fees. We leave the discussion on fairness for future work.  4. Consensus Data and Networking In this section, we introduce the core data types of LibraBFT, called records and discuss the communication framework used to synchronize node states over the network.  8  C1  C2  C3  V1  V2  V3  V1 h0  B1  V2 B2  V3 B3  V1  V2  V3  Figure 2: A chain of records in LibraBFT  4.1. Records The core state of a LibraBFT node consists of a set of records. We define four kinds of records: • blocks, proposed by leaders at a given round number and containing commands to execute. • votes, by which a node votes for a block and its execution state. • quorum certificates (QCs), which hold a quorum of votes for a given block and its execution state — and optionally a commitment meant for Libra clients. • timeouts, by which a node certifies that its current round has reached a timeout. Precise data structures in Rust are provided in the next paragraph. Most importantly: • Records are signed by their authors. • Blocks are chained (Figure 2): they must include the hash of the QC of a block at a lower round — or at the beginning of an epoch, a fixed hash value ℎinit set during the initialization of the epoch. • Votes and QCs include the hash of the block and the execution state that are the objects of the votes. A QC is created by gathering enough votes to form a quorum (Section 2.3) in favor of the same execution state, according to the voting rights of the current epoch. The fact that quorum certificates are signed by their author is not essential to the protocol: this is meant only to limit the influence of the next leader(s) regarding voter rewards (see Section 9 on economic incentives). Votes and QCs include an optional commitment value prepared for Libra clients. When a voter detects that gathering a QC on the voted block will trigger a commit for an earlier block in the chain, it must populate the field commitment with the execution state of that earlier block (see also the commit rule in Section 5.3 below). In this way, a QC with a non-empty field commitment acts as commit certificate — that is, a short cryptographic proof that a particular state was committed. Since we included the epoch identifier in the QC, such a commit certificate can be verified in isolation as long as one knows the set of validators for this epoch. We include a redundant round number in votes and QCs for technical reasons related to datasynchronization messages (Appendix A.3). When necessary, we will distinguish network records — records that have just been received from the network — from verified records, which have been thoroughly verified to ensure strong invariants defined in the next section (Section 4.2). However, we generally use records for verified records when the context is clear. Importantly, invariants enforced by record verification (e.g., chaining rules) and the initial conditions guarantee that the records of a given epoch form a tree — with the exception of timeouts, which are  9  not chained. For each node, the data structure holding the records of an epoch is called a record store. We say that a node knows a record if it is present in the record store of its current epoch. We sketch the possible interfaces of the record store object in Appendix A.2. We will make explicit when nodes may delete (clean up) records from their record stores to minimize storage as part of the description of the protocol in Section 5.7. Data structures in Rust. We assume the following primitive data types: • • • • • •  EpochId (an integer). Round (an integer). NodeTime (the system time of a node). BlockHash and QuorumCertificateHash (hash values). Author (identifier of a consensus node). Signature (a digital signature).  The network records of LibraBFT are specified using Rust syntax in Table 1. Hashing and Signing. We assume that the data fields of records can be hashed to produce deterministic hashing values. By deterministic hashes, we mean that the hashes of two data structures should be equal if and only if the content of the data structures are equal (also see Section 2.4 on cryptographic assumptions). The hashing of records should include a type-related tag followed by all the fields in the record, except the field signature. Signatures of records apply to their hashing value. The signatures in the vector of votes of a QC are copied from the original Vote records that were selected by the author of the QC.  4.2. Veriﬁcation of Network Records At the beginning of an epoch, consensus nodes agree on an initial value ℎinit of type QuorumCertificateHash. For example, we may define ℎinit = hash(seed || epoch_id) for some fixed value seed. Every consensus node sequentially verifies all the records that it receives from the network: • All signatures should be valid signatures from a node of the current epoch. • BlockHash values should refer to previously verified blocks. • QuorumCertificateHash values should refer to verified quorum certificates or the initial hash ℎinit . • Rounds should be strictly increasing for successive blocks in a chain of blocks and quorum certificates. Round numbers in proposed blocks restart at round 1 at every epoch. • The author of a QC should be the author of the previous block. • Epoch identifiers in timeouts, votes, and QCs must match the current epoch. • Round values in votes and QCs must match the round of the certified block. • The commitment value in a vote or in quorum certificate should be consistent with the commit rule (Section 5.3). If present, it should be signed by the same set of authors as the certified block. • Network records that fail to verify should be skipped. Given the constraints on hashes of blocks and QCs, except for timeouts, the verified records known to a node form a tree whose root is the value ℎinit (Lemma S1).  10  /// A record read from the network. enum Record { /// Proposed block, containing a command, e.g. a set of Libra transactions. Block(Block), /// A single vote on a proposed block and its execution state. Vote(Vote), /// A quorum of votes related to a given block and execution state. QuorumCertificate(QuorumCertificate), /// A signal that a particular round of an epoch has reached a timeout. Timeout(Timeout), } struct Block { /// User-defined command to execute in the state machine. command: Command, /// Time proposed for command execution. time: NodeTime, /// Hash of the quorum certificate of the previous block. previous_quorum_certificate_hash: QuorumCertificateHash, /// Number used to identify repeated attempts to propose a block. round: Round, /// Creator of the block. author: Author, /// Signs the hash of the block, that is, all the fields above. signature: Signature, } struct Vote { /// The current epoch. epoch_id: EpochId, /// The round of the voted block. round: Round, /// Hash of the certified block. certified_block_hash: BlockHash, /// Execution state. state: State, /// Execution state of the ancestor block (if any) that will match /// the commit rule when a QC is formed at this round. commitment: Option<State>, /// Creator of the vote. author: Author, /// Signs the hash of the vote, that is, all the fields above. signature: Signature, } struct QuorumCertificate { /// The current epoch. epoch_id: EpochId, /// The round of the certified block. round: Round, /// Hash of the certified block. certified_block_hash: BlockHash, /// Execution state state: State, /// Execution state of the ancestor block (if any) that matches /// the commit rule thanks to this QC. commitment: Option<State>, /// A collections of votes sharing the fields above. votes: Vec<(Author, Signature)>, /// The leader who proposed the certified block should also sign the QC. author: Author, /// Signs the hash of the QC, that is, all the fields above. signature: Signature, } struct Timeout { /// The current epoch. epoch_id: EpochId, /// The round that has timed out. round: Round, /// Creator of the timeout object. author: Author, /// Signs the hash of the timeout, that is, all the fields above. signature: Signature, }  Table 1: Network records in LibraBFT 11  4.3. Communication Framework In LibraBFT, the communication framework builds a peer-to-peer overlay for the reliable dissemination of protocol records (Section 4.1) among the validators. The framework API consists of two primitives actions: (i) send, where the node updates a peer with the records it has; (ii) broadcast, where the node disseminates updates to all its peers. In order to provide reliable delivery guarantees to honest nodes, LibraBFT builds gossip overlay on top of a point-to-point synchronization protocol. Briefly, a node that wishes to broadcast sends its data to a random subset of at least 𝐾 nodes, for some fixed value 𝐾 (0 < 𝐾 ≤ 𝑁 ). Receiving nodes reshare relevant data in the same way. The exact nature of the “relevant” data sent and reshared during network actions is part of the description of the LibraBFT protocol (Section 7.11). In the first reading, one may simply consider that a node reshares every valid record that it knows every time its record store is updated. Resharing data is important to make the broadcast action reliable [24] in the following sense: if an honest node receives (relevant) data, then — with high probability — every other honest node will know about these data shortly after. Importantly, this holds even if the origin of the data is a malicious node. Randomized gossip provides the following broadcast guarantee: (probabilistic-reliable-broadcast) After GST, if an honest node receives or possesses data that requires gossiping, then — with high probability — before an (unknown) time delay 𝛿𝐺 > 0, every other honest node will have received these data. We interpret this requirement in a broad sense that allows data to be updated along the way. We also allow several senders to initiate the gossiping of the same data in parallel during an interval of time [𝑡1 ; 𝑡2 ] and assume that the data is received by all honest nodes by time 𝑡2 + 𝛿𝐺 . Note on choosing the fan-out factor. The fan-out factor 𝐾 is generally chosen to be much smaller than the number of nodes, as a trade-off between networking delay and scalability. We leave for future work how to choose a value 𝐾 so that the requirement above follows from the classic assumption (eventually-synchronous-network) on single messages after GST. For now, we may simply choose 𝐾 = 𝑁 to include every node, and let 𝛿𝐺 = 𝛿𝑀 .  4.4. Data Synchronization Because of resharing and the possibility of crashes, a naive approach, where senders push their records directly to receivers, would lead to receiving and retransmitting the same data many times. In general, we wish to let senders initiate communication but give more control to receivers on how they consume data. In LibraBFT, this is addressed by introducing an exchange protocol called data synchronization. Sending data from one sender to other nodes is done in multiple steps: • Make the new data available (i.e., passively publish it) as part of the data-synchronization service of the sender. • Send a notification, called a DataSyncNotification message, to each receiver according to the nature of the communication: a single receiver for point-to-point communication, a random subset otherwise.  12  • Let receivers connect back to the sender with DataSyncRequest message and retrieve data contained in a DataSyncResponse message. When the exchange is completed, a receiver should validate received data immediately, then make all valid and relevant data available as a server. As mentioned earlier, new data may require notifying further nodes to complete the reliable broadcast.  4.5. Runtime Environment For the purpose of specifying and simulating LibraBFT, we abstract away details about processes, networking, timers, and, generally, the operating systems of nodes under the generic term runtime environment. We will specify the behavior of LibraBFT nodes as the combination of a private local state and a small number of algorithms called handlers. A handler typically mutates the local state of the current node and returns a value. Specifications will require the runtime environment to call handlers at specific times and interpret returned values right away.  4.6. Data-Synchronization Handlers We now specify the handlers for data synchronization. We require the runtime environment to carry the messages sketched in Section 4.4 (namely DataSyncNotification, DataSyncRequest, and DataSyncResponse) to their intended recipient in an authenticated channel, at a speed depending on the current network conditions. We create three corresponding handlers to be called when a message is received and returning a possible answer. An additional handler create_notification is used when the main handler of node (Section 5.6) requests the environment to send a notification to specific senders. Interface in Rust. We express the data-synchronization handlers as a Rust trait as follows: trait DataSyncNode { /// Sender role: what to send to initiate a data-synchronization exchange. /// We only include our current vote when notifying a proposer. fn create_notification(&self, include_vote: bool) -> DataSyncNotification; /// Sender role: handle a request from a receiver. fn handle_request(&mut self, request: DataSyncRequest) -> DataSyncResponse; /// Receiver role: accept or refuse a notification from an authenticated sender. fn handle_notification( &mut self, authenticated_sender: Author, notification: DataSyncNotification, smr_context: &mut SMRContext, ) -> Option<DataSyncRequest>; /// Receiver role: receive data from an authenticated sender. fn handle_response( &mut self, authenticated_sender: Author, response: DataSyncResponse, smr_context: &mut SMRContext, ); }  Data-synchronization handlers continuously query and update the record store of a node, independently from the main handler of the protocol, which will be presented in Section 5. Possible definitions for the three message types are given in Appendix A.3.  13  4.7. Mathematical Notations We have seen that records that fail to be verified are rejected from receiving nodes. Unless mentioned otherwise, all records considered from now on are verified records. We use the letter 𝛼 to denote a node of the protocol. We write record_store(𝛼) for the record store of 𝛼 at a given time. We use the symbol || to denote the concatenation of bit strings. We introduce the following notations regarding records: • We use the letter 𝐵 to denote block values; 𝐶 to denote quorum certificates; 𝑉 for votes; 𝑇 for timeouts; and, finally, 𝑅 to denote either blocks or certificates. • We use ℎ, ℎ1 , etc., to denote hash values of type QuorumCertificateHash or BlockHash. We use letters 𝑛, 𝑛1 , etc., for rounds. • We write round(𝐵) for the field round of a block and, more generally, foo(𝑅) for any field foo of a record 𝑅. • If ℎ = certified_block_hash(𝐶), we write ℎ ← 𝐶. Similarly, we write ℎ ← 𝑉 in case of a single vote 𝑉 . If ℎ = previous_quorum_certificate_hash(𝐵), we write ℎ ← 𝐵. • More generally, we see ← as a relation between hashes, blocks, votes, and quorum certificates. We may write 𝐵 ← 𝐶 instead of hash(𝐵) ← 𝐶, 𝐵 ← 𝑉 for hash(𝐵) ← 𝑉 , and 𝐶 ← 𝐵 instead of hash(𝐶) ← 𝐵. • Finally, we write ←∗ for the transitive and reflexive closure of ←, that is: 𝑅0 ←∗ 𝑅𝑛 if and only if 𝑅0 ← 𝑅1 … ← 𝑅𝑛 , 𝑛 ≥ 0.  5. The LibraBFT Protocol 5.1. Overview of the Protocol Each consensus node 𝛼 maintains a local tree of records for the current epoch, previously noted record_store(𝛼). The initial root of the tree, a QC hash noted ℎinit , is agreed upon as part of the setup for the consensus epoch. Each branch in the tree is a chain of records, alternating between blocks 𝐵𝑖 and quorum certificates 𝐶𝑖 . Formally, such as a chain is denoted: ℎinit ← 𝐵1 ← 𝐶1 … ← 𝐵𝑛 [← 𝐶𝑛 ]. When a node acts as a leader (Figure 3), it must propose a new block of transactions 𝐵𝑛+1 , usually extending the tail quorum certificate 𝐶𝑛 of (one of) its longest branch(es) ( 1 ). Assuming that the proposal 𝐵𝑛+1 is successfully broadcast, honest nodes will verify the data, execute the new block, and send back a vote to the leader ( 2 ). In the absence of execution bugs, honest nodes should agree on the execution state after 𝐵𝑛+1 . Upon receiving enough votes agreeing with this execution state, the proposer will create a quorum certificate 𝐶𝑛+1 for this block and broadcast it ( 3 ). The chain length has now increased by one: ℎinit ← 𝐵1 ← 𝐶1 … ← 𝐵𝑛+1 ← 𝐶𝑛+1 . At this point, the leader is considered done, and another leader is expected to extend the tree with a new proposal. Due to network delays and malicious nodes, honest nodes may not always agree on the “best” branch to extend and for which blocks to vote. Under BFT assumption (Section 2.3), the voting constraints observed by honest nodes guarantee that when a branch grows enough to include a block 𝐵 that satisfies the commit rule, 𝐵 and its predecessors cannot be challenged by conflicting proposals anymore. These blocks are thus committed in order to advance the replicated state machine. To guarantee progress despite malicious nodes or unresponsive leaders, each proposal includes a round number. A round will time out after a certain time. When the next round becomes active, a new  14  B1  V1  C1  B2  V2  C2  B3  1  2  3  1  2  3  1  V3  C3  node3  node2  node1  node0  round 1 (leader: node3)  round 2 (leader: node0)  2  3  round 3 (leader: node1)  Figure 3: Overview of the LibraBFT protocol (simplified, excluding round synchronization)  leader is expected to propose a block. The pacemaker abstraction (Section 7.3) aims to make honest nodes agree on a unique, active round for sufficiently long periods of time. We can now rephrase the main goals of the LibraBFT protocol as follows: (safety) New commits always extend a chain containing all the previous commits. (liveness) If the network is synchronous for a sufficiently long time, eventually a new commit is produced. Layout of the description of LibraBFT. In the rest of this section, we make precise the commit rule (Section 5.3) and the voting constraints (Section 5.4 and Section 5.5). Then, using the communication framework described previously in Section 4, we proceed to describe the local state and the behaviors of nodes in the LibraBFT protocol (Section 5.6 and Section 5.7). This section provides the prerequisites for the proof of safety given in Section 6. Liveness mechanisms will be presented in Section 7 and followed by the proof of liveness in Section 8.  5.2. Chains A 𝑘-chain is a sequence of 𝑘 blocks and 𝑘 QCs: 𝐵0 ← 𝐶0 ← … ← 𝐵𝑘−1 ← 𝐶𝑘−1 𝐵0 is called the head of such a chain. 𝐶𝑘−1 is called the tail. Recall that by definition of the notion of round for blocks, rounds must be strictly increasing along a chain: round(𝐵𝑖 ) < round(𝐵𝑖+1 ). When rounds increase exactly by one — that is, round(𝐵𝑖 ) + 1 = round(𝐵𝑖+1 ) — we say that the chain has contiguous rounds. In practice, the round numbers of a chain may fail to be contiguous for many reasons. For example, a dishonest leader may propose an invalid block, or a leader may fail to gather a quorum of votes in a timely manner because of network issues. When a quorum certificate is not produced in a round, a leader at a higher round will eventually propose a block that breaks contiguity in the chain.  15  5.3. Commit Rule A block 𝐵0 is said to match the commit rule of HotStuff in the record store of a node if and only if it is the head of a 3-chain with contiguous rounds, that is, there exist 𝐶0 , 𝐵1 , 𝐶1 , 𝐵2 , 𝐶2 such that 𝐵0 ← 𝐶 0 ← 𝐵 1 ← 𝐶 1 ← 𝐵 2 ← 𝐶 2 and round(𝐵2 ) = round(𝐵1 ) + 1 = round(𝐵0 ) + 2 When such a commit rule is observed by a node 𝛼, the blocks preceding 𝐵0 in the record store of 𝛼, and 𝐵0 itself becomes committed. Following our previous discussion (Section 4.1) on commitments, a valid quorum certificate in the position of 𝐶2 acts as a commit certificate: it must include a non-empty field value commitment to authenticate that the execution state state(𝐶0 ) was committed in the current epoch. Note that if commitment is empty, then 𝐶2 is not a valid record, and it should be ignored (Section 4.2).  5.4. First Voting Constraint: Increasing Round Safety of the commit rule relies on two voting constraints. The first one concerns the rounds of voted blocks: (increasing-round) An honest node that voted once for 𝐵 in the past may only vote for 𝐵′ if round(𝐵) < round(𝐵′ ). This voting constraint is important for quorum certificates (see Section 6). In practice, a node 𝛼 will track the round of its latest vote in a local variable noted latest_voted_round(𝛼), and only vote for a block 𝐵 if round(𝐵) > latest_voted_round(𝛼).  5.5. Second Voting Constraint: Locked Round The locked round of a node 𝛼, written locked_round(𝛼), is the highest round of the head of a 2-chain ever known to 𝛼, if any, and zero otherwise. In practice, we may initialize the value locked_round(𝛼) to 0 and update it to round(𝐵0 ) whenever a new 2-chain 𝐵0 ← 𝐶0 ← 𝐵1 ← 𝐶1 such that round(𝐵0 ) > locked_round(𝛼) is found in record_store(𝛼). We also define the previous round of a block 𝐵 as follows: if there exist 𝐵′ and 𝐶 ′ such that 𝐵′ ← 𝐶 ′ ← 𝐵, we let previous_round(𝐵) = round(𝐵′ ); otherwise, previous_round(𝐵) = 0. We can now formulate our second voting constraint: (locked-round) An honest node 𝛼 may only vote for a block 𝐵 if it currently holds that previous_round(𝐵) ≥ locked_round(𝛼). The voting constraint (locked-round) was adapted from the most recent version of HotStuff [5]. In LibraBFT, it is simplified into a single-clause condition.  5.6. Local State of a Consensus Node and Main Handler API We can now describe the protocol followed by a LibraBFT node in terms of a local state and a handler called by the runtime environment (Section 4.5).  16  Local state. As mentioned previously, the core component of the state of a node 𝛼 consists of the current record store (Section 4.2) — written as record_store(𝛼) — which contains all the verified records that 𝛼 knows for its current epoch. The state of node also includes a number of variables related to leader election. We group them into a special object called a pacemaker and describe them in Section 7.3. Additional state variables needed by an instance of the protocol include: • • • • •  The current epoch identifier epoch_id(𝛼), used to detect the end of the current epoch; The identifier of 𝛼 as an author of records, written local_author(𝛼); The round of the latest voted block latest_voted_round(𝛼), (initial value: 0); The locked round locked_round(𝛼), (initial value: 0); The identities and the active rounds of the latest nodes that synchronized with us, denoted latest_senders (initial value: the empty list); and • The system time of the last broadcast latest_broadcast(𝛼), (initial value: the starting time of the epoch). The state of a node also includes an object, called data tracker, responsible for tracking data that the main handler has already processed, notably commits, and deciding if new data need to be reshared. We give more details about the data tracker in Section 7.11. Finally, the state of a node includes the record stores of all the previous epochs. Those epochs are now stopped, meaning that no new records can be inserted. Main handler API. In LibraBFT, the main handler of a consensus node consists of a single algorithm update_node that must be called by the runtime environment on three occasions: • Whenever the node starts or restarts after a crash; • Whenever a data-synchronization exchange was completed (Section 4.6); and • Regularly, at a given time scheduled by the last run of the handler itself. The main handler reacts to the changes observed in the record store or in the clock by returning a list of action items to be carried by the runtime environment. Specifically: • The main handler may require that a new call to update_node be scheduled at a given time in the future; • It may specify that a data notification should be sent to a particular leader; and • It may ask to broadcast data notifications. The implementation of the main handler is the core of the LibraBFT protocol. It is described in detail in Section 5.7. Rust definitions. In Rust, the local state of a node is written as follows: struct NodeState { /// Module dedicated to storing records for the current epoch. record_store: RecordStoreState, /// Module dedicated to leader election. pacemaker: PacemakerState, /// Current epoch. epoch_id: EpochId, /// Identity of this node. local_author: Author, /// Highest round voted so far. latest_voted_round: Round, /// Current locked round. locked_round: Round, /// Time of latest broadcast. latest_broadcast: NodeTime, /// Names and rounds of the latest senders during network communication.  17  latest_senders: Vec<(Author, Round)>, /// Track data to which the main handler has already reacted. tracker: DataTracker, /// Record stores from previous epochs. past_record_stores: HashMap<EpochId, RecordStoreState>, }  The main handler API, update_node, is written: trait ConsensusNode { fn update_node(&mut self, clock: NodeTime, smr_context: &mut SMRContext) -> NodeUpdateActions; }  This definition assumes that the runtime environment provides the following inputs: • The current node state (self in Rust); • The current system time (clock); and • A context for SMR operations such as command execution (smr_context). Recall that the trait ConsensusNode comes in addition to the trait DataSyncNode (Section 4.6), which provide handlers for data synchronization. The trait SMRContext is made precise in Appendix A.1. Action items returned by the function update_node are held in the following data structure: struct NodeUpdateActions { /// Time at which to call `update_node` again, at the latest. should_schedule_update: Option<NodeTime>, /// Whether we need to send a notification to a leader. should_notify_leader: Option<Author>, /// Whether we need to send notifications to a random subset of nodes. should_broadcast: bool, }  5.7. Main Handler Implementation We now describe the implementation of the main handler of a LibraBFT node in Rust (Table 2). At a high level, the algorithm realizes the following operations: • Run the pacemaker module and execute requested pacemaker actions, such as creating a timeout or proposing a block. • Execute and vote for a valid proposed block, if any, while respecting the two voting constraints regarding the latest voted round (Section 5.4) and the locked round (Section 5.5). • If the node proposed a block and if a quorum of votes for the same state was received, create a quorum certificate. • Check for newly found commits in the record store and deliver them to the state-machine replication context. • Check for a commit that would terminate the current epoch, and start a new one if needed. • Decide if the node should reshare (i.e., gossip) its data. This algorithm relies on the record store of a node for computing the round of the highest head of a known 2-chain (highest_2chain_head_round), the head of the highest commit rule (highest_committed_round), and the tail QC of a commit rule (commit_certificate). The method chain_between_quorum_certificates takes two rounds 𝑚 and 𝑛 (𝑚 ≤ 𝑛) as inputs. If the record store contains a chain 𝐵0 ← 𝐶0 ← 𝐵1 ← 𝐶1 ← … ← 𝐵𝑘 ← 𝐶𝑘 with round(𝐵0 ) = 𝑚 and round(𝐵𝑘 ) = 𝑛, then it will return an iterator on the QCs 𝐶1 , … , 𝐶𝑘 , in this order. (See Appendix A.2 for detailed interfaces.)  18  impl ConsensusNode for NodeState { fn update_node(&mut self, clock: NodeTime, smr_context: &mut SMRContext) -> NodeUpdateActions { // Update pacemaker state and process pacemaker actions (e.g., creating a timeout, proposing a block). let latest_senders = self.read_and_reset_latest_senders(); let pacemaker_actions = self.pacemaker.update_pacemaker( self.local_author, &self.record_store, self.latest_broadcast, latest_senders, clock, ); let mut actions = self.process_pacemaker_actions(pacemaker_actions, smr_context); // Update locked round. self.locked_round = std::cmp::max(self.locked_round, self.record_store.highest_2chain_head_round()); // Vote on a valid proposal block designated by the pacemaker, if any. if let Some((block_hash, block_round, proposer)) = self.record_store.proposed_block(&self.pacemaker) { // Enforce voting constraints. if block_round > self.latest_voted_round && self.record_store.previous_round(block_hash) >= self.locked_round { // Update latest voted round. self.latest_voted_round = block_round; // Try to execute the command contained the a block and create a vote. if self.record_store.create_vote(self.local_author, block_hash, smr_context) { // Ask that we reshare the proposal. actions.should_broadcast = true; // Ask to notify and send our vote to the author of the block. actions.should_notify_leader = Some(proposer); } } } // Check if our last proposal has reached a quorum of votes and create a QC. if self.record_store.check_for_new_quorum_certificate(self.local_author, smr_context) { // The new QC may cause a change in the pacemaker state: schedule a new run of this handler now. actions.should_schedule_update = Some(clock); } // Check for new commits and verify if we should start a new epoch. for commit_qc in self .record_store .chain_between_quorum_certificates( self.tracker.highest_committed_round, self.record_store.highest_committed_round(), ) .cloned() { // Deliver the new committed state, together with a short certificate (if any). smr_context.commit(&commit_qc.state, self.record_store.commit_certificate(&commit_qc)); // If the current epoch ended.. let epoch_id = smr_context.read_epoch_id(&commit_qc.state); if self.epoch_id != epoch_id { // .. create a new record store and switch to the new epoch. self.start_new_epoch(epoch_id, commit_qc, smr_context); // .. stop delivering commits after an epoch change. break; } } // Update the data tracker and ask that we reshare data if needed. if self.tracker.update_and_decide_resharing(self.epoch_id, &self.record_store) { actions.should_broadcast = true; } // Return desired node actions to environment. actions } }  Table 2: Main handler of a LibraBFT node  19  The main handler also uses additional interfaces related to liveness and described in later sections: • The Pacemaker trait provides a function update_pacemaker to control leader election, timeouts, and proposals; the returned action items are processed by a method process_pacemaker_actions; and the method proposed_block of RecordStore also uses the pacemaker to select an active proposal that a node can vote for, if any (Section 7.3). • The DataTracker object provides the latest commit processed so far, as well as a method update_and_decide_resharing to update the latest commit value (among others) and control gossiping (Section 7.11). Epoch changes. As soon as a node delivers a commit QC that ends the current epoch, it stops delivering commits for this epoch, archives its current record store, and creates a record store for the new epoch. Nodes must keep the record stores of the previous epochs to ensure liveness: during data synchronization, a node must be able to follow the chain of commits and execute commands up to the latest commit rule of the latest epoch of any sender (see also Section 3.1 and Section 7.11). The chain of commits between the block containing the epoch changes and the block triggering the commit rule may be arbitrarily long depending on network conditions. To avoid persisting data that will not be committed, we may require proposers to propose only empty commands once an epoch change is detected on a branch.  6. Proof of Safety In the proof of safety, we consider the set of all records ever seen by honest nodes in the current epoch and prove that committed blocks must form a linear chain ℎinit ← 𝐵1 ← 𝐶1 ← 𝐵2 … ← 𝐵𝑛 , starting from the initial QC hash ℎinit of the epoch.  6.1. Preliminaries We start by recalling the proof of the classical BFT lemma: Lemma B1: Under BFT assumption, for every two quorums of nodes in the same epoch, there exists an honest node that belongs to both quorums. Proof: Let 𝑀𝑖 ≥ 𝑁 −𝑓 (𝑖 = 1, 2) be the combined voting power of each quorum. The voting powers 𝑀𝑖′ of each quorum, excluding Byzantine nodes, satisfies 𝑀𝑖′ ≥ 𝑀𝑖 − 𝑓 ≥ 𝑁 − 2𝑓. We note that if the two sets were disjoint, the voting power of the union 𝑀1′ + 𝑀2′ ≥ 2𝑁 − 4𝑓 > 𝑁 − 𝑓 would exceed the voting power of all honest nodes. Therefore, there exists an honest node in both quorums. □ Next, we prove two new lemmas. The first one concerns the chaining of records. Lemma S1: For any records 𝑅, 𝑅0 , 𝑅1 , 𝑅2 : • ℎinit ←∗ 𝑅; • If 𝑅0 ← 𝑅2 and 𝑅1 ← 𝑅2 then 𝑅0 = 𝑅1 ; and • If 𝑅0 ←∗ 𝑅2 , 𝑅1 ←∗ 𝑅2 and round(𝑅0 ) < round(𝑅1 ) then 𝑅0 ←∗ 𝑅1 .  20  Proof: • By definition of verified records (Section 4.2). • By definition of chaining, assuming that hashing is perfectly collision resistant. • By induction on the derivation 𝑅1 ←∗ 𝑅2 , using the previous item and the fact that rounds cannot decrease in a chain. □ The second lemma concerns the first voting rule and the notion of quorum certificates. Lemma S2: Consider two blocks with QCs: 𝐵 ← 𝐶 and 𝐵′ ← 𝐶 ′ . Under BFT assumption, if round(𝐵) = round(𝐵′ ), then 𝐵 = 𝐵′ and state(𝐶) = state(𝐶 ′ ). In particular, for every 𝑘 > 0, there is a unique block that has the highest round amongst the heads of 𝑘-chains known to a node. Proof: Under BFT assumption, there must exist an honest node that voted both for the winning proposal state(𝐶) in 𝐶 and for state(𝐶 ′ ) in 𝐶 ′ . By the voting rule (increasing-round), we must have 𝐵 = 𝐵′ and state(𝐶) = state(𝐶 ′ ). □  6.2. Main Safety Argument We say that two (distinct) records 𝑅, 𝑅′ are conflicting when neither 𝑅 ←∗ 𝑅′ nor 𝑅′ ←∗ 𝑅. Lemma S3: Assume a 3-chain starting at round 𝑛0 and ending at round 𝑛2 . For every certified block 𝐵 ← 𝐶, such that round(𝐵) > 𝑛2 , under BFT assumption, we have previous_round(𝐵) ≥ 𝑛0 . Proof: Let 𝐵0 ← 𝐶0 ← 𝐵1 ← 𝐶1 ← 𝐵2 ← 𝐶2 be a 3-chain starting at round 𝑛0 = round(𝐵0 ) and ending at round 𝑛2 = round(𝐵2 ). Under BFT assumption, there exists an honest node 𝛼 whose vote is included both in 𝐶2 (voting for 𝐵2 ) and 𝐶 (voting for 𝐵). Since round(𝐵) > 𝑛2 , by the voting rule (increasing-round), 𝛼 must have voted for 𝐵2 first. At that time, 𝛼 had already seen the 2-chain starting with 𝐵0 . Since the locked round never decreases, its locked round was at least round(𝐵0 ) = 𝑛0 . At the later time of voting for 𝐵, the locked round of 𝛼 was again at least 𝑛0 . Therefore, the voting rule (locked-round) implies that previous_round(𝐵) ≥ 𝑛0 . □ Proposition S4: Assume a 3-chain with contiguous rounds starting with a block 𝐵0 at round 𝑛0 . For every certified block 𝐵 ← 𝐶, such that round(𝐵) ≥ 𝑛0 , under BFT assumption, we have 𝐵0 ←∗ 𝐵. Proof: By induction on round(𝐵) ≥ 𝑛0 . Let 𝐵0 ← 𝐶0 ← 𝐵1 ← 𝐶1 ← 𝐵2 ← 𝐶2 be a 3-chain starting with 𝐵0 and with contiguous rounds: round(𝐵0 )+2 = round(𝐵1 )+1 = round(𝐵2 ) = 𝑛0 +2. If round(𝐵) ≤ 𝑛0 + 2, then round(𝐵) is one of the values 𝑛0 , 𝑛0 + 1, 𝑛0 + 2. By Lemma S2, 𝐵 is one of the values 𝐵0 , 𝐵1 , 𝐵2 ; therefore, 𝐵0 ←∗ 𝐵. Otherwise, assume round(𝐵) > 𝑛0 + 2, that is, round(𝐵) > round(𝐵2 ). By Lemma S3, we have previous_round(𝐵) ≥ 𝑛0 . Since 𝑛0 = round(𝐵0 ) > 0, this means there exists a chain 𝐵3 ← 𝐶3 ← 𝐵 such that round(𝐵3 ) ≥ 𝑛0 . Since round(𝐵3 ) ≥ 𝑛0 and round(𝐵3 ) < round(𝐵), we may apply the induction hypothesis on 𝐵3 to deduce that 𝐵0 ←∗ 𝐵3 . Therefore, 𝐵0 ←∗ 𝐵3 ←∗ 𝐵 concludes the proof. □ Theorem S5 (Safety): Under BFT assumption, two blocks that match the commit rule cannot be conflicting.  21  Proof: Consider the commit rules of two blocks 𝐵0 and 𝐵0′ that match the commit rule: 𝐵0 ← 𝐶0 ← 𝐵1 ← 𝐶1 ← 𝐵2 ← 𝐶2 and 𝐵0′ ← 𝐶0′ ← 𝐵1′ ← 𝐶1′ ← 𝐵2′ ← 𝐶2′ (with contiguous rounds in both cases). Without loss of generality, we may assume that round(𝐵0′ ) ≥ round(𝐵0 ). By Proposition S4, this implies 𝐵0 ←∗ 𝐵0′ . □ Corollary S6: Under BFT assumption, the set of all commits seen by any honest node since the beginning of the current epoch form a linear chain ℎinit ← 𝐵1 ← 𝐶1 ← 𝐵2 … ← 𝐵𝑛 . Proof: Using Theorem S5, by induction on the number of commits. □  7. Liveness Mechanisms of LibraBFT LibraBFT follows the example of HotStuff [5] and delegates leader election to a special module called a pacemaker. We now describe the pacemaker of LibraBFT in detail, as well as the policies for resharing data and cleaning the record store. These mechanisms are all crucial for liveness, as we will see in the proofs of Section 8.  7.1. Timeout certiﬁcates. We define a Timeout Certificate (TC) as a set of timeout objects at the same round 𝑛, such that the combined voting power of timeout authors exceeds 𝑓. The round of a timeout certificate is the common round value 𝑛.  7.2. Overview of the Pacemaker Given the latest committed block and its QC, noted 𝐵𝑐 ← 𝐶𝑐 , we assign a leader and a maximum duration to every round number 𝑛 > round(𝐵𝑐 ) + 2. A consensus node enters a round 𝑛 whenever it receives a QC at round 𝑛 − 1, or enough timeouts to form a TC at round 𝑛 − 1, whichever comes first. The round 𝑛 is then considered active until the node enters round 𝑛 + 1. While a node has active round 𝑛, it may only vote for a block at round 𝑛 authored by the leader of round 𝑛. To achieve liveness despite malicious or unresponsive leaders, nodes start a timer when they enter a round and verify that the elapsed time has not passed the current maximum duration of the round. New timeout objects are gossiped immediately. Once enough timeouts have been shared, a node will observe a TC and change their active round — that is, unless the node learned a QC first, which also updates the active round. The maximum duration of an active round is subject to be updated if the latest committed block changes while the round is active. Importantly, when a node enters a round that makes it a leader, it must wait for a quorum of nodes to confirm that they entered the round as well before proposing a block. This is important so that the proposed block can provably meet the second voting constraint computed by a quorum of honest nodes (Section 8.3). Finally, during periods of asynchrony, some nodes may become unaware of the latest committed block. To recover from this situation after GST, we make sure that nodes broadcast their states at least once per period of time 𝐼 > 0.  22  7.3. Pacemaker State and Update API We specify the pacemaker module at a high level in the same way as the node itself (Section 5.6) in terms of local state and an update API. Visible pacemaker state. The pacemaker module is in charge of driving leader election. As such, it exposes two important state values to the other components of a LibraBFT node: • The current active round, denoted active_round(𝛼), (initial value: 0); • The leader of the current round, written active_leader(𝛼), (initial value: ⊥). These two values are accessed notably by the function proposed_block of the record store, which was used earlier in the main handler update_node (Section 5.7). Specifically, this function ensures that the block 𝐵 that we may vote for satisfies round(𝐵) = active_round(𝛼) and author(𝐵) = active_leader(𝛼). The rest of the pacemaker state will be described in Section 7.9. Pacemaker update API. We expect a pacemaker module to provide a method update_pacemaker, meant to be called by the higher-level method update_node. This method should refresh the state of the pacemaker in the function of the input arguments and return a list of action items for the main handler of a node to process. Action items returned by update_pacemaker are similar to the action items returned by update_node, augmented with the two internal action items: • The pacemaker may instruct the node to create a timeout object for the given round. • The pacemaker may require that the node act as the leader and propose a new block. Rust definitions. The Rust trait for a pacemaker module is written as follows: trait Pacemaker { /// Update our state from the given data and return some action items. fn update_pacemaker( &mut self, // Identity of this node local_author: Author, // Tree of records record_store: &RecordStore, // Local time of the latest broadcast by us latest_broadcast: NodeTime, // Known active rounds of recent senders latest_senders: Vec<(Author, Round)>, // Current local time clock: NodeTime, ) -> PacemakerUpdateActions; /// Current active round and current leader. fn active_round(&self) -> Round; fn active_leader(&self) -> Option<Author>; }  The parameters passed to update_pacemaker were described previously in the main handler update_node (Table 2). The pacemaker action items returned by update_pacemaker can be described as follows: struct PacemakerUpdateActions { /// Time at which to call `update_pacemaker` again, at the latest. should_schedule_update: Option<NodeTime>, /// Whether we should create a timeout object for the given round. should_create_timeout: Option<Round>, /// Whether we need to send our records to the given next leader. should_notify_leader: Option<Author>, /// Whether we need to broadcast our records.  23  should_broadcast: bool, /// Whether to propose a block and on top of which QC hash. should_propose_block: Option<QuorumCertificateHash>, }  These action items are interpreted by update_node and turned into node action items as follows: impl NodeState { fn process_pacemaker_actions( &mut self, pacemaker_actions: PacemakerUpdateActions, smr_context: &mut SMRContext, ) -> NodeUpdateActions { let mut actions = NodeUpdateActions::new(); actions.should_schedule_update = pacemaker_actions.should_schedule_update; actions.should_broadcast = pacemaker_actions.should_broadcast; actions.should_notify_leader = pacemaker_actions.should_notify_leader; if let Some(round) = pacemaker_actions.should_create_timeout { self.record_store.create_timeout(self.local_author, round, smr_context); } if let Some(previous_qc_hash) = pacemaker_actions.should_propose_block { self.record_store.propose_block( self.local_author, previous_qc_hash, self.latest_broadcast, smr_context, ); } actions } }  7.4. Equivalence of Quorum Certiﬁcates To define leaders and maximum durations rigorously, including at the beginning of an epoch, we must take some precautions and introduce the notion of QC equivalence. Two QC hashes ℎ and ℎ′ are said to be equivalent, noted ℎ ≈ ℎ′ , if and only if one of the two conditions is fulfilled: 1) ℎ = ℎ′ ; or 2) There exist two quorum certificates 𝐶 and 𝐶 ′ and a block 𝐵, such as ℎ = hash(𝐶), ℎ′ = hash(𝐶 ′ ), 𝐵 ← 𝐶, 𝐵 ← 𝐶 ′ , and state(𝐶) = state(𝐶 ′ ). The first condition matters only for initial hashes. In the second case, we say that the quorum certificates 𝐶 and 𝐶 ′ are equivalent and write 𝐶 ≈ 𝐶 ′ . The reason behind this definition is that a block may be known to have a valid QC, for example, 𝐵0 ← 𝐶0 , but consensus nodes may temporarily disagree on 𝐶0 . Indeed, although we required 𝐶0 to be signed by the author of 𝐵0 , a dishonest proposer could select and aggregate votes in different ways and broadcast several variants of 𝐶0 . We have seen that under BFT assumption, all variants must be equivalent (Lemma S2) in the sense defined above. Notations: In the following, we will ℎ𝑐 for the hash of the QC of the latest committed block, if any; otherwise, the initial hash of the epoch. We will require that formulas for leaders and maximum durations depend on ℎ𝑐 up to equivalence — they may depend on the state and the committed block, but not the voters. When we use ℎ𝑐 as an input for a function, we will rely on the fact that a record store is available locally to each node and can resolve the latest commit hash ℎ𝑐 into actual data.  24  7.5. Prerequisite: Assigning Leaders to Rounds Given a suitable record store, a QC hash ℎ𝑐 , and a round 𝑛, we assume an algorithm leader(ℎ𝑐 , 𝑛) that returns an author in a fair way, meaning that all sequences of 𝑘 authors (𝑘 > 0) are equally frequent. As explained above (Section 7.4), we also require that ℎ𝑐 ≈ ℎ′𝑐 implies leader(ℎ𝑐 , 𝑛) = leader(ℎ′𝑐 , 𝑛). Assuming equal voting rights, the simplest approach is to let leader(ℎ𝑐 , 𝑛) = author(hash(𝑛) mod 𝑁 ), where 𝑁 is the number of nodes. However, this lets anyone predict leaders for a long time in advance. This is problematic as it facilitates the preparation of targeted attacks on leaders. We also note that depending on ℎ𝑐 in a naive way is not possible because of grinding attacks — a leader at round 𝑛 could try to select transactions, or votes, so that leader(ℎ𝑐 , 𝑛′ ) (𝑛′ > 𝑛 + 2) points to a particular node once 𝑛 = round(ℎ𝑐 ). To mitigate both risks, we intend to use a verifiable random function (VRF) [39] in the future. If the certified block under the QC hash ℎ𝑐 contains some seed 𝑠 = VRFauthor(ℎ𝑐 ) (epoch_id || round(ℎ𝑐 )), then we may define leader(ℎ𝑐 , 𝑛) = author(PRF𝑠 (𝑛) mod 𝑁 ) where PRF stands for the implementation of a pseudo-random function.  7.6. Prerequisite: Minimum Broadcast Interval We assume a time delay 𝐼 > 0 fixed as part of the protocol. The shorter the delay, the more responsive to a newly synchronous network we will be.  7.7. Prerequisite: Delay Coefﬁcients We assume that every node can compute some values Δ(ℎ𝑐 ) > 0 and 𝛾(ℎ𝑐 ) > 0 in function of available data in the chain of records ending with the QC hash ℎ𝑐 . • Δ(ℎ𝑐 ) represents the amount of time available for the first block proposer on top of ℎ𝑐 . • 𝛾(ℎ𝑐 ) is the exponent used to increase the time for subsequent proposers. • As before, we assume that ℎ𝑐 ≈ ℎ′𝑐 implies Δ(ℎ𝑐 ) = Δ(ℎ′𝑐 ) and 𝛾(ℎ𝑐 ) = 𝛾(ℎ′𝑐 ). As far as theoretical liveness is concerned, those values could be fixed: Δ(ℎ𝑐 ) = Δ0 > 0 and 𝛾(ℎ𝑐 ) = 2. However, we expect practical performance to depend on more meaningful values.  7.8. Assigning Durations to Rounds Assume a latest commit QC hash ℎ𝑐 at round 𝑛𝑐 . Let us write Δ = Δ(ℎ𝑐 ) and 𝛾 = 𝛾(ℎ𝑐 ) > 0 for the constants mentioned previously. We define a sequence of maximum duration for each 𝑛 > 𝑛𝑐 + 2: duration(ℎ𝑐 , 𝑛) = Δ ⋅ (𝑛 − 𝑛𝑐 − 2)𝛾 We make the following observations: • For the first round after the commit rule, duration(ℎ𝑐 , 𝑛𝑐 + 3) = Δ. • Since as Δ > 0 and 𝛾 > 0, when 𝑛 grows, duration(ℎ𝑐 , 𝑛) keeps increasing and is not bounded.  25  7.9. Pacemaker State We may now complete the specifications of the pacemaker state: struct PacemakerState { /// Active round active_round: Round, /// Leader of the active round active_leader: Option<Author>, /// Time at which we entered the round active_round_start: NodeTime, /// Nodes known to have switched to the same active round active_nodes: HashSet<Author>, /// Maximal time allowed between two broadcasts. broadcast_interval: Duration, /// Maximal duration of the first round after a commit rule. delta: Duration, /// Exponent to increase round durations. gamma: f64, }  7.10. Pacemaker Update Handler We now describe the implementation of the pacemaker update function seen in Section 7.3 using Rust (Table 3). The algorithm computes a set of actions to be interpreted by the node as follows: • After initializing actions with default values, the current active round active_round(𝛼) is set to 𝑘 + 1, where 𝑘 is the maximum value between: – The highest round of a quorum certificate (QC) in record_store(𝛼), if any; – The highest round of a timeout certificate (TC) in record_store(𝛼), if any; and – 0 = round(ℎinit ). • The current active leader active_leader(𝛼) is set to leader(ℎ𝑐 , active_round(𝛼)). • If active_round(𝛼) was just changed above: – Start a timer to track the duration of the round; – Reset the list of active nodes, defined as the set of nodes which have communicated to us that they shared the same active round as us (this is only useful if we are the leader of this round); and – Request that the node notify the new leader active_leader(𝛼) to be counted as an active node. • If the round of the latest_senders argument is the current active round active_round(𝛼), then we add the corresponding author to the set of active nodes. • If active_leader(𝛼) points to local_author(𝛼) and the set of active nodes form a quorum, then request that the node fetch a command and propose a block. Also, force a re-evaluation of the main handler so that we vote on our proposal immediately. • If we have not broadcast in an interval of time 𝐼, request a new broadcast. • If this active round has exceeded its maximal duration and we have not created a timeout yet, then request the creation of a timeout at round active_round(𝛼) and request a broadcast. • Finally, reschedule a run of the main handler (hence this function) to a time where we may have to broadcast or create a timeout.  26  fn update_pacemaker( &mut self, local_author: Author, record_store: &RecordStore, mut latest_broadcast: NodeTime, latest_senders: Vec<(Author, Round)>, clock: NodeTime, ) -> PacemakerUpdateActions { // Initialize actions with default values. let mut actions = PacemakerUpdateActions::new(); // Recompute the active round. let active_round = std::cmp::max( record_store.highest_quorum_certificate_round(), record_store.highest_timeout_certificate_round(), ) + 1; // If the active round was just updated.. if active_round > self.active_round { // .. store the new value self.active_round = active_round; // .. start a timer self.active_round_start = clock; // .. recompute the leader self.active_leader = Some(Self::leader(record_store, active_round)); // .. reset the set of nodes known to have entered this round (useful for leaders). self.active_nodes = HashSet::new(); // .. notify the leader to be counted as an "active node". actions.should_notify_leader = self.active_leader; } // Update the set of "active nodes", i.e. received synchronizations at the same active round. for (author, round) in latest_senders { if round == active_round { self.active_nodes.insert(author); } } // If we are the leader and have seen a quorum of active node.. if self.active_leader == Some(local_author) && record_store.is_quorum(&self.active_nodes) && record_store.proposed_block(&*self) == None { // .. propose a block on top of the highest QC that we know. actions.should_propose_block = Some(record_store.highest_quorum_certificate_hash().clone()); // .. force an immediate update to vote on our own proposal. actions.should_schedule_update = Some(clock); } // Enforce sufficiently frequent broadcasts. if clock >= latest_broadcast + self.broadcast_interval { actions.should_broadcast = true; latest_broadcast = clock; } // If we have not yet, create a timeout after the maximal duration for rounds. let deadline = if record_store.has_timeout(local_author, active_round) { NodeTime::never() } else { self.active_round_start + self.duration(record_store, active_round) }; if clock >= deadline { actions.should_create_timeout = Some(active_round); actions.should_broadcast = true; } // Make sure this update function is run again soon enough. actions.should_schedule_update = Some(std::cmp::min( actions.should_schedule_update.unwrap_or(NodeTime::never()), std::cmp::min(latest_broadcast + self.broadcast_interval, deadline), )); actions }  Table 3: Update function of the pacemaker  27  7.11. Resharing and Cleaning Records We have sketched the requirements for the data-synchronization protocol between nodes in a previous section (Section 4.4). We are now making more precise the minimal amount of data that must be exchanged during an instance of data synchronization. Whenever a node 𝛼 synchronizes with an honest sender 𝛼0 , we expect that: (sync-epoch) The current epoch of 𝛼 after synchronization is at least as recent as the one of 𝛼0 . Besides, whenever 𝛼 and 𝛼0 share the same epoch at the end of the synchronization (the usual case), we expect the following properties to hold: (sync-commits) The highest commit known to 𝛼 after synchronization is at least as high as the one of 𝛼0 . (sync-QCs) The highest QC known to 𝛼 after synchronization is at least as high as the one of 𝛼0 . (sync-TCs) If 𝛼0 knows a TC at a higher round than the highest QC of 𝛼, then the highest TC known to 𝛼 after synchronization is at least as high as the highest TC of 𝛼0 . (sync-vote) If 𝛼0 just voted for a block proposed by 𝛼, then 𝛼 receives this vote; no other votes are sent by 𝛼0 . (sync-timeouts) If 𝛼0 knows timeouts at its current active round, then 𝛼 receives these timeouts. (sync-block) If 𝛼0 knows one block proposed by the current leader of its current active round, then 𝛼 receives this block and learns the chain of previous blocks and QCs. Regarding the last item (sync-block), note that honest leaders are expected to propose only one block, so we can stop gossiping conflicting proposals. Regarding the property (sync-epoch), the data-synchronization exchange between the two nodes relies on the past record stores of 𝛼0 (Section 5.6) so that 𝛼 can follow the chains of commits and the commit rules of all the epochs known to 𝛼0 . A solution for data synchronization is described in Appendix A.3. Record cleanups. The requirements above condition which data can be cleaned from the record store of receiving nodes after an update. • We define the current round of the record store to be one plus the round of the highest QC or TC in the store. • In terms of QCs and blocks with QCs, the record store only needs to keep the two chains ending with the highest QC and the last QC of the latest commit rule. • In terms of blocks without QCs, only one proposal at the current round is needed. • In terms of votes, if we just proposed a block at the current round, only the votes at the current round are needed; otherwise, only one vote authored by us at the current round is needed. • In terms of timeouts, only the timeouts at the current active round are needed, together with one set of timeouts that form a TC at the current round minus one, if any such TC exists. Note that the current round of the record store becomes the active round of the node only after the main handler is called and update_pacemaker has started its timer.  28  Data tracker. The requirements above also translate into the following specifications for the datatracker abstraction that was used previously in the main handler update_node to control resharing (Section 5.7). struct DataTracker { /// Latest epoch identifier that was processed. epoch_id: EpochId, /// Round of the latest commit that was processed. highest_committed_round: Round, /// Round of the latest highest QC that was processed. highest_quorum_certificate_round: Round, /// Latest current round of the record store that was processed. current_round: Round, /// Number of timeouts in the current round. num_current_timeouts: usize, } impl DataTracker { fn update_and_decide_resharing(&mut self, epoch_id: EpochId, record_store: &RecordStore) -> bool { let mut should_broadcast = false; if epoch_id != self.epoch_id { self.epoch_id = epoch_id; self.highest_committed_round = Round(0); self.highest_quorum_certificate_round = Round(0); self.current_round = Round(1); self.num_current_timeouts = 0; should_broadcast = true; } let highest_committed_round = record_store.highest_committed_round(); if highest_committed_round > self.highest_committed_round { self.highest_committed_round = highest_committed_round; should_broadcast = true; } let highest_quorum_certificate_round = record_store.highest_quorum_certificate_round(); if highest_quorum_certificate_round > self.highest_quorum_certificate_round { self.highest_quorum_certificate_round = highest_quorum_certificate_round; should_broadcast = true; } let current_round = record_store.current_round(); if current_round > self.current_round { self.current_round = current_round; self.num_current_timeouts = 0; should_broadcast = true; } else { let num_current_timeouts = record_store.num_current_timeouts(); if num_current_timeouts > self.num_current_timeouts { self.num_current_timeouts = num_current_timeouts; should_broadcast = true; } } should_broadcast } }  8. Proof of Liveness We now consider the liveness of the LibraBFT protocol. We argue that the liveness mechanisms described in Section 7 ensure that commits are being produced in a timely manner whenever the network becomes synchronous. Without loss of generality, we assume that the current epoch continues indefinitely. We will also rely on the fact that after GST, network and nodes are responsive; hence, we only take into account network propagation delays. Note that we address only the question of chain growth. How committed transactions are picked — aka fairness — is left for future work (see previous note in Section 3.3).  29  8.1. Active Rounds In the following, we use maximal active round to refer to the maximal active round between honest nodes. Recall that the set of honest nodes is unknown to the participants, yet stays the same during an epoch. Next, we prove that active rounds are always increasing over time or when nodes synchronize with each other. Lemma L1: If a node changes its active round from 𝑛 to 𝑛′ , then 𝑛 < 𝑛′ . Proof: By construction, discarding records (Section 5.7) never decreases the rounds of the highest QC and TC in the record store of a node. Therefore, given the definition of the method update_pacemaker (Section 7.10), any change in the active round must result from a higher QC or a higher TC. □ Lemma L2: If a node 𝛼 has synchronized with a node of active round 𝑛 in the past, then the active round of 𝛼 is at least 𝑛. Proof: This is a consequence of Lemma L1 and the properties of data synchronization (sync-QCs) and (sync-TCs). □ Lemma L3: Assume that the maximal active round amongst honest nodes changes from 𝑛 to 𝑛′ , then 𝑛′ = 𝑛 + 1. Proof: We have seen with Lemma L1 that the maximal active round cannot decrease. While the maximal active round is 𝑛, honest nodes can only vote for blocks or sign timeout objects at round lower or equal than 𝑛. The voting power necessary to produce a QC or a TC requires at least one honest node to collaborate; therefore, no QC or TC at a round greater than 𝑛 can be produced. Given the definition of the method update_pacemaker (Section 7.10), this implies 𝑛′ ≤ 𝑛 + 1. □ Note: • Since active rounds do not decrease by Lemma L1, the definition of the method proposed_block used by the main handler (Section 7.3) implies that the first voting constraint (Section 5.4) is always fulfilled the first time that a node wishes to vote on a proposal at a given round. • The fact that maximal active round increases sequentially is very important for the liveness argument. In particular, given that leaders are predictable until a new commit is produced, after GST, we must not allow malicious nodes and the network to cause a round to be skipped. • Assuming that nodes agree on the latest commit hash ℎ𝑐 , a similar argument as in the proof of Lemma L3 shows that the maximal active round 𝑛 will stay the same for at least a time duration(ℎ𝑐 , 𝑛) unless a new commit is produced or the leader at round 𝑛 successfully produces a QC sooner.  8.2. Synchronization of Active Rounds In the previous section, we discussed necessary conditions for the maximal active round to change. We now aim at sufficient conditions for the minimal active round between honest nodes to increase. Note that by Lemma L1, the minimal active round never decreases. Let 𝛿𝐺 be the time taken by a full gossip-based broadcast after GST. Assuming that the current minimal active round is 𝑛, we say that the system is synchronized if the following two conditions hold:  30  (i) Every honest node with an active round equal to 𝑛 has initiated a broadcast, including its timeout object at round 𝑛, at least once after GST (if any such timeout exists). (ii) Every honest node with an active round greater than 𝑛 (if any such node exists) entered its active round less than 𝛿𝐺 time ago. Note that the last condition can be stated formally as a function on the current states by using the timers in active rounds. Lemma L4: If an honest node 𝛼 is first to switch to a new maximal active round 𝑛 + 1 at time 𝑡 > 𝐺𝑆𝑇 , then at time 𝑡+𝛿𝐺 , the system is synchronized with a minimal active round at least 𝑛+1. Proof: Given the definition of the method update_pacemaker (Section 7.10), 𝛼 switches round because it learned a QC or a TC at round 𝑛. According to the protocol (Section 5.7), this QC or TC was gossiped immediately by 𝛼. Since 𝛼 is first in the round, other honest nodes will fully propagate this QC or TC (or any higher one that might be produced in the meantime); therefore, the assumption on broadcasting after GST applies. By Lemma L2, at time 𝑡 + 𝛿𝐺 , all honest nodes will have entered an active round greater than 𝑛, less than 𝛿𝐺 time ago. Note that no timeout at round greater than 𝑛 could be created before GST, thus condition (i) holds as well and the system is synchronized. □ Note: Assuming that nodes agree on the latest commit hash ℎ𝑐 and that duration(ℎ𝑐 , 𝑛) ≥ 𝛿𝐺 , in the condition of Lemma L4, all honest nodes will switch exactly to the round 𝑛 + 1 between time 𝑡 and 𝑡 + 𝛿𝐺 , unless a new commit is produced or the leader at round 𝑛 + 1 successfully produces a QC before 𝑡 + 𝛿𝐺 . We can now show that system synchronization persists after GST. Proposition L5: If the system is synchronized at time 𝑡 > 𝐺𝑆𝑇 + 𝛿𝐺 , then it stays synchronized at later times 𝑡′ > 𝑡. Proof: Condition (i) is clearly propagated for old timeouts and true for new ones. We prove (ii) by contradiction. Let 𝑡′ > 𝑡 be the earliest time at which the system is no longer synchronized. Let 𝑛0 be the minimal active round at time 𝑡′ . By assumption, there exist nodes with an active round 𝑛 > 𝑛0 that switched their active rounds no less than 𝛿𝐺 ago. Let 𝛼 be the node that switched first to the greatest active round 𝑛1 > 𝑛0 amongst those nodes. Since 𝑡′ is the earliest time of desynchronization, and the minimal active round never decreases, 𝛼 switched to 𝑛1 exactly at time 𝑡′ − 𝛿𝐺 . Besides, 𝑛1 was the maximal active round at time 𝑡′ − 𝛿𝐺 . Since 𝛼 was the first to switch and 𝑡′ − 𝛿𝐺 > 𝐺𝑆𝑇 , Lemma L2 applies and shows that the system is synchronized at time 𝑡′ . □ Lemma L6: If the maximal active round between honest nodes is 𝑛 at time 𝑡 > 𝐺𝑆𝑇 , then the system is synchronized at time 𝑡 + 𝐼 + 𝛿𝐺 , with a minimal active round at least 𝑛. Proof: Condition (i) follows from the mechanism of regular broadcasts (Section 7.6) with interval 𝐼. The same mechanism also entails that every node 𝛼 that has the maximal active round 𝑛 at time 𝑡 will initiate a broadcast no later than time 𝑡 + 𝐼. This broadcast will gossip a QC or TC that justifies the active round 𝑛 or any later round learned in the meantime. Given Lemma L2 and the assumption on reliable broadcast after GST, every honest node will have an active round at least 𝑛 by time 𝑡 + 𝐼 + 𝛿𝐺 . Let 𝑛′ ≥ 𝑛 be the minimal active round at time 𝑡 + 𝐼 + 𝛿𝐺 . We prove Condition (ii) by contradiction: assume that there exists a node with an active round 𝑛″ > 𝑛′ and that this node switched to 𝑛″ on or before time 𝑡 + 𝐼. Since 𝑛″ > 𝑛, any such node must have switched after 𝑡 > 𝐺𝑆𝑇 . By considering  31  the first node to switch to a new maximal active round, Lemma L4 and Proposition L5 imply that the system is synchronized at time 𝑡 + 𝐼 + 𝛿𝐺 . □ Proposition L7: Assume that the highest commit hash ℎ𝑐 is shared between honest nodes and stays the same. If the system is synchronized with a minimal active round at least 𝑛 at time 𝑡 > 𝐺𝑆𝑇 + 𝛿𝐺 , then it is synchronized with a minimal active round at least 𝑛 + 1 at time 𝑡 + 2𝛿𝐺 + duration(ℎ𝑐 , 𝑛). Proof: If all honest nodes have an active round greater than 𝑛 at time 𝑡, then the result follows from Lemma L1. Otherwise, let us assume that the minimal active round at time 𝑡 is exactly 𝑛. If some honest nodes have an active round greater than 𝑛 at time 𝑡, then, by condition (ii), they must have switched less than 𝛿𝐺 time ago. Since 𝑡 − 𝛿𝐺 > 𝐺𝑆𝑇 , we can conclude by Lemma L4 and Proposition L5 as before. Otherwise, every honest node has an active round equal to 𝑛 at time 𝑡. By time 𝑡 + duration(ℎ𝑐 , 𝑛), each honest node will have created a timeout object for round 𝑛. This timeout object may be created before (or at) time 𝑡, or after time 𝑡. Given condition (i) of synchronized systems, timeout objects that existed already at time 𝑡 were already broadcast at least once after GST. Timeout objects created after 𝑡 are also broadcast immediately by definition of the pacemaker. Given the assumption on gossiping delay after GST, by time 𝑡 + 𝛿𝐺 + duration(ℎ𝑐 , 𝑛), one honest node will first learn enough old and new timeouts to form a TC at round 𝑛, thus switching to the next active round 𝑛 + 1. We then conclude by Lemma L4 and Proposition L5. □  8.3. Optimistic Responsiveness Following the authors of the original HotStuff [5], we prove an important property for the liveness of the protocol called “Optimistic Responsiveness.” Proposition L8 (Optimistic Responsiveness): Assume that a quorum of nodes (𝛼) communicated their highest 1-chains to a proposer at times (𝑡𝛼 ). Let 𝐵 ← 𝐶 be the highest 1-chain amongst all those communicated. If such a node 𝛼 is honest, we further assume that it has not voted on any proposal since 𝑡𝛼 . Then, under BFT assumption, any proposal 𝐵′ such that 𝐵 ← 𝐶 ← 𝐵′ is compatible with the voting rule (locked-round) of any honest node. Proof: Let 𝑛0 be the current locked round of an honest node 𝛼0 . By definition of the locked round, 𝛼0 once knew a 2-chain 𝐵0 ← 𝐶0 ← 𝐵1 ← 𝐶1 such that round(𝐵0 ) = 𝑛0 . Under BFT assumption, there exists an honest node 𝛼 that voted for 𝐵1 a time 𝑡0 and communicated its highest 1-chain at time 𝑡𝛼 . Since 𝛼 has not voted since 𝑡𝛼 , we have 𝑡0 ≤ 𝑡𝛼 . At time 𝑡0 , 𝛼 knew the 1-chain 𝐵0 ← 𝐶0 . Since its highest 1-chain at later time 𝑡𝛼 is not higher than 𝐵 and the round of the highest 1-chain in the record store of node never decreases (see record cleanups in Section 7.11), we deduce 𝑛0 ≤ round(𝐵). Therefore, 𝛼0 can vote for 𝐵′ according to the voting rule (locked-round). □  8.4. Main Proof Let 𝛿𝑀 be the transmission delay for one message after GST. Theorem L9 (Liveness): Let ℎ𝑐 be the highest commit QC hash known to honest nodes at a time 𝑡0 > 𝐺𝑆𝑇 . Let 𝑡1 = 𝑡0 + 𝐼 + 𝛿𝐺 and 𝑛1 be the maximal active round at time 𝑡1 . Let 𝑛 ≥ 𝑛1 be such that leader(ℎ𝑐 , 𝑛), leader(ℎ𝑐 , 𝑛 + 1), and leader(ℎ𝑐 , 𝑛 + 2) are honest and such that  32  duration(ℎ𝑐 , 𝑛) ≥ 2 𝛿𝑀 + 2 𝛿𝐺 . Then, the next commit after ℎ𝑐 is received by every node no later than time: 𝑛+2  𝑡 = 𝑡0 + 𝐼 + (2 𝑛 − 2 𝑛1 + 3) 𝛿𝐺 + ∑ duration(ℎ𝑐 , 𝑘) 𝑘=𝑛1  Proof: If the highest commit hash ℎ𝑐 known to honest nodes changes before time 𝑡 − 𝛿𝐺 , then given that 𝑡 − 𝛿𝐺 ≥ 𝐺𝑆𝑇 , the assumption on gossiping after GST implies the next commit after ℎ𝑐 is received by every node no later than time 𝑡. By contradiction, assume that ℎ𝑐 does not change before 𝑡 − 𝛿𝐺 . Given the regular broadcasts (Section 7.6) and the properties of data synchronization (sync-commits), at time 𝑡1 = 𝑡0 + 𝐼 + 𝛿𝐺 , all honest nodes have the same highest commit QC hash ℎ𝑐 . From then on and until time 𝑡 − 𝛿𝐺 , this means every honest node agrees on the leader and the duration of each round. By Lemma L6 and Proposition L5, the system is also synchronized on and after time 𝑡1 . By the assumption on gossiping after GST and condition (ii) of synchronization at time 𝑡1 , the minimal active round at time 𝑡2 = 𝑡1 + 𝛿𝐺 is at least 𝑛1 . By Lemma L3, the maximal active rounds on and after 𝑡2 follow the sequence of values 𝑛1 , 𝑛1 + 1, 𝑛1 + 2, etc. Using the minimal active round as a lower bound for the maximal active round, by Lemma L4 and Proposition L7, and given that duration(ℎ𝑐 , 𝑛) > 𝛿𝐺 , we deduce that there exists a 𝑛−1 time 𝑡3 with 𝑡3 ≤ 𝑡2 + 2 (𝑛 − 𝑛1 ) 𝛿𝐺 + ∑𝑘=𝑛 duration(ℎ𝑐 , 𝑘) such that all honest nodes switch to 1 round 𝑛 between 𝑡3 and 𝑡3 + 𝛿𝐺 . By definition of the pacemaker, these nodes immediately notifies the leader of round 𝑛. Given that duration(ℎ𝑐 , 𝑛) ≥ 2 𝛿𝑀 + 2 𝛿𝐺 , no honest node will timeout and all of them will stay at active round 𝑛 until the leader at round 𝑛 has completed its expected tasks: • Receive the synchronizations from a quorum of nodes (time cost ≤ 𝛿𝑀 ); • Pick a valid block compatible with voting rules (see Proposition L8 on optimistic responsiveness); • Broadcast it (time cost ≤ 𝛿𝐺 ); • Gather a quorum of votes (time cost ≤ 𝛿𝑀 ); and • Broadcast its QC (time cost ≤ 𝛿𝐺 ). Recall that 2 𝛿𝑀 + 2 𝛿𝐺 ≤ duration(ℎ𝑐 , 𝑛) ≤ duration(ℎ𝑐 , 𝑛 + 1) ≤ duration(ℎ𝑐 , 𝑛 + 2). Therefore, the same reasoning applies to the next leaders at round 𝑛 + 1 and 𝑛 + 2. This means that all honest nodes will receive three QCs at contiguous rounds 𝑛, 𝑛 + 1, and 𝑛 + 2 – hence a commit, before time 𝑛+2 𝑡4 = 𝑡3 + 𝛿𝐺 + ∑𝑘=𝑛 duration(ℎ𝑐 , 𝑘). Using previous equations, we have: 𝑛+2  𝑛−1  𝑡4 ≤ 𝑡0 + 𝐼 + 2 𝛿𝐺 + 2 (𝑛 − 𝑛1 ) 𝛿𝐺 + ∑ duration(ℎ𝑐 , 𝑘) + 𝛿𝐺 + ∑ duration(ℎ𝑐 , 𝑘) = 𝑡 𝑘=𝑛1  𝑘=𝑛  □ Note: For subsequent commits after GST, we may assume that the system is already synchronized and that the highest commit known to honest nodes was just broadcast at time 𝑡0 > 𝐺𝑆𝑇 . In this case, a similar proof shows that we can spare the term 𝐼. Specifically, the next commit will be received 𝑛′ +2 no later than 𝑡′ = 𝑡0 + (2𝑛′ − 2𝑛′1 + 3) 𝛿𝐺 + ∑𝑘=𝑛′ duration(ℎ𝑐 , 𝑘) when 𝑛′ is defined as 𝑛 above, 1 but based on the highest QC round 𝑛′1 at time 𝑡′1 = 𝑡0 + 𝛿𝐺 .  33  9. Economic Incentives Finally, we sketch how to economically incentivize LibraBFT nodes for their behaviors in the consensus protocol. Specifically, we show how to reward timely leaders and voters and how to detect violations of voting constraints and conflicting proposals. This covers the essential behaviors of participants. In the future, we intend to study how to cover more behaviors, such as timeouts. The execution of rewards and punishments is meant to be entirely delegated to the execution module of the Libra Blockchain and programmed using the Move language [38]. Rewards are handled by adding consensus-provided arguments to the execution callbacks. We sketch possible SMR APIs in Appendix A.1. In the case of punishments, we will rely on a whistleblower node to detect a violation, gather cryptographic evidence (see the conditions given below), and submit a punishment request through consensus. We leave for future work the exact specifications of the corresponding interactions between mempool, execution, and consensus.  9.1. Leaders and Voters Assume a proposal 𝐵 on top of a quorum certificate 𝐶0 , that is, 𝐵0 ← 𝐶0 ← 𝐵. Thanks to cryptographic chaining, during the execution of the block 𝐵, we may introspect 𝐵0 and 𝐶0 to suggest rewards for the author of 𝐵0 and the authors of the votes included in the quorum certificate 𝐶0 . APIs to communicate lists of authors and voters to the execution are proposed in Appendix A.1. We emphasize that rewards concerning 𝐵0 are computed as part of the speculative execution of some next block 𝐵. They become final when 𝐵 is committed. Note that we cannot so easily punish unsuccessful leaders leader(ℎ𝑐 , 𝑛) for round(𝐵0 ) < 𝑛 < round(𝐵) because there may not be agreement between consensus nodes on ℎ𝑐 , the latest commit preceding 𝐵0 .  9.2. Detecting Safety Violations Looking at the proof of Proposition S4, we notice that the proof of safety relies only on Lemma S2 and Lemma S3. Interestingly, these two lemmas are merely properties of the tree of records. Therefore, we can translate them into the following conditions to prove that a node 𝛼 is trying to break safety: (conflicting-votes) There exist two votes, 𝐵1 ← 𝑉1 and 𝐵2 ← 𝑉2 , such that round(𝐵1 ) = round(𝐵2 ), author(𝑉1 ) = author(𝑉2 ) = 𝛼, and either 𝐵1 ≠ 𝐵2 or state(𝑉1 ) ≠ state(𝑉2 ). (locked-round-violation) There exist a vote following a 2-chain 𝐵0 ← 𝐶0 ← 𝐵1 ← 𝐶1 ← 𝐵2 ← 𝑉2 and a vote 𝐵 ← 𝑉 , such that author(𝑉2 ) = author(𝑉 ) = 𝛼, round(𝐵) > round(𝐵2 ), and previous_round(𝐵) < round(𝐵0 ). Proposition E1 (Safe detection): A node that respects the voting rules (increasing-round) and (locked-round) never triggers the conditions (conflicting-votes) and (locked-round-violation). Proof: This was proved as part of the proofs of Lemma S2 and Lemma S3, respectively. □ Proposition E2 (Complete detection): If no more than 𝑓 nodes ever triggered the conditions (conflicting-votes) and (locked-round-violation), then safety holds.  34  Proof: As mentioned above, the proofs of Proposition S4, or safety, rely only on Lemma S2 and Lemma S3. We prove these lemmas under the new assumption by considering a non-violating node (instead of an honest node) at the intersection of the two QCs mentioned at the beginning of the original proofs. □  9.3. Detecting Conﬂicting Proposals According to the protocol, the leader of a round should make only one proposal. Making several proposals does not endanger safety, but it makes other nodes consume more resources than needed (e.g., CPU, network). This undesirable behavior is easy to detect: (conflicting-proposals) There exist two proposals 𝐵1 and 𝐵2 such that round(𝐵1 ) = round(𝐵2 ), 𝐵1 ≠ 𝐵2 , and author(𝐵1 ) = author(𝐵2 ) = 𝛼.  10. Conclusion We have presented LibraBFT, a state machine replication system based on the HotStuff protocol [5] and designed for the Libra Blockchain [2]. LibraBFT provides safety and liveness in a Byzantine setting when up to one-third of voting rights are held by malicious actors, assuming that the network is partially synchronous. In this report, we have presented detailed proofs of safety and liveness and covered many important practical considerations, such as networking and data structures. We have shown that LibraBFT is compatible with proof of stake and can generate incentives for a variety of behaviors, such as proposing blocks and voting. Thanks to the simplicity of the safety argument in LibraBFT, we also provided criteria to detect malicious attempts to break safety. These criteria will be instrumental for the progressive migration of the Libra infrastructure to a permissionless model. Future work. This report constitutes an initial proposal for LibraBFT and is meant to be updated in the future. In the next version, we intend to share the code for our reference implementation in a simulated environment and provide experimental results, both using this simulation and using the production implementation currently developed by Calibra engineers. In the future, we would like to improve our theoretical analysis in several ways. We plan to make our networking assumptions more precise, with additional studies on message sizes and probabilistic gossiping. Regarding the integration of LibraBFT with the Libra Blockchain, we would like to cover fairness and discuss how light clients can authenticate the set of validators for each epoch. Economic incentives should reward additional positive behaviors, such as creating timeouts, and specifications should provide an external protocol for auditors to report violations of safety rules. On a practical level, we have not yet analyzed resource consumption (memory, CPU, etc.) in the presence of malicious participants. Heuristics for leader selection, a precise description of the VRF solution, and possibly adaptive policies will likely be required to increase the robustness of the system in case of malicious leaders or targeted attacks on leaders. In the long term, we hope that our efforts on precise specifications and detailed proofs will pave the way for mechanized proofs of safety and liveness of LibraBFT.  Acknowledgments We would like to thank the following people for helpful discussions and feedback on this paper: Tarun Chitra, Ittay Eyal, Klaus Kursawe, John Mitchell, and Jared Saia.  35  A. Programming Interfaces Note: This section will evolve in the future as we integrate engineering optimizations and make progress in the software implementation of LibraBFT.  A.1. State Machine Replication We now present possible programming interfaces for state machine replication (Table 4). We assume two abstract data types: • Values of type State are authenticators that refer to a concrete execution state in the Libra Blockchain. • Command values are meant to be executed on top of a State value. At the beginning of the first epoch, we assume that the SMR module of every node is initialized with the same initial value of type State. As mentioned above (Section 3), in practice, State values contain a hash value that points to a persistent local storage outside the SMR module. The SMR module communicates with the other modules of a Libra validator through a number of APIs (i.e., Rust traits): • CommandFetcher lets the SMR module fetch user commands from the mempool. • StateComputer produces a new state hash from the hash of a base state, a command to execute, and additional contextual data, including a proposed system time and signals for economic incentives (Section 9). • StateFinalizer lets the SMR module eventually declare whether each state hash was successfully committed or not. In the case of a commit, we pass the quorum certificate that contains the corresponding commitment value, as discussed in Section 4.1. • EpochReader lets the SMR module retrieve a possibly updated epoch identifier from a state, as well as the current voting rights.  A.2. Record Store The implementation of the record store is assumed to provide the APIs outlined in Table 5. In the simulator used as a reference for this report, we implement the RecordStore APIs using the in-memory data structures described in Table 6. Note that the data structures described here do not cover constant-time cleanups, persistent storage, and resistance to potential crashes while the record store is being updated.  A.3. Data-Synchronization Messages The messages of the data-synchronization protocol (Section 4.6) used in our current simulator are described in Table 7. For simplicity, we have assumed that data are transmitted over authenticated channels and omitted message signatures.  36  trait CommandFetcher { /// How to fetch valid commands to submit to the consensus protocol. fn fetch(&mut self) -> Command; } trait StateComputer { /// How to execute a command and obtain the next state. /// If execution fails, the value `None` is returned, meaning that the /// command should be rejected. fn compute( &mut self, // The state before executing the command. base_state: &State, // Command to execute. command: Command, // Time associated to this execution step, in agreement with // other consensus nodes. time: NodeTime, // Suggest to reward the author of the previous block, if any. previous_author: Option<Author>, // Suggest to reward the voters of the previous block, if any. previous_voters: Vec<Author>, ) -> Option<State>; } /// How to communicate that a state was committed or discarded. trait StateFinalizer { /// Report that a state was committed. fn commit(&mut self, state: &State, certificate: Option<&QuorumCertificate>); /// Report that a state was discarded. fn discard(&mut self, state: &State); } /// Hold voting rights for a give epoch. struct EpochConfiguration { voting_rights: BTreeMap<Author, usize>, total_votes: usize, } /// How to communicate that a state was committed or discarded. trait EpochReader { /// Read the id of the epoch in a state. fn read_epoch_id(&self, state: &State) -> EpochId; /// Return the configuration (i.e. voting rights) for the current epoch. fn configuration(&self) -> EpochConfiguration; } trait SMRContext: CommandFetcher + StateComputer + StateFinalizer + EpochReader {}  Table 4: Programming interfaces for State Machine Replication  37  trait RecordStore { /// Return the hash of a QC at the highest round, or the initial hash. fn highest_quorum_certificate_hash(&self) -> QuorumCertificateHash; /// Query the round of the highest QC. fn highest_quorum_certificate_round(&self) -> Round; /// Query the round of the highest TC. fn highest_timeout_certificate_round(&self) -> Round; /// Query the round of highest commit. fn highest_committed_round(&self) -> Round; /// Query the round of the highest 2-chain. fn highest_2chain_head_round(&self) -> Round; /// Current round as seen by the record store. fn current_round(&self) -> Round; /// Number of timeouts objects known at the current round. fn num_current_timeouts(&self) -> usize; /// Iterate on a chain of QCs starting after the QC at round `after_round` and ending with the QC at round /// `until_round` fn chain_between_quorum_certificates<'a>( &'a self, after_round: Round, until_round: Round, ) -> ForwardQuorumCertificateIterator<'a>; /// Find a QC whose `commitment` field is the state of the input QC. fn commit_certificate(&self, qc: &QuorumCertificate) -> Option<&QuorumCertificate>; /// Access the block proposed by the leader chosen by the Pacemaker (if any). fn proposed_block(&self, pacemaker: &Pacemaker) -> Option<(BlockHash, Round, Author)>; /// Check if a timeout already exists. fn has_timeout(&self, author: Author, round: Round) -> bool; /// Create a timeout. fn create_timeout(&mut self, author: Author, round: Round, smr_context: &mut SMRContext); /// Fetch a command from mempool and propose a block. fn propose_block( &mut self, local_author: Author, previous_qc_hash: QuorumCertificateHash, clock: NodeTime, smr_context: &mut SMRContext, ); /// Execute the command contained in a block and vote for the resulting state. /// Return false if the execution failed. fn create_vote( &mut self, local_author: Author, block_hash: BlockHash, smr_context: &mut SMRContext, ) -> bool; /// Try to create a QC for the last block that we have proposed. fn check_for_new_quorum_certificate( &mut self, local_author: Author, smr_context: &mut SMRContext, ) -> bool; /// Compute the previous round of a block. fn previous_round(&self, block_hash: BlockHash) -> Round; /// Determine if a set of nodes form a quorum. fn is_quorum(&self, authors: &HashSet<Author>) -> bool; /// Pick an author based on a seed, with chances proportional to voting rights. fn pick_author(&self, seed: u64) -> Author; /// APIs supporting data synchronization. fn highest_commit_certificate(&self) -> Option<&QuorumCertificate>; fn highest_quorum_certificate(&self) -> Option<&QuorumCertificate>; fn timeouts(&self) -> Vec<Timeout>; fn current_vote(&self, local_author: Author) -> Option<&Vote>; fn block(&self, block_hash: BlockHash) -> Option<&Block>; fn known_quorum_certificate_rounds(&self) -> BTreeSet<Round>; fn verify_quorum_certificate_without_previous_block(&self, qc: &QuorumCertificate) -> Result<()>; fn unknown_records(&self, known_qc_rounds: BTreeSet<Round>) -> Vec<Record>; fn insert_network_record(&mut self, record: Record, smr_context: &mut SMRContext); }  Table 5: Programming interfaces for the record store 38  struct RecordStoreState { /// Epoch initialization. epoch_id: EpochId, configuration: EpochConfiguration, initial_hash: QuorumCertificateHash, initial_state: State, /// Storage of verified blocks and QCs. blocks: HashMap<BlockHash, Block>, quorum_certificates: HashMap<QuorumCertificateHash, QuorumCertificate>, round_to_qc_hash: HashMap<Round, QuorumCertificateHash>, current_proposed_block: Option<BlockHash>, /// Computed round values. highest_quorum_certificate_round: Round, highest_timeout_certificate_round: Round, current_round: Round, highest_2chain_round: Round, highest_committed_round: Round, /// Storage of verified timeouts at the highest TC round. highest_timeout_certificate: Option<Vec<Timeout>>, /// Storage of verified votes and timeouts at the current round. current_timeouts: HashMap<Author, Timeout>, current_votes: HashMap<Author, Vote>, /// Computed weight values. current_timeouts_weight: usize, current_election: ElectionState, } /// Counting votes for a proposed block and its execution state. enum ElectionState { Ongoing { ballot: HashMap<(BlockHash, State), usize> }, Won { block_hash: BlockHash, state: State }, Closed, }  Table 6: In-memory data structures for the record store  struct DataSyncNotification { /// Current epoch identifier. current_epoch: EpochId, /// Tail QC of the highest commit rule. highest_commit_certificate: Option<QuorumCertificate>, /// Highest QC. highest_quorum_certificate: Option<QuorumCertificate>, /// Timeouts in the highest TC, then at the current round, if any. timeouts: Vec<Timeout>, /// Sender's vote at the current round, if any (meant for the proposer). current_vote: Option<Vote>, /// Known proposed block at the current round, if any. proposed_block: Option<Block>, /// Active round of the sender's pacemaker. active_round: Round, } struct DataSyncRequest { /// Current epoch identifier. current_epoch: EpochId, /// Selection of rounds for which the receiver already knows a QC. known_quorum_certificates: BTreeSet<Round>, } struct DataSyncResponse { /// Current epoch identifier. current_epoch: EpochId, /// Records for the receiver to insert, for each epoch, in the given order. records: Vec<(EpochId, Vec<Record>)>, /// Active round of the sender's pacemaker. active_round: Round, }  Table 7: Data-synchronization messages  39  References [1] The Libra Association, “An Introduction to Libra.” https://libra.org/en-us/whitepaper. [2] Z. Amsden et al., “The Libra Blockchain.” https://developers.libra.org/docs/the-libra-blockchainpaper. [3] L. Lamport, R. Shostak, and M. Pease, “The Byzantine generals problem,” ACM Transactions on Programming Languages and Systems (TOPLAS’82), vol. 4, no. 3, pp. 382–401, 1982. [4] S. Bano et al., “Moving toward permissionless consensus.” https://libra.org/permissionless-bloc kchain. [5] M. Yin, D. Malkhi, M. K. Reiterand, G. G. Gueta, and I. Abraham, “HotStuff: BFT consensus in the lens of blockchain,” 2019. http://arxiv.org/abs/1803.05069v4 [6] M. Yin, D. Malkhi, M. K. Reiterand, G. G. Gueta, and I. Abraham, “HotStuff: BFT consensus with linearity and responsiveness,” in 38th ACM symposium on Principles of Distributed Computing (PODC’19), 2019. [7] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” 2008. http://bitcoin.org/bitcoin. pdf [8] digiconomist.net, https://digiconomist.net/bitcoin-energy-consumption [9] arewedecentralizedyet.com, https://arewedecentralizedyet.com/ [10] C. Cachin and M. Vukolić, “Blockchain consensus protocols in the wild,” 2017. https://arxiv.or g/abs/1707.01873 [11] S. Bano et al., “Consensus in the age of blockchains,” 2017. https://arxiv.org/abs/1711.03936 [12] I. Abraham, D. Malkhi, and others, “The blockchain consensus layer and BFT,” Bulletin of EATCS, vol. 3, no. 123, 2017. [13] M. Ben-Or, “Another advantage of free choice: Completely asynchronous agreement protocols,” in 2nd ACM symposium on Principles of Distributed Computing (PODC’83), 1983, pp. 27–30. [14] P. Feldman and S. Micali, “Optimal algorithms for byzantine agreement,” in 20th annual ACM symposium on Theory of Computing, 1988, pp. 148–161. [15] A. Miller, Y. Xia, K. Croman, E. Shi, and D. Song, “The honey badger of BFT protocols,” in 23rd ACM SIGSAC conference on Computer and Communications Security (CCS’16), 2016, pp. 31–42. [16] R. Canetti and T. Rabin, “Fast asynchronous byzantine agreement with optimal resilience,” in 25th annual ACM symposium on Theory of Computing (STOC’93), 1993, pp. 42–51. [17] I. Abraham, D. Malkhi, and A. Spiegelman, “Validated asynchronous byzantine agreement with optimal resilience and asymptotically optimal time and word communication.” 2018. https://arxiv. org/abs/1811.01332 [18] C. Dwork, N. Lynch, and L. Stockmeyer, “Consensus in the presence of partial synchrony,” Journal of the ACM (JACM), vol. 35, no. 2, pp. 288–323, 1988. [19] M. Castro and B. Liskov, “Practical byzantine fault tolerance,” in 3rd symposium on Operating Systems Design and Implementation (OSDI’99), 1999, vol. 99, pp. 173–186. [20] A. Bessani, J. Sousa, and E. E. P. Alchieri, “State machine replication for the masses with BFTSMART,” in 44th annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN’14), 2014, pp. 355–362.  40  [21] E. Androulaki et al., “Hyperledger fabric: A distributed operating system for permissioned blockchains,” in 13th EuroSys conference (EuroSys’18), 2018, p. 30. [22] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong, “Zyzzyva: Speculative byzantine fault tolerance,” in 21st ACM SIGOPS Symposium on Operating Systems Principles (SOSP’07), 2007, pp. 45–58. [23] A. Clement et al., “Upright cluster services,” in 22nd ACM Symposium on Operating Systems Principles (SOSP’09), 2009, pp. 277–290. [24] C. Cachin, R. Guerraoui, and L. Rodrigues, Introduction to reliable and secure distributed programming. 2011. [25] M. K. Reiter, “The rampart toolkit for building high-integrity services,” in Theory and practice in distributed systems, 1995, pp. 99–110. [26] E. K. Kogias, P. Jovanovic, N. Gailly, I. Khoffi, L. Gasser, and B. Ford, “Enhancing bitcoin security and performance with strong consistency via collective signing,” in 25th USENIX security symposium (USENIX security ’16), 2016, pp. 279–296. [27] G. G. Gueta et al., “SBFT: A scalable decentralized trust infrastructure for blockchains,” 2018. https://arxiv.org/abs/1804.01626 [28] D. Boneh, B. Lynn, and H. Shacham, “Short signatures from the Weil pairing,” in Advances in Cryptology (ASIACRYPT 2001), 2001, pp. 514–532. [29] E. Buchman, J. Kwon, and Z. Milosevic, “The latest gossip on BFT consensus,” 2018. http: //arxiv.org/abs/1807.04938v2 [30] V. Buterin and V. Griffith, “Casper, the friendly finality gadget,” 2017. https://arxiv.org/abs/ 1710.09437 [31] T. H. Chan, R. Pass, and E. Shi, “PaLa: A simple partially synchronous blockchain.” 2018. https://eprint.iacr.org/2018/981 [32] Y. Sompolinsky and A. Zohar, “Secure high-rate transaction processing in bitcoin,” in 19th international conference on financial cryptography and data security (FC’15), 2015, pp. 507–527. [33] C. Li, P. Li, W. Xu, F. Long, and A. C.-c. Yao, “Scaling Nakamoto consensus to thousands of transactions per second,” 2018. https://arxiv.org/abs/1805.03870 [34] G. Danezis and D. Hrycyszyn, “Blockmania: From block DAGs to consensus,” 2018. http: //arxiv.org/abs/1809.01620 [35] “The swirlds hashgraph consensus algorithm: Fair, fast, byzantine fault tolerance,” 2016. [36] F. B. Schneider, “Implementing fault-tolerant services using the state machine approach: A tutorial,” ACM Computing Surveys (CSUR), pp. 299–319, 1990. [37] D. Malkhi and M. Reiter, “Byzantine quorum systems,” in 29th annual ACM symposium on Theory of Computing (STOC’97), 1997. [38] S. Blackshear et al., “Move: A language with programmable resources.” https://developers.libra .org/docs/move-paper. [39] S. Micali, M. Rabin, and S. Vadhan, “Verifiable random functions,” in 40th annual Symposium on Foundations of Computer Science (FOCS’99), 1999, pp. 120–130.  41  Moving Toward Permissionless Consensus Shehar Bano, Christian Catalini, George Danezis, Nick Doudchenko, Ben Maurer, Alberto Sonnino, Nils Wernerfelt*  This document outlines some of the questions, decisions, and challenges the Libra Association will face on the journey to permissionless governance and consensus. A key distinction in the blockchain space is the one between permissioned systems, in which only a defined set of entities can shape consensus and governance, and permissionless systems, where anyone that follows the rules of the protocol and contributes the right types of resources (e.g., computing power in the case of a proof-of-work system) can do so. This distinction is important not only from a technical perspective but also from an economic one: permissionless systems have low barriers to entry and innovation, are resistant to censorship attacks, and encourage healthy competition among infrastructure providers (e.g., who can participate in consensus) as well as the developers of applications on top of the network [1]. Since nobody can exclude others from the market or censor their transactions, permissionless systems provide stronger guarantees to participants that no single party will be able to unilaterally change the rules of the network to their advantage at a future date. At their core, permissionless systems make irreversible commitments to operating as open networks where changes can only be implemented if they are democratically supported by a majority of constituents. For all these reasons, and to ensure that Libra is truly open and always operates in the best interest of its users, our ambition is for the Libra network to become permissionless. The challenge is that as of today we do not believe that there is a proven solution that can deliver the scale, stability, and security needed to support billions of people and transactions across the globe through a permissionless network. With the testnet, the association starts the journey toward building a permissionless system. And while there are a number of technical and economic challenges that will need to be solved together with the open-source community to make this a reality, we believe that for the Libra network to achieve its full potential, it needs to be permissionless. As a result, one of the association’s directives will be to work with the community to research and implement this transition, which will begin within five years of the public launch of the Libra Blockchain and ecosystem. Essential to the spirit of Libra, in both its permissioned and permissionless states, the Libra Blockchain will be open to everyone — any consumer, developer, or business can use the Libra network, build products on top of it, and add value through their services. Open access ensures low barriers to entry and innovation and encourages healthy competition that benefits consumers. This is foundational to the goal of building more inclusive financial options for the world.  ∗  The authors work at Calibra, a subsidiary of Facebook, Inc., and contribute this paper to the Libra Association under a Creative Commons Attribution 4.0 International License.  1  1 Starting Point Since the association had to start as a permissioned system, it introduced measures to mitigate some of the major concerns about permissioned networks. 1. No Centralized Control: The degree of influence platform architects retain over permissioned networks and their ability to change the rules of protocol to their advantage after others have joined (what economists call a “hold-up problem”) was an area of concern. • Therefore, the Libra Association was established, composed of entities acting as validator nodes, collectively and democratically making decisions on the future of the network and protocol. The Libra Association is an independent, not-for-profit entity that no single Founding Member can control. All decisions require participation by a majority of the Founding Members. • To ensure a plurality of opinions and voices, Founding Members of the association are geographically distributed and diverse businesses, nonprofit and multilateral organizations, and academic institutions. 2. Distributed Reserve Collateral: We introduced the reserve to ensure that Libra is a great medium of exchange and store of value from day one. However, we did not want the assets used to back coins to become a single point of failure or for the reserve to be able to mint coins without backing, which would introduce inflation in the system. • This is why the reserve will be held by a geographically distributed and highly respected global network of custodians with investment-grade credit rating to limit counterparty risk. Safeguarding the assets, providing extremely high auditability and transparency, avoiding the risks of a centralized reserve, and achieving operational efficiency are the key parameters we will follow in custody selection and design. • The reserve cannot print coins without full backing and can only use as assets certain currencies and treasuries of highly stable and respected central banks. Changes to the composition of the basket require a supermajority vote by the association. 3. Progressive Opening of Participation in the Network: While at launch the Founding Members will advance the governance of the association and will be in charge of running validator nodes and protecting the network, irreversible commitments have been made to rapidly open up participation to new members. • With each new release of Libra Investment Tokens, new members will be able to enter, own a stake in the network (through the Libra Investment Token), and start contributing to consensus and governance by operating a validator node on the network. • It was determined that membership decisions should neither become an area of contention nor a way to increase barriers to entry for new players. Anyone that meets a comprehensive set of criteria will be able to participate in future offerings of the Libra Investment Tokens. Nonprofit and multilateral organizations and social impact partners will also be able to apply for grants to join the network. • There will be a reliance on these criteria (which the association will expand over time) to ensure that the entities joining the network in its early phases — when it is most vulnerable — have an established reputation offline and can be trusted with securely operating validator nodes and defending the network.  2  • As the technology matures, the Libra Blockchain will transition from relying on ownership of the Libra Investment Tokens — in order to operate validator nodes and vote on governance — to relying on ownership of Libra coins. The basic intuition is that at scale the network should be owned by its users and should always evolve in a way to protect their interests and assets.  2 Open Technical and Economic Challenges Before the network can be truly permissionless, the association will need to find technical and economic answers to the open problems below. Some of these are challenging, unsolved research and development questions, and some involve high-assurance engineering, which takes time. With the help of the open-source community, substantial progress will be made toward solving them. The association will also dedicate grants for researchers and entrepreneurial teams interested in helping us get there.  2.1 Technical and economic challenges: • Defining on-chain governance contracts. Some governance actions and processes, such as the definition of the validator node set in epochs, distribution of fees, abuse reporting, etc., may initially be performed off-chain by the association. For the network to become permissionless, progressively more of these governance actions should be implemented on-chain, including the ability to modify the rules of the network over time. • Market design. We have started to understand the governance and equilibrium structure of a financial system based on stake holdings and consumer confidence in wallets and other delegates. In the process, we have identified new market design trade-offs between the Libra approach and more established alternatives like proof of work. Nevertheless, more research is needed to determine how best to maintain long-run competition in the ecosystem while ensuring the security and efficiency of the network. Furthermore, as stake-based governance introduces path dependence in influence, it is essential to explore mechanisms for protecting smaller stakeholders and service providers [2]. • Scaling. The initial system that will be built aims to have 1,000 tx/sec for a set of 100 validator nodes. As the validator node set grows, that will lead to a decline in performance. The association needs to understand how to maintain acceptable performance while expanding participation and increasing the number of validators. • Understanding the Sybil resistance assumption. The current assumption is that more than two-thirds of the votes are coming from honest participants. In the beginning, this requires that two-thirds of the holders of the Libra Investment Token act in the best interest of the network. Once the network transitions, it will require that at least two-thirds of coin holders are acting honestly. Since a malicious entity can accumulate coin holdings and try to attack the system, safeguards will need to be in place to prevent such behavior. In time, such safeguards can be relaxed once the total value of Libra in circulation is large enough to make such an attack impractical. • Defining an effective and fair model for delegation of stake and for responding to validator node misbehavior. Since anyone holding Libra will be able to become a validator node when the network is permissionless, the association needs to define how users that do not wish to take part in the consensus process will be able to delegate this role to entities they can trust, and, importantly, what information needs to be provided to them to update their decision if abuse is detected. Rules for responding to abuse will also need to be established. Fairness concerns will also need to be addressed to allow the ecosystem to thrive in the long run.  3  • Defining a strategy for extending the Move platform on top of a permissionless network. The Move language is currently only exposed to built-in smart contracts such as the ones that manage the validator node set and the Libra coin. As the language stabilizes, the association plans to open up the language to third-party development. In order to enable effective decision making after the transition to permissionless, the association will need to establish guiding principles that help the community make decisions around key questions, such as the direction of innovation of the language, and the pace of core updates to it. The association will also need to establish community best practices around formal verification, security, etc. • Decentralizing the reserve function. The reserve allows users of the system to enjoy a relatively stable medium of exchange from the start. At the same time, it also represents a centralized function. For the network to be fully permissionless, the association will have to explore ways to further distribute and decentralize the reserve, including automating the verification of the assets in the basket and the process of minting and burning of coins. Increasing market competition in the custody and management of the reserve will also be explored to improve the efficiency of this service, and the costs it imposes on users and custodians over time.  2.2 Governance Challenges • Defining processes for the evolution of the protocol. Protocols need to evolve and introduce new features in response to new needs that the network will discover as it scales. This work happens through the interaction between the open-source community and the association. While at the beginning the evolution of the protocol will be supported by the coordination work of the Founding Members, as the network transitions to permissionless, the community will need to develop a robust process and clear principles for managing improvement proposals and providing open-source leadership. • Understanding how the association can support a healthy ecosystem. When governance is distributed, the association will have to design robust on-chain procedures for coordinating responses and stopping the most dangerous abuses of the network. • Defining emergency governance and breaks. What happens if the network comes under a large-scale attack, connectivity breaks, or other global events take place that affect the operations of the network? How can a response be effectively coordinated when ownership and governance are fully distributed across coin holders? To ensure safety, the association will need to develop robust processes for coordination to emerge in a decentralized way.  Acknowledgments We would like to thank the following people for helpful discussions and feedback on this paper: Adrien Auclert, Morgan Beller, Dan Boneh, Ravi Jagadeesan, Scott Duke Kominers, Roberto Rigobon, Catherine Tucker, and Kevin Zhang.  References [1] C. Catalini and J. S. Gans, “Some simple economics of the blockchain.” National Bureau of Economic Research, 2016.  4  [2] C. Catalini S. D. Kominers and R. Jagadeesan, “Market design for a blockchain-based financial system.” Social Science Research Network, 2019.  5  Move: A Language With Programmable Resources Sam Blackshear, Evan Cheng, David L. Dill, Victor Gao, Ben Maurer, Todd Nowacki, Alistair Pott, Shaz Qadeer, Rain, Dario Russi, Stephane Sezer, Tim Zakian, Runtian Zhou*  Abstract. We present Move, a safe and flexible programming language for the Libra Blockchain [1][2]. Move is an executable bytecode language used to implement custom transactions and smart contracts. The key feature of Move is the ability to define custom resource types with semantics inspired by linear logic [3]: a resource can never be copied or implicitly discarded, only moved between program storage locations. These safety guarantees are enforced statically by Move’s type system. Despite these special protections, resources are ordinary program values — they can be stored in data structures, passed as arguments to procedures, and so on. First-class resources are a very general concept that programmers can use not only to implement safe digital assets but also to write correct business logic for wrapping assets and enforcing access control policies. The safety and expressivity of Move have enabled us to implement significant parts of the Libra protocol in Move, including Libra coin, transaction processing, and validator management.  1. Introduction The advent of the internet and mobile broadband has connected billions of people globally, providing access to knowledge, free communications, and a wide range of lower-cost, more convenient services. This connectivity has also enabled more people to access the financial ecosystem. Yet, despite this progress, access to financial services is still limited for those who need it most. The mission of Libra is to change this state of affairs [1]. In this paper, we present Move, a new programming language for implementing custom transaction logic and smart contracts in the Libra protocol [2]. To introduce Move, we: 1. Describe the challenges of representing digital assets on a blockchain (Section 2). 2. Explain how our design for Move addresses these challenges (Section 3). 3. Give an example-oriented overview of Move’s key features and programming model (Section 4). 4. Dig into the technical details of the language and virtual machine design (Section 5, Section 6, and Appendix A). 5. Conclude by summarizing the progress we have made on Move, describing our plans for language evolution, and outlining our roadmap for supporting third-party Move code on the Libra Blockchain (Section 7).  ∗  The authors work at Calibra, a subsidiary of Facebook, Inc., and contribute this paper to the Libra Association under a Creative Commons Attribution 4.0 International License. For more information on the Libra ecosystem, please refer to the Libra white paper [1].  1  Audience. This paper is intended for two different audiences: • Programming language researchers who may not be familiar with blockchain systems. We encourage this audience to read the paper from cover-to-cover, but we warn that we may sometimes refer to blockchain concepts without providing enough context for an unfamiliar reader. Reading [2] before diving into this paper will help, but it is not necessary. • Blockchain developers who may not be familiar with programming languages research but are interested in learning about the Move language. We encourage this audience to begin with Section 3. We caution that Section 5, Section 6, and Appendix A contain some programming language terminology and formalization that may be unfamiliar.  2. Managing Digital Assets on a Blockchain We will begin by briefly explaining a blockchain at an abstract level to help the reader understand the role played by a “blockchain programming language” like Move. This discussion intentionally omits many important details of a blockchain system in order to focus on the features that are relevant from a language perspective.  2.1. An Abstract View of a Blockchain A blockchain is a replicated state machine [4][5]. Replicators in the system are known as validators. Users of the system send transactions to validators. Each validator understands how to execute a transaction to transition its internal state machine from the current state to a new state. Validators leverage their shared understanding of transaction execution to follow a consensus protocol for collectively defining and maintaining the replicated state. If • the validators start from the same initial state, and • the validators agree on what the next transaction should be, and • executing a transaction produces a deterministic state transition, then the validators will also agree on what the next state is. Repeatedly applying this scheme allows the validators to process transactions while continuing to agree on the current state. Note that the consensus protocol and the state transition components are not sensitive to each other’s implementation details. As long as the consensus protocol ensures a total order among transactions and the state transition scheme is deterministic, the components can interact in harmony.  2.2. Encoding Digital Assets in an Open System The role of a blockchain programming language like Move is to decide how transitions and state are represented. To support a rich financial infrastructure, the state of the Libra Blockchain must be able to encode the owners of digital assets at a given point in time. Additionally, state transitions should allow the transfer of assets. There is one other consideration that must inform the design of a blockchain programming language. Like other public blockchains, the Libra Blockchain is an open system. Anyone can view the current blockchain state or submit transactions to a validator (i.e., propose state transitions). Traditionally, software for managing digital assets (e.g., banking software) operates in a closed system with special  2  administrative controls. In a public blockchain, all participants are on equal footing. A participant can propose any state transition she likes, yet not all state transitions should be allowed by the system. For example, Alice is free to propose a state transition that transfers assets owned by Bob. The state transition function must be able to recognize that this state transition is invalid and reject it. It is challenging to choose a representation of transitions and state that encodes ownership of digital assets in an open software system. In particular, there are two properties of physical assets that are difficult to encode in digital assets: • Scarcity. The supply of assets in the system should be controlled. Duplicating existing assets should be prohibited, and creating new assets should be a privileged operation. • Access control. A participant in the system should be able to protect her assets with access control policies. To build intuition, we will see how these problems arise during a series of strawman proposals for the representation of state transitions. We will assume a blockchain that tracks a single digital asset called a StrawCoin. The blockchain state G is structured as a key-value store that maps user identities (represented with cryptographic public keys) to natural number values that encode the StrawCoin held by each user. A proposal consists of a transaction script that will be evaluated using the given evaluation rule, producing an update to apply to the global state. We will write G[𝐾] := 𝑛 to denote updating the natural number stored at key 𝐾 in the global blockchain state with the value 𝑛. The goal of each proposal is to design a system that is expressive enough to allow Alice to send StrawCoin to Bob, yet constrained enough to prevent any user from violating the scarcity or access control properties. The proposals do not attempt to address security issues such as replay attacks [6] that are important, but unrelated to our discussion about scarcity and access control. Scarcity. The simplest possible proposal is to directly encode the update to the state in the transaction script: Transaction Script Format  Evaluation Rule  ⟨𝐾, 𝑛⟩  G[𝐾] := 𝑛  This representation can encode sending StrawCoin from Alice to Bob. But it has several serious problems. For one, this proposal does not enforce the scarcity of StrawCoin. Alice can give herself as many StrawCoin as she pleases “out of thin air” by sending the transaction ⟨Alice, 100⟩. Thus, the StrawCoin that Alice sends to Bob are effectively worthless because Bob could just as easily have created those coins for himself. Scarcity is an important property of valuable physical assets. A rare metal like gold is naturally scarce, but there is no inherent physical scarcity in digital assets. A digital asset encoded as some sequence of bytes, such as G[Alice] → 10, is no physically harder to produce or copy than another sequence of bytes, such as G[Alice] → 100. Instead, the evaluation rule must enforce scarcity programmatically. Let’s consider a second proposal that takes scarcity into account: Transaction Script Format  Evaluation Rule  ⟨𝐾𝑎 , 𝑛, 𝐾𝑏 ⟩  if G[𝐾𝑎 ] ≥ 𝑛 then G[𝐾𝑎 ] := G[𝐾𝑎 ] - 𝑛 G[𝐾𝑏 ] := G[𝐾𝑏 ] + 𝑛  3  Under this scheme, the transaction script specifies both the public key 𝐾𝑎 of the sender, Alice, and the public key 𝐾𝑏 of the recipient, Bob. The evaluation rule now checks that the number of StrawCoin stored under 𝐾𝑎 is at least 𝑛 before performing any update. If the check succeeds, the evaluation rule subtracts 𝑛 from the StrawCoin stored under the sender’s key and adds 𝑛1 to the StrawCoin stored under the recipient’s key. Under this scheme, executing a valid transaction script enforces scarcity by conserving the number of StrawCoin in the system. Alice can no longer create StrawCoin out of thin air — she can only give Bob StrawCoin debited from her account. Access control. Though the second proposal addresses the scarcity issue, it still has a problem: Bob can send transactions that spend StrawCoin belonging to Alice. For example, nothing in the evaluation rule will stop Bob from sending the transaction ⟨Alice, 100, Bob⟩. We can address this by adding an access control mechanism based on digital signatures: Transaction Script Format  Evaluation Rule  𝑆𝐾𝑎 (⟨𝐾𝑎 , 𝑛, 𝐾𝑏 ⟩)  if verify_sig(𝑆𝐾𝑎 (⟨𝐾𝑎 , 𝑛, 𝐾𝑏 ⟩)) && G[𝐾𝑎 ] ≥ 𝑛 then G[𝐾𝑎 ] := G[𝐾𝑎 ] - 𝑛 G[𝐾𝑏 ] := G[𝐾𝑏 ] + 𝑛  This scheme requires Alice to sign the transaction script with her private key. We write 𝑆𝐾 (𝑚) for signing the message 𝑚 using the private key paired with public key 𝐾. The evaluation rule uses the verify_sig function to check the signature against Alice’s public key 𝐾𝑎 . If the signature does not verify, no update is performed. This new rule solves the problem with the previous proposal by using the unforgeability of digital signatures to prevent Alice from debiting StrawCoin from any account other than her own. As an aside, notice that there was effectively no need for an evaluation rule in the first strawman proposal — the proposed state update was applied directly to the key-value store. But as we progressed through the proposals, a clear separation between the preconditions for performing the update and the update itself has emerged. The evaluation rule decides both whether to perform an update and what update to perform by evaluating the script. This separation is fundamental because enforcing access control and scarcity policies inevitably requires some form of evaluation — the user proposes a state change, and computation must be performed to determine whether the state change conforms to the policy. In an open system, the participants cannot be trusted to enforce the policies off-chain and submit direct updates to the state (as in the first proposal). Instead, the access control policies must be enforced on-chain by the evaluation rule.  2.3. Existing Blockchain Languages StrawCoin is a toy language, but it attempts to capture the essence of the Bitcoin Script [7][8] and Ethereum Virtual Machine bytecode [9] languages (particularly the latter). Though these languages are more sophisticated than StrawCoin, they face many of the same problems: 1. Indirect representation of assets. An asset is encoded using an integer, but an integer value is not the same thing as an asset. In fact, there is no type or value that represents Bitcoin/Ether/ StrawCoin! This makes it awkward and error-prone to write programs that use assets. Patterns such as passing assets into/out of procedures or storing assets in data structures require special language support.  1  For simplicity, we will ignore the possibility of integer overflow here.  4  2. Scarcity is not extensible. The language only represents one scarce asset. In addition, the scarcity protections are hardcoded directly in the language semantics. A programmer that wishes to create a custom asset must carefully reimplement scarcity with no support from the language. 3. Access control is not flexible. The only access control policy the model enforces is the signature scheme based on the public key. Like the scarcity protections, the access control policy is deeply embedded in the language semantics. It is not obvious how to extend the language to allow programmers to define custom access control policies. Bitcoin Script. Bitcoin Script has a simple and elegant design that focuses on expressing custom access control policies for spending Bitcoin. The global state consists of a set of unspent transaction output (UTXOs). A Bitcoin Script program provides inputs (e.g., digital signatures) that satisfy the access control policies for the old UTXOs it consumes and specifies custom access control policies for the new UTXOs it creates. Because Bitcoin Script includes powerful instructions for digital signature checking (including multisignature [10] support), programmers can encode a rich variety of access control policies. However, the expressivity of Bitcoin Script is fundamentally limited. Programmers cannot define custom datatypes (and, consequently, custom assets) or procedures, and the language is not Turingcomplete. It is possible for cooperating parties to perform some richer computation via complex multi-transaction protocols [11] or informally define custom assets via “colored coins” [12][13]. However, these schemes work by pushing complexity outside the language and, thus, do not enable true extensibility. Ethereum Virtual Machine bytecode. Ethereum is a ground-breaking system that demonstrates how to use blockchain systems for more than just payments. Ethereum Virtual Machine (EVM) bytecode programmers can publish smart contracts [14] that interact with assets such as Ether and define new assets using a Turing-complete language. The EVM supports many features that Bitcoin Script does not, such as user-defined procedures, virtual calls, loops, and data structures. However, the expressivity of the EVM has opened the door to expensive programming mistakes. Like StrawCoin, the Ether currency has a special status in the language and is implemented in a way that enforces scarcity. But implementers of custom assets (e.g., via the ERC20 [15] standard) do not inherit these protections (as described in (2)) — they must be careful not to introduce bugs that allow duplication, reuse, or loss of assets. This is challenging due to the combination of the indirect representation problem described in (1) and the highly dynamic behavior of the EVM. In particular, transferring Ether to a smart contract involves dynamic dispatch, which has led to a new class of bugs known as re-entrancy vulnerabilities [16]. High-profile exploits, such as the DAO attack [17] and the Parity Wallet hack [18], have allowed attackers to steal millions of dollars worth of cryptocurrency.  3. Move Design Goals The Libra mission is to enable a simple global currency and financial infrastructure that empowers billions of people [1]. The Move language is designed to provide a safe, programmable foundation upon which this vision can be built. Move must be able to express the Libra currency and governance rules in a precise, understandable, and verifiable manner. In the longer term, Move must be capable of encoding the rich variety of assets and corresponding business logic that make up a financial infrastructure. To satisfy these requirements, we designed Move with four key goals in mind: first-class assets, flexibility, safety, and verifiability.  5  3.1. First-Class Resources Blockchain systems let users write programs that directly interact with digital assets. As we discussed in Section 2.2, digital assets have special characteristics that distinguish them from the values traditionally used in programming, such as booleans, integers, and strings. A robust and elegant approach to programming with assets requires a representation that preserves these characteristics. The key feature of Move is the ability to define custom resource types with semantics inspired by linear logic [3]: a resource can never be copied or implicitly discarded, only moved between program storage locations. These safety guarantees are enforced statically by Move’s type system. Despite these special protections, resources are ordinary program values — they can be stored in data structures, passed as arguments to procedures, and so on. First-class resources are a very general concept that programmers can use not only to implement safe digital assets but also to write correct business logic for wrapping assets and enforcing access control policies. Libra coin itself is implemented as an ordinary Move resource with no special status in the language. Since a Libra coin represents real-world assets managed by the Libra reserve [19], Move must allow resources to be created (e.g., when new real-world assets enter the Libra reserve), modified (e.g., when the digital asset changes ownership), and destroyed (e.g., when the physical assets backing the digital asset are sold). Move programmers can protect access to these critical operations with modules. Move modules are similar to smart contracts in other blockchain languages. A module declares resource types and procedures that encode the rules for creating, destroying, and updating its declared resources. Modules can invoke procedures declared by other modules and use types declared by other modules. However, modules enforce strong data abstraction — a type is transparent inside its declaring module and opaque outside of it. Furthermore, critical operations on a resource type T may only be performed inside the module that defines T.  3.2. Flexibility Move adds flexibility to Libra via transaction scripts. Each Libra transaction includes a transaction script that is effectively the main procedure of the transaction. A transaction script is a single procedure that contains arbitrary Move code, which allows customizable transactions. A script can invoke multiple procedures of modules published in the blockchain and perform local computation on the results. This means that scripts can perform either expressive one-off behaviors (such as paying a specific set of recipients) or reusable behaviors (by invoking a single procedure that encapsulates the reusable logic)2 . Move modules enable a different kind of flexibility via safe, yet flexible code composition. At a high level, the relationship between modules/resources/procedures in Move is similar to the relationship between classes/objects/methods in object-oriented programming. However, there are important differences — a Move module can declare multiple resource types (or zero resource types), and Move procedures have no notion of a self or this value. Move modules are most similar to a limited version of ML-style modules [20].  3.3. Safety Move must reject programs that do not satisfy key properties, such as resource safety, type safety, and memory safety. How can we choose an executable representation that will ensure that every 2  By contrast, Ethereum transactions only support the second use-case — they are constrained to invoking a single smart contract method.  6  program executed on the blockchain satisfies these properties? Two possible approaches are: (a) use a high-level programming language with a compiler that checks these properties, or (b) use low-level untyped assembly and perform these safety checks at runtime. Move takes an approach between these two extremes. The executable format of Move is a typed bytecode that is higher-level than assembly yet lower-level than a source language. The bytecode is checked on-chain for resource, type, and memory safety by a bytecode verifier 3 and then executed directly by a bytecode interpreter. This choice allows Move to provide safety guarantees typically associated with a source language, but without adding the source compiler to the trusted computing base or the cost of compilation to the critical path for transaction execution.  3.4. Veriﬁability Ideally, we would check every safety property of Move programs via on-chain bytecode analysis or runtime checks. Unfortunately, this is not feasible. We must carefully weigh the importance and generality of a safety guarantee against the computational cost and added protocol complexity of enforcing the guarantee with on-chain verification. Our approach is to perform as much lightweight on-chain verification of key safety properties as possible, but design the Move language to support advanced off-chain static verification tools. We have made several design decisions that make Move more amenable to static verification than most general-purpose languages: 1. No dynamic dispatch. The target of each call site can be statically determined. This makes it easy for verification tools to reason precisely about the effects of a procedure call without performing a complex call graph construction analysis. 2. Limited mutability. Every mutation to a Move value occurs through a reference. References are temporary values that must be created and destroyed within the confines of a single transaction script. Move’s bytecode verifier uses a “borrow checking” scheme similar to Rust to ensure that at most one mutable reference to a value exists at any point in time. In addition, the language ensures that global storage is always a tree instead of an arbitrary graph. This allows verification tools to modularize reasoning about the effects of a write operation. 3. Modularity. Move modules enforce data abstraction and localize critical operations on resources. The encapsulation enabled by a module combined with the protections enforced by the Move type system ensures that the properties established for a module’s types cannot be violated by code outside the module. We expect this design to enable exhaustive functional verification of important module invariants by looking at a module in isolation without considering its clients. Static verification tools can leverage these properties of Move to accurately and efficiently check both for the absence of runtime failures (e.g., integer overflow) and for important program-specific functional correctness properties (e.g., the resources locked in a payment channel can eventually be claimed by a participant). We share more detail about our plans for functional verification in Section 7.  3  This design is similar to the load-time bytecode verification performed by the Java Virtual Machine [21] and Common Language Runtime [22].  7  4. Move Overview We introduce the basics of Move by walking through the transaction script and module involved in a simple peer-to-peer payment. The module is a simplified version of the actual Libra coin implementation. The example transaction script demonstrates that a malicious or careless programmer outside the module cannot violate the key safety invariants of the module’s resources. The example module shows how to implement a resource that leverages strong data abstraction to establish and maintain these invariants. The code snippets in this section are written in a variant of the Move intermediate representation (IR). Move IR is high-level enough to write human-readable code, yet low-level enough to have a direct translation to Move bytecode. We present code in the IR because the stack-based Move bytecode would be more difficult to read, and we are currently designing a Move source language (see Section 7). We note that all of the safety guarantees provided by the Move type system are checked at the bytecode level before executing the code.  4.1. Peer-to-Peer Payment Transaction Script public main(payee: address, amount: u64) { let coin: 0x0.Currency.Coin = 0x0.Currency.withdraw_from_sender(copy(amount)); 0x0.Currency.deposit(copy(payee), move(coin)); } This script takes two inputs: the account address of the payment’s recipient and an unsigned integer that represents the number of coins to be transferred to the recipient. The effect of executing this script is straightforward: amount coins will be transferred from the transaction sender to payee. This happens in two steps. In the first step, the sender invokes a procedure named withdraw_from_sender from the module stored at 0x0.Currency. As we will explain in Section 4.2, 0x0 is the account address4 where the module is stored and Currency is the name of the module. The value coin returned by this procedure is a resource value whose type is 0x0.Currency.Coin. In the second step, the sender transfers the funds to payee by moving the coin resource value into the 0x0.Currency module’s deposit procedure. This example is interesting because it is quite delicate. Move’s type system will reject small variants of the same code that would lead to bad behavior. In particular, the type system ensures that resources can never be duplicated, reused, or lost. For example, the following three changes to the script would be rejected by the type system: Duplicating currency by changing move(coin) to copy(coin). Note that each usage of a variable in the example is wrapped in either copy() or move(). Move, following Rust and C++, implements move semantics. Each read of a Move variable x must specify whether the usage moves x’s value out of the variable (rendering x unavailable) or copies the value (leaving x available for continued use). Unrestricted values like u64 and address can be both copied and moved. But resource values can only be moved. Attempting to duplicate a resource value (e.g., using copy(coin) in the example above) will cause an error at bytecode verification time. Reusing currency by writing move(coin) twice. Adding the line 0x0.Currency.deposit(copy(some_other_payee), move(coin)) to the example above would let the sender “spend” coin twice — the first time with payee and the second with some_other_payee. This undesirable behavior would not be possible with a physical asset. Fortunately, Move will reject 4  Addresses are 256-bit values that we abbreviate as 0x0, 0x1, etc., for convenience.  8  this program. The variable coin becomes unavailable after the first move, and the second move will trigger a bytecode verification error. Losing currency by neglecting to move(coin). The Move language implements linear [3][23] resources that must be moved exactly once5 . Failing to move a resource (e.g., by deleting the line that contains move(coin) in the example above) will trigger a bytecode verification error. This protects Move programmers from accidentally — or intentionally — losing track of the resource. These guarantees go beyond what is possible for physical assets like paper currency. We use the term resource safety to describe the guarantees that Move resources can never be copied, reused, or lost. These guarantees are quite powerful because Move programmers can implement custom resources that also enjoy these protections. As we mentioned in Section 3.1, even the Libra currency is implemented as a custom resource with no special status in the Move language.  4.2. Currency Module In this section, we will show how the implementation of the Currency module used in the example above leverages resource safety to implement a secure fungible asset. We will begin by explaining a bit about the blockchain environment in which Move code runs. Primer: Move execution model. As we explained in Section 3.2, Move has two different kinds of programs: transaction scripts, like the example outlined in Section 4.1 and modules, such as the Currency module that we will present shortly. Transaction scripts like the example are included in each user-submitted transaction and invoke procedures of a module to update the global state. Executing a transaction script is all-or-nothing — either execution completes successfully, and all of the writes performed by the script are committed to global storage, or execution terminates with an error (e.g., due to a failed assertion or out-of-gas error), and nothing is committed. A transaction script is a single-use piece of code — after its execution, it cannot be invoked again by other transaction scripts or modules. By contrast, a module is a long-lived piece of code published in the global state. The module name 0x0.Currency used in the example above contains the account address 0x0 where the module code is published. The global state is structured as a map from account addresses to accounts.  0x0  0x1  0x2  modules/Currency  modules/MyModule1  modules/MyModule2  modules/MyModule3 0x0.Currency.Coin { … } 0x0.Currency.Coin { … }  0x1.MyModule.T { … }  0x0.Currency.Coin { … }  Figure 1: A example global state with three accounts.  Each account can contain zero or more modules (depicted as rectangles) and one or more resource values (depicted as cylinders). For example, the account at address 0x0 contains a module 0x0.Currency and a resource value of type 0x0.Currency.Coin. The account at address 0x1 has two resources and one module; the account at address 0x2 has two modules and a single resource value. 5  This is similar to Rust, which implements affine resources that can be moved at most once.  9  Accounts can contain at most one resource value of a given type and at most one module with a given name. The account at address 0x0 would not be allowed to contain an additional 0x0.Currency.Coin resource or another module named Currency. However, the account at address 0x1 could add a module named Currency. In that case, 0x0 could also hold a resource of type 0x1.Currency.Coin. 0x0.Currency.Coin and 0x1.Currency.Coin are distinct types that cannot be used interchangeably; the address of the declaring module is part of the type. Note that allowing at most a single resource of a given type in an account is not restrictive. This design provides a predictable storage schema for top-level account values. Programmers can still hold multiple instances of a given resource type in an account by defining a custom wrapper resource (e.g., resource TwoCoins { c1: 0x0.Currency.Coin, c2: 0x0.Currency.Coin }). Declaring the Coin resource. Having explained how modules fit into the Move execution model, we are finally ready to look inside the Currency module: module Currency { resource Coin { value: u64 } // ... } This code declares a module named Currency and a resource type named Coin that is managed by the module. A Coin is a struct type with a single field value of type u64 (a 64-bit unsigned integer). The structure of Coin is opaque outside of the Currency module. Other modules and transaction scripts can only write or reference the value field via the public procedures exposed by the module. Similarly, only the procedures of the Currency module can create or destroy values of type Coin. This scheme enables strong data abstraction — module authors have complete control over the access, creation, and destruction of their declared resources. Outside of the API exposed by the Currency module, the only operation another module can perform on a Coin is a move. Resource safety prohibits other modules from copying, destroying, or double-moving resources. Implementing deposit. Let’s investigate how the Currency.deposit procedure invoked by the transaction script in the previous section works: public deposit(payee: address, to_deposit: Coin) { let to_deposit_value: u64 = Unpack<Coin>(move(to_deposit)); let coin_ref: &mut Coin = BorrowGlobal<Coin>(move(payee)); let coin_value_ref: &mut u64 = &mut move(coin_ref).value; let coin_value: u64 = *move(coin_value_ref); *move(coin_value_ref) = move(coin_value) + move(to_deposit_value); } At a high level, this procedure takes a Coin resource as input and combines it with the Coin resource stored in the payee’s account. It accomplishes this by: 1. Destroying the input Coin and recording its value. 2. Acquiring a reference to the unique Coin resource stored under the payee’s account. 3. Incrementing the value of payee’s Coin by the value of the Coin passed to the procedure. There are some aspects of the low-level mechanics of this procedure that are worth explaining. The Coin resource bound to to_deposit is owned by the deposit procedure. To invoke the procedure, the caller needs to move the Coin bound to to_deposit into the callee (which will prevent the caller from reusing it).  10  The Unpack procedure invoked at the first line is one of several module builtins for operating on the types declared by a module. Unpack<T> is the only way to delete a resource of type T. It takes a resource of type T as input, destroys it, and returns the values bound to the fields of the resource. Module builtins like Unpack can only be used on the resources declared in the current module. In the case of Unpack, this constraint prevents other code from destroying a Coin, which, in turn, allows the Currency module to set a custom precondition on the destruction of Coin resources (e.g., it could choose only to allow the destruction of zero-valued Coins). The BorrowGlobal procedure invoked on the third line is also a module builtin. BorrowGlobal<T> takes an address as input and returns a reference to the unique instance of T published under that address6 . This means that the type of coin_ref in the code above is &mut Coin — a mutable reference to a Coin resource, not Coin — which is an owned Coin resource. The next line moves the reference value bound to coin_ref in order to acquire a reference coin_value_ref to the Coin’s value field. The final lines of the procedure read the previous value of the payee’s Coin resource and mutate coin_value_ref to reflect the amount deposited7 . We note that the Move type system cannot catch all implementation mistakes inside the module. For example, the type system will not ensure that the total value of all Coins in existence is preserved by a call to deposit. If the programmer made the mistake of writing *move(coin_value_ref) = 1 + move(coin_value) + move(to_deposit_value) on the final line, the type system would accept the code without question. This suggests a clear division of responsibilities: it is the programmer’s job to establish proper safety invariants for Coin inside the confines of the module, and it is the type system’s job to ensure that clients of Coin outside the module cannot violate these invariants. Implementing withdraw_from_sender. In the implementation above, depositing funds via the deposit procedure does not require any authorization — deposit can be called by anyone. By contrast, withdrawing from an account must be protected by an access control policy that grants exclusive privileges to the owner of the Currency resource. Let us see how the withdraw_from_sender procedure called by our peer-to-peer payment transaction script implements this authorization: public withdraw_from_sender(amount: u64): Coin { let transaction_sender_address: address = GetTxnSenderAddress(); let coin_ref: &mut Coin = BorrowGlobal<Coin>(move(transaction_sender_address)); let coin_value_ref: &mut u64 = &mut move(coin_ref).value; let coin_value: u64 = *move(coin_value_ref); RejectUnless(copy(coin_value) >= copy(amount)); *move(coin_value_ref) = move(coin_value) - copy(amount); let new_coin: Coin = Pack<Coin>(move(amount)); return move(new_coin); } This procedure is almost the inverse of deposit, but not quite. It: 1. Acquires a reference to the unique resource of type Coin published under the sender’s account. 2. Decreases the value of the referenced Coin by the input amount. 3. Creates and returns a new Coin with value amount. The access control check performed by this procedure is somewhat subtle. The deposit procedure allows the caller to specify the address passed to BorrowGlobal, but withdraw_from_sender can only pass the address returned by GetTxnSenderAddress. This procedure is one of several transaction 6 7  If no instance of T exists under the given address, then the builtin will fail. If the addition on the final line results in an integer overflow, it will trigger a runtime error.  11  builtins that allow Move code to read data from the transaction that is currently being executed. The Move virtual machine authenticates the sender address before the transaction is executed. Using the BorrowGlobal builtin in this way ensures that that the sender of a transaction can only withdraw funds from her own Coin resource. Like all module builtins, BorrowGlobal<Coin> can only be invoked inside the module that declares Coin. If the Currency module does not expose a procedure that returns the result of BorrowGlobal, there is no way for code outside of the Currency module to get a reference to a Coin resource published in global storage. Before decreasing the value of the transaction sender’s Coin resource, the procedure asserts that the value of the coin is greater than or equal to the input formal amount using the RejectUnless instruction. This ensures that the sender cannot withdraw more than she has. If this check fails, execution of the current transaction script halts and none of the operations it performed will be applied to the global state. Finally, the procedure decreases the value of the sender’s Coin by amount and creates a new Coin resource using the inverse of Unpack — the Pack module builtin. Pack<T> creates a new resource of type T. Like Unpack<T>, Pack<T> can only be invoked inside the declaring module of resource T. Here, Pack is used to create a resource new_coin of type Coin and move it to the caller. The caller now owns this Coin resource and can move it wherever she likes. In our example transaction script in Section 4.1, the caller chooses to deposit the Coin into the payee’s account.  5. The Move Language In this section, we present a semi-formal description of the Move language, bytecode verifier, and virtual machine. Appendix A lays out all of these components in full detail, but without any accompanying prose. Our discussion here will use excerpts from the appendix and occasionally refer to symbols defined there. Global state. Σ ∈ GlobalState = Account =  AccountAddress ⇀ Account (StructID ⇀ Resource) × (ModuleName ⇀ Module)  The goal of Move is to enable programmers to define global blockchain state and securely implement operations for updating global state. As we explained in Section 4.2, the global state is organized as a partial map from addresses to accounts. Accounts contain both resource data values and module code values. Different resources in an account must have distinct identifiers. Different modules in an account must have distinct names. Modules. Module = ModuleID = StructID = StructDecl =  ModuleName × (StructName ⇀ StructDecl) × (ProcedureName ⇀ ProcedureDecl) AccountAddress × ModuleName ModuleID × StructName Kind × (FieldName ⇀ NonReferenceType)  A module consists of a name, struct declarations (including resources, as we will explain shortly), and  12  procedure declarations. Code can refer to a published module using a unique identifier consisting of the module’s account address and the module’s name. The module identifier serves as a namespace that qualifies the identifiers of its struct types and procedures for code outside of the module. Move modules enable strong data abstraction. The procedures of a module encode rules for creating, writing, and destroying the types declared by the module. Types are transparent inside their declaring module and opaque outside. Move modules can also enforce preconditions for publishing a resource under an account via the MoveToSender instruction, acquiring a reference to a resource under an account via the BorrowGlobal instruction, and removing a resource from an account via the MoveFrom instruction. Modules give Move programmers the flexibility to define rich access control policies for resources. For example, a module can define a resource type that can only be destroyed when its f field is zero, or a resource that can only be published under certain account addresses. Types. PrimitiveType = StructType = 𝒯 ⊆ NonReferenceType = Type ::=  AccountAddress ∪ Bool ∪ UnsignedInt64 ∪ Bytes StructID × Kind StructType ∪ PrimitiveType 𝒯 | &mut 𝒯 | & 𝒯  Move supports primitive types, including booleans, 64-bit unsigned integers, 256-bit account addresses, and fixed-size byte arrays. A struct is a user-defined type declared by a module. A struct type is designated as a resource by tagging it with a resource kind. All other types, including nonresource struct types and primitive types, are called unrestricted types. A variable of resource type is a resource variable; a variable of unrestricted type is an unrestricted variable. The bytecode verifier enforces restrictions on resource variables and struct fields of type resource. A resource variable cannot be copied and must always be moved. Both a resource variable and a struct field of resource type cannot be reassigned — doing so would destroy the resource value previously held in the storage location. In addition, a reference to a resource type cannot be dereferenced, since this would produce a copy of the underlying resource. By contrast, unrestricted types can be copied, reassigned, and dereferenced. Finally, an unrestricted struct type may not contain a field with a resource type. This restriction ensures that (a) copying an unrestricted struct does not result in the copying of a nested resource, and (b) reassigning an unrestricted struct does not result in the destruction of a nested resource. A reference type may either be mutable or immutable; writes through immutable references are disallowed. The bytecode verifier performs reference safety checks that enforce these rules along with the restrictions on resource types (see Section 5.2). Values. Resource = Struct = UnrestrictedValue = 𝑣 ∈ Value = 𝑔 ∈ GlobalResourceKey = 𝑎𝑝 ∈ AccessPath ::= 𝑟 ∈ RuntimeValue ::=  FieldName ⇀ Value FieldName ⇀ UnrestrictedValue Struct ∪ PrimitiveValue Resource ∪ UnrestrictedValue AccountAddress × StructID 𝑥 | 𝑔 | 𝑎𝑝 . 𝑓 𝑣 | ref 𝑎𝑝  13  In addition to structs and primitive values, Move also supports reference values. References are different from other Move values because they are transient. The bytecode verifier does not allow fields of reference type. This means that a reference must be created during the execution of a transaction script and released before the end of that transaction script. The restriction on the shape of struct values ensures the global state is always a tree instead of an arbitrary graph. Each storage location in the state tree can be canonically represented using its access path [24] — a path from a root in the storage tree (either a local variable 𝑥 or global resource key 𝑔) to a descendant node marked by a sequence of field identifiers 𝑓. The language allows references to primitive values and structs, but not to other references. Move programmers can acquire references to local variables with the BorrowLoc instruction, to fields of structs with the BorrowField instruction, and to resources published under an account with the BorrowGlobal instruction. The latter two constructs can only be used on struct types declared inside the current module. Procedures and transaction scripts. ∗  ProcedureSig =  Visibility × (VarName ⇀ Type) × Type  ProcedureDecl = Visibility ::= ℓ ∈ InstrIndex = TransactionScript = ProcedureID =  ProcedureSig × (VarName ⇀ Type) × [Instrℓ ]ℓ=0 public | internal UnsignedInt64 ProcedureDecl ModuleID × ProcedureSig  ℓ=𝑖  A procedure signature consists of visibility, typed formal parameters, and return types. A procedure declaration contains a signature, typed local variables, and an array of bytecode instructions. Procedure visibility may be either public or internal. Internal procedures can only be invoked by other procedures in the same module. Public procedures can be invoked by any module or transaction script. The blockchain state is updated by a transaction script that can invoke public procedures of any module that is currently published under an account. A transaction script is simply a procedure declaration with no associated module. A procedure can be uniquely identified by its module identifier and its signature. The Call bytecode instruction requires a unique procedure ID as input. This ensures that all procedure calls in Move are statically determined — there are no function pointers or virtual calls. In addition, the dependency relationship among modules is acyclic by construction. A module can only depend on modules that were published earlier in the linear transaction history. The combination of an acyclic module dependency graph and the absence of dynamic dispatch enforces a strong execution invariant: all stack frames belonging to procedures in a module must be contiguous. Thus, there is no equivalent of the re-entrancy [16] issues of Ethereum smart contracts in Move modules. In the rest of this section, we introduce bytecode operations and their semantics (Section 5.1) and describe the static analysis that the bytecode verifier performs before allowing module code to be executed or stored (Section 5.2).  14  5.1. Bytecode Interpreter 𝜎 ∈ InterpreterState = 𝑣𝑠𝑡𝑘 ∈ ValueStack ::= 𝑐𝑠𝑡𝑘 ∈ CallStack ::= 𝑐 ∈ CallStackFrame = Locals =  ValueStack × CallStack × GlobalRefCount × GasUnits [] | 𝑟 :: 𝑣𝑠𝑡𝑘 [] | 𝑐 :: 𝑐𝑠𝑡𝑘 Locals × ProcedureID × InstrIndex VarName ⇀ RuntimeValue  Move bytecode instructions are executed by a stack-based interpreter similar to the Common Language Runtime [22] and Java Virtual Machine [21]. An instruction consumes operands from the stack and pushes results onto the stack. Instructions may also move and copy values to/from the local variables of the current procedure (including formal parameters). The bytecode interpreter supports procedure calls. Input values passed to the callee and output values returned to the caller are also communicated via the stack. First, the caller pushes the arguments to a procedure onto the stack. Next, the caller invokes the Call instruction, which creates a new call stack frame for the callee and loads the pushed values into the callee’s local variables. Finally, the bytecode interpreter begins executing the bytecode instructions of the callee procedure. The execution of bytecode proceeds by executing operations in sequence unless there is a branch operation that causes a jump to a statically determined offset in the current procedure. When the callee wishes to return, it pushes the return values onto the stack and invokes the Return instruction. Control is then returned to the caller, which finds the output values on the stack. Execution of Move programs is metered in a manner similar to the EVM [9]. Each bytecode instruction has an associated gas unit cost, and any transaction to be executed must include a gas unit budget. The interpreter tracks the gas units remaining during execution and halts with an error if the amount remaining reaches zero. We considered both a register-based and a stack-based bytecode interpreter and found that a stack machine with typed locals is a very natural fit for the resource semantics of Move. The low-level mechanics of moving values back and forth between local variables, the stack, and caller/callee pairs closely mirror the high-level intention of a Move program. A stack machine with no locals would be much more verbose, and a register machine would make it more complex to move resources across procedure boundaries. Instructions. Move supports six broad categories of bytecode instructions: • Operations such as CopyLoc/MoveLoc for copying/moving data from local variables to the stack, and StoreLoc for moving data from the stack to local variables. • Operations on typed stack values such as pushing constants onto the stack, and arithmetic/logical operations on stack operands. • Module builtins such as Pack and Unpack for creating/destroying the module’s declared types, MoveToSender/MoveFrom for publishing/unpublishing the module’s types under an account, and BorrowField for acquiring a reference to a field of one of the module’s types. • Reference-related instructions such as ReadRef for reading references, WriteRef for writing references, ReleaseRef for destroying a reference, and FreezeRef for converting a mutable reference into an immutable reference. • Control-flow operations such as conditional branching and calling/returning from a procedure.  15  • Blockchain-specific builtin operations such as getting the address of the sender of a transaction script and creating a new account. Appendix A gives a complete list of Move bytecode instructions. Move also provides cryptographic primitives such as sha3, but these are implemented as modules in the standard library rather than as bytecode instructions. In these standard library modules, the procedures are declared as native, and the procedure bodies are provided by the Move VM. Only the VM can define new native procedures, which means that these cryptographic primitives could instead be implemented as ordinary bytecode instructions. However, native procedures are convenient because the VM can rely on the existing mechanisms for invoking a procedure instead of reimplementing the calling convention for each cryptographic primitive.  5.2. Bytecode Veriﬁer 𝐶 ∈ Code = 𝑧 ∈ VerificationResult ::= 𝐶⇝𝑧  TransactionScript ∪ Module ok | stack_err | type_err | reference_err | … bytecode verification  The goal of the bytecode verifier is to statically enforce safety properties for any module submitted for publication and any transaction script submitted for execution. No Move program can be published or executed without passing through the bytecode verifier. The bytecode verifier enforces general safety properties that must hold for any well-formed Move program. We aim to develop a separate offline verifier for program-specific properties in future work (see Section 7). The binary format of a Move module or transaction script encodes a collection of tables of entities, such as constants, type signatures, struct definitions, and procedure definitions. The checks performed by the verifier fall into three categories: • Structural checks to ensure that the bytecode tables are well-formed. These checks discover errors such as illegal table indices, duplicate table entries, and illegal type signatures such as a reference to a reference. • Semantic checks on procedure bodies. These checks detect errors such as incorrect procedure arguments, dangling references, and duplicating a resource. • Linking uses of struct types and procedure signatures against their declaring modules. These checks detect errors such as illegally invoking an internal procedure and using a procedure identifier that does not match its declaration. In the rest of this section, we will describe the phases of semantic verification and linking. Control-flow graph construction. The verifier constructs a control-flow graph by decomposing the instruction sequence into a collection of basic blocks (note that these are unrelated to the “blocks” of transactions in a blockchain). Each basic block contains a contiguous sequence of instructions; the set of all instructions is partitioned among the blocks. Each basic block ends with a branch or return instruction. The decomposition guarantees that branch targets land only at the beginning of some basic block. The decomposition also attempts to ensure that the generated blocks are maximal. However, the soundness of the bytecode verifier does not depend on maximality. Stack balance checking. Stack balance checking ensures that a callee cannot access stack locations that belong to callers. The execution of a basic block happens in the context of an array of local  16  variables and a stack. The parameters of the procedure are a prefix of the array of local variables. Passing arguments and return values across procedure calls is done via the stack. When a procedure starts executing, its arguments are already loaded into its parameters. Suppose the stack height is n when a procedure starts executing. Valid bytecode must satisfy the invariant that when execution reaches the end of a basic block, the stack height is n. The verifier ensures this by analyzing each basic block separately and calculating the effect of each instruction on the stack height. It checks that the height does not go below n, and is n at the basic block exit. The one exception is a block that ends in a Return instruction, where the height must be n+m (where m is the number of values returned by the procedure). Type checking. The second phase of the verifier checks that each instruction and procedure (including both builtin procedures and user-defined procedures) is invoked with arguments of appropriate types. The operands of an instruction are values located either in a local variable or on the stack. The types of local variables of a procedure are already provided in the bytecode. However, the types of stack values are inferred. This inference and the type checking of each operation is done separately for each basic block. Since the stack height at the beginning of each basic block is n and does not go below n during the execution of the block, we only need to model the suffix of the stack starting at n for type checking the block instructions. We model this suffix using a stack of types on which types are pushed and popped as the instruction sequence in a basic block is processed. The type stack and the statically known types of local variables are sufficient to type check each bytecode instruction. Kind checking. The verifier enforces resource safety via the following additional checks during the type checking phase: • Resources cannot be duplicated: CopyLoc is not used on a local variable of kind resource, and ReadRef is not used on a stack value whose type is a reference to a value of kind resource. • Resources cannot be destroyed: PopUnrestricted is not used on a stack location of kind resource, StoreLoc is not used on a local variable that already holds a resource, and WriteRef is not performed on a reference to a value of kind resource. • Resources must be used: When a procedure returns, no local variables may hold a resource value, and the callee’s segment of the evaluation stack must only hold the return values of the procedure. A non-resource struct type cannot have a field of kind resource, so these checks cannot be subverted by (e.g.) copying a non-resource struct with a resource field. Resources cannot be destroyed by a program execution that halts with an error. As we explained in Section 4.2, no state changes produced by partial execution of a transaction script will ever be committed to the global state. This means that resources sitting on the stack or in local variables at the time of a runtime failure will (effectively) return to wherever they were before execution of the transaction began. In principle, a resource could be made unreachable by a nonterminating program execution. However, the gas metering scheme described in Section 5.1 ensures that execution of a Move program always terminates. An execution that runs out of gas halts with an error, which will not result in the loss of a resource (as we explained above). Reference checking. The safety of references is checked using a combination of static and dynamic analyses. The static analysis uses borrow checking in a manner similar to the Rust type system, but performed at the bytecode level instead of at the source code level. These reference checks ensure two strong properties: • All references point to allocated storage (i.e., there are no dangling references).  17  • All references have safe read and write access. References are either shared (with no write access and liberal read access) or exclusive (with limited read and write access). To ensure that these properties hold for references into global storage created via BorrowGlobal, the bytecode interpreter performs lightweight dynamic reference counting. The interpreter tracks the number of outstanding references to each published resource. It uses this information to halt with an error if a global resource is borrowed or moved while references to that resource still exist. This reference checking scheme has many novel features and will be a topic of a separate paper. Linking with global state. 𝐷 ∈ Dependencies = deps ∈ Code → Dependencies 𝑙 ∈ LinkingResult ::= ⟨𝐷, Σ⟩ ↪ 𝑙  ∗  ∗  StructType × ProcedureID computing dependencies success | fail linking dependencies with global state  During bytecode verification, the verifier assumes that the external struct types and procedure ID’s used by the current code unit exist and are represented faithfully. The linking step checks this assumption by reading the struct and procedure declarations from the global state Σ and ensuring that the declarations match their usage. Specifically, the linker checks that the following declarations in the global state match their usage in the current code unit: • Struct declarations (name and kind). • Procedure signatures (name, visibility, formal parameter types, and return types).  6. Move Virtual Machine: Putting It All Together 𝑇 ∈ Transaction = 𝐵 ∈ Block = 𝐸 ∈ TransactionEffect = apply ∈ (GlobalState × TransactionEffect) → GlobalState ⟨𝑇 , 𝐸, Σ⟩ ⇓ 𝐸 ′ ⟨𝐵, Σ⟩ ⇓ 𝐸  ∗  ∗  TransactionScript × PrimitiveValue × Module ×AccountAddress × GasUnits … ∗ Transaction × … AccountAddress ⇀ Account updating global state transaction evaluation block evaluation  The role of the Move virtual machine is to execute a block 𝐵 of transactions from a global state Σ and produce a transaction effect 𝐸 representing modifications to the global state. The effect 𝐸 can then be applied to Σ to generate the state Σ′ resulting from the execution of 𝐵. Separating the effects from the actual state update allows the VM to implement transactional semantics in the case of execution failures. Intuitively, the transaction effect denotes the update to the global state at a subset of the accounts. A transaction effect has the same structure as the global state: it is a partial map from account addresses to accounts, which contain canonically-serialized representations of Move modules and resource values. The canonical serialization implements a language-independent 1-1 function from a Move module or resource to a byte array.  18  To execute the block 𝐵 from state Σ𝑖−1 , the VM fetches a transaction 𝑇𝑖 from 𝐵, processes it to produce an effect 𝐸𝑖 , then applies 𝐸𝑖 to Σ𝑖−1 to produce a state Σ𝑖 to use as the initial state for the next transaction in the block. The effect of the entire block is the ordered composition of the effects of each transaction in the block. Each transaction is processed according to a workflow that includes steps such as verifying the bytecode in the transaction and checking the signature of the transaction sender. The entire workflow for executing a single transaction is explained in more detail in [2]. Today, transactions in a block are executed sequentially by the VM, but the Move language has been designed to support parallel execution. In principle, executing a transaction could produce a set of reads as well as a set of write effects 𝐸. Each transaction in the block could be speculatively executed in parallel and re-executed only if its read/write set conflicts with another transaction in the block. Checking for conflicts is straightforward because Move’s tree memory model allows us to uniquely identify a global memory cell using its access path. We will explore speculative execution schemes in the future if virtual machine performance becomes a bottleneck for the Libra Blockchain.  7. What's Next for Move So far, we have designed and implemented the following components of Move: • A programming model suitable for blockchain execution. • A bytecode language that fits this programmable model. • A module system for implementing libraries with both strong data abstraction and access control. • A virtual machine consisting of a serializer/deserializer, bytecode verifier, and bytecode interpreter. Despite this progress, there is a long road ahead. We conclude by discussing some immediate next steps and longer-term plans for Move. Implementing core Libra Blockchain functionality. We will use Move to implement the core functionality in the Libra Blockchain: accounts, Libra coin, Libra reserve management, validator node addition and removal, collecting and distributing transaction fees, cold wallets, etc. This work is already in progress. New language features. We will add parametric polymorphism (generics), collections, and events to the Move language. Parametric polymorphism will not undermine Move’s existing safety and verifiability guarantees. Our design adds type parameters with kind (i.e, resource or unrestricted) constraints to procedure and structs in a manner similar to [25]. In addition, we will develop a trusted mechanism for versioning and updating Move modules, transaction scripts, and published resources. Improved developer experience. The Move IR was developed as a testing tool for the Move bytecode verifier and virtual machine. To exercise these components, the IR compiler must intentionally produce bad bytecode that will be (e.g.) be rejected by the bytecode verifier. This means that although the IR is suitable for prototyping Move programs, it is not particularly user-friendly. To make Move more attractive for third-party development, we will both improve the IR and work toward developing an ergonomic Move source language.  19  Formal specification and verification. We will create a logical specification language and automated formal verification tool that leverage Move’s verification-friendly design (see Section 3.4). The verification toolchain will check program-specific functional correctness properties that go beyond the safety guarantees enforced by the Move bytecode verifier (Section 5.2). Our initial focus is to specify and verify the modules that implement the core functionality of the Libra Blockchain. Our longer-term goal is to promote a culture of correctness in which users will look to the formal specification of a module to understand its functionality. Ideally, no Move programmer will be willing to interact with a module unless it has a comprehensive formal specification and has been verified to meet to that specification. However, achieving this goal will present several technical and social challenges. Verification tools should be precise and intuitive. Specifications must be modular and reusable, yet readable enough to serve as useful documentation of the module’s behavior. Support third-party Move modules. We will develop a path to third-party module publishing. Creating a good experience for both Libra users and third-party developers is a significant challenge. First, opening the door to general applications must not affect the usability of the system for core payment scenarios and associated financial applications. Second, we want to avoid the reputational risk that scams, speculation, and buggy software present. Building an open system while encouraging high software quality is a difficult problem. Steps such as creating a marketplace for high-assurance modules and providing effective tools for verifying Move code will help.  Acknowledgments We thank Tarun Chitra, Sophia Drossopoulou, Susan Eisenbach, Maurice Herlihy, John Mitchell, James Prestwich, and Ilya Sergey for their thoughtful feedback on this paper.  20  A. Move Language Reference In this appendix, we present the structure of programs and state in the Move bytecode language. Identifiers 𝑛 ∈ StructName 𝑓 ∈ FieldName 𝑥 ∈ VarName ProcedureName ModuleName  Types and Kinds 𝑎 ∈ AccountAddress 𝑏 ∈ Bool 𝑢 ∈ UnsignedInt64 𝑏⃗ ∈ Bytes Kind ::= ModuleID = StructID = StructType = PrimitiveType = 𝒯 ⊆ NonReferenceType = Type ::=  resource | unrestricted AccountAddress × ModuleName ModuleID × StructName StructID × Kind AccountAddress ∪ Bool ∪ UnsignedInt64 ∪ Bytes StructType ∪ PrimitiveType 𝒯 | &mut 𝒯 | & 𝒯  Values Resource = Struct = PrimitiveValue ::= UnrestrictedValue = 𝑣 ∈ Value = 𝑔 ∈ GlobalResourceKey = 𝑎𝑝 ∈ AccessPath ::= 𝑟 ∈ RuntimeValue ::=  FieldName ⇀ Value FieldName ⇀ UnrestrictedValue 𝑎 | 𝑏 | 𝑢 | 𝑏⃗ Struct ∪ PrimitiveValue Resource ∪ UnrestrictedValue AccountAddress × StructID 𝑥 | 𝑔 | 𝑎𝑝 . 𝑓 𝑣 | ref 𝑎𝑝  Global State Σ ∈ GlobalState = Account =  AccountAddress ⇀ Account (StructID ⇀ Resource) × (ModuleName ⇀ Module)  21  Modules and Transaction Scripts Module = TransactionScript = StructDecl = ProcedureSig =  ModuleName × (StructName ⇀ StructDecl) × (ProcedureName ⇀ ProcedureDecl) ProcedureDecl Kind × (FieldName ⇀ NonReferenceType) ∗ Visibility × (VarName ⇀ Type) × Type  ProcedureDecl = Visibility ::= ℓ ∈ InstrIndex =  ProcedureSig × (VarName ⇀ Type) × [Instrℓ ]ℓ=0 public | internal UnsignedInt64  ℓ=𝑖  Interpreter State 𝜎 ∈ InterpreterState = 𝑣𝑠𝑡𝑘 ∈ ValueStack ::= 𝑐𝑠𝑡𝑘 ∈ CallStack ::= 𝑐 ∈ CallStackFrame = Locals = 𝑝 ∈ ProcedureID = GlobalRefCount = GasUnits =  ValueStack × CallStack × GlobalRefCount × GasUnits [] | 𝑟 :: 𝑣𝑠𝑡𝑘 [] | 𝑐 :: 𝑐𝑠𝑡𝑘 Locals × ProcedureID × InstrIndex VarName ⇀ RuntimeValue ModuleID × ProcedureSig GlobalResourceKey ⇀ UnsignedInt64 UnsignedInt64  Evaluation ∗  𝑇 ∈ Transaction = 𝐵 ∈ Block = 𝐸 ∈ TransactionEffect = apply ∈ (GlobalState × TransactionEffect) → GlobalState ⟨𝐵, Σ⟩ ⇓ 𝐸 ⟨𝑇 , 𝐸, Σ⟩ ⇓ 𝐸 ′ ′  ⟨𝜎, 𝐸, Σ⟩ ⇓ 𝜎 , 𝐸  block evaluation transaction evaluation  ′  interpreter state evaluation  Verification 𝐶 ∈ Code = 𝑧 ∈ VerificationResult ::= 𝐶⇝𝑧 𝐷 ∈ Dependencies = deps ∈ Code → Dependencies 𝑙 ∈ LinkingResult ::= ⟨𝐷, Σ⟩ ↪ 𝑙  ∗  TransactionScript × PrimitiveValue × Module ×AccountAddress × GasUnits … ∗ Transaction × … AccountAddress ⇀ Account updating global state  TransactionScript ∪ Module ok | stack_err | type_err | reference_err | … bytecode verification ∗ ∗ StructType × ProcedureID computing dependencies success | fail linking dependencies with global state  22  Instructions LocalInstr ::= MoveLoc< 𝑥 > | CopyLoc< 𝑥 > | StoreLoc< 𝑥 > | BorrowLoc< 𝑥 > ReferenceInstr ::= ReadRef | WriteRef | ReleaseRef | FreezeRef CallInstr ::= Call< 𝑝 > | Return ModuleBuiltinInstr ::= Pack< 𝑛 > | Unpack< 𝑛 > | BorrowField< 𝑓 > | MoveToSender< 𝑛 >† | MoveFrom< 𝑛 >† | BorrowGlobal< 𝑛 >† | Exists< 𝑛 > TxnBuiltinInstr ::= GetGasRemaining | GetTxnSequenceNumber | GetTxnPublicKey | GetTxnSenderAddress | GetTxnMaxGasUnits | GetTxnGasUnitPrice SpecialInstr ::= PopUnrestricted | RejectUnless† | CreateAccount† ConstantInstr ::= LoadTrue | LoadFalse | LoadU64< 𝑢 > | LoadAddress< 𝑎 > | LoadBytes< 𝑏⃗ > StackInstr ::= Not | Add† | Sub† | Mul† | Div† | Mod† | BitOr | BitAnd | Xor | Lt  † indicates an instruction whose execution may fail at runtime Push value stored in 𝑥 on the stack. 𝑥 is now unavailable. Push value stored in 𝑥 on the stack. Pop the stack and store the result in 𝑥. 𝑥 is now available. Create a reference to the value stored in 𝑥 and push it on the stack. Pop Pop Pop Pop  𝑟 and push ∗𝑟 on the stack. two values 𝑣 and 𝑟, perform the write ∗𝑟 = 𝑣. 𝑟 and decrement the appropriate refcount if 𝑟 is a global reference. mutable reference 𝑟, push immutable reference to the same value.  Pop arguments 𝑟∗ , load into 𝑝’s formals 𝑥∗ , transfer control to 𝑝. Return control to the previous frame in the call stack. Pop Pop Pop Pop Pop Pop Pop  arguments 𝑣∗ , create struct of type 𝑛 with 𝑓𝑖 : 𝑣𝑖 , push it on the stack. struct of type 𝑛 from the stack and push its field values 𝑣∗ on the stack. reference to a struct and push a reference to field 𝑓 of the struct. resource of type 𝑛 and publish it under the sender’s address. address 𝑎, remove resource of type 𝑛 from 𝑎, push it. address 𝑎, push a reference to the resource of type 𝑛 under 𝑎. address 𝑎, push bool encoding “a resource of type 𝑛 exists under 𝑎”.  Push Push Push Push Push Push  u64 representing remaining gas unit budget. u64 encoding the transaction’s sequence number. byte array encoding the transaction sender’s public key. address encoding the sender of the transaction. u64 representing the initial gas unit budget. u64 representing the Libra coin per gas unit price.  Pop a non-resource value. Pop bool 𝑏 and u64 𝑢, fail with error code 𝑢 if 𝑏 is false. Pop address 𝑎, create a LibraAccount.T under 𝑎. Push Push Push Push Push Pop Pop Pop Pop Pop Pop Pop Pop Pop Pop  true on the stack. false on the stack. the u64 𝑢 on the stack. the address 𝑎 on the stack. the byte array 𝑏⃗ on the stack.  boolean 𝑏 and push ¬𝑏. two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and two u64’s 𝑢1 and 𝑢2 and  23  push push push push push push push push push  𝑢1 + 𝑢2 . Fail on overflow. 𝑢1 - 𝑢2 . Fail on underflow. 𝑢1 × 𝑢2 . Fail on overflow. 𝑢1 ÷ 𝑢2 . Fail if 𝑢2 is zero. 𝑢1 mod 𝑢2 . Fail if 𝑢2 is zero. 𝑢1 | 𝑢2 . 𝑢1 & 𝑢2 . 𝑢1 ⊕ 𝑢2 . 𝑢1 < 𝑢2 .  Instructions  † indicates an instruction whose execution may fail at runtime  | Gt | Le | Ge | Or | And | Eq | Neq ControlFlowInstr ::= Branch< ℓ > | BranchIfTrue< ℓ > | BranchIfFalse< ℓ > Instr = LocalInstr ∪ ReferenceInstr ∪ CallInstr ∪ ModuleBuiltinInstr ∪ TxnBuiltinInstr ∪ SpecialInstr ∪ ConstantInstr ∪ StackInstr ∪ ControlFlowInstr  Pop Pop Pop Pop Pop Pop Pop  two two two two two two two  u64’s 𝑢1 and 𝑢2 and push 𝑢1 > 𝑢2 . u64’s 𝑢1 and 𝑢2 and push 𝑢1 ≤ 𝑢2 . u64’s 𝑢1 and 𝑢2 and push 𝑢1 ≥ 𝑢2 . booleans 𝑏1 and 𝑏2 and push 𝑏1 ∨ 𝑏2 . booleans 𝑏1 and 𝑏2 and push 𝑏1 ∧ 𝑏2 . values 𝑟1 and 𝑟2 and push 𝑟1 = 𝑟2 . values 𝑟1 and 𝑟2 and push 𝑟1 ≠ 𝑟2 .  Jump to instruction index ℓ in the current procedure. Pop boolean, jump to instruction index ℓ in the current procedure if true. Pop boolean, jump to instruction index ℓ in the current procedure if false.  24  References [1] The Libra Association, “An Introduction to Libra.” https://libra.org/en-us/whitepaper. [2] Z. Amsden et al., “The Libra Blockchain.” https://developers.libra.org/docs/the-libra-blockchainpaper. [3] J. Girard, “Linear logic,” Theor. Comput. Sci., 1987. [4] L. Lamport, “Using time instead of timeout for fault-tolerant distributed systems,” ACM Trans. Program. Lang. Syst., 1984. [5] M. Baudet et al., “State machine replication in the Libra Blockchain.” https://developers.libra.o rg/docs/state-machine-replication-paper. [6] M. Saad et al., “Exploring the attack surface of blockchain: A systematic overview,” CoRR, 2019. [7] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,” 2008. http://bitcoin.org/bitcoin. pdf [8] bitcoin.org, “Bitcoin script.”. https://en.bitcoin.it/wiki/Script [9] G. Wood, “Ethereum: A secure decentralised generalised transaction ledger,” 2014. https://ethe reum.github.io/yellowpaper/paper.pdf [10] bitcoin.org, “Multisignature.”. https://en.bitcoin.it/wiki/Multisignature [11] M. Bartoletti and R. Zunino, “BitML: A calculus for Bitcoin smart contracts,” in Proceedings of the ACM SIGSAC conference on computer and communications security, 2018. [12] bitcoin.org, “Colored coin.”. https://en.bitcoin.it/wiki/Colored_Coin [13] K. Crary and M. J. Sullivan, “Peer-to-peer affine commitment using bitcoin,” in Proceedings of the ACM SIGPLAN conference on programming language design and implementation, 2015. [14] N. Szabo, “Formalizing and securing relationships on public networks,” First Monday, vol. 2, no. 9, 1997. https://ojphi.org/ojs/index.php/fm/article/view/548 [15] V. Buterin and F. Vogelsteller, “ERC20 token standard.” 2015. https://theethereum.wiki/w/i ndex.php/ERC20_Token_Standard [16] I. Sergey and A. Hobor, “A concurrent perspective on smart contracts,” in Financial cryptography and data security, 2017. [17] V. Buterin, “Critical update re DAO.” 2016. https://ethereum.github.io/blog/2016/06/17/criti cal-update-re-dao-vulnerability [18] “The Parity wallet hack explained.” 2017. https://blog.zeppelin.solutions/on-the-parity-walletmultisig-hack-405a8c12e8f7 [19] C. Catalini, O. Gratry, J. M. Hou, S. Parasuraman, and N. Wernerfelt, “The Libra reserve.” https://libra.org/about-currency-reserve. [20] R. Milner, M. Tofte, and R. Harper, “Definition of standard ML,” 1990. [21] T. Lindholm and F. Yellin, The Java virtual machine specification. Addison-Wesley, 1997. [22] E. Meijer, R. Wa, and J. Gough, “Technical overview of the common language runtime.” 2000.  25  [23] D. Walker, “Substructural type systems,” in Advanced topics in types and programming languages, The MIT Press, 2004. [24] N. D. Jones and S. S. Muchnick, “Flow analysis and optimization of LISP-like structures,” in POPL, 1979. [25] K. Mazurak, J. Zhao, and S. Zdancewic, “Lightweight linear types in System F,” in Proceedings of the ACM SIGPLAN international workshop on types in languages design and implementation, 2010.  26  The Libra Reserve Christian Catalini, Oliver Gratry, J. Mark Hou, Sunita Parasuraman, Nils Wernerfelt*  This document contains several parts: (1) What is the purpose of the reserve? (2) How is it getting set up? (3) How do entities interact with it? And (4) How will it change over time?  1 What Is the Purpose of the Reserve? Many cryptocurrencies today (e.g., Bitcoin and Ether) have no underlying assets to back them. As a result, speculation and investment have been primary use cases, as with a potential for substantial appreciation many people have acquired these coins hoping to sell them at a higher price later. As beliefs over the longterm value of these currencies and success of their networks fluctuate, so too have their prices, yielding at times massive swings in value. To drive widespread adoption, Libra is designed to be a currency where any user will know that the value of a Libra today will be close to its value tomorrow and in the future. Just as consumers in Europe know the number of Euros it takes them to buy a coffee today will be similar to the number of Euros it will take them tomorrow, holders of Libra, too, can be confident the value of their coins today will be relatively stable across time. The reserve is the key mechanism for achieving value preservation. By fully backing each coin with a set of stable and liquid assets (described later) and by working with a competitive group of exchanges and other liquidity providers, users can have confidence that they will be able to sell any Libra coin at or close to the value of the reserve at any time. This gives the coin intrinsic value on day one and helps protect against the speculative swings of other cryptocurrencies. The mechanics of the reserve and the various actors in the system are described later in this section, but, at the outset, it is important to highlight why the reserve was created in the first place — to support stability and value preservation.  2 How Is the Reserve Getting Set Up? There are several parts to this section. First, where is the money for the reserve coming from? Second, how will the reserve be invested? Third, where is it being held? And fourth, what are the actual assets backing each Libra coin? Where is the money for the reserve coming from? The money in the reserve will come from two sources: investors in the separate Investment Token, and users of Libra. The association will pay out incentives in Libra coin to Founding Members to encourage adoption by users, merchants, and developers. The funds for the coins that will be distributed as incentives will come from a private placement to investors. On the user side, for new Libra coins to be created, there must be an ∗  The authors work at Calibra, a subsidiary of Facebook, Inc., and contribute this paper to the Libra Association under a Creative Commons Attribution 4.0 International License.  1  equivalent purchase of Libra for fiat and transfer of that fiat to the reserve. Hence, the reserve will grow as users’ demand for Libra increases. In short, on both the investor and user side, there is only one way to create more Libra — by purchasing more Libra for fiat and growing the reserve. How will the reserve be invested? Users of Libra do not receive a return from the reserve. The reserve will be invested in low-risk assets that will yield interest over time. The revenue from this interest will first go to support the operating expenses of the association — to fund investments in the growth and development of the ecosystem, grants to nonprofit and multilateral organizations, engineering research, etc. Once that is covered, part of the remaining returns will go to pay dividends to early investors in the Libra Investment Token for their initial contributions. Because the assets in the reserve are low risk and low yield, returns for early investors will only materialize if the network is successful and the reserve grows substantially in size. How is the reserve being held? The reserve will be held by a geographically distributed network of custodians with investment-grade credit rating to limit counterparty risk. Safeguarding the reserve’s assets, providing high auditability and transparency, avoiding the risks of a centralized reserve, and achieving operational efficiency are the key parameters in custody selection and design. What are the actual assets that will be backing each Libra coin? The actual assets will be a collection of low-volatility assets, including bank deposits and government securities in currencies from stable and reputable central banks. As the value of Libra will be effectively linked to a basket of fiat currencies, from the point of view of any specific currency, there will be fluctuations in the value of Libra. The makeup of the reserve is designed to mitigate the likelihood and severity of these fluctuations, particularly in the negative direction (i.e., even in economic crises). To that end, the above basket has been structured with capital preservation and liquidity in mind. On the capital preservation point, the association will only invest in debt from stable governments with low default probability that are unlikely to experience high inflation. In addition, the reserve has been diversified by selecting multiple governments, rather than just one, to further reduce the potential impact of such events. In terms of liquidity, the association plans to rely on short-dated securities issued by these governments, that are all traded in liquid markets that regularly accommodate daily trading volume in the tens or even hundreds of billions. This allows the size of the reserve to be easily adjusted as the number of Libra in circulation expands or contracts.  2.1 How Do Entities Interact With the Reserve? Users will not directly interface with the reserve. Rather, to support higher efficiency, there will be authorized resellers who will be the only entities authorized by the association to transact large amounts of fiat and Libra in and out of the reserve. These authorized resellers will integrate with exchanges and other institutions that buy and sell cryptocurrencies to users, and will provide these entities with liquidity for users who wish to convert from cash to Libra and back again. The association does not set monetary policy. It mints and burns coins only in response to demand from authorized resellers. Users do not need to worry about the association introducing inflation into the system or debasing the currency. For new coins to be minted, there must be a commensurate payment of fiat by resellers into the reserve. Through interaction with authorized resellers, the association automatically mints new coins when demand increases and destroys them when the demand contracts. Because the reserve will not be actively managed, any appreciation or depreciation in the value of the Libra will come solely as a result of FX market movements. The association will encourage the listing of Libra on multiple regulated electronic exchanges throughout the world. These exchanges offer both web portals and mobile apps for users to buy and sell Libra. The association is also discussing ongoing relationships with principal cryptocurrency trading  2  firms and top banking institutions as authorized resellers to allow people the opportunity to exchange their local currencies for Libra as easily as possible. Consumer financial protection and law enforcement Consumers of financial services and products can be vulnerable. There is a commitment to strong consumer protection in the Libra ecosystem, and the Libra Association recognizes that regulators charged with consumer protection will be keen to engage with those building services to be offered in their jurisdictions. In the early development of the Libra network, its Founding Members are committed to working with authorities to shape a regulatory environment that encourages technological innovation while maintaining high standards of consumer protection. As with any currency or financial infrastructure, bad actors will try to exploit the Libra network. While the network is open and accessible to everyone with internet access, the network’s main endpoints, in the form of exchanges and wallets, will need to follow applicable laws and regulations and collaborate with law enforcement. In addition, like many other blockchains, the ledger of transactions on the Libra Blockchain will be publicly accessible so that it is possible for third parties to do analysis to detect and penalize fraud.  3 How Will the Reserve Change Over Time? The reserve will remain fully backed across time. This means that today, tomorrow, and in the future, users can have confidence that any Libra coin they hold can be sold for fiat currency at a narrow spread above or below the value of the underlying reserve, when a competitive market for exchanges is present. The association may occasionally change the composition of the basket in response to significant changes in market conditions (e.g., to respond to an economic crisis in one of the represented regions), but the goal will always be value preservation. Further, such a change would require exceptional circumstances and a supermajority vote by the association’s council. Importantly, the size of the reserve is determined by the size of the balances that users are holding in Libra. Hence, unlike some other cryptocurrencies, supply is not restricted by outside factors. This has the important role of allowing the Libra ecosystem to grow or shrink with demand. It also discourages “runs on the bank” since the typical rationale behind a run is that a coin is only fractionally backed, so users want to get their backing out before others do. With a fully backed coin and a competitive ecosystem of exchanges, it will be possible to convert coins back to fiat at a narrow spread above or below their current value, no matter how many coins are in circulation or how many people have already sold their Libra. The market value of the reserve always supports the value of the fiat currency that users receive if they sell their Libra. Our goal is for Libra to exist alongside existing currencies. Since Libra will be global, the association decided not to develop its own monetary policy but to inherit the policies of the central banks represented in the basket. As discussed earlier, the mechanics of interfacing with our reserve make our approach very similar to the way in which currency boards (e.g., of Hong Kong) have operated. Whereas central banks can print money at their own discretion, currency boards typically only print local currency when there are sufficient foreign exchange assets to fully back a new minting of notes and coins. Two of the major reasons for implementing such a system are stability — as the underlying assets are selected for their lower volatility — and protection from future abuse (e.g., printing of additional coins in the absence of backing). A key objective of the project is to provide billions of people with access to a low-volatility cryptocurrency that can serve as a low-friction medium of exchange on an international basis from day one, and can support new digital-native use cases such as micropayments. The Libra Reserve plays a vital role  3  in supporting value preservation, building trust, and protecting the resources users, merchants, and developers bring to the network. Over time, as the network moves to permissionless (see Moving Toward Permissionless Consensus), the association will explore ways to further increase the geographic distribution and resilience of the reserve to economic shocks and to improve how it achieves stability and value preservation for its users. We look forward to feedback and ideas from researchers and the Libra community as we work together with the association Founding Members toward these goals.  4 Acknowledgments We would like to thank the following people for helpful discussions and feedback on this paper: Adrien Auclert, Ravi Jagadeesan, Scott Duke Kominers, Roberto Rigobon, Catherine Tucker, and Kevin Zhang.  4  An Introduction to Libra White Paper  •  From the Libra Association Members  Libra’s mission is to enable a simple global currency and financial infrastructure that empowers billions of people. This document outlines our plans for a new decentralized blockchain, a low-volatility cryptocurrency, and a smart contract platform that together aim to create a new opportunity for responsible financial services innovation.  Problem Statement The advent of the internet and mobile broadband has empowered billions of people globally to have access to the world’s knowledge and information, high-fidelity communications, and a wide range of lower-cost, more convenient services. These services are now accessible using a $40 smartphone from almost anywhere in the world.1 This connectivity has driven economic empowerment by enabling more people to access the financial ecosystem. Working together, technology companies and financial institutions have also found solutions to help increase economic empowerment around the world. Despite this progress, large swaths of the world’s population are still left behind — 1.7 billion adults globally remain outside of the financial system with no access to a traditional bank, even though one billion have a mobile phone and nearly half a billion have internet access. 2 For too many, parts of the financial system look like telecommunication networks pre-internet. Twenty years ago, the average price to send a text message in Europe was 16 cents per message.3 Now everyone with a smartphone can communicate across the world for free with a basic data plan. Back then, telecommunications prices were high but uniform, whereas today, access to financial services is limited or restricted for those who need it most — those impacted by cost, reliability, and the ability to seamlessly send money. All over the world, people with less money pay more for financial services. Hard-earned income is eroded by fees, from remittances and wire costs to overdraft and ATM charges. Payday loans can charge annualized interest rates of 400 percent or more, and finance charges can be as high as $30 just to borrow $100.4 When people are asked why they remain on the fringe of the existing financial system, those who remain “unbanked” point to not having sufficient funds, high and unpredictable fees, banks being too far away, and lacking the necessary documentation.5 Blockchains and cryptocurrencies have a number of unique properties that can potentially address some of the problems of accessibility and trustworthiness. These include distributed governance, which ensures that no single entity controls the network; open access, which allows anybody with an internet connection to participate; and security through cryptography, which protects the integrity of funds.  1 / 12  But the existing blockchain systems have yet to reach mainstream adoption. Mass-market usage of existing blockchains and cryptocurrencies has been hindered by their volatility and lack of scalability, which have, so far, made them poor stores of value and mediums of exchange. Some projects have also aimed to disrupt the existing system and bypass regulation as opposed to innovating on compliance and regulatory fronts to improve the effectiveness of anti-money laundering. We believe that collaborating and innovating with the financial sector, including regulators and experts across a variety of industries, is the only way to ensure that a sustainable, secure, and trusted framework underpins this new system. And this approach can deliver a giant leap forward toward a lower-cost, more accessible, and more connected global financial system.  The Opportunity As we embark on this journey together, we think it is important to share our beliefs to align the community and ecosystem we intend to spark around this initiative: •• We believe that many more people should have access to financial services and to cheap capital. •• We believe that people have an inherent right to control the fruit of their legal labor. •• We believe that global, open, instant, and low-cost movement of money will create immense economic opportunity and more commerce across the world. •• We believe that people will increasingly trust decentralized forms of governance. •• We believe that a global currency and financial infrastructure should be designed and governed as a public good. •• We believe that we all have a responsibility to help advance financial inclusion, support ethical actors, and continuously uphold the integrity of the ecosystem.  2 / 12  Introducing Libra The world truly needs a reliable digital currency and infrastructure that together can deliver on the promise of “the internet of money.” Securing your financial assets on your mobile device should be simple and intuitive. Moving money around globally should be as easy and cost-effective as — and even more safe and secure than — sending a text message or sharing a photo, no matter where you live, what you do, or how much you earn. New product innovation and additional entrants to the ecosystem will enable the lowering of barriers to access and cost of capital for everyone and facilitate frictionless payments for more people. Now is the time to create a new kind of digital currency built on the foundation of blockchain technology. The mission for Libra is a simple global currency and financial infrastructure that empowers billions of people. Libra is made up of three parts that will work together to create a more inclusive financial system: 1. It is built on a secure, scalable, and reliable blockchain; 2. It is backed by a reserve of assets designed to give it intrinsic value; 3. It is governed by the independent Libra Association tasked with evolving the ecosystem. The Libra currency is built on the “Libra Blockchain.” Because it is intended to address a global audience, the software that implements the Libra Blockchain is open source — designed so that anyone can build on it, and billions of people can depend on it for their financial needs. Imagine an open, interoperable ecosystem of financial services that developers and organizations will build to help people and businesses hold and transfer Libra for everyday use. With the proliferation of smartphones and wireless data, increasingly more people will be online and able to access Libra through these new services. To enable the Libra ecosystem to achieve this vision over time, the blockchain has been built from the ground up to prioritize scalability, security, efficiency in storage and throughput, and future adaptability. Keep reading for an overview of the Libra Blockchain, or read the technical paper. The unit of currency is called “Libra.” Libra will need to be accepted in many places and easy to access for those who want to use it. In other words, people need to have confidence that they can use Libra and that its value will remain relatively stable over time. Unlike the majority of cryptocurrencies, Libra is fully backed by a reserve of real assets. A basket of bank deposits and short-term government securities will be held in the Libra Reserve for every Libra that is created, building trust in its intrinsic value. The Libra Reserve will be administered with the objective of preserving the value of Libra over time. Keep reading for an overview of Libra and the reserve, or read more here. The Libra Association is an independent, not-for-profit membership organization headquartered in Geneva, Switzerland. The association’s purpose is to coordinate and provide a framework for governance for the  3 / 12  network and reserve and lead social impact grant-making in support of financial inclusion. This white paper is a reflection of its mission, vision, and purview. The association’s membership is formed from the network of validator nodes that operate the Libra Blockchain. Members of the Libra Association will consist of geographically distributed and diverse businesses, nonprofit and multilateral organizations, and academic institutions. The initial group of organizations that will work together on finalizing the association’s charter and become “Founding Members” upon its completion are, by industry:  •• Payments: Mastercard, PayPal, PayU (Naspers’ fintech arm), Stripe, Visa •• Technology and marketplaces: Booking Holdings, eBay, Facebook/Calibra, Farfetch, Lyft, •• •• •• ••  Mercado Pago, Spotify AB, Uber Technologies, Inc. Telecommunications: Iliad, Vodafone Group Blockchain: Anchorage, Bison Trails, Coinbase, Inc., Xapo Holdings Limited Venture Capital: Andreessen Horowitz, Breakthrough Initiatives, Ribbit Capital, Thrive Capital, Union Square Ventures Nonprofit and multilateral organizations, and academic institutions: Creative Destruction Lab, Kiva, Mercy Corps, Women’s World Banking  We hope to have approximately 100 members of the Libra Association by the target launch in the first half of 2020. Facebook teams played a key role in the creation of the Libra Association and the Libra Blockchain, working with the other Founding Members. While final decision-making authority rests with the association, Facebook is expected to maintain a leadership role through 2019. Facebook created Calibra, a regulated subsidiary, to ensure separation between social and financial data and to build and operate services on its behalf on top of the Libra network. Once the Libra network launches, Facebook, and its affiliates, will have the same commitments, privileges, and financial obligations as any other Founding Member. As one member among many, Facebook’s role in governance of the association will be equal to that of its peers. Blockchains are described as either permissioned or permissionless in relation to the ability to participate as a validator node. In a “permissioned blockchain,” access is granted to run a validator node. In a “permissionless blockchain,” anyone who meets the technical requirements can run a validator node. In that sense, Libra will start as a permissioned blockchain. To ensure that Libra is truly open and always operates in the best interest of its users, our ambition is for the Libra network to become permissionless. The challenge is that as of today we do not believe that there is a proven solution that can deliver the scale, stability, and security needed to support billions of people and transactions across the globe through a permissionless network. One of the association’s directives will be to work with the community to research and implement this transition, which will begin within five years of the public launch of the Libra Blockchain and ecosystem. Essential to the spirit of Libra, in both its permissioned and permissionless state, the Libra Blockchain will be open to everyone: any consumer, developer, or business can use the Libra network, build products on top of it, and add value through their services. Open access ensures low barriers to entry and innovation and encourages healthy competition that benefits consumers. This is foundational to the goal of building more inclusive financial options for the world.  4 / 12  The Libra Blockchain The goal of the Libra Blockchain is to serve as a solid foundation for financial services, including a new global currency, which could meet the daily financial needs of billions of people. Through the process of evaluating existing options, we decided to build a new blockchain based on these three requirements:  •• Able to scale to billions of accounts, which requires high transaction throughput, low latency, and an efficient, high-capacity storage system. •• Highly secure, to ensure safety of funds and financial data. •• Flexible, so it can power the Libra ecosystem’s governance as well as future innovation in financial services. The Libra Blockchain is designed from the ground up to holistically address these requirements and build on the learnings from existing projects and research — a combination of innovative approaches and wellunderstood techniques. This next section will highlight three decisions regarding the Libra Blockchain: 1. Designing and using the Move programming language. 2. Using a Byzantine Fault Tolerant (BFT) consensus approach. 3. Adopting and iterating on widely adopted blockchain data structures. “Move” is a new programming language for implementing custom transaction logic and “smart contracts” on the Libra Blockchain. Because of Libra’s goal to one day serve billions of people, Move is designed with safety and security as the highest priorities. Move takes insights from security incidents that have happened with smart contracts to date and creates a language that makes it inherently easier to write code that fulfills the author’s intent, thereby lessening the risk of unintended bugs or security incidents. Specifically, Move is designed to prevent assets from being cloned. It enables “resource types” that constrain digital assets to the same properties as physical assets: a resource has a single owner, it can only be spent once, and the creation of new resources is restricted. The Move language also facilitates automatic proofs that transactions satisfy certain properties, such as payment transactions only changing the account balances of the payer and receiver. By prioritizing these features, Move will help keep the Libra Blockchain secure. By making the development of critical transaction code easier, Move enables the secure implementation of the Libra ecosystem’s governance policies, such as the management of the Libra currency and the network of validator nodes. Move will accelerate the evolution of the Libra Blockchain protocol and any financial innovations built on top of it. We anticipate that the ability for developers to create contracts will be opened up over time in order to support the evolution and validation of Move. To facilitate agreement among all validator nodes on the transactions to be executed and the order in which they are executed, the Libra Blockchain adopted the BFT approach by using the LibraBFT consensus protocol. This approach builds trust in the network because BFT consensus protocols are designed to function correctly even if some validator nodes — up to one-third of the network — are compromised or fail. This class  5 / 12  of consensus protocols also enables high transaction throughput, low latency, and a more energy-efficient approach to consensus than “proof of work” used in some other blockchains. In order to securely store transactions, data on the Libra Blockchain is protected by Merkle trees, a data structure used by other blockchains that enables the detection of any changes to existing data. Unlike previous blockchains, which view the blockchain as a collection of blocks of transactions, the Libra Blockchain is a single data structure that records the history of transactions and states over time. This implementation simplifies the work of applications accessing the blockchain, allowing them to read any data from any point in time and verify the integrity of that data using a unified framework. The Libra Blockchain is pseudonymous and allows users to hold one or more addresses that are not linked to their real-world identity. This approach is familiar to many users, developers, and regulators. The Libra Association will oversee the evolution of the Libra Blockchain protocol and network, and it will continue to evaluate new techniques that enhance privacy in the blockchain while considering concerns of practicality, scalability, and regulatory impact. For more details, read the technical paper on the Libra Blockchain. Detailed information is also available on the Move programming language and the LibraBFT consensus protocol. We’ve open sourced an early preview of the Libra testnet, with accompanying documentation. The testnet is still under development, and APIs are subject to change. Our commitment is to work in the open with the community and hope you will read, build, and provide feedback.  6 / 12  The Libra Currency and Reserve We believe that the world needs a global, digitally native currency that brings together the attributes of the world’s best currencies: stability, low inflation, wide global acceptance, and fungibility. The Libra currency is designed to help with these global needs, aiming to expand how money works for more people around the world. Libra is designed to be a stable digital cryptocurrency that will be fully backed by a reserve of real assets — the Libra Reserve — and supported by a competitive network of exchanges buying and selling Libra. That means anyone with Libra has a high degree of assurance they can convert their digital currency into local fiat currency based on an exchange rate, just like exchanging one currency for another when traveling. This approach is similar to how other currencies were introduced in the past: to help instill trust in a new currency and gain widespread adoption during its infancy, it was guaranteed that a country’s notes could be traded in for real assets, such as gold. Instead of backing Libra with gold, though, it will be backed by a collection of low-volatility assets, such as bank deposits and short-term government securities in currencies from stable and reputable central banks. It is important to highlight that this means one Libra will not always be able to convert into the same amount of a given local currency (i.e., Libra is not a “peg” to a single currency). Rather, as the value of the underlying assets moves, the value of one Libra in any local currency may fluctuate. However, the reserve assets are being chosen to minimize volatility, so holders of Libra can trust the currency’s ability to preserve value over time. The assets in the Libra Reserve will be held by a geographically distributed network of custodians with investment-grade credit rating to provide both security and decentralization of the assets. The assets behind Libra are the major difference between it and many existing cryptocurrencies that lack such intrinsic value and hence have prices that fluctuate significantly based on expectations. Libra is indeed a cryptocurrency, though, and by virtue of that, it inherits several attractive properties of these new digital currencies: the ability to send money quickly, the security of cryptography, and the freedom to easily transmit funds across borders. Just as people can use their phones to message friends anywhere in the world today, with Libra, the same can be done with money — instantly, securely, and at low cost. Interest on the reserve assets will be used to cover the costs of the system, ensure low transaction fees, pay dividends to investors who provided capital to jumpstart the ecosystem (read “The Libra Association” here), and support further growth and adoption. The rules for allocating interest on the reserve will be set in advance and will be overseen by the Libra Association. Users of Libra do not receive a return from the reserve. For more on the reserve policy and the details of the Libra currency, please read here.  7 / 12  The Libra Association To make the mission of Libra a reality — a simple global currency and financial infrastructure that empowers billions of people — the Libra Blockchain and Libra Reserve need a governing entity that is comprised of diverse and independent members. This governing entity is the Libra Association, an independent, not-for-profit membership organization, headquartered in Geneva, Switzerland. Switzerland has a history of global neutrality and openness to blockchain technology, and the association strives to be a neutral, international institution, hence the choice to be registered there. The association is designed to facilitate the operation of the Libra Blockchain; to coordinate the agreement among its stakeholders — the network’s validator nodes — in their pursuit to promote, develop, and expand the network, and to manage the reserve. The association is governed by the Libra Association Council, which is comprised of one representative per validator node. Together, they make decisions on the governance of the network and reserve. Initially, this group consists of the Founding Members: businesses, nonprofit and multilateral organizations, and academic institutions from around the world. All decisions are brought to the council, and major policy or technical decisions require the consent of two-thirds of the votes, the same supermajority of the network required in the BFT consensus protocol. Through the association, the validator nodes align on the network’s technical roadmap and development goals. In that sense, it is similar to other not-for-profit entities, often in the form of foundations, which govern open-source projects. As Libra relies on a growing distributed community of open-source contributors to further itself, the association is a necessary vehicle to establish guidance as to which protocols or specifications to develop and to adopt. The Libra Association also serves as the entity through which the Libra Reserve is managed, and hence the stability and growth of the Libra economy are achieved. The association is the only party able to create (mint) and destroy (burn) Libra. Coins are only minted when authorized resellers have purchased those coins from the association with fiat assets to fully back the new coins. Coins are only burned when the authorized resellers sell Libra coin to the association in exchange for the underlying assets. Since authorized resellers will always be able to sell Libra coins to the reserve at a price equal to the value of the basket, the Libra Reserve acts as a “buyer of last resort.” These activities of the association are governed and constrained by a Reserve Management Policy that can only be changed by a supermajority of the association members. In these early years of the network, there are additional roles that need to be performed on behalf of the association: the recruitment of Founding Members to serve as validator nodes; the fundraising to jumpstart the ecosystem; the design and implementation of incentive programs to propel the adoption of Libra, including the distribution of such incentives to Founding Members; and the establishment of the association’s social impact grant-making program.  8 / 12  An additional goal of the association is to develop and promote an open identity standard. We believe that decentralized and portable digital identity is a prerequisite to financial inclusion and competition. An important objective of the Libra Association is to move toward increasing decentralization over time. This decentralization ensures that there are low barriers to entry for both building on and using the network and improves the Libra ecosystem’s resilience over the long term. As discussed above, the association will develop a path toward permissionless governance and consensus on the Libra network. The association’s objective will be to start this transition within five years, and in so doing will gradually reduce the reliance on the Founding Members. In the same spirit, the association aspires to minimize the reliance on itself as the administrator of the Libra Reserve. For more on the Libra Association, please read here.  9 / 12  What’s Next for Libra? Today we are publishing this document outlining our goals for Libra and launching libra.org as a home for the association and all things Libra. It will continue to be updated over the coming months. We are also opensourcing the code for the Libra Blockchain and launching Libra’s initial testnet for developers to experiment with and build upon. There is much left to do before the target launch in the first half of 2020.  •• The Libra Blockchain: Over the coming months, the association will work with the community to gather feedback on the Libra Blockchain prototype and bring it to a production-ready state. In particular, this work will focus on ensuring the security, performance, and scalability of the protocol and implementation.  ◦◦ The Libra Association will construct well-documented APIs and libraries to enable users to interact ◦◦ ◦◦ ◦◦  with the Libra Blockchain. The Libra Association will create a framework for the collaborative development of the technology behind the Libra Blockchain using the open-source methodology. Procedures will be created for discussing and reviewing changes to the protocol and software that support the blockchain. The association will perform extensive testing of the blockchain, which range from tests of the protocol to constructing a full-scale test of the network in collaboration with entities such as wallet services and exchanges to ensure the system is working before launch. The association will work to foster the development of the Move language and determine a path for third parties to create smart contracts once language development has stabilized — after the launch of the Libra ecosystem.  Together with the community, the association will research the technological challenges on the path to a permissionless ecosystem so that we can meet the objective to begin the transition within five years of the launch. •• The Reserve: The association will work to establish a geographically distributed and regulated group of global institutional custodians for the reserve. The association will establish operational procedures for the reserve to interact with authorized resellers and ensure high-transparency and auditability. The association will establish policies and procedures that establish how the association can change the composition of the reserve basket.  ◦◦ ◦◦ ◦◦  10 / 12  •• The Libra Association: We will work to grow the Libra Association Council to around 100 geographically distributed and diverse members, all serving as the initial validator nodes of the Libra Blockchain. The association will develop and adopt a comprehensive charter and set of bylaws for the association on the basis of the currently proposed governance structure. We will recruit a Managing Director for the association and work with her/him to continue hiring for the association’s executive team. We will identify social impact partners aligned with our joint mission and will work with them to establish a Social Impact Advisory Board and a social impact program.  ◦◦ ◦◦ ◦◦ ◦◦  How to Get Involved The association envisions a vibrant ecosystem of developers building apps and services to spur the global use of Libra. The association defines success as enabling any person or business globally to have fair, affordable, and instant access to their money. For example, success will mean that a person working abroad has a fast and simple way to send money to family back home, and a college student can pay their rent as easily as they can buy a coffee. Our journey is just beginning, and we are asking the community to help. If you believe in what Libra could do for billions of people around the world, share your perspective and join in. Your feedback is needed to make financial inclusion a reality for people everywhere.  If you are a researcher or protocol developer, an early preview of the Libra testnet is available under the Apache 2.0 Open Source License, with accompanying documentation. This is just the start of the process, and the testnet is still an early prototype under development, but you can read, build, and provide feedback right away. Since the current focus is on stabilizing the prototype, the project may initially be slower to take community contributions. However, we are committed to building a community-oriented development process and opening the platform to developers — starting with pull requests — as soon as possible.  If you want to learn about the Libra Association, read more here.  If your organization is interested in becoming a Founding Member or applying for social impact grants from the Libra Association, read more here. The association will work with the global community in the coming months and continue to partner with policymakers worldwide to further the mission.  11 / 12  Conclusion This is the goal for Libra: A stable currency built on a secure and stable open-source blockchain, backed by a reserve of real assets, and governed by an independent association. Our hope is to create more access to better, cheaper, and open financial services — no matter who you are, where you live, what you do, or how much you have. We recognize that the road to delivering this will be long, arduous, and won’t be achieved in isolation — it will take coming together and forming a real movement around this pursuit. We hope you’ll join us and help turn this dream into a reality for billions of people around the world.  1 Best Buy. “AT&T prepaid Alcatel CAMEOX device purchase.” Bestbuy.com. Available: https://www.bestbuy.com/site/at-t-prepaid-alcatel-cameox-4g-lte-with16gb-memory-cell-phone-arctic-white/6008102.p?skuId=6008102 (Accessed: May 15, 2019). 2 A. Demirgüç-Kunt, L. Klapper, D. Singer, S. Ansar, and J. Hess. The Global Findex database 2017: Measuring financial inclusion and the fintech revolution. World Bank Group, 2018. Accessed: May 15 2019. Globalfindex.worldbank.org. [Online]. Available: https://globalfindex.worldbank.org/sites/globalfindex/ files/2018-04/2017%20Findex%20full%20report_0.pdf 3 OECD. Mobile phones: Pricing structures and trends. Paris, France: OECD Publishing, 2000, p. 67. [Online]. Available: https://books.google.com/books?id=pcP84M_GBeoC&pg=PA6&lpg=PA6&dq=1999+price+SMS+europe&source=bl&ots=TIbwgZWCmj&sig=ACfU3U2Z_yRawxW78qVSVO_wHCtRupoqoA&hl=en&sa=X&ved=2ahUKEwjOmeG9tMHiAhVVFzQIHU8eBEMQ6AEwD3oECAkQAQ#v=onepage&q=SMS&f=false 4 Consumer Federation of America. “How payday loans work.” Payday Loan Consumer Information. Available: https://paydayloaninfo.org/facts (Accessed: May 19, 2019). 5 A. Demirgüç-Kunt, L. Klapper, D. Singer, S. Ansar, and J. Hess. The Global Findex database 2017: Measuring financial inclusion and the fintech revolution. World Bank Group, 2018. Accessed: May 15 2019. Globalfindex.worldbank.org. [Online]. Available: https://globalfindex.worldbank.org/sites/globalfindex/ files/2018-04/2017%20Findex%20full%20report_0.pdf  12 / 12  Partial list of Risks Thanks for your interest in MasterCoin! You should be aware that investing in currency experiments is really, absurdly risky. There are a huge number of things that could go wrong. I'll list a few of them here, but this is by no means all of them:          I could lose the bitcoins and thus not be able to continue funding this project. While I'm taking every precaution I can think of to make sure funds sent to the Exodus Address are not lost or stolen (the wallet is backed up and only accessed from an offline-only computer), history shows that people have a hard time keeping their bitcoins safe. Your own bitcoin wallet could be lost or stolen, taking your MasterCoins with it. I could be slow. I don't plan on quitting my job (unless millions of dollars are raised), and regardless of how much money is raised, I have three small children and my lovely wife who take priority over anything else. Someone else could do it better. I may have first-mover advantage, but first-mover is rarely the long-term winner. Especially if I am slow, there is a big risk someone else will take the lead. My protocol ideas might not work. I think they will, but that remains to be proven. There could be legal trouble. If the government tells me to quit working on this, I probably will. I'd gladly hand over the project to someone else, but there's no guarantee that someone will step up. Future bitcoin changes might break MasterCoin. I've been careful to only use the most basic bitcoin features, but MasterCoin could be negatively impacted if bitcoin starts doing things in a radically different way.  Please also be aware that MasterCoins are not in any way equity. They don't give you ownership of any company I might start, and they certainly don't give you voting or control over this project. I plan to be the biggest MasterCoin purchaser, but even if someone else buys more than I do, that doesn't give them majority ownership, or any other rights or privileges. MasterCoins are an experimental currency built on top of another experimental currency (bitcoin). The risks (and potential rewards) are extreme. Finally, anything I say here or anywhere else should not be considered investment advice. Please consult a financial advisor before investing in MasterCoin (they will probably have a heart attack at the thought).  The Second Bitcoin Whitepaper vs. 0.5 (Draft for Public Comment)  Introduction Summary We claim that the existing bitcoin network can be used as a protocol layer, on top of which new currency layers with new rules can be built without changing the foundation. We further claim that the new protocol layers described in this document: •  Will fix the two biggest barriers to widespread bitcoin adoption: instability and insecurity.  •  Will financially benefit the entire bitcoin user community, including those who don’t use the new protocol layers.  •  Will provide initial funds to hire developers to build software which implements the new protocol layers, and ongoing funds to pay for maintenance of this software.  •  Will richly reward early adopters of the new protocol, in proportion to how successful it is.  Assumptions Our claims are built on the following assumptions: •  Alternate block chains compete with bitcoins financially, confuse our message to the world, and dilute our efforts. These barriers interfere with the adoption momentum of bitcoin and the adoption momentum of alternate currencies as well, regardless of how well-conceived their rules may be.  •  New protocol layers on top of the bitcoin protocol will increase bitcoin values, consolidate our message to the world, and concentrate our efforts, while still allowing individuals and groups to issue new currencies with experimental new rules. The success of any experimental currency protocol layer will enhance the value and success of the foundational bitcoin protocol.  •  Getting consensus and widespread adoption from the bitcoin community is not needed to add protocol layers, since no changes to the foundational bitcoin protocol are required.  •  Tiny bitcoin transactions can be encoded into the block chain to support and represent transactions in higher protocol layers. Any protocol transaction described in this document can be assumed to be represented in the block chain by tiny bitcoin transactions.  •  A protocol can pay for its own software development, “bootstrapping” itself into existence, utilizing a trusted entity to hold funds and hire developers.  •  It is possible to create tools to allow end users to create currency protocol layers which have a stable value, pegged to an external currency or commodity. In this way, users of these currencies can own stabilized virtual currency tied to U.S. Dollars, Euros, ounces of gold, barrels of oil, etc.  •  It is possible for users of these new currencies to exchange between currencies with each other using simple rules and no central exchange.  Visualization The proposed protocol layers can be visualized as follows, with arrows representing users exchanging between currencies:  Note that all transfers of value are still stored in the normal bitcoin block chain, but higher layers of the protocol assign additional meaning to some transactions.  MasterCoin Design The “MasterCoins” protocol layer between the existing bitcoin protocol and users’ currencies is intended to be a base upon which anyone can build their own currency. The software implementing MasterCoins will contain simple tools which will allow anyone to design and release their own currency with their own rules without doing any software development.  The “Trusted Entity” The concept of a “trusted entity” is needed for initial distribution of MasterCoins and initial protocol software development. This entity: •  Should have a publicly known identity and location.  •  Should devote the funds received to protocol software development, marketing, and business expenses. Funds could also be used for legal costs if necessary.  •  Should devote any remaining funds to other projects which benefit those who are using the new protocol layers (banking applications, merchant services, etc)  •  Should vow to return all remaining funds to their original owners if they are for some reason unable to continue working on the software.  •  Should commit to financial transparency, and should undergo regular external audits.  •  Should obey the laws of whatever country they are situated in, and should be prepared to move to a different country if necessary.  •  Will be given a large (slowly vesting) financial stake in the success of the new protocol layers so that their interests are aligned with the interests of the early adopters.  Choosing this entity is the first and most important decision to be made if the changes in this document are to be implemented. We don’t expect a shortage of volunteers to manage the money, but we are very interested in community feedback on who can be trusted to manage it.  Initial Distribution of MasterCoins Initial distribution of MasterCoins will essentially be a fundraiser for the trusted entity to provide money to pay developers to write the software which fully implements the protocol. The distribution is very simple, and will proceed as follows: 1. The trusted entity publishes the bitcoin addresses to be used for collecting bitcoins as MasterCoins are sold to the public. 2. The trusted entity publishes the final date at which MasterCoins may initially be purchased (i.e. July 31st, 2012) 3. Anyone sending bitcoins to those addresses before the specified deadline is recognized by the protocol as owning an equivalent number of MasterCoins. For instance, if I send 100 bitcoins to the address before the deadline, I own 100 MasterCoins after the deadline. This ratio will be tweaked slightly to reward earlier buyers more than later buyers.  Reward MasterCoins For every 10 MasterCoins sold, an additional “reward MasterCoin” will also be created, which the trusted entity will receive slowly over the following years. These delayed MasterCoins will ensure that the trusted entity has plenty of motivation to increase the value of MasterCoins by completing the features desired by users. The reward will be structured so that they have received 50% of the reward by one year after the initial sale, 75% by a year later, 87.5% by a year later, and so on:  The 10:1 ratio and 50%/year vesting rate are only suggestions at this point. More or less reward with a faster or slower vesting rate may be appropriate, depending largely on how widely this document is accepted, how trusted the entity is, and the likelihood that a different entity will try to supplant them with a similar enterprise. These factors all impact the likelihood of success, which in turn impacts how much reward incentive the trusted entity will need. It is our hope that we (the author’s royal/academic we) can endorse an entity who represents the most trusted choice of the majority of bitcoin users, that the author can work with that entity to make these ideas into reality, and that the community will be largely behind us, lowering the odds that some other entity will try to do something similar.  Ongoing Balance of MasterCoin and Bitcoin Values If the user currencies based on MasterCoins are even modestly successful, we would expect exponential growth in MasterCoin values. What then happens to the bitcoins supporting all this? The value of bitcoins and MasterCoins must be manipulated to support two goals: 1. In order to ensure sufficient hashing power, Bitcoins must gain value at the same time that MasterCoins gain value. 2. In order to reward early MasterCoin adopters, Bitcoin values must not be forced up as quickly as MasterCoin values are rising. To accomplish these two goals, a minimum bitcoin value and maximum MasterCoin value will be set as follows:  This limit will be enforced by the protocol creating new MasterCoins for sale in exchange for destroying bitcoins (by sending them to a fake address) whenever the limit is exceeded. When these new MasterCoins are created, the trusted entity would not receive new bitcoins to spend, but they would still receive the corresponding reward MasterCoins over time, as described above. The numbers 0.2 and 0.8 ultimately represent that bitcoin values can never fall under 20% of MasterCoin values. That is, 5 bitcoins will always be worth at least one MasterCoin. In the event that the MasterCoin community feels they have too much or too little hashing power, they could change this minimum bitcoin value by voting on what they believe the minimum bitcoin value should be. Voting is further described later in this document.  Using MasterCoins All transactions and operations involving MasterCoins are encoded into the block chain using tiny bitcoin transactions. MasterCoins (and user currencies derived from them) will allow a much wider diversity of transactions, including at least the following: •  I hereby transfer these coins to <address>  •  I hereby mark these coins (in my savings wallet) as having a 2 week waiting period to complete any transfer.  •  I hereby offer for sale these 10 coins at a price of 5 bitcoins each.  •  I hereby accept your offer – I would like to buy 2 of your coins at that price.  •  I hereby offer a bet B1 that address X will not send a value greater than Y before date Z. (Y could represent a sports score, a stock price, or any other data that people might want to bet on.)  •  I hereby take your bet B2.  •  I hereby create a user currency with the following properties . . .  Some of these transactions are elaborated below.  Distributed Exchange •  Someone who is holding MasterCoins (or a currency derived from them) can simply broadcast a message that they are for sale, and a price in another currency (such as bitcoins).  •  Someone wishing to buy some coins which are for sale can broadcast their intention to buy some or all of them.  •  When a buyer is confirmed in the block chain to be the first to broadcast this intention, they may then transfer the offered price to the seller, and they are then recognized as the new owner of the MasterCoins.  •  If an offer has not yet been accepted, the seller may broadcast that the coins are no longer for sale.  Special Security Features MasterCoins, and any user currencies based on them, will have additional security features built-in to increase the confidence people have in holding these currencies. Any holder of such currencies will be able to place restrictions on their “savings account” such as: •  Temporary transaction reversibility – for instance, a user could mark their savings account as having a 30-day window in which they can reverse any transactions exceeding 10 MasterCoins/month. This reversal would be accomplished by sending a coded transaction from a secondary wallet file they control. Any reversible transfer would not be considered complete until the window was past.  •  Multiple Signers – a user could require large transactions from their online wallet to also be approved by a confirming transaction from the wallet on their home PC, or vice-versa. Similarly, a virtual company could require that a majority of stakeholders sign-off on large expenditures.  •  Duress Protection – To discourage kidnapping and other forms of payment under duress, a user can appoint a trusted entity with the power to prevent or slow large transactions which they believe to be made under duress. Kidnappers and corrupt governments would know that no matter how much the currency owner is motivated to pay them off, they simply cannot until they can prove to this arbitrator that they and their loved ones are not being threatened.  •  Arbitration – A user will also be able to designate other trusted entities to arbitrate In the event that a hacker is attempting to gain control of their wallets, ensuring that the funds return to the proper owner.  Conditional Transfers MasterCoins, and any user currencies based on them, will support transfers which are conditional, that is, dependent on another transaction. In this way, users can bet on any data which is published in the block chain. For instance: 1. User A starts publishing a stock price from his bitcoin address 2. User B publishes a bet that the stock price will fall below $2 in the next 7 days. User B’s money is now locked until the bet is over or he withdraws the bet offer 3. User C accepts the bet. Money for User B and User C is locked until the bet is over. 4. If user A publishes a value below $2 or 7 days passes, the protocol recognizes the winner as the new owner of the funds that were bet. No middleman website is needed. Betters only need to trust the data source and the shared protocol. We expect that betting on financial instruments, sports, and world events using user-created currencies will become very widespread. MasterCoins will probably be too volatile to be used in betting.  User Currencies The whole point of MasterCoins is to allow users to create their own currencies out of existing MasterCoins. For the purposes of demonstrating how user currencies will work, we will use an example currency called “GoldCoins”, which are intended to track the value of one ounce of gold, and which may be stored, transferred, bought, and sold similarly to MasterCoins.  Stability Concept The first problem is how to drive the value of these GoldCoins to their target value, when demand for them may surge and decline. The price of GoldCoins is decided by the balance of supply and demand. Since we can’t control the demand for GoldCoins, we must control the supply. The key to accomplishing this is a transfer of volatility from holders of the stabilized currency (who want stability) to speculators (who want risk and reward). In order to transfer volatility, we need to split the currency into two parts, GoldCoins and GoldShares:  Publishing the User Currency Value How does everyone agree on what the value of one ounce of gold is? The creator of the GoldCoins/GoldShares currency is responsible to designate a “ticker” bitcoin address which periodically publishes the value of an ounce of gold (denominated in bitcoins, MasterCoins, or another user currency such as USDCoins) with a tiny bitcoin transaction. If for some reason that address is compromised or starts publishing invalid data, currency stakeholders (GoldCoin and GoldShare holders) can vote to use a different ticker.  Stability Mechanism The volatility transfer is accomplished by destroying GoldShares (and issuing new GoldCoins) when GoldCoin values are too high, and by destroying GoldCoins (and issuing new GoldShares) when GoldCoin values are too low:  Owning GoldShares is equivalent to betting that demand for GoldCoins will rise. Owning GoldCoins is equivalent to owning ounces of gold (betting that the value of gold will rise).  Escrow Fund (Insurance Against Panic) As with everything else in the world, both GoldCoins and GoldShares derive their value from the confidence people have in them to be worth something in the future. What if something better comes along, and nobody wants GoldCoins OR GoldShares anymore? What if people panic and everybody tries to sell their GoldCoins and GoldShares at once? To handle the eventual demise of GoldCoins (tomorrow or in a thousand years) we propose an escrow fund of MasterCoins to be held as insurance for each user-created currency. For example, when the GoldCoins currency is first created, MasterCoins would be stored in escrow in exchange for the first GoldShares purchased by speculators. As long as the GoldShares have some value above zero, GoldCoin stability can be maintained. If GoldShare values fall to zero, the protocol will begin buying GoldCoins in exchange for the stored MasterCoins worth some amount probably less than the face value of the GoldCoins, depending on the size of the escrow fund:  Thus, as long as MasterCoins have value, GoldCoins are worth something because of the MasterCoins held in escrow. Panic selling can be further discouraged by liquidating the escrow fund in a way that rewards those who wait longer to cash in their GoldCoins. For instance, if 10% of the escrow fund is used to purchase 20% of GoldCoins outstanding, there are more MasterCoins in the escrow fund per outstanding GoldCoin that users held onto. This culling process might even jump-start the currency to life again as GoldCoins approach their target value and GoldShare speculators wonder if demand for GoldCoins might again exceed supply. As GoldCoin values rose, the old worthless GoldShares would undergo a reverse split (i.e. 1 Million GoldShares might eventually become 1 GoldShare, since the market got flooded with GoldShares as the initial crash happened). Once GoldCoins regain their target value, there are only a small number of GoldShares left from the original batch (the exact number left would be determined by the currency creator when they launched GoldCoins). At this point, new GoldShares are sold, the escrow fund is replenished, and life goes on. One interesting side effect of the potential exponential rise in MasterCoin values is that early user currencies may find themselves with a large surplus in escrow. The creator of GoldCoins would decide at the beginning what to do with these funds – to what extent they should be distributed to GoldShare holders, or held in escrow to increase the attractiveness of GoldCoins to users.  New Currency Creation At the time GoldCoins are created, the user must decide the following: •  Ticker symbol (“GLD”)  •  Short name for what is being tracked (“Gold”, which will determine “GoldCoins” and “GoldShares”)  •  Long name for what is being tracked (“US dollar value of one ounce of gold”)  •  Bitcoin address which will publish the ticker  •  Compact URL pointing to a webpage with more information about the currency (i.e. “tinyurl.com/aboutgoldcoins/”)  •  Various parameters governing the currency’s behavior, such as: o  The minimum allowed value of funds to be stored in escrow before correctional action is taken (i.e. “When the escrow fund falls below 50% of the face value of all outstanding GoldCoins, the protocol shall sell new GoldShares on the open market until the escrow fund is restored to 50% or GoldShare values fall to zero”)  o  The maximum allowed value of funds to be stored in escrow (i.e. Escrow funds exceeding 150% of the face value of all outstanding GoldCoins shall be used by the protocol to buy back GoldShares on the open market.)  o  “Proportional” gain of the PID loop governing value corrections (i.e. “Increase GoldCoin supply by 1% per year for every 2% GoldCoins are below target. Decrease GoldCoin supply by 1% per year for every 3% GoldCoins are above target.”)  o  “Integral” gain of the PID loop governing value corrections (i.e. “Increase GoldCoin supply by 1% per year for every 1% GoldCoins stay below their target value multiplied by the number of years they have been below target. Decrease GoldCoin supply by 1% per year for every 0.5% GoldCoins stay above their target value multiplied by the number of years they have been above target.  o  “Differential” gain of the PID loop governing value corrections (i.e. “Increase GoldCoin supply by 1% per year for every 5% per year GoldCoin values drop against their target. Decrease GoldCoin supply by 1% per year for every 7% per year GoldCoin values rise against their target.”  •  •  •  Parameters governing crashes (GoldShare values falling to zero) and recoveries (GoldCoin values regaining their target), such as: o  What constitutes a crash (i.e. “If GoldShare values fall to zero and then in the next 48 hours do not achieve a price above zero for more than 8 consecutive hours, the currency is considered to be crashed and escrow fund liquidation will begin.”)  o  Aggressiveness of increasing GoldCoin values when liquidating the escrow fund (i.e. “When liquidating the escrow fund, 5% of the fund should be liquidated each time 10% of GoldCoins outstanding are purchased, as GoldCoin holders choose to sell at that price. That 5% figure shall rise at the rate of 2% per year until the escrow fund is completely liquidated or GoldCoins regain their target value.”)  o  Ratio of old GoldShares to new GoldShares after a crash recovery (i.e. “If GoldShare prices should crash to zero, GoldShares will be reverse split such that 10000 GoldShares remain outstanding. If GoldCoins later regain their target value, increase GoldShares by 10% per day until the escrow fund regains its minimum value or GoldShare values reach zero again.”)  Parameters governing voting, such as: o  Number of shares/coins required to propose a change (i.e. “Individuals proposing a change must control 0.5% of GoldShares or 0.5% of GoldCoins or an equivalent combination of the two such as 0.4% of GoldShares and 0.1% of GoldCoins)  o  Number of shares/coins required to endorse a proposed change before it is put to a vote (i.e. “Proposed changes must be endorsed by 10% or GoldShares or 10% of GoldCoins or an equivalent combination of the two such as 2% of GoldShares and 8% of GoldCoins”)  o  Number of days for which votes are collected from GoldShare and GoldCoin holders (i.e. 30 days)  o  Percentage of votes in favor of a change required to make the change official. (i.e. For a proposal to be adopted, 50% of GoldShare voters AND 50% of GoldCoin voters must vote for it, or an equivalent combination of the two, such as 80% of GoldShare holders and 20% of GoldCoin holders)  Amount of MasterCoins to pay to the ticker registry address. This determines the placement of the ticker in the list that users see, represented by transactions sent to a bitcoin address owned by the trusted entity. Paying more than a trivial amount is optional, and merely demonstrates that the currency creator is serious in their commitment to making the currency a success.  This information and any other similar information is published into the block chain, and the currency creator (and anyone else) can buy some GoldShares to kick things off. Once the values above are set, they can only be changed by a vote of the currency stakeholders (GoldCoin and GoldShare holders), as implied by the rules above. Note that changes to the MasterCoin protocol would require ratification by a similar voting system for holders of MasterCoins and the currencies which hold MasterCoins in escrow.  Appendix A – Legal Issues We believe that the trusted entity can avoid liability for any laws broken by users of new protocol layers. For instance, if a government-issued currency loses value due to the rise of distributed currencies, that government might wish to mount a legal attack against the trusted entity for engaging in “economic terrorism”. The counter-argument would be that the protocol and software merely provide a set of tools which can be used by anyone to create and use new currencies pegged to external values of the users’ choosing. As with file-sharing, there are many uses for such a protocol, some legal, some not. Any software tools released for the creation and use of user-generated currencies should explicitly warn users to determine for themselves the laws pertaining to the legality, regulation, and taxation of any new currency protocols they create or use. In the event that laws are passed that specifically target distributed currencies, the trusted entity must be prepared to move to a country with a more favorable legal climate.  Appendix B – Moral Issues While moral issues are outside the traditional scope of a whitepaper such as this, we feel compelled to raise them. Modest success of this protocol could make early investors (and even those who simply hold bitcoins) very rich, hopefully without much disruption to the rest of the world. However, should this protocol and its children (or perhaps some other similar protocol) become more than modestly successful and start to replace traditional currencies, the consequences could be huge, possibly even apocalyptic. Early investors could be the richest people in the world, maybe even trillionaires by today’s accounting. The social and economic upheaval could be cataclysmic, and many people could suffer greatly if the transition is not handled well by governments and the new rich. We urge our readers to examine their hearts, and to engage in more constructive fantasies than buying fast cars and private islands. Are you willing to devote the vast majority of your wealth to helping people in need, especially those who suffer because of the forces that made you rich? Will you pursue new entrepreneurial projects which create jobs for these people? Will you help your government adapt to the new paradigm? Will the world be a better place because of your good fortune? The suffering of these people will be on all of our heads, and they (and a higher power) will hold us accountable if we behave selfishly. In such a world, we must all encourage each other to do the right thing, by words and by example.  Appendix C – Disclaimer Investing in experimental currencies is really, absurdly risky. This paper is not investment advice, and anyone predicting what will happen with experimental currencies such as those described here is indulging in the wildest sort of speculation, and that includes the speculations in the previous appendix. Please consult your financial adviser before investing in ANY wild scheme such as this (hint: they will probably tell you to RUN and not look back unless you assure them that it is money you are totally prepared to lose). Anyone who puts their rent money or life savings into an experiment of this type is a fool, and deserves the financial ruin they will inevitably reap from this or some other risky enterprise.  MasterCoin Complete Specification vs. 1.1 (Smart Property Edition)  Summary We claim that the existing bitcoin network can be used as a protocol layer, on top of which new currency layers with new rules can be built without changing the foundation. We further claim that the new protocol layers described in this document:   Will fix the two biggest barriers to widespread bitcoin adoption: instability and insecurity.    Will financially benefit the entire bitcoin user community, including those who don’t use the new protocol layers.    Will provide initial funds to hire developers to build software which implements the new protocol layers, and ongoing funds to pay for maintenance of this software.    Will richly reward early adopters of the new protocol, in proportion to how successful it is.  Assumptions Our claims are built on the following assumptions:   Alternate block chains compete with bitcoins financially, confuse our message to the world, and dilute our efforts. These barriers interfere with the adoption momentum of bitcoin and the adoption momentum of alternate currencies as well, regardless of how well-conceived their rules may be.    New protocol layers on top of the bitcoin protocol will increase bitcoin values, consolidate our message to the world, and concentrate our efforts, while still allowing individuals and groups to issue new currencies with experimental new rules. The success of any experimental currency protocol layer will enhance the value and success of the foundational bitcoin protocol.    Getting consensus and widespread adoption from the bitcoin community is not needed to add protocol layers, since no changes to the foundational bitcoin protocol are required.    Tiny bitcoin transactions can be encoded into the block chain to support and represent transactions in higher protocol layers.    A protocol can pay for its own software development, “bootstrapping” itself into existence, utilizing a trusted entity to hold funds and hire developers.    It is possible to create tools to allow end users to create currency protocol layers which have a stable value, pegged to an external currency or commodity. In this way, users of these currencies can own stabilized virtual currency tied to U.S. Dollars, Euros, ounces of gold, barrels of oil, etc.    It is possible for users of these new currencies to exchange between currencies with each other using simple rules and no central exchange.  Visualization The proposed protocol layers can be visualized as follows, with arrows representing users exchanging between currencies:  Note that all transfers of value are still stored in the normal bitcoin block chain, but higher layers of the protocol assign additional meaning to some transactions.  Document History   Version 0.5 released 1/6/2012 (No packet definitions, overly-complicated currency stabilization)    Version 0.7 released 7/29/2013 (Preview of 1.0, but without revealing the Exodus Address)    Version 1.0 released 7/31/2013 (Version used during the fund-raiser)    Version 1.1 (Smart Property. Also, improvements for easier parsing and better escrow fund health)  Previous versions of this document can be found at https://sites.google.com/site/2ndbtcwpaper/  MasterCoin Design The “MasterCoins” protocol layer between the existing bitcoin protocol and users’ currencies is intended to be a base upon which anyone can build their own currency. The software implementing MasterCoins will contain simple tools which will allow anyone to design and release their own currency with their own rules without doing any software development.  The “Exodus Address” Perhaps you have heard of the “Genesis Block” which launched the bitcoin protocol. The MasterCoin protocol has a similar starting point in the block chain, called the “Exodus Address” - the bitcoin address from which the first MasterCoins will be sold. The Exodus Address is: 1EXoDusjGwvnjZUyKkxZ4UHEf77z6A5S4P (please read the “Special Considerations” section below before sending bitcoins to this address!) Initial distribution of MasterCoins will essentially be a fundraiser to provide money to pay developers to write the software which fully implements the protocol. The distribution is very simple, and will proceed as follows: 1. Anyone sending bitcoins to the Exodus Address before August 31st, 2013 is recognized by the protocol as owning 100x that number of MasterCoins. For instance, if I send 100 bitcoins to the Exodus Address before August 31st, my bitcoin address owns 10,000 MasterCoins after August 31st. 2. Early buyers get additional MasterCoins. In order to encourage adoption momentum, buyers will get an additional 10% bonus MasterCoins if they make their purchase a week before the deadline, 20% extra if they purchase two weeks early, and so on, including partial weeks. Thus, if I send 100 bitcoins to the exodus address 1.5 weeks before August 31st, the protocol recognizes my bitcoin address as owning 11,500 MasterCoins (10000 + 15% bonus). 3. Attempts to send funds to the Exodus Address on or after September 1 st 2013 (as determined by bitcoin block chain records) will not be considered MasterCoin purchases. (Update 9/8/2013: Transactions sent before the deadline but not included in a block until after the deadline may be included. This is still under discussion at the time of this writing) In the event that a purchase has multiple inputs, the input address contributing the most funds is recognized as owning the MasterCoins. Note that anyone who purchases MasterCoins also receives the same number of “Test MasterCoins” which will be used for testing new features before they are available for use in the MasterCoin protocol. Initially, the only valid MasterCoin transaction will be a “simple transfer” (defined later in this document), but the additional features described in this document will also become valid in the future (at specific pre-announced block milestones) once they are fully tested.  Reward MasterCoins For every 10 MasterCoins sold, an additional “reward MasterCoin” will also be created, which will be awarded to the Exodus Address slowly over the following years. These delayed MasterCoins will ensure that we have plenty of motivation to increase the value of MasterCoins by completing the features desired by users. The reward will be structured so that we receive 50% of the reward by one year after the initial sale, 75% by a year later, 87.5% by a year later, and so on:  Hiding MasterCoin Protocol Data in the Block Chain Bitcoin has some little-known advanced features (such as scripting) which many people imagine will enable it to perform fancy new tricks someday. MasterCoin uses exactly NONE of those advanced features, because support for them is not guaranteed in the future, and MasterCoin doesn't need them anyway. MasterCoin transactions are defined as a series of bitcoin payments from a bitcoin address where the payments match this pattern:   A “reference” payment (of any amount) to another bitcoin address (called the reference address), such as the person being paid MasterCoins    One or more “data” payments (of any amount) to fake bitcoin addresses. (A fake bitcoin address can contain 20 bytes of arbitrary data, not including overhead such as the version number and check-sum of the address.) This is data used by the protocol, such as the number of MasterCoins being paid    A “marker” payment (of any amount) to the Exodus Address marking this series of payments to be interpreted as a MasterCoin transaction.  These payments may be sent one at a time or all at once (via bitcoin's “sendmany” function), and in any order (even if we sent them in a particular order, there is no way to force miners to keep them in that order). The only requirement is they cannot have long time gaps between them (not more than 6 blocks apart in the block chain), and they must be distinguishable from other MasterCoin transactions by using a 6-block time gap or the “sendmany” function. (see update below)  Update 9/8/2013: Current implementations of MasterCoin only recognize “sendmany” transactions. We may someday support transactions sent one-at-a-time, but please don’t count on it. Also, at the request of bitcoin core devs, we are working on alternate ways of storing the “data address” packets which do not create unspendable bitcoins. Gavin Andresen has assured us that there are multiple alternate ways to store the data, and once we settle on a better way, this spec will be updated, and we will use the new method for any features beyond the “simple send” transaction. “Simple Send” must remain in its current form so that people can transfer their MasterCoins out of basic unmodified bitcoin wallets.  Putting Data Packets in Order Since the packets are stored unordered, some consideration must be made to how to tell these payments apart and put data packets in their proper order. For ordering data packets, each 20-byte payload starts with a 1-byte sequence number which is incremented for each data packet. In order to distinguish data packets from the reference address, the data packet sequence numbers start at n+1 where n is the sequence number of the reference packet if it were treated as a data packet. Any additional data packets can continue to use up sequence numbers n+2, n+3, and so on until all sequence numbers are used except for n-1. As an example of how this works, let's imagine a MasterCoin transaction that has two data packets. If the reference address happens to have a sequence number of 62, then the first data packet has a sequence number of 63 and the second has a sequence number of 64. Note that sequence number 255 is followed by 0. The theoretical limit on the amount of protocol data that could be stored in one MasterCoin transaction is therefore 254 data payments * 19 bytes = 4826 bytes, but we expect that most common transactions should only need one or two data payments. All numbers are stored big-endian (most significant digits first).  Special Considerations to Avoid Invalid Transactions Not every wallet lets you choose which address bitcoins come from when you make a payment, and MasterCoin transactions must all come from the address which holds the MasterCoins. If a wallet contains bitcoins stored in multiple addresses, the wallet must first consolidate the bitcoins by sending ALL of them to the address which is going to initiate a MasterCoin transaction. Then, any MasterCoinrelated bitcoin transactions will be sent from that address. Wallets which do not allow you to consolidate to one address and send from that address (such as online web wallet providers) will not work for MasterCoin unless they are modified to do so. For this  reason, attempting to purchase MasterCoins from an online web wallet will likely result in the permanent loss of those MasterCoins. All transaction outputs should be for the same amount, which should be above the “dust” threshold, and should include an appropriate fee to ensure that miners include them in the block chain in a timely manner. For instance, the first MasterCoin transactions had three outputs of 0.00006 BTC each, with a 0.0001 BTC transaction fee. If these recommendations are not followed, the MasterCoin transaction may be malformed and therefore invalid.  Transferring MasterCoins Say you want to transfer 1 MasterCoin to another address. Only 16 bytes are needed, which fits into a single data payment. The data stored is: 1. Transaction type = 0 for simple transfer (32-bit unsigned integer, 4 bytes) 2. Currency identifier = 1 for MasterCoin (32-bit unsigned integer, 4 bytes) 3. Amount to transfer = 100,000,000 (1.00000000 MasterCoins) (64-bit unsigned integer, 8 bytes, should not exceed number owned, but if it does, assume user is transferring all of them) Note that the amount to transfer is multiplied by 100,000,000 before it is stored, which allows for MasterCoins to be sent with the same precision as bitcoins (eight decimal places). The reference payment (described earlier) determines the address receiving the MasterCoins. Note that if the transfer comes from an address which has been marked as “Savings”, there is a time window in which the transfer can be undone. Otherwise MasterCoin transactions are not reversible.  Marking an Address as “Savings” Say you want to back up your savings wallet in the cloud, but if someone manages to hack into it, you want transactions out of that wallet to be reversible for up to 30 days. Doing this takes 8 bytes, which fits into a single data payment: 1. Transaction type = 10 for marking savings (32-bit unsigned integer, 4 bytes) 2. Reversibility period = 2,592,000 seconds (30 days) (32-bit unsigned integer, 4 bytes) The maximum reversibility period is 365 days (1,892,160,000 seconds) to avoid accidents. Marking an address as savings is PERMANENT and cannot be undone. If an address is marked as savings, the reversibility rules affect not only MasterCoins, but any MasterCoin-derived child currency stored at that address. When marking an address as savings, the reference payment should point to a “guardian” address authorized to reverse fraudulent transactions. The guardian address should preferably be from an unused offline or paper wallet. When a fraudulent transaction is reversed, any pending funds go to the guardian address, rather than going back to the compromised savings address. Also, any funds which remain in the compromised address also go to the guardian wallet.  An address marked as savings can only do simple transfers (transaction type=0). All other transaction types require addresses without a reversibility time window.  Marking a Savings Address as Compromised Say you notice that the address you marked as savings has been compromised, and you want to reverse transactions and transfer everything to the guardian address. Doing this takes 4 bytes, which fits into a single data payment 1. Transaction type = 11 for marking a compromised savings address (32-bit unsigned integer, 4 bytes) This transaction must be sent from the guardian address. The reference payment must be to the compromised savings address. Funds from any pending transactions and any remaining funds will then be transferred to the guardian address, both MasterCoins and any currencies derived from MasterCoins.  Advantages of the Savings/Guardian Model The savings/guardian model is intended to allow the user to take extreme precautions against accidental loss of the savings address (for instance, by storing lots of backups, including in the cloud), and extreme precautions against theft of the guardian address. Although reasonable precautions should be taken, if your savings address gets hacked, or the key to your guardian address gets lost or destroyed, the coins can still be recovered. This model also facilitates estate planning. You simply give your heir(s) a paper copy to the private key of your savings address, but you keep the guardian address key to yourself. If you die, your heirs can simply transfer the funds out of your savings (they will have to wait for the reversibility period to pass), but they can't steal from you while you are alive since you are the only one with the key to the guardian address and can reverse their transaction if they try. It should be obvious that anyone accepting MasterCoins or MasterCoin-derived currencies for payment should check that the payment is not reversible before completing the transaction!  Selling MasterCoins for Bitcoins Say you want to publish an offer to sell 1.5 MasterCoins for 1000 bitcoins. Doing this takes 33 bytes, which fits into two data payments: 1. Transaction type = 20 for currency trade offer for bitcoins (32-bit unsigned integer, 4 bytes) 2. Currency identifier for sale = 1 for MasterCoin (32-bit unsigned integer, 4 bytes) 3. Amount for sale = 150,000,000 (1.50000000 MasterCoins) (64-bit unsigned integer, 8 bytes, should not exceed the number owned, but if it does, assume the user is selling all of them) 4. Amount of bitcoins desired = 100,000,000,000 (1000.00000000 bitcoins) (64-bit unsigned integer, 8 bytes) 5. Time limit = 10 (10 blocks in which to send payment after counter-party accepts these terms) (8bit unsigned integer, 1 byte) 6. Minimum bitcoin transaction fee = 10,000,000 (require that the buyer pay a hefty 0.1 BTC transaction fee to the miner, discouraging fake offers) (64-bit unsigned integer, 8 bytes)  Selling MasterCoins for Other MasterCoin-Derived Currencies Say you want to publish an offer to sell 2.5 MasterCoins for 50 GoldCoins (coins which each represent one ounce of gold, derived from MasterCoins and described later in this document). For the sake of example, we'll assume that GoldCoins have currency identifier 3. Doing this takes 28 bytes, which fits into two data payments: 1. Transaction type = 21 for currency trade offer for another MasterCoin-derived currency (32-bit unsigned integer, 4 bytes) 2. Currency identifier for sale = 1 for MasterCoin (32-bit unsigned integer, 4 bytes) 3. Amount for sale = 250,000,000 (2.50000000 MasterCoins) (64-bit unsigned integer, 8 bytes, should not exceed the number owned, but if it does, assume the user is selling all of them) 4. Currency identifier desired = 3 for GoldCoin (32-bit unsigned integer, 4 bytes) 5. Amount of GoldCoins desired = 5,000,000,000 (50.00000000 GoldCoins) (64-bit unsigned integer, 8 bytes)  Changing an Offer Say you decide you want to change the number of coins you are offering for sale, or change the asking price. Simply re-send the offer with the new details. If your change gets into the block chain before someone accepts your old offer, your offer has been updated. If you decide you want to cancel an offer, simply send the offer again, but enter the number of coins for sale as zero.  Purchasing a Currency Offered For Sale Say you see an offer such as those listed above, and wish to accept it. Doing so takes 16 bytes, which fits into 1 data payment: 1. Transaction type = 22 for accepting currency trade offer (32-bit unsigned integer, 4 bytes) 2. Currency identifier you are purchasing = 1 for MasterCoin (32-bit unsigned integer, 4 bytes) 3. Amount you are purchasing = 130,000,000 (1.30000000 MasterCoins) (64-bit unsigned integer, 8 bytes, should not exceed number available for sale, but if it does, assume buyer is purchasing all of them) The reference address should point to the seller's address, to identify whose offer you are accepting. If you are purchasing with bitcoins, make sure your total expenditures on bitcoin transaction fees while accepting the purchase meet the minimum fee requested! You will need to send the appropriate amount of bitcoins before the time limit expires to complete the purchase. Note that you must send the bitcoins from the same address which initiated the purchase. If you send less than the correct amount of bitcoins, your purchase will be for that amount. Update 9/8/2013: In order to make parsing MasterCoin transactions easier, you must also include an output to the Exodus Address when sending the bitcoins to complete a purchase of MasterCoins. The output can be for any amount, but should be above the dust threshold. If you are purchasing with MasterCoin or a MasterCoin-derived currency such as GoldCoins, your purchase is complete as soon as you accept the offer (assuming you are recorded in the block chain as the first to accept the offer). If you have less than the correct amount on hand, your purchase will be for that amount. Note that when only some coins are purchased, the rest are still for sale with the same terms.  Registering a Data Stream Say you decide you would like to start publishing the price of Gold in the block chain. Registering your data stream takes a varying number of bytes due to the use of null-terminated strings. This example uses 57 bytes, which fits in 3 data payments: 1. Transaction type = 30 for registering a data stream (32-bit unsigned integer, 4 bytes) 2. Parent Currency Identifier = 1 for MasterCoin (32-bit unsigned integer, 4 bytes) (Meaning that the price of Gold will be published in units of MasterCoin) 3. Category = “Commodities\0” (12 bytes) 4. Sub-Category = “Metals\0” (7 bytes) 5. Label = “Gold\0” (5 bytes) (if a second “Gold” is registered in this sub-category, it will be shown as “Gold-2”) 6. Description/Notes = “tinyurl.com/kwejgoig\0” (22 bytes) (Please save space in the block chain by linking to your description!) 7. Display Multiplier = 10,000 (if the ticker publishes 0.00150000, the price of an ounce of gold is currently 15.0000 MasterCoins. (32-bit unsigned integer, 4 bytes) The reference payment should be to the bitcoin address which will be publishing the data. Only the first payment sent from that address in a given day (as determined by block-chain timestamps) will be considered ticker data. Update 9/8/2013: Data published by a ticker should also have an output to the Exodus Address – this will make it easier to find ticker data in the block chain data. The output can be for any amount, but should be above the dust threshold. Each data stream gets a unique identifier, determined by the order in which they were registered. For instance, if your data stream was the third data stream ever registered, your data stream identifier would be 3. Since anyone can cheaply register a data stream, and thereby create categories and subcategories, we can assume that there will be a lot of noise. Anyone writing code to display data stream categories should note which data streams are the most actively used, and order categories and subcategories by descending activity, thereby pushing unused categories to the bottom of the list. If you ever need to change the description/notes for your data stream (for instance, if some poor sport takes down your website), simply re-register it from the same address with the same category, subcategory, and label. When re-registering, you can also change the ticker address by choosing a different address for the reference payment (for instance, if your ticker address gets compromised), or change the display multiplier.  Offering a Bet Say you want to use USDCoins (another hypothetical currency derived from MasterCoin, each USDCoin being worth one U.S. Dollar) to bet $200 that the gold ticker will not rise above 20 MasterCoins/Ounce in the next 30 days at 2:1 odds. For the sake of example, we will assume that USDCoins have currency identifier 5. Creating this bet takes 36 bytes which fits into 2 data payments 1. Transaction type = 40 for creating a bet offer (32-bit unsigned integer, 4 bytes) 2. Bet Currency identifier = 5 for USDCoin (32-bit unsigned integer, 4 bytes) 3. Data Stream identifier = 3 for the Gold ticker, per our data stream example (32-bit unsigned integer, 4 bytes) 4. Bet Type = 35 for “Will not exceed on or before” (See table below) (16-bit unsigned integer, 2 bytes) 5. Bet threshold = 200,000 (0.00200000 BTC, which equates to a ticker value of 20 per our data stream example) (32-bit unsigned integer, 4 bytes) 6. Days out = 30 (16-bit unsigned integer, 2 bytes) 7. Amount of wager = 20,000,000,000 (200.00000000 USDCoins) (64-bit unsigned integer, 8 bytes) 8. Amount of counter-wager = 10,000,000,000 (100.00000000 USDCoins) (64-bit unsigned integer, 8 bytes) By offering $200 against $100, the desired 2:1 odds are implied. It is not possible to change a bet (you must cancel and then broadcast a new bet). To cancel your bet, rebroadcast it with all the same data except set the amount of wager to zero. Table of Bet Types 0  Will equal on  32 Will equal on or before  1  Will not equal on  33 Will not equal on or before  2  Will exceed on  34 Will exceed on or before  3  Will not exceed on  35 Will not exceed on or before  4  Will be below on  36 Will be below on or before  5  Will not be below on  37 Will not be below on or before  Accepting a Bet Say you see a bet which you would like to accept. Simply publish the inverse bet with matching odds and the same end date, and the MasterCoin protocol will match them automatically (that is, everyone parsing MasterCoin data will mark both bets as accepted). Here is what a bet matching our last example published 5 days later (with 25 days to go) would look like: 1. Transaction type = 40 for creating a bet offer (32-bit unsigned integer, 4 bytes) 2. Bet Currency identifier = 5 for USDCoin (32-bit unsigned integer, 4 bytes) 3. Data Stream identifier = 3 for the Gold ticker, per our data stream example (32-bit unsigned integer, 4 bytes) 4. Bet Type = 34 for “Will exceed on or before” (See table above) (16-bit unsigned integer, 2 bytes) 5. Bet threshold = 200,000 (0.00200000 BTC, which equates to a ticker value of 20 per our data stream example) (32-bit unsigned integer, 4 bytes) 6. Days out = 25 (16-bit unsigned integer, 2 bytes) 7. Amount of wager = 5,000,000,000 (50.00000000 USDCoins) (64-bit unsigned integer, 8 bytes) 8. Amount of counter-wager = 10,000,000,000 (100.00000000 USDCoins) (64-bit unsigned integer, 8 bytes) Note that this bet will be matched against only half of the previous example, because while the odds match (2:1 vs. 1:2), the amount of this bet is for less. This bet is only for $50, so would only win $100 if they win, as opposed to the full $200. Once the bets are matched, the first bet still has $100 available for someone else to bet $50 against. Once GoldCoins reach a value of 20 or the bet deadline passes, the bet winner gets 99.5% of the money at stake. The other 0.5% goes to the creator of the data stream.  Smart Property Update 9/8/2013: MasterCoin supports creating property tokens to be used for titles, deeds, userbacked currencies, and even shares in a company. Whenever property is created, it gets assigned the next available currency ID, so any property can be bought, sold, transferred, and even used for betting, just like other MasterCoin-derived currencies.  New Property Creation Say you want to create a new IPO for your company “Quantum Miner”. Doing so will use a varying number of bytes, due to the use of a null-terminated string. This example uses 37 bytes, which fits in 2 data payments: 1. Transaction type = 50 for creating new property (32-bit unsigned integer, 4 bytes) 2. Property Type = 1 for indivisible shares (2 is divisible currency) (32-bit unsigned integer, 4 bytes) 3. Property Name = “Quantum Miner Shares\0” (21 bytes) 4. Number Properties = 1,000,000 indivisible shares (64-bit unsigned integer, 8 bytes) As with data streams, properties are awarded currency identifiers in the order in which they are created. MasterCoin is currency identifier 1 (bitcoin is 0), and Test MasterCoins have currency identifier 2. If creating a title to a house or deed to land, the number of properties should be 1. Don’t set number of properties to 10 for 10 pieces of land – create a new property for each piece of land, since each piece of land inherently has a different value, and they are not interchangeable. If creating 1,000,000 units of a divisible currency, the user would have chosen property type 2 and would have entered 100,000,000,000,000 for the number of properties (1 million divisible to 8 decimal places). Once property has been created, the creator owns them at the address which broadcast the message creating them.  Escrow-Backed User Currencies The most important feature of MasterCoins is the built-in support for users to create their own currencies out of existing MasterCoins. For the purposes of demonstrating how user currencies will work, we will use an example currency called “GoldCoins”, which are intended to track the value of one ounce of gold, and which may be stored, transferred, bought, and sold similarly to MasterCoins.  Stability Concept So how do we drive the value of these GoldCoins to their target value, when demand for them may surge and decline? The price of GoldCoins is decided by the balance of supply and demand. Since we can’t control the demand for GoldCoins, we must control the supply. The key to accomplishing this is to use an escrow fund which holds MasterCoins, as shown below:  The escrow fund operates like a battery on the power grid, charging when there is excess energy then discharging where there isn't enough. When there are too few GoldCoins (GoldCoin price is too high), the escrow fund releases new GoldCoins, and the escrow-battery “charges” by holding MasterCoins in escrow. When there are too many GoldCoins (GoldCoin price is too low), the escrow fund purchases some of the excess GoldCoins, thereby “discharging” the escrow-battery as it releases the stored MasterCoins.  New Currency Creation Say you want to create the GoldCoin currency described above, using the Gold data stream we defined. Doing so will use a varying number of bytes, due to the use of a null-terminated string. This example uses 38 bytes, which exactly fits in 2 data payments: 5. Transaction type = 100 for creating a new child currency (32-bit unsigned integer, 4 bytes) 6. Data Stream identifier = 3 for the Gold ticker, per our data stream example (32-bit unsigned integer, 4 bytes) 7. Escrow fund delay = 4 for 4 days (see below) (8-bit unsigned integer, 1 byte) 8. Escrow fund aggression factor = 1,000,000 for 1% (See below) (32-bit unsigned integer, 4 bytes) 9. Currency Name = “GoldCoin\0” (9 bytes) 10. Escrow Fund Initial Size = 100,000,000,000 for 1,000 MasterCoins (64-bit unsigned integer, 8 bytes, causes 1,000 MasterCoins to be debited from the currency creator and credited to the escrow fund. This number should not exceed the amount owned by the creator, but if it does, assume they are crediting all their MasterCoins to the escrow fund) 11. Escrow Fund Minimum Size = 99,000,000 for 99% (32-bit unsigned integer, 4 bytes, if the escrow fund value is ever less than 99% of all GoldCoins, the currency is dissolved and the escrow fund is distributed to GoldCoin holders who would take a 1% loss) 12. Sale/Transfer Penalty = 100,000 for 0.1% (32-bit unsigned integer, 4 bytes, any time GoldCoins are sold or transferred, 0.1% are destroyed, which improves the health of the escrow fund) As with properties, currencies are awarded currency identifiers in the order in which they are created. MasterCoin is currency identifier 1 (bitcoin is 0), and Test MasterCoins have currency identifier 2, so if GoldCoin is the first MasterCoin-derived currency, it will get a currency identifier of 3. The currency held in escrow is the parent currency of the data stream. In this example it is MasterCoins, but it could also be any currency derived from MasterCoins. For instance, GoldCoins could later be held in escrow to support a currency whose data stream uses GoldCoins as a parent currency. The escrow fund delay of 4 days means that the price of GoldCoins must be too high (or too low) for 4 days in a row before the escrow fund will take any action. The escrow fund aggression factor determines how aggressively the escrow fund corrects the price of GoldCoins when their price diverges from their target. An escrow fund with aggression factor of 0 will never take any action. If the aggression factor is 100%, the escrow fund will take the maximum possible action (buying every GoldCoin for sale above the target price, or selling new GoldCoins to every buyer below the target price).  In the case of a 1% aggression factor, the escrow fund's first action will be to fix 1% of the error. If the error the next day is still in the same direction, the escrow fund will fix 2% of the error, then 3% the next day, and so on until it reaches 100% or the error changes direction. Once the error changes its direction, the escrow fund has done its job and it starts counting again from zero. Update 9/8/2013: Items 6-8 above were added in response to the “bytemaster/d’aniel attack”, which becomes possible once malicious actors are able to short these currencies. The attack only works on currencies with underfunded escrows, and consists of a malicious actor creating a competing GoldCoin with a healthy escrow fund, which the market would presumably prefer over the GoldCoin with the unhealthy escrow fund. The malicious actor could then profit by shorting the unhealthy GoldCoin until people panicked and fled for the healthy version. More information about unhealthy escrow funds can be found in the next section.  Unhealthy Escrow Funds What if the price of MasterCoins falls 95%, and the value of the escrow fund is now only 5% of the target value of all GoldCoins? Using the battery analogy, this escrow fund now has less “charge” and is therefore less capable of intervening to correct prices. If the currency creator had set the minimum escrow size to 100% the escrow fund would never get into this situation because it would simply dissolve and pay out to currency stakeholders as soon as the escrow fund value dropped to parity, with zero or minimal losses. For currencies which are set up to allow continued operation once unhealthy, the protocol responds by adjusting the aggression factor accordingly. In the example of GoldCoins backed by only 5% given above, the 1% aggression factor would be multiplied by 5% to get 0.05%, meaning that the adjustments will be of much smaller magnitude, and it will take a lot longer to get to 100% aggression. Note that escrow funds holding funds worth more than their currency do not get more aggressive. That is, if the GoldCoins escrow fund is worth twice the value of all GoldCoins in existence, the aggression factor is still 1%.  Maintaining Escrow Fund Health Given a reasonably stable MasterCoin, escrow funds should generally grow healthier over time. Our GoldCoin escrow fund, when it does act, is buying GoldCoins when they are cheap, and selling them when they are expensive. Thus it will generally tend to make a profit, and the MasterCoins held by the escrow fund will grow. The larger the escrow fund, the lower the chance of the currency failing to maintain its value. Update 9/8/2013: Additionally, the currency creator can optionally supply initial escrow funds if desired, and the currency can be tuned to destroy some GoldCoins with every sale or transfer, further increasing escrow fund health. When an escrow fund is unhealthy, lowering the aggression factor makes the escrow fund take more profitable trades, which increases the likelihood of recovery. For instance, if it is buying excess GoldCoins, the cheapest 0.05% can be purchased at a better average price than the cheapest 1% on the market. Escrow funds should generally be tuned to act slowly. This will allow arbitrage traders to do the heavy lifting, as the knowledge that the escrow fund will eventually get the price back to the target makes for a self-fulfilling prophecy when traders act on that knowledge. If the escrow fund acts too quickly, it loses money when the bitcoin version of a security leads the real-world version, as would happen if someone was engaging in insider trading anonymously using the bitcoin version.  Appendix A – Horrible, Awful, Very Bad Things Speaking of insider trading, it should be clear by now that MasterCoins can be used for some very bad things. Anyone working on an implementation of the MasterCoin protocol should be very careful to warn users to not break the law of their country of residence. It is up to the user to know the laws of their country, and not (for instance) engage in sports betting in a country where sports betting is not legal. Also, we are not securities experts, but one has to imagine that insider trading is illegal most places, if not everywhere. Also, if you think bitcoin has a reputation problem for money laundering now, just wait until you can store “USDCoins” in the block chain! At last, drug dealers and human traffickers will have the perfect currency to enable their work. However, stable distributed currencies will be incredibly useful in a huge number of legal applications, and even modest success of this protocol could make early investors (and even those who simply hold bitcoins) very rich. MasterCoins, much like guns, are just tools, capable of being used for good or for evil. We urge our early adopters to consider how they may use MasterCoin for good, and if you get rich, how to use that money for good. It will take a lot of work to make the good outshine the evil.  Appendix B – Disclaimer Investing in experimental currencies is really, absurdly risky. This paper is not investment advice, and anyone predicting what will happen with experimental currencies such as those described here is indulging in the wildest sort of speculation, and that includes the speculations in the previous appendix. Please consult your financial adviser before investing in ANY wild scheme such as this (hint: they will probably tell you to RUN and not look back unless you assure them that it is money you are totally prepared to lose). Anyone who puts their rent money or life savings into an experiment of this type is a fool, and deserves the financial ruin they will inevitably reap from this or some other risky enterprise.  Mimblewimble Andrew Poelstra∗ 2016-10-06 (commit e9f45ec)  Abstract At about 04:30 UTC on the morning of August 2nd, 2016, an anonymous person using the name Tom Elvis Jedusor signed onto a Bitcoin research IRC channel, dropped a document hosted on a Tor hidden service[Jed16], then signed out. The document, titled Mimblewimble, described a blockchain with a radically different approach to transaction construction from Bitcoin, supporting noninteractive merging and cut-through of transactions, confidential transactions, and full verification of the current chainstate without requiring new users to verify the full history of any coins. Unfortunately, while the paper was detailed enough to comminicate its main idea, it contained no arguments for security, and even one mistake1 . The purpose of this paper is to make precise the original idea, and add further scaling improvements developed by the author. In particular, Mimblewimble shrinks the transaction history such that a chain with Bitcoin’s history would need 15Gb of data to record every transaction (not including the UTXO set, which including rangeproofs, would take over 100Gb). Jedusor left open a problem of how to reduce this; we solve this, and combine it with existing research for compressing proof-of-work blockchains, to reduce the 15Gb to less than a megabyte.  License. This work is released into the public domain.  ∗ grindelwald@wpsoftware.net 1 https://www.reddit.com/r/Bitcoin/comments/4vub3y/mimblewimble_noninteractive_coinjoin_ and_better/d61n7yd  1  Contents 1  2  Introduction  2  1.1  Overview and Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3  1.2  Trust Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  4  Preliminaries  5  2.1  Cryptographic Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  5  2.1.1  Standard Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  5  2.1.2  Sinking Signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  6  Mimblewimble Primitives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  8  2.2 3  The Mimblewimble Payment System  9  3.1  Fundamental Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  9  3.2  The Blockchain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  10  3.3  Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  11  3.3.1  Block Headers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  11  3.3.2  Proven and Expected Work . . . . . . . . . . . . . . . . . . . . . . . . . .  13  3.3.3  Sinking Signatures and Compact Chains . . . . . . . . . . . . . . . . . . .  14  4  Extensions and Future Research  16  5  Acknowledgements  17  1  Introduction  In 2009, Satoshi Nakamoto introduced the Bitcoin cryptocurrency[Nak09], an online currency system to allow peer-to-peer transfer of digital tokens. These tokens’ ownership is ascertained by the use of public keys, and transfer is accomplished by means of a public ledger, a globally replicated list of transactions which destroy unspent transaction outputs (UTXOs) and create new ones of equal value but different keys. All Bitcoin coins originate in coinbase transactions, transactions which create new outputs without destroying any old ones, and which are limited in value to maintain a fixed inflation schedule. Once created, they change hands by means of ordinary transactions. This means that new users, in 10  order to fully verify that their view of the system is uncompromised (absent theft or illegal inflation), must download and validate the entire history from the system’s genesis in 2009 until today. In other words, they must download and “replay” every transaction that has ever occurred. Today the Bitcoin blockchain sits just shy of 100Gb on the author’s disk. Had Bitcoin used Confidential Transactions[Max15] (CT), each of the approximately 430 million outputs would consume 2.5Kb each, totalling over a terabyte of historical data2 . In this paper, we describe a research project to design a Bitcoin-like cryptocurrency, which achieves dramatically better scaling and privacy properties than Bitcoin. In particular, it allows 2 The  author is aware of unpublished research that would reduce this quantity by about 25%; the total is still formidable.  2  removal of most historic blockchain data, including spent transaction outputs, rangeproofs and all, while still allowing users to fully verify the chain. Indeed, it is possible for the system to function 20  even if no users retain the vast majority of historic blockchain data. Further, this currency allows transactions to be noninteractively combined (a la Coinjoin[Max13a]) and cut-through[Max13b], eliminating much of the topological structure of the transaction graph. If this is done without publishing the original component transactions, the result is an enormous boon to user privacy, amplified by CT. Unfortunately, it accomplishes these goals at cost of sacrificing functionality.  1.1  Overview and Goals  Mimblewimble is a design for a cryptocurrency whose history can be compacted and quickly verified with trivial computing hardware even after many years of chain operation. As a secondary goal, it should support strong user privacy by means of confidential transactions and an obfuscated trans30  action graph. However, to achieve these goals, Mimblewimble cannot support a general-purpose scripting system such as that in Bitcoin. This precludes such functionality as zero-knowledge contingent payments[Max16], cross-chain atomic swaps[Nol13] and micropayment channels[PD16]. Further research is needed to emulate these functionalities on top of Mimblewimble, and is discussed in Section 4. More precisely, • Direct transfer of value between parti(es) to other parti(es) should be possible on the system. • All transactions should use confidential transactions to blind their output amounts. • Transactions should be noninteractively aggregable[Mou13], i.e. support a noninteractive variant of CoinJoin[Max13a, Max13b], such that parties not privy to the original transactions  40  are unable to separate them. This can improve censorship resistance and privacy, though it is unclear how to design a safe peer-to-peer network capable of exploiting this ability. • For a new participant, the amount of bandwidth and processing power needed to catch up with the system should be proportional to the current state of the system, i.e. the size of the UTXO set plus rangeproofs, rather than the total size of all historical transactions. As described in the next section, and more thoroughly in Section 3.3, Mimblewimble has a scheme for compressing blockchain history to a size polylog in the original size, which in practice for a system with Bitcoin’s scale should allow a decade or more of transaction history to be verified in several seconds using a couple megabytes of data3 . 3 Experiments with the compact chains described in this paper suggest that a 500000 block chain may have a compact length of around 300 blocks (though with high variance across multiple experiments), each containing sinking signatures which average ten 96-byte elliptic curve points, merkle commitments to previous blocks consisting of forty 40-byte (blockhash, difficulty) pairs, and some extra block header data, totalling less than 3Kb of data. Across 300 blocks this is 900Kb total of compact chain data. Verification time is dominated by pairing computations, of which there are on average ten per block, or 3000. On the author’s laptop using Ben Lynn’s libpbc library, these can be done in under 20 seconds using a single core. By more careful choice of elliptic curve, and focused optimization work, this number can surely be improved.  3  On the other hand, Bitcoin’s current state of 40 million unspent outputs would grow to 100Gb and a few days of verification on a modern CPU, thanks to Mimblewimble’s use of confiden-  50  tial transitions. It is an open problem how to improve this. We observe that even without cryptographic improvements, it would go a long way to simply cap the UTXO set size as a function of blockheight and let the transaction-fee market take care of it.  1.2  Trust Model  Like Bitcoin, Mimblewimble is a blockchain-based cryptocurrency intended to be instantiated in a decentralized manner in which users may join or leave the system at any time. Further, upon (re)joining the network, users should be able to determine the network state, at least up to some recent time, and verify that this state was obtained by a series of valid state transitions, without trusting the honesty of any third parties. 60  However, there are two points of departure from Bitcoin’s trust model: 1. Unlike Bitcoin, whose blockchain describes every transaction in its entirety, and therefore allows all users to agree on and verify the precise series of transactions that led to the current chainstate, Mimblewimble only allows users to verify the essential features: • A transaction, once committed to the block, cannot be reversed without rewriting the block (or by the owner(s) of its outputs explicitly undoing it). • The current state of all coins was obtained by zero net theft or inflation: there are exactly as many coins in circulation as there should be, and there for each unspent output there exists a path of transactions leading to it, all of which are committed in the chain and authorized.  70  Note that there may be other paths which have also been committed in the chain, during which some transactions may have been invalid or unauthorized or inflationary, but since a legitimate path exists, all these things must have netted out to zero. 2. Like Bitcoin, verifiers may see multiple differing blockchains, and select the valid one as the one with the greatest total work. This is detailed in Section 3.3.2. However, in Bitcoin the total work represents both the expected work to produce such a blockchain as well as the proven work of the chain, in the sense that any party who expends significantly less than this much work will be unable to produce such a chain except with negligible probability. Mimblewimble, on the other hand, separates these. The total work still represents the ex-  80  pected work to produce the blockchain, and therefore incentivizes rational actors to contribute to the most-work chain rather than rewriting it. The proven work, on the other hand, is capped at some fixed value independent of the length of the chain, and serves to make forgery by irrational lucky actors prohibitively expensive.  4  2  Preliminaries  2.1  Cryptographic Primitives  Groups. Throughout, G1 , G2 will denote elliptic curve groups adorned with an efficiently computable bilinear pairing ê : G1 × G2 → GT , with GT equal to the multiplicative group of Fqk for some prime q, small positive integer k. We further require both G1 and G2 to have prime order r. Let G, H be fixed generators of G1 whose discrete logarithms relative to each other are unknown (i.e. they are 90  nothing-up-my-sleeve (NUMS) points; G2 does not need any canonical generators. We will make computational hardness assumptions about these groups as needed. We write Zr for the integers modulo r, and write + for the group operation in all groups. All cryptographic schemes will have an implicit GenParams(1λ ) phase which generates r, G1 , G2 , GT , G and H uniformly randomly given a security parameter λ . 2.1.1  Standard Primitives  Definition 1. A commitment scheme is a pair of algorithms Commit(v, r) → G , Open(v, r,C) → {true, false} such that Open(v, r, Commit(v, r)) accepts for all (v, r) in the domain of Commit. It must satisfy the following security properties. • Binding. The scheme is binding if for all (v, r) in the domain of Commit, there is no (v0 , r0 ) 6= 100  (v, r) such that Open(v0 , r0 , Commit(v, r)) accepts. It is computationally binding if no PPT algorithm can produce such a (v0 , r0 ) with nonneglible probability. • Hiding. The scheme is (perfectly, computationally) hiding if the distribution of Commit(v, r) for uniformly random r is (equal, computationally indistinguishable) for different values of v. Definition 2. We define a homomorphic commitment scheme as one for which there is a group operations on commitments and Commit is homomorphic in its value parameter. That is, one where commitments to v, v0 can be added to obtain a commitment to v + v0 having the same security properties. Example 1. Define a Pedersen commitment as the following scheme: Commit : Z2r → G maps (v, r) to vH + rG, and Open : Z2r × G → {true, false} checks that Commit(v, r) equals vH + rG.  110  If the discrete logarithm problem in G is hard, then this is a computationally binding, perfectly hiding homomorphic commitment scheme[Ped01]. Definition 3. Given a homomorphic encryption C, we define a rangeproof on C as a cryptographic proof that the committed value of C lies in some given range [a, b]. We further require that rangeproofs are zero-knowledge proofs of knowledge (zkPoK) of the opening information of the commitments. Unless otherwise stated, rangeproofs will commit to the range [0, 2n ] where n is small enough that no practical amount of commitments can be summed to produce an overflow. We may use, for example, the rangeproofs described in [Max15] for Pedersen commitments, which satisfy these criteria. 5  120  2.1.2  Sinking Signatures  This brings us to the first new primitive needed for Mimblewimble. Definition 4. We define a sinking signature as the following collection of algorithms: • Setup(1λ ) outputs a keypair (sk, pk); • Sign(sk, h) takes a secret key sk and nonnegative integer “height” h which is polynomial in λ , and outputs a signature s. • Verify(pk, h, s) takes a public key pk and signature s and outputs from {true, false}. satisfying the following security and correctness properties: • Correctness. For all polynomial h, (sk, pk) ← Setup(1λ ), s ← Sign(sk, h), we have that Verify(pk, h, s) accepts. 130  • Security. Let (·, pk) ← Setup(1λ ). Then given pk and an oracle H which given h returns Sign(sk, h), no PPT algorithm A can produce a pair (s, h0 ) with h0 > h for all h given to the oracle, and Verify(pk, h0 , s) accepts. (Except with negligible probability.) The name “sinking signature” is motivated by the fact that given a signature s on height h, it may be possible for a forger to create a signature s0 on height h0 with the same public key and h0 ≤ h, thus “decreasing the height” of the signature. We will use this feature later to aid scalability for a Mimblewimble chain. Definition 5. We say a sinking signature is aggregatable or summable if given a a linear combination pk of pki values computed from Setup(1λ ), the same linear combination of si ← Sign(ski , h) (for fixed h) it is possible to compute a signature s such that Verify(pk, h, s) accepts.  140  For a summable sinking signature, we generalize the above security game to allow the adversary A to play polynomially many times in parallel and win with an arbitrary linear combination of its received public keys. (It must also provide the coefficients of the linear combination.) Definition 6. We propose the following summable sinking signature (Poelstra, Kulkarni). Let H2 be a random oracle hash with values in G2 . • Setup(1λ ) chooses a uniformly random sk ← Zr and sets pk = sk · G. • Sign(sk, x) first computes the sequence {x0 , . . . , xn } where x0 = x and xi+1 is obtained by subtracting from xi the largest power of 2 that divides it (i.e., by clearing its least significant 1 bit). We observe that xn = 0 and that n is one plus the number of one bits in x and is therefore O(log2 x).  150  Finally, it outputs s = {sk · H2 (xi }ni=0 . • Verify(pk, x, {si }) computes S as the sum of all si , computes H = ∑ni=0 H2 (xi ), and checks that e(G, S) = e(pk, H).  6  Observe that the verification step uses only the sum S of the elements of the signature. However, the extra data will be useful later, so we state it here so we can prove the scheme is secure with it. Correctness and summability of the scheme are immediate. Theorem 1. This is a secure summable sinking signature, in the following sense: if an adversary A exists which wins the game described in Definition 5, a simulator B exists which can solve the computational co-Diffie-Hellman (co-CDH) problem for (G2 , G1 ), given oracle access to A . Proof. B answers random oracle queries to H and works in the following way. (Recall that the 160  game in Definition 5 is the game from Definition 4 played in parallel.) We suppose without loss that before making signature queries on any height x, the adversary first all random oracle queries needed to verify such a signature; similarly before producing a forgery on height x∗ it makes the required queries. We then suppose that in total it requests at most q p public keys and makes at most qh random oracle queries. 1. B receives a co-CDH challenge (G, P, Q) from its challenger, where G, P ∈ G1 , Q ∈ G2 , and the goal is to produce R ∈ G2 such that ê(P, Q) = ê(G, R). 2. B responds to the ith public key request by generating a uniformly random keypair (xi , Pi ) and replies with Pi + P. 3. B responds to the jth random oracle query by generating a uniformly random keypair (y j , H j ) $  170  except that H j∗ = Q for j∗ ← − {1, . . . , qh }, and replying with Hi . 4. B responds to the kth signature query on height hk and pubkey Pk as follows: it computes the sequence {hkn } and checks if any of the H(hkn ) values is Q; if so, it aborts. Otherwise, for each i{0, . . . , n} it knows a zi such that H(hki ) = zi G and produces s = {zi P}i . 5. Finally, A wins by producing a forgery consisting of coefficients {ci }m i=1 with not all ci zero, ∗ ∗ a pubkey P∗ = ∑m i=1 ci (Pi + P), a height h greater than any h thus queried on, and a signature ∗  ∗  s∗ = {Si } such that ∑ni=0 e(G, Si ) = ∑ni=0 e(P∗ , H(h∗i )). 6. If some H(h∗i∗ ) = Q, then for the above sum to hold, writing x∗ for P∗ = x∗ G, we must have n∗  n∗  i=0  i=0  ∑ Si = ∑ y∗i P∗ n∗  for y∗i such that H(h∗i ) = y∗i G  m  = ∑ ∑ [c j x j y∗i G + c j y∗i P] i=0 j=1 n∗  m  = ∑ ∑ [c j x j H(h∗i ) + c j y∗i P] i=0 j=1 m  =  n∗  ∑ c jR + ∑  j=1  m  ∑ [c j x j H(h∗i ) + c j y∗i P]  i=0 j=1 i6=i∗  where R is the solution to B’s CDH challenge, and every other term is computable by B.  7  To complete the proof, we must show that we abort with probability bounded away from 1, and that our win condition occurs with nonneglible probability. We observe that we only abort if 180  the attacker asks for a signature that requires Q be used, and we win if Q is used in the attacker’s forgery. Both occur if H(h∗ ) = Q, which has probability 1/qh , which completes the proof.  2.2  Mimblewimble Primitives  Definition 7. A Mimblewimble transaction is the following data: • A list of homomorphic commitments termed the inputs, with attached rangeproofs. Alternately, inputs may be given as explicit amounts, in which case they are treated as homomorphic commitments to the given amount with zero blinding factor. • A list of homomorphic commitments termed the outputs, with attached rangeproofs. • A blockheight x. • An excess commitment to zero, with a summable sinking signature on blockheight x with this  190  as pubkey. We make the further restriction on transactions that every input within a transaction be unique. Definition 8. We define the sum of a transaction to be its outputs minus its inputs, plus its excess. If • the sum is zero4 , and • the rangeproofs and sinking signature are valid then we say the transaction is valid. Definition 9. We define the canonical form of a transaction T as the transaction equal to T except if any input is equal to an output, both are removed. 200  Notice that the canonical form of any transaction has equal sum to the original transaction; in particular a transaction is valid if and only if its canonical form is valid. We observe that all valid transactions are noninflationary; the total output value must be equal to the total input value. Definition 10. Given a finite set of transactions {Ti } with pairwise disjoint input sets, we define the cut-through of {Ti } as the canonical form of the union of all Ti ’s. Next, we define some terms that will allow us to treat Mimblewimble transactions as mechanisms for the transfer of value within a blockchain. Definition 11. (Ownership.) We say that a party S owns a set of transaction outputs if she knows the opening of the sum of the outputs. 4 By  zero we mean the homomorphic commitment which commits to zero with zero blinding factor.  8  210  Definition 12. (Sending n coins.) To send n coins from S to R, S produces a transaction: chooses inputs, creates uniformly random change output(s) and a uniformly random excess, whose sum is a commitment to n. S sends this to R along with the opening information of the sum. Definition 13. (Receiving n coins.) To receive n coins, R receives a transaction T 0 which sums to a commitment of m ≥ n coins, along with opening information of this sum, and completes it to a valid transaction T by the following process: 1. R first produces uniformly random outputs whose total committed value is n, and adds these to the transaction. 2. If m = n, R negates the sum of this transaction and adds it to the excess of the transaction, so that the total sum is now zero. (It also computes a summable sinking signature for the amount added, and adds this to the original signature, so that the final excess has a valid signature  220  on it.) Otherwise the transaction now has sum committing to m − n, so R adds a uniformly random value to excess, updates the signature, and gives the opening information of the new sum to another recipient who can take the remaining value. When we define a blockchain and require transaction inputs to be outputs of earlier transactions, we will show that after this process, R is the only party who owns any of the outputs that he added.  3  The Mimblewimble Payment System  3.1  Fundamental Theorem  Theorem 2. (Fundamental Theorem of Mimblewimble) Suppose we have a binding, hiding ho230  momorphic commitment scheme. Then no algorithm A can win at the following game against a challenger C except with negligible probability: 1. (Setup.) C computes a finite list L of uniformly random homomorphic commitments. C sends L to A . 2. (Challenge.) At most polynomially many times, A selects some (integer) linear combination Ti of L and requests the opening of this combination from C . C obliges. 3. (Forgery.) A then chooses a new linear combination T which is not a linear combination of {Ti } and reveals the opening information of T . Proof. Consider the lattice Λ of formal linear combinations generated by L, that is, the set {∑A∈L bA A : bA ∈ Z}. Consider the quotient lattice Λ/Γ where Γ is the sublattice of Λ generated by the queries  240  {Ti }. We may consider every element of Λ/Γ to be a homomorphic commitment by using some canonical representative. In particular, every element of Λ/Γ is a homomorphic commitment which satisfies the same hiding/blinding properties that the original scheme did.  9  Then the projection of every {Ti } into Λ/Γ is zero, and the projection of T is nonzero. Therefore A has learned no information about the projection of T from its queries; however, if A knows the opening of T then it also knows the opening of the projection of T , contradicting the hiding property of the homomorphic commitment scheme.  3.2  The Blockchain  Mimblewimble consists of a blockchain, which is a weighted-vertex directed rooted tree of blocks (defined below) for which the highest-weighted path is termed the consensus history. 250  Definition 14. We define a Mimblewimble block as the following data: • A canonical transaction, whose inputs are of one of the two forms: – A reference to an output of the transaction in an earlier block; or – An explicit (i.e. with zero blinding factor) input restricted by rules above those given in this paper5 . • A block header, which has a binding commitment to earlier blocks in the chain, termed backlinks; a commitment to the current UTXO set after this block’s transaction has taken effect; and the transaction included in this block. If the block’s transaction is valid, we say the block is valid. In Section 3.3, we will describe how blocks are weighted, and which previous blocks specifically  260  should be linked to. For now we may use Definition 14 as our definition of validity, though note that there will be additional requirements on the commitments. Definition 15. We define the consensus chain state of a Mimblewimble blockchain as the cutthrough of all transactions on the consensus history. If the consensus chain state is valid, we call the blockchain valid. Note that for a blockchain to be valid, it is not required that all blocks be valid, only that the entire chain sum to a valid transaction. Next, we prove that Mimblewimble is a sound payment system, in the sense of the following two theorems. Theorem 3. (No inflation.) The total value committed by the outputs of the consensus chainstate of  270  a valid blockchain is equal to the value of the explicit inputs in each block. Proof. By hypothesis the consensus chain state, which is the canonical form of the cut-through of all transactions, is valid. Since every non-explicit input of every block is the output of a previous transaction, it does not appear in this canonical form. Therefore the only inputs of the chain state are the explicit ones. 5A  typical rule would be that each block can have only a single coinbase input of fixed value.  10  Lemma 1. (Unique ownership.) Suppose that all outputs of a transaction were created by receiving coins as in Definition 13 or sending as in Definition 12, so that all blinding factors are kept secret. Then for every subset of outputs in which not all have not been sent, the only owner of that subset is the person who created all its outputs. (In particular, if the subset contains outputs created by different parties, then that subset has no owner.) 280  Proof. Let O be an output in the subset which has not been sent. Then the only combination of outputs containing O whose commitment included O that may have been revealed also included some uniformly random excess E which was chosen when O was created. Further no other sum containing E was ever revealed, so that any combination including O but not E is not a linear combination of combinations whose opening information has been revealed. The subset in question does not contain E, since E is an excess not an output. Therefore by Theorem 2 nobody knows the opening information of the subset except the person who created O. Theorem 4. (No theft.) Consider a valid blockchain. An output x created as in Definition 13 cannot be removed from the consensus chain state without replacing the block in which it appeared, i.e., forming a higher-weighted valid blockchain not containing this block, except by parties who  290  (collectively) own a set U of outputs containing x. Proof. Suppose otherwise; then there exists a higher-weighted chain containing the block B in which x appeared, but for which the consensus chain state does not contain x. Consider the transaction T which is the canonical form of the cut-through of the first block after B to the tip of the chain. (Note that T may not be valid; we know only that the full chain states are valid.) Then the outputs of T are a subset of the outputs of the new chain state; in particular they contain rangeproofs which are proofs of knowledge of the openings of the outputs. Similarly, the excess value is also known. We conclude that the parties who created these blocks (and therefore T ) know  300  the openings of all outputs and sum of the excess value, and therefore own the set of all inputs of T . However, the inputs of T form a set of outputs containing x, completing the proof.  3.3  Consensus  Mimblewimble uses a hashcash[Bac02] style blockchain in which every block in a blockchain is labelled by a weight called its difficulty. A blockchain is valid if every block of difficulty D has a header which hashes into a range of size 1/D of the total space of hashes. We define a directed edge from block A to B iff A commits to B in its header, and require each block commit to its unique parent. We can then refine our definition in Section 3.2 of consensus history as the highest-weighted path terminating at the root. This is the same as Bitcoin’s design. 3.3.1 310  Block Headers  However, in we also define a second graph structure on blocks as vertices, called the compact blockchain. We define the compact blockchain iteratively as follows.  11  1. The genesis block is in the compact blockchain. 2. The first block after the genesis is added to the compact blockchain, and is assigned effective difficulty equal to its difficulty. 3. Each new block is added to the tip of the compact blockchain, and may cause blocks to be removed from the compact chain as follows: (a) First, its effective difficulty is calculated as follows. Consider the “maximum possible difficulty” M of the block, which is the size of the hash space divided by the hash of the block. (This may be larger than the actual difficulty.) The effective difficulty is determined by starting with the block’s difficulty, then adding the effective difficulties of as many consecutive blocks as possible, starting from the tip,  320  so that the total is less than or equal to M. (b) All blocks whose effective difficulty was used in the above calculation, except the new block itself, are dropped from the compact chain. The compact blockchain is encoded in the real blockchain by having every block commit to a merkle sum tree (with effective difficulty the quantity being summed) of all blocks in the current compact chain. We next prove several theorems to give an intuition of the properties of the compact blockchain. Lemma 2. The expected work required to produce a block with effective difficulty D is equivalent to 330  computing D hashes; similarly to produce several blocks with total effective difficulty D0 one must do expected work of computing D0 hashes. Proof. This is immediate in the random oracle model. Theorem 5. The expected amount of work to replace a block B in the compact chain (i.e. produce a blockchain of greater or equal total difficulty whose compact chain does not contain B) is greater than or equal to the work needed to replace B, its parent in the non-compact chain, its parent, and so on, up to but not including B’s parent in the compact chain. Proof. This follows immediately from Lemma 2 and the fact that the effective difficulty of every block is defined to be greater than or equal to the sum of the difficulty of the skipped blocks. Corallary 1. The expected work required to produce a compact blockchain is at least as large as  340  the expected work required to produce a full chain containing the same blocks. Theorem 6. Assuming constant difficulty, given a blockchain of length N, the expected length of the compact chain will be O(log N). Proof. The compact chain has been defined such that the proof in [BCD+ 14, Appendix A] still holds. We summarize it here:  12  1. First, consider starting from the tip and scanning backward until we find a block that can skip all remaining blocks back to the genesis. By construction this block will be in the compact chain. The probability that such a block exists within the first x blocks we check is x  N −i N −x x = 1− = N − i + 1 N N i=1  1−∏  and the expectation of this over all 1 ≤ x ≤ N is  N+1 2 ,  i.e. we expect the chain length to be  halved by this one skip. 350  2. Inductively, we can scan back from the tip until we find a block that skips back to the block in the previous step. This halves (in expectation) the remaining chain, and so on. The number of times we repeat this process until we have no more blocks to skip is the length of the compact chain, since each step added one more block to the compact chain, and since each step halved the number of remaining blocks, we see that there are only logarithmically many steps. We observe that small variations in difficulty do not affect the character of this proof, and therefore the constant-difficulty case can be considered a good approximation to the real-world situation. Theorem 7. Given a blockchain B (for the purpose of this theorem, we considering B to be only the most-work path), there is exactly one compact chain C ⊆ B. Further, verifiers can determine that that C is the compact chain given only the blocks of C and the opening of each block’s commitment  360  to the previous in the compact chain. Finally, no blocks in B \ C will appear in the compact chain of any extension of B (i.e. once a block is dropped it may be forgotten forever). Proof. By construction, a compact chain C exists which is a subset of B. Suppose that some other compact chain C0 6= C of B also exists. Let C← be the longest path from the tip contained in both C0 and C (since the tip itself is in both C0 and C by construction, this is nonempty), and let β be the deepest block of C← . Now, the block preceding β must differ in C and C; however, this is impossible since β commits to an ordered Merkle sum tree of previous blocks in the compact chain, the “preceding block” must be the first one that β ’s hash is not small enough to skip, and this is uniquely specified by the shape of the Merkle sum tree. We conclude that C0 = C.  370  Next, we argue that when B is extended, no blocks of B \C are added to the compact chain. But this is immediate, since by construction only new blocks are ever added to a compact chain. 3.3.2  Proven and Expected Work  However, while the expected work can be computed to be the same, the the compact chain does not, in general, prove as much work as the full chain. Here by “proving an amount of work” we mean that a prover who does less than this amount of work has negligible chance of producing the chain. For example, consider a blockchain of total difficulty D across n blocks, whose compact chain has log n blocks. Suppose an attacker attempts to produce this chain in εD work, where 0 < ε < 1. Then this requires, on average, that each individual block be produced in ε the expected time. Each block’s 13  380  production time is an independent variable, so the Chernoff bound lets us approximate this more precisely: if ε < 1 then the probability decays exponentially with the number of blocks in the chain. For the full chain this means probability O(exp[(1 − ε)n]) (exponential in n); for the compact chain O(exp[(1 − ε) log n]) or O(n1−ε ) (sublinear in n). In fact, the extreme case is even worse: a compact chain may consist of only a single block which has difficulty D, in which case the probability is simply O(exp(1 − ε)). This means an attacker willing to expend some fixed percentage of the total chain work has the same probability of successfully rewriting the chain regardless of the length of the chain. To be conservative, we conclude that compact chains, as described in this paper, prove no work.6 .  390  So what good are they? In Mimblewimble, we expect verifiers of a chain to demand all blocks from the most recent two months, say, and a compact chain from there to the start (in Section 3.3.3 we will see how full verification can proceed using only a compact chain). The resulting composite will: • be forgeable with expected work equal to the entire chain’s work; but • only prove the most recent two months of work Unlike Bitcoin, where the expected work to forge a blockchain is the same (asymptotically) to its proven work, in Mimblewimble these quantities are different. The expected work affects incentives: rational actors will choose to extend the most-work chain rather than attempting forgeries, just as in Bitcoin; on the other hand, the proven work affects verifiers’ certainty about the state of the world:  400  they know at least two months worth of work has been done to produce the chain they see, that it was not an accident, and that if it is a forgery it was a very expensive one that cannot be reliably repeated. 3.3.3  Sinking Signatures and Compact Chains  In this section, we describe how sinking signatures interact with compact chains, and in particular we find that it is possible to do a full Mimblewimble verification with only a compact chain. We introduce a notion of compact validity of blocks which which is weaker than validity as described in in Definition 14. Nonetheless, we preserve the trust model of Section 1.2. To do this, we modify the Merkle sum tree of previous blocks and their effective difficulty to 410  sum not only difficulty, but (a) the excess of the blocks’ transactions (Definition 7), (b) the sinking signatures on this excess. The excess values are summed in the obvious way, added as points, but the sinking signatures are more complicated. Rather than directly adding the signatures, we combine sets of signatures from each child at every node of the Merkle tree, in the following fashion: for any blockheight x, let {xn } denote the decreasing sequence defined in Definition 6. Then the signature set on a node is the union of the signature set of its children, except that whenever two heights x < y appear in the set such that 6 This says nothing about the compact SPV proofs described in, e.g. [KLS16], which put lower bounds on the length of compact proofs in order to upper-bound the probability a less-than-expected-work attacker can succeed. We cannot take this approach because we are using these proofs in consensus code and therefore need Theorem 7.  14  {xn } ⊆ {yn }, then both signatures are dropped and replaced by the signature on {xn } obtained by adding the corresponding components of the original signature. Therefore for a block at height h, the root of the Merkle sum tree committing to its ancestor blocks will have O(log h) sinking signatures, one for every power of 2 between 0 and h. 420  With this structure in place, we are able to define validity of a block. Definition 16. A block is compact valid if • It is valid in the sense of Definition 14. • Its Merkle sum tree commits to the deepest block allowable under the rules of Section 3.3.1. • This commitment is valid, in the sense that all the summing rules are obeyed. • The above commitment will be a Merkle proof consisting of a path from the commitment to the Merkle root of the form {ci , c0i } where c0 is a direct commitment to the block; for all i c0i is the sibling in the Merkle tree of ci ; and ci+1 is a commitment to {ci , c0i }. We require the first c0i that is a right sibling in the tree to have a valid set of sinking signatures on it. (If there is no such c0i then this block is skipping back to the genesis, so we instead  430  require that all the signatures at the root of the tree are correct.) Definition 17. A block is fully valid if it is compact valid and • It commits to its immediate predecessor in the full chain (and the excess/signature committed to in the tree match the previous block). • It has a valid commitment to the current UTXO set at the time of its creation. (The latter condition allows the UTXO set to be verified using only the compact chain; it also prevents consensus attacks whereby users are given the same chain but differing UTXO sets that it commits to. The former condition ensures that compact blocks’ contents don’t differ from full blocks’ contents, as long as full verifiers are watching.) (And if nobody is watching, it’s a moot point anyway.)  440  The conditions of Definition 16 are very technical. We can summarize them simply as follows: whenever a block at height h skips back to h0 , we sink the signatures of every intervening block to the lowest height greater than h0 possible, then aggregate all the signatures that wound up on the same height. This mess of Merkle sum trees is only to show how this condition can be checked by a verifier given only polylogarithmically much data. We argue that this design preserves the trust model. In particular: Theorem 8. No transaction can be removed without (doing as much work as) rewriting the block it appeared in. Proof. Every transaction has a sinking signature on the height h of the block it appears in. When a block is removed from the compact chain, the above construction may cause this signature to only  450  be verified after being sunk to some lower height h0 .  15  However, since h0 is always guaranteed to lay between the same two blocks in the compact chain that h does, this signature cannot be removed except be rewriting the more recent of these two blocks. However, by construction, this has expected work greater than or equal to rewriting the block that the transaction originally appeared in. Theorem 9. The commitments required by Definition 16 take O(log3 ) space in the height of the full chain. Proof. The commitment contains logarithmically many nodes from the Merkle tree, each of which contain logarithmically many sinking signatures, each of them are logarithmic in size.  4 460  Extensions and Future Research  Multisignature Outputs. We observe that CT rangeproofs can be produced interactively in the same ways that Schnorr signatures can to produce multisignature outputs. Similarly the sinking signatures can be trivially produced in a multiparty way. So support for multiparty signatures, while not addressed in this article, is simply a matter of wallet support and requires no further changes to the system. Payment channels.  Bitcoin’s script system supports off-chain payment channels, which are used  by the Lightning network[PD16] to support unlimited transaction capacity in constant blockchain space. A scaling-focused blockchain design such as Mimblewimble ought to support such a scheme. It is an open problem to produce payment channels that are as ergonomic and flexible as those in Bitcoin, but to show that this ought to be possible, here is an example of a primitive Mimblewimble 470  payment channel. Suppose that a party A wants to send a series of payments to party B of maximum value V . 1. First A creates a spend of V coins to a multisignature output requiring both A’s and B’s signature to sign. A “timelocks” this by taking the highest 0 bit i in the current blockheight h, then asking B to sign a transaction with height h + 2i returning the coins to A. Only after receiving this signature, A publishes the spend. The signing signature construction ensures that such a refund transaction cannot be spent in any block between h and h + 2i . On the other hand, this means that if A wants a locktime of 2i blocks he must wait for a blockheight that is a multiple of 2i+1 to create it. 2. Now that all V coins are in a multisignature output requiring both A’s and B’s signatures to  480  spend (with the coins going back to A after 2i blocks in case of stall), A can send an amount v to B by signing a transaction from this output sending v to B and (V − n) to A. 3. To increase the value v, A simply signs a new transaction with the new value v. B will not sign and publish until the channel is near-closing, at which point B can publish one transaction taking the whole value v, even if it was actually produced over the course of many interactions.  16  Sidechain support.  If Mimblewimble blocks commit to a structure containing all peg-in trans-  actions and all peg-out transactions (in the same way they commit to the UTXO set, except these structures will never shrink), then it is possible for Mimblewimble to be implemented as a pegged sidechain. This would provide a tremendous scaling benefit to its parent chain, since most blockchain 490  transactions are of the simple transfer-of-value sort that Mimblewimble supports, and also reduce the risk to users of Mimblewimble from quantum computers, since it is easy to move coins off of a sidechain. On a technical level, both peg-ins and peg-outs may look like transaction excess values which, instead of signing blockheights, sign an output on the destination chain. Then verifiers add this excess plus as ±vH to the total utxoset value (where v is the value of the output). Working out the details of this, and arguing security, is left as future research, but it appears on a high-level that there are no open problems. Quantum resistance.  Since Mimblewimble depends on the discrete logarithm problem for se-  curity against theft and inflation, it is highly susceptible to attacks by quantum computers. Find500  ing quantum-secure analogues to the primitives used in this paper would allow a quantum-secure Mimblewimble with the same asymptotic scaling properties of the original. Some research in this direction includes: • Pedersen commitments can be replaced by a quantum analogue such as [CDG+ 15]. Note that these commitments are not homomorphic in the strong sense of allowing arbitrarily many commitments to be added, since after a fixed noise threshold the commitments will no longer open — however, this is not a problem since Mimblewimble only requires that (unmerged) transactions sum to (a non-hiding) commitment to zero. • Sinking signatures can likely be replaced by a LWE-based candidate, or a variation of the NIZK proof-of-openings given in the above paper. The only interesting property required of these is that signatures on same value can be added to form multisignatures, which should be  510  algebraically easy to obtain. • The author is unaware of any quantum-secure rangeproofs. • The blockchain commitments, being based on hashes, are already quantum secure, and the compact chain arguments go through unchanged.  5  Acknowledgements  The author would like to thank • Avi Kulkarni for developing the sinking signature construction described in the paper (and breaking the author’s original construction). • Gregory Sanders for asking probing questions about Mimblewimble’s guarantees until a clear 520  picture of its consensus structure appeared.  17  References [Bac02]  A. Back, Hashcash — a denial of service counter-measure, 2002, http://hashcash. org/papers/hashcash.pdf.  [BCD+ 14] A. Back, M. Corallo, L. Dashjr, M. Friedenbach, G. Maxwell, A. Miller, A. Poelstra, J. Timón, and P. Wuille, Enabling blockchain innovations with pegged sidechains, 2014, https://www.blockstream.com/sidechains.pdf. [CDG+ 15] D. Cabarcas, D. Demirel, F. Göpfert, J. Lancrenon, and T. Wunderer, An unconditionally hiding and long-term binding post-quantum commitment scheme, Cryptology ePrint Archive, Report 2015/628, 2015, http://eprint.iacr.org/2015/628. [Jed16]  T.E.  Jedusor,  2016,  Mimblewimble,  Defunct  //5pdcbgndmprm4wud.onion/mimblewimble.txt.  hidden Reddit  service,  http:  discussion  at  https://www.reddit.com/r/Bitcoin/comments/4vub3y/mimblewimble_ noninteractive_coinjoin_and_better/. [KLS16]  A. Kiayias, N. Lamprou, and A.-P. Stouka, Proofs of proofs of work with sublinear complexity, Financial Cryptography and Data Security: FC 2016 International Workshops, BITCOIN, VOTING, and WAHC, Christ Church, Barbados, February 26, 2016, Revised Selected Papers (Berlin, Heidelberg) (J. Clark, S. Meiklejohn, Y.A.P. Ryan, D. Wallach, M. Brenner, and K. Rohloff, eds.), Springer Berlin Heidelberg, 2016, pp. 61–78.  [Max13a] G. Maxwell, CoinJoin: Bitcoin privacy for the real world, 2013, BitcoinTalk post, https://bitcointalk.org/index.php?topic=279249.0. [Max13b]  , Transaction cut-through, 2013, BitcoinTalk post, https://bitcointalk. org/index.php?topic=281848.0.  [Max15]  , Confidential transactions, 2015, Plain text, https://people.xiph.org/ ~greg/confidential_values.txt.  [Max16]  , 2016,  The Blog  first  successful  post,  zero-knowledge  contingent  payment,  https://bitcoincore.org/en/2016/02/26/  zero-knowledge-contingent-payments-announcement/. [Mou13]  Y. M. Mouton, Increasing anonymity in Bitcoin ... (possible alternative to Zerocoin?), 2013, BitcoinTalk post, https://bitcointalk.org/index.php?topic=290971.  [Nak09]  S. Nakamoto, Bitcoin: A peer-to-peer electronic cash system, 2009, https://www. bitcoin.org/bitcoin.pdf.  [Nol13]  T. Nolan, Re: Alt chains and atomic transfers, https://bitcointalk.org/index. php?topic=193281.msg2224949#msg2224949, 2013.  18  [PD16]  J. Poon and T. Dryja, The bitcoin lightning network, 2016, https://lightning. network/lightning-network-paper.pdf.  [Ped01]  T. Pedersen, Non-interactive and information-theoretic secure verifiable secret sharing, Lecture Notes in Computer Science 576 (2001), 129–140.  19  PPCoin: Peer-to-Peer Crypto-Currency with Proof-of-Stake Sunny King, Scott Nadal (sunnyking9999@gmail.com, scott.nadal@gmail.com) August 19th, 2012 Abstract A peer-to-peer crypto-currency design derived from Satoshi Nakamoto’s Bitcoin. Proof-of-stake replaces proof-of-work to provide most of the network security. Under this hybrid design proof-of-work mainly provides initial minting and is largely non-essential in the long run. Security level of the network is not dependent on energy consumption in the long term thus providing an energyefficient and more cost-competitive peer-to-peer crypto-currency. Proof-of-stake is based on coin age and generated by each node via a hashing scheme bearing similarity to Bitcoin’s but over limited search space. Block chain history and transaction settlement are further protected by a centrally broadcasted checkpoint mechanism.  Introduction Since the creation of Bitcoin (Nakamoto 2008), proof-of-work has been the predominant design of peer-to-peer crypto currency. The concept of proof-of-work has been the backbone of minting and security model of Nakamoto’s design. In October 2011, we have realized that, the concept of coin age can facilitate an alternative design known as proof-of-stake, to Bitcoin’s proof-of-work system. We have since formalized a design where proof-of-stake is used to build the security model of a peer-to-peer crypto currency and part of its minting process, whereas proof-of-work mainly facilitates the initial part of the minting process and gradually reduces its significance. This design attempts to demonstrate the viability of future peer-to-peer crypto-currencies with no dependency on energy consumption. We have named the project ppcoin. Coin Age The concept of coin age was known to Nakamoto at least as early as 2010 and used in Bitcoin to help prioritize transactions, for example, although it didn’t play much of an critical role in Bitcoin’s security model. Coin age is simply defined as currency amount times holding period. In a simple to understand example, if Bob received 10 coins from Alice and held it for 90 days, we say that Bob has accumulated 900 coin-days of coin age. Additionally, when Bob spent the 10 coins he received from Alice, we say the coin age Bob accumulated with these 10 coins had been consumed (or destroyed).  In order to facilitate the computation of coin age, we introduced a timestamp field into each transaction. Block timestamp and transaction timestamp related protocols are strengthened to secure the computation of coin age. Proof-of-Stake Proof-of-work helped to give birth to Nakamoto’s major breakthrough, however the nature of proof-of-work means that the crypto-currency is dependent on energy consumption, thus introducing significant cost overhead in the operation of such networks, which is borne by the users via a combination of inflation and transaction fees. As the mint rate slows in Bitcoin network, eventually it could put pressure on raising transaction fees to sustain a preferred level of security. One naturally asks whether we must maintain energy consumption in order to have a decentralized crypto-currency? Thus it is an important milestone both theoretically and technologically, to demonstrate that the security of peer-to-peer crypto-currencies does not have to depend on energy consumption. A concept termed proof-of-stake was discussed among Bitcoin circles as early as 2011. Roughly speaking, proof-of-stake means a form of proof of ownership of the currency. Coin age consumed by a transaction can be considered a form of proof-of-stake. We independently discovered the concept of proof-of-stake and the concept of coin age in October 2011, whereby we realized that proof-of-stake can indeed replace most proof-ofwork’s functions with careful redesign of Bitcoin’s minting and security model. This is mainly because, similar to proof-of-work, proof-of-stake cannot be easily forged. Of course, this is one of the critical requirements of monetary systems - difficulty to counterfeit. Philosophically speaking, money is a form of ‘proof-of-work’ in the past thus should be able to substitute proof-of-work all by itself. Block Generation under Proof-of-Stake In our hybrid design, blocks are separated into two different types, proof-of-work blocks and proof-of-stake blocks.  Kernel input Stake input Stake input  Stake output (pay to stake owner himself)  Figure: Structure of Proof-of-Stake (Coinstake) Transaction The proof-of-stake in the new type of blocks is a special transaction called coinstake (named after Bitcoin’s special transaction coinbase). In the coinstake transaction block owner pays himself thereby consuming his coin age, while gaining the privilege of  generating a block for the network and minting for proof-of-stake. The first input of coinstake is called kernel and is required to meet certain hash target protocol, thus making the generation of proof-of-stake blocks a stochastic process similar to proof-ofwork blocks. However an important difference is that the hashing operation is done over a limited search space (more specifically one hash per unspent wallet-output per second) instead of an unlimited search space as in proof-of-work, thus no significant consumption of energy is involved. The hash target that stake kernel must meet is a target per unit coin age (coin-day) consumed in the kernel (in contrast to Bitcoin’s proof-of-work target which is a fixed target value applying to every node). Thus the more coin age consumed in the kernel, the easier meeting the hash target protocol. For example, if Bob has a wallet-output which accumulated 100 coin-years and expects it to generate a kernel in 2 days, then Alice can roughly expect her 200 coin-year wallet-output to generate a kernel in 1 day. In our design both proof-of-work hash target and proof-of-stake hash target are adjusted continuously rather than Bitcoin’s two-week adjustment interval, to avoid sudden jump in network generation rate. Minting based on Proof-of-Stake A new minting process is introduced for proof-of stake blocks in addition to Bitcoin’s proof-of-work minting. Proof-of-stake block mints coins based on the consumed coin age in the coinstake transaction. A mint rate of 1 cent per coin-year consumed is chosen to give rise to a low future inflation rate. Even though we kept proof-of-work as part of the minting process to facilitate initial minting, it is conceivable that in a pure proof-of-stake system initial minting can be seeded completely in genesis block via a process similar to stock market initial public offer (IPO). Main Chain Protocol The protocol for determining which competing block chain wins as main chain has been switched over to use consumed coin age. Here every transaction in a block contributes its consumed coin age to the score of the block. The block chain with highest total consumed coin age is chosen as main chain. This is in contrast to the use of proof-of-work in Bitcoin’s main chain protocol, whereas the total work of the block chain is used to determine main chain. This design alleviates some of the concerns of Bitcoin’s 51% assumption, where the system is only considered secure when good nodes control at least 51% of network mining power. First the cost of controlling significant stake might be higher than the cost of acquiring significant mining power, thus raising the cost of attack for such powerful entities. Also attacker’s coin age is consumed during the attack, which may render it  more difficult for the attacker to continue preventing transactions from entering main chain. Checkpoint: Protection of History One of the disadvantages of using total consumed coin age to determine main chain is that it lowers the cost of attack on the entire block chain of history. Even though Bitcoin has relatively strong protection over the history Nakamoto still introduced checkpoints in 2010 as a mechanism to solidify the block chain history, preventing any possible changes to the part of block chain earlier than the checkpoint. Another concern is that the cost of double-spending attack may have been lowered as well, as attacker may just need to accumulate certain amount of coin age and force reorganization of the block chain. To make commerce practical under such a system, we decided to introduce an additional form of checkpoints that are broadcasted centrally, at much shorter intervals such as a few times daily, to serve to freeze block chain and finalize transactions. This new type of checkpoint is broadcasted similar to Bitcoin’s alert system. Laurie (2011) has argued that Bitcoin has not completely solved the distributed concensus problem as the mechanism for checkpointing is not distributed. We attempted to design a practical distributed checkpointing protocol but found it difficult to secure against network split attack. Although the broadcasted checkpointing mechanism is a form of centralization, we consider it acceptable before a distributed solution is available. Another technical reason entails the use of centrally broadcasted checkpointing. In order to defend against a type of denial-of-service attack coinstake kernel must be verified before a proof-of-stake block can be accepted into the local database (block tree) of each node. Due to Bitcoin node’s data model (transaction index specifically) a deadline of checkpointing is needed to ensure all nodes’ capability of verifying connection of each coinstake kernel before accepting a block into the block tree. Because of the above practical considerations we decided not to modify node’s data model but use central checkpointing instead. Our solution is to modify the coin age computation to require a minimum age, such as one month, below which the coin age is computed as zero. Then the central checkpointing is used to ensure all nodes can agree upon past transactions older than one month thus allowing the verification of coinstake kernel connection as a kernel requires non-zero coin age thus must use an output from more than one month ago. Block Signatures and Duplicate Stake Protocol Each block must be signed by its owner to prevent the same proof-of-stake from being copied and used by attackers. A duplicate-stake protocol is designed to defend against an attacker using a single proofof-stake to generate a multitude of blocks as a denial-of-service attack. Each node collects the (kernel, timestamp) pair of all coinstake transactions it has seen. If a received  block contains a duplicate pair as another previously received block, we ignore such duplicate-stake block until a successor block is received as an orphan block. Energy Efficiency When the proof-of-work mint rate approaches zero, there is less and less incentive to mint proof-of-work blocks. Under this long term scenario energy consumption in the network may drop to very low levels as disinterested miners stop mining proof-of-work blocks. The Bitcoin network faces such risk unless transaction volume/fee rises to high enough levels to sustain the energy consumption. Under our design even if energy consumption approaches zero the network is still protected by proof-of-stake. We call a crypto-currency long-term energy-efficient if energy consumption on proof-of-work is allowed to approach zero. Other Considerations We modified the proof-of-work mint rate to be not determined by block height (time) but instead determined by difficulty. When mining difficulty goes up, proof-of-work mint rate is lowered. A relatively smooth curve is chosen as opposed to Bitcoin’s step functions, to avoid artificially shocking the market. More specifically, a continuous curve is chosen such that each 16x raise of mining difficulty halves the block mint amount. Over longer term the proof-of-work mint curve would not be too dissimilar to that of Bitcoin in terms of the inflationary behavior, given the continuation of Moore’s Law. We consider it wise to follow the traditional observation that the Market favors a lowinflation currency over a high-inflation one, despite of significant criticism of Bitcoin from some mainstream economists due to ideological reasons in our opinion. Babaioff et al. (2011) studied the effect of transaction fee and argued that transaction fee is an incentive to not cooperate between miners. Under our system this attack is exacerbated so we no longer give transaction fees to block owner. We decided to destroy transaction fees instead. This removes the incentive to not acknowledge other minter’s blocks. It also serves as a deflationary force to counter the inflationary force from the proof-of-stake minting. We also choose to enforce transaction fees at protocol level to defend against block bloating attack. During our research we have also discovered a third possibility besides proof-of-work and proof-of-stake, which we termed proof-of-excellence. Under this system typically a tournament is held periodically to mint coins based on the performance of the tournament participants, mimicking the prizes of real-life tournaments. Although this system tends to consume energy as well when artificial intelligence excels at the game involved, we still found the concept interesting even under such situation as it provides a somewhat intelligent form of energy consumption.  Conclusion Upon validation of our design in the Market, we expect proof-of-stake designs to become a potentially more competitive form of peer-to-peer crypto-currency to proof-of-work designs due to the elimination of dependency on energy consumption, thereby achieving lower inflation/lower transaction fees at comparable network security levels. Acknowledgement Many thanks to Richard Smith for helping out with testing and various network/fork related work. We would like to thank Satoshi Nakamoto and Bitcoin developers whose brilliant pioneering work opened our minds and made a project like this possible. References Babaioff M. et al. (2011): On Bitcoin and red balloons. Laurie B. (2011): Decentralised currencies are probably impossible (but let’s at least make them efficient). (http://www.links.org/files/decentralised-currencies.pdf) Nakamoto S. (2008): Bitcoin: A peer-to-peer electronic cash system. (http://www.bitcoin.org/bitcoin.pdf)  PonzICO: Let’s Just Cut to the Chase Josh Cincinnati @acityinohio https://ponzico.win 0x1ce7986760ADe2BF0F322f5EF39Ce0DE3bd0C82B May 12, 2017  Abstract Token sales—aka “Initial Coin Offerings” where the ‘i’ in Coin is optional— are hotter than Instagram filters on Venus. But to date, nearly all of them have presold non-functional products at ludicrous valuations. The author is offended. Not at the scheme per se—kudos to the developers for discovering a class of rich, pseudo-intellectual suckers willing to value the work they haven’t done at multipliers that would give a private equity partner an aneurysm. But the inefficiency of it all! In this very legitimate, LATEXcomposed, long-planned[1] white paper, the author proposes a method to achieve the same goal with a magnitude less work and presents a productionready Ethereum smart contract address—where production-ready is as at least as code- and concept-tested as The DAO’s Code is Law. As a bonus, the scheme doesn’t require tokens on top of Ethereum, which were a really fun growth-hacky mirage for awhile but ultimately a distraction.[2] As a double bonus, if everyone stays irrational it might greatly enrich the author, providing future funding for blockchain performance art. If that sounds groovy to you—or you just want to support “blockchain performance art,” whatever that means—don’t bother reading further; just invest with your web3 browser at https://ponzico.win/.  1  Contents 1 A Token Introduction: Chuck E. Cheese’s and the Yap People  3  2 Tokens Today: #ScamThePlanet  3  3 ICO-nomics is a Dismal Science  5  4 Let’s Be Real Though, Everything’s a Scam  6  5 I Barely Know Solidity, But Fuck It Let’s Build PonzICO  8  6 Anticipating PonzICO Critiques  10  7 A Dedicated Section on What I’d Do With All My Ether  10  8 Conclusion  11  2  1  A Token Introduction: Chuck E. Cheese’s and the Yap People  Context—and wide margins—are the bedrock of every white paper. To understand the modern crowdsale token, history matters. But correctness doesn’t—any particular liberal retelling of history can suffice. The modern token crowdsale traces its origins to an unassuming corner of San Jose, where in 1977 the first Chuck E. Cheese’s Pizza Time Theatre opened. Here Nolan Bushnell succeeded in swindling youths to trade money for brass tokens of questionable value and aesthetic taste. The tokens were tightly controlled—indeed, the first successful crowdsale netted Nolan something like $41.47 in 2017 dollars, and the “crowd” consisted of one pimply adolescent. But it was the first of many sales, and successes compounded. Soon its success enabled Chuck E. Cheese’s to expand into franchises and pump millions of R&D dollars into the unholy creation of semi-sentient, human-sized animatronic rats. After the creation of Chuck E. Cheese’s nightmarish robo-rat army of darkness, the token crowdsale skeptics were silenced. Whether by the weight of Chuck E. Cheese’s overwhelming success or through surgical assassinations by the elite Munch’s Make Believe Team Six1 is not germane to this discussion. In any event, the future was cast in brass. The Token Crowdsale was here to stay. With the advent of cryptocurrency and the rediscovery of the Yap peoples’ Rai Stones2 , crowdsales would become more decentralized...at least superficially.  2  Tokens Today: #ScamThePlanet  Since the bygone era of exchanging rat tokens for freedom from Bushnell’s gulag arcades, we’ve evolved. The platform is no longer brass controlled by a single entity— but a distributed, decentralized, Web 3.0, blockchain, DLT platform, operating far above the rat race: ETHEREUM. Ethereum itself is rather neat. It can reach 1  https://www.youtube.com/watch?v=o5qfXHtkIWM Rai Stones were unearthed by blockchain consultants to make oblique references in pitch decks and TED talks, and I’m forcing it here under (questionable) advisement. 2  3  consensus on state between different, potentially adversarial parties using clever incentives and cryptography. And yet it’s accessible enough for junior developers to program smart contracts handling millions of dollars. This has posed some problems—the EVM seems most efficient at transpiling developers who make bad Node.js into developers who make bad Solidity. A particular breed of these bad Solidity developers have pioneered the newest iteration of the token crowdsale: “The Initial Coin Offering” (hereafter abbreviated ICO—with all due apologies to the PlayStation 2 cult classic ICO, by all accounts a far less tarnished product). Dozens of teams have unveiled amazing products that have transformed the world. Nearly all lawyers have been replaced by a few lines of Solidity. Venture capitalists have been forced to sell their fourth homes for liquidity. Weathermen and Vegas bookies are trading war stories, wistfully remembering the era when they could just make shit up instead of demurring before decentralized prediction markets. Democracy has melted under the bright light of blockchain governance, petty nationalism is a dying religion, and we are one step closer to a quilted Singularity where every square’s a token. ...at least, that’s how these tokens are being priced. But reality is divorced from this vision—indeed, the destruction of value after most of these scams are revealed will ironically employ more lawyers than Ethereum would otherwise claim to make redundant.[3] You might be saying, “scam is a serious indictment.” Well: • The author would encourage you not to say anything while you’re reading a white paper. You won’t grok as much and you’ll look silly speaking to your iPad. • Vanishingly few of these tokens have any released products tied to them—let alone “disruptive” products—despite increasingly high valuations for increasingly frequent crowdsales. We’re living in a world where developers are Kickstarting Pets.com every week. And their customers are metaphorically preordering the failed Coin card[4], a product with no use beyond its novelty...but it’s an (initial) coin (offering) instead of Coin card. And the coin is valued like Instagram. Inexplicably.3 3  At the time of this white paper’s publication, someone just raised $12.7 million for the TokenCard, which seems to be “like Coin card, but Blockchain.” We are doomed as a species.  4  It seems the only successful product of these token sales is Ethereum, which has shown itself to be an excellent platform for repeated, rapid token sales. But therein lies the cryptorub. Why would developers build tokens on top of Ethereum when Ethereum already has a token called ether ? Unsurprisingly the answer is, “because they can.”  3  ICO-nomics is a Dismal Science  Like a Node.js developer downloading a gigabyte of dependencies to serve a static 20 kilobyte website, if you give a nefarious developer an inch, they’re going to scam people out of billions. These tokens have (pre)sold well, which is encouraging more and more interest, and less and less scruples. But for all their paper success, these tokens are sleight of hand. Nevermind that these tokens get pumped before the presales by even more unscrupulous parties who get in on the ‘pre-pre-sales’—you can imagine this activity like like an after-party in the hotel mezzanine, except it’s a pre-pre party with some mezzanine financing and it ain’t R. Kelly hitting the ignition.4 No, the pre-pumping is Christian Bale-level misdirection in the Prestige. The real magic is when David Bowie gives Huge Jackman an ERC20 template. All of these tokens were exchanged for ether. Even if you opt to trade these tokens for fiat directly—and even if these tokens somehow manage to offer some functionality not possible by ether—trading for ether will always be the most direct, cheapest method of exchange for every ICO’d token. Perhaps you don’t agree? No matter, because all of them require ether to engage in transactions. Cars don’t run without gas5 , and neither do tokens. They are abstractions on top of Ethereum itself, extra layers that obfuscate the truth: Ethereum is a state machine collecting transactions signed by private keys and verified by public keys. Your key pair—and associated address—is what matters, along with the native token balance you hold (i.e., ether). As long as Ethereum transactions require ether-denominated gas, this will not change. 4 5  There’s a crass joke in here. I’m above making it, but not above making you think about it. Although the Tesla I buy after PonzICO’s tremendous success won’t need gas.  5  “But,” you may be saying smugly, “what about tokens with a limited treasury, or tethered to the physical world, like gold or USD? Or compute power? Or um, like, predictive, reputational whosists? They could make an address tied to very little ether worth considerably more.” Well: • What did the author say about speaking while reading an white paper? You are the laughingstock at Starbucks now. • Anything that requires a centralized treasury/authority to dictate a trade in physical property within a decentralized system is going to have the same risk profile as its weakest link: the centralized system. • But say you make it more decentralized, somehow. Maybe through some wild multisig scheme. The author posits that every attempt to increase decentralization will make the trade more expensive, until it reaches Parity6 with simply exchanging ether for physical goods directly—ether is the most maximally decentralized token that can possibly exist on Ethereum. Ergo, while not all Ethereum developers are scammers, most tokens are scams. Especially one where the developers keep 95% of the tokens. The author doesn’t need a decentralized prediction market to tell you that’s got a 99% chance of being a scam.  4  Let’s Be Real Though, Everything’s a Scam  Let us level with one another, reader. The author does not intend to cast disrepute upon these token developers—in reality, they are simply short-circuiting a process that has existed since the dawn of securitization. Most tokens are scams, sure—but then again, so is everything. The Efficient Market Hypothesis is a scam draped with bad math. To play the stock market is to play musical chairs under the chord progression of a bid-ask spread. Individuals may win, but in the long run we’re all dead. And as the average lifetime of companies and products continues to shrink geometrically, the long run isn’t so long anymore. In today’s age, it seems better to promote the plausibility of future profit rather than waste energy on actually delivering. 6  Don’t grimace. You love the wordplay, or you wouldn’t have read this far.  6  Figure 1: The trajectory of every company that has ever existed or will exist, where “Present” corresponds to the time of a company’s peak valuation. It contains no units to bolster its accuracy relative to GAAP accounting and investment banker analysts.  The world of physical capitalism concerned itself with scamming enough people for long enough to eke out a modicum of societal progress. The system functions well enough, though clearly—as evidenced by the continued existence of Chuck E. Cheese’s—it’s far from perfect. But token capitalism constrains the timeline considerably. With freedom to structure token sales in complex, pump-and-dump-friendly terms—and pre-pump marketeers greasing the wheels—token makers can dispense with the illusion of future intrinsic value and build castles in the distributed cloud. The author does not begrudge this behavior. We are all consenting adults here— we can play musical chairs whenever we damn like; it’s what separates us from the children. But the least we can do is exchange honestly, and treat each other with the mutual respect we all deserve. That is why I’m urging you to invest your life savings into my revolutionary new concern, PonzICO.  7  5  I Barely Know Solidity, But Fuck It Let’s Build PonzICO  PonzICO is the first ICO to respect your intelligence by calling a spade a spade. Here’s how it differs from your everyday ICO: • It dispense with tokens entirely—we all want that sweet, sweet ether. • There is no limited time window for “investment.” Time is an illusion. • PonzICO also dispenses with the notion that it needs a meaningful product attached to its future earning potential. It is the product. • All profits from PonzICO are carried in balances linked to prior participants in proportion to their prior investment on a transaction by transaction basis...with a trivial, 50% service fee for yours truly, as nominal recompense for spending a solid couple of days re-learning LATEXand Solidity. That’s it. With every new transaction into the PonzICO invest method, PonzICO immediately balances that ETH to prior participants based on their current proportion of ownership (minus my extremely minor, entirely fair 50% service fee). Upon updating the balances, the new participant is added to the list to receive future disbursements, and PonzICO dilutes everyone else’s stake accordingly. When they are ready to receive their balance, they simply use the withdraw method. Preexisting holders can always put more ETH into PonzICO to reduce their dilution— especially with the convenient reinvest method—at the cost of that transaction being disbursed to current stakeholders. PonzICO elegantly distills an ICO to its barest essentials, and this author believes it will prove more sustainable than any other ICO on the market.7 Still confused? Not sure why, honestly it’s frightfully easy to understand compared to literally every other ICO white paper. But fine, let’s use our favorite sock puppets Alice and Bob. Suppose Alice is the first brilliant investor to PonzICO. She’s conservative, so she sends a paltry 1000 ETH via the invest method. 7  So long as every ETH holder participates at least once and Ethereum’s monetary supply remains ambiguously infinite.  8  Alice Invested 1000 ETH Returned 0 PonzICO Owned 100%  Josh Incalculable Intellectual Energy 500 ETH 0% Because It’s His Gift To The Community  As the first investor, Alice now owns 100% of PonzICO, while the author takes his modest fee—a mere pittance in the grand scheme. Now consider our second investor Bob. Bob isn’t some weakass punk like Alice; he’s a real believer in the transformative power of PonzICO. He sends 3000 ETH.  Alice Invested 1000 ETH Returned 1500 ETH PonzICO Owned 25%  Bob 3000 ETH 0 ETH 75%  Josh Incalculable Intellectual Energy 2000 ETH 0% Like All Those CEOs with $1 Salaries  Despite Alice’s complete lack of faith and her modest investment, she’s already made her money back by collecting Bob’s full investment...minus the author’s very meager fee as PonzICO’s symbolic Keeper of the Flame. But in the process, her stake went from 100% to 25%, and Bob’s ownership went to 75%—a move that will surely reward Bob in the future. At this point Alice could reinvest her returns to reduce her dilution against Bob’s, but that’s up to her. In the mean time, PonzICO is making huge waves, and Carol comes along and invests a whopping 5000 ETH just to show Bob who the fuck is boss.  Alice Invested 1000 ETH Returned 2125 ETH PonzICO Owned 11.1%  Bob 3000 ETH 1875 ETH 33.3%  Carol 5000 ETH 0 ETH 55.5%  Josh Incalculable Intellectual Energy 4500 ETH Amazingly Still 0%  Alice has nearly doubled her investment, while Bob has nearly made his money back. But Carol is in the lead for ownership stake, and her profits will surely make Bob and Alice’s gains look weak by comparison. The author continues to scrape by on purely symbolic fees—but don’t weep for him, it is a simple pleasure to see his creation prosper in this hypothetical world. 9  The author will not dive into the details of the Solidity but rest assured it’s been battle-tested by his web-based IDE and formally verified (that it does, in fact, run in the EVM). You can probably find the ABI somewhere if you’re really curious. The main point here is that it works, and nary a token in sight but ether. Lest you think the author does not believe in his beautiful creation, consider his skin in the game. The author has put a significant portion of his ETH holdings into PonzICO, constituting many trillions of wei, not to mention staking his considerable reputation on this innovative endeavor. If you are concerned about the author’s potentially lucrative fees—you shouldn’t worry. The author has graciously capped his maximum fee disbursement to 100,000 ETH within the contract; after that, he’ll have to participate in PonzICO just like everybody else. Additionally—to prevent less sophisticated investors and encourage the most subtle tinge of pretension— the contract requires at least 0.1 ETH investment per transaction, using a very responsible AccreditedInvestor modifier.  6  Anticipating PonzICO Critiques  ...is an irrelevant section, because there is absolutely nothing wrong with any of this.  7  A Dedicated Section on What I’d Do With All My Ether  Assuming PonzICO returns the maximum amount of fees to the author—a safe assumption, given the many improvements over traditional ICOs—the author expects to earn 100,000 ETH for his considerable efforts to reform the space. While the longterm value of ETH is uncertain, $1000 per ETH is a fair wild-ass guess. That would place his gains somewhere south of $100 million. In the name of full transparency, the author would like to outline his plans for these funds. First, he’d exchange 70% of his ETH into a cryptocurrency with a monetary policy that doesn’t scare the living shit out of him. He’d keep 20% in ETH just in case he’s completely wrong on the whole monetary policy thing. With the final 10%, he’d invest in other ICOs because the VC-codified 70-20-10 10  Pareto Hallucination haunts him like a ghost. After that, he’d probably get a Tesla, maybe a two bedroom condo in San Francisco, and (most expensively) a ticket to Consensus. If there’s any money left (unlikely) he’ll focus on his considerable blockchain research efforts, including the much-anticipated investigative follow-up into Accenture’s shenanigans to make a “reversible blockchain.”[5]  8  Conclusion  The author proposed a continuous, tokenless ICO that dispenses with all the fanciful illusions pervading the space. He demonstrated the soundness of his approach with selective history, a hand-wavy graph, tables, self-referential citations, and vague commitments to show his smart contract code eventually. He anticipated no critiques to this approach, and fully commits to ignoring critics who emerge on social media by labeling them trolls. Still more honest than 99% of the hucksters who have trapped us in this darkest-timeline dystopic token bubble. Please contribute at https://ponzico.win/. In some seriousness, the author will view the fees generated from these disbursements as validation for his blockchain satire, and your generous investment will undoubtedly encourage him to make more. Don’t want him to make more? Fuck you. PonzICO. With win as the TLD, you can’t possibly lose.8 If you’re long on other tokens, there’s absolutely no excuse for skipping on PonzICO. 8  This claim has not been verified by any investment specialists, lawyers, or priests, but is the sole perspective of the author—who practically embodies the classic lone genius archetype.  11  References [1] Cincinnati, ...honestly I may code this. Or write a sardonic manifesto. Both seem productive, https://twitter.com/acityinohio/status/735251297494913026 [2] Cincinnati, The only tokens that matter are native. Abstracted tokens are a mirage, https://twitter.com/acityinohio/status/859159773077413888 [3] Cincinnati, At its current rate of unbridled exuberance, Ethereum will lead to more employed lawyers for post-ICO shenanigans, https://twitter.com/acityinohio/ status/860720787489136640 [4] Cincinnati, One Card Instead of All Your Cards, https://www.youtube.com/ watch?v=0YjIrHZOwf8 [5] Cincinnati, The Upside of the Internet: I Can Correct “The Downside of Bitcoin: A Ledger That Cant Be Corrected”, https://medium.com/@cincinnati  12  Primecoin: Cryptocurrency with Prime Number Proof-of-Work Sunny King (sunnyking9999@gmail.com) July 7th, 2013 Abstract A new type of proof-of-work based on searching for prime numbers is introduced in peer-to-peer cryptocurrency designs. Three types of prime chains known as Cunningham chain of first kind, Cunningham chain of second kind and bi-twin chain are qualified as proof-of-work. Prime chain is linked to block hash to preserve the security property of Nakamoto’s Bitcoin, while a continuous difficulty evaluation scheme is designed to allow prime chain to act as adjustable-difficulty proof-of-work in a Bitcoin like cryptocurrency.  Introduction Since the creation of Bitcoin [Nakamoto 2008], hashcash [Back 2002] type of proof-ofwork has been the only type of proof-of-work design for peer-to-peer cryptocurrency. Bitcoin’s proof-of-work is a hashcash type based on SHA-256 hash function. In 2011, ArtForz implemented scrypt hash function for cryptocurrency Tenebrix. Even though there have been some design attempts at different types of proof-of-work involving popular distributed computing workloads and other scientific computations, so far it remains elusive for a different proof-of-work system to provide minting and security for cryptocurrency networks. In March 2013, I realized that searching for prime chains could potentially be such an alternative proof-of-work system. With some effort a pure prime number based proof-ofwork has been designed, providing both minting and security for cryptocurrency networks similar to hashcash type of proof-of-work. The project is named primecoin. Prime Numbers, An Odyssey Prime numbers, a simple yet profound construct in arithmetic, have perplexed generations of brilliant mathematicians. Its infinite existence was known as early as Euclid over 2000 years ago, yet the prime number theorem, regarding the distribution of prime numbers, was only proven in 1896, following Bernhard Riemann’s study of its connection to the Riemann zeta function. There remain still numerous unsolved conjectures to this day. The world records in prime numbers have been largely focused on Mersenne prime 2p–1, named after French monk Marin Mersenne (1588-1648), due to its long history and importance in number theory, and the fact that modulo 2p–1 can be computed without  Bernhard Riemann 1826-1866  division for the efficient Lucas-Lehmer test. Currently the top 10 largest known primes are all Mersenne primes. Two well-known types of prime pairs are, twin primes, where both p and p+2 are primes, and Sophie Germain (1776-1831) primes, where both p and 2p+1 are primes. Extending the concept of Sophie Germain prime pairs, a chain of nearly doubled primes is named after Allan Cunningham (1842-1928), where Cunningham chain of the first kind has each prime one Édouard Lucas 1842-1891 more than the double of previous prime in chain, and where Cunningham chain of the second kind has each prime one less than the double of previous prime in chain. A variation of the form is known as bi-twin chain, that is, a chain of twin primes where each twin pair basically doubles the previous twin pair. Let’s look at some small examples to better understand these prime chains. 5 and 7 are twin primes, 6 is their center. Let’s double 6, arriving at 12, whereas 11 and 13 are twin primes again. So 5, 7, 11, 13 is a bi-twin chain of length 4, also known as bi-twin chain of one link (a link from twin 5, 7 to twin 11, 13). The bi-twin chain can actually be split from their centers, giving one Cunningham chain of first kind, and one Cunningham chain of second kind. Now if we split through centers 6, 12 of bi-twin chain 5, 7, 11, 13, those below the centers are 5, 11, a Cunningham chain of first kind, those above the centers are 7, 13, a Cunningham chain of second kind. I call the first center, the number 6 in this example, the origin of the prime chain. From this origin you can keep doubling to find your primes immediately adjacent to the center numbers. There are also other prime formations known as prime constellations or tuplets, and prime arithmetic progressions. Of interest to these prime pairs and formations, is that their distribution seems to follow a similar but more rare pattern than the distribution of prime numbers. Heuristic distribution formulas have been conjectured, however, none of their infinite existence is proven (the twin prime conjecture being the most well known among them [Goldston 2009]), let alone their distribution. Efficient Verification of Proof-of-Work In order to act as proof-of-work for cryptocurrency, the work needs to be efficiently verifiable by all nodes of the network. This would require the primes not to be too large, such as record-breakingly large. It then precludes Mersenne primes and leads to the use of prime chain as primecoin’s work, since finding a prime chain gets exponentially harder (with our current theoretical and algorithmic understanding) as the chain length increases, yet verification of a reasonably sized prime is efficient. So for the primecoin design three types of prime chains are accepted as proof-of-work: Cunningham chain of first kind, Cunningham chain of second kind, and bi-twin chain. The primes in the prime  Pierre de Fermat 1601-1665  chain are subject to a maximum size protocol in order to ensure efficient verification on all nodes.  Paul Erdıs 1913-1996  The classical Fermat test [Caldwell 2002] of base 2 is used together with Euler-Lagrange-Lifchitz test [Lifchitz 1998] to verify probable primality for the prime chains. Note we do not require strict primality proof during verification, as it would unnecessarily burden the efficiency of verification. Composite number that passes Fermat test is commonly known as pseudoprime. Since it is known by the works of Erdös and Pomerance [Pomerance 1981] that pseudoprimes of base 2 are much more rare than primes, it suffices to only verify probable primality.  Non-Reusability of Proof-of-Work Another important property of proof-of-work for cryptocurrency is non-reusability. That is, the proof-of-work on a particular block should not be reusable for another block. To achieve this, the prime chain is linked to the block header hash by requiring that its origin be divisible by the block header hash. The quotient of the division then becomes the proof-of-work certificate. Block hash, the value that is embedded in the child block, is derived from hashing the header together with the proof-of-work certificate. This not only prevents the proof-ofwork certificate from being tampered with, but also defeats attempt at generating a single proof-of-work certificate usable on multiple blocks on the block chain, since the block header hash of a descendant block then depends on the certificate itself. Note that, if an attacker generates a different proof-of-work certificate for an existing block, the block would then have a different block hash even though the block content remains the same other than the certificate, and would be accepted to the block chain as a sibling block to the existing block. Block header hash is subject to a lower bound so performing hashcash type of work is of no help to prime mining. Varying nonce value generally does not help with prime mining, as prime mining is done typically by fixing the block header hash and generating a sieve. In one case, varying nonce and finding a block header hash that is divisible by a small primorial number – the product of all primes smaller than a given prime p – can help only slightly. It allows the prime miner to work on somewhat smaller primes, like maybe a few digits shorter, for prime numbers of typically 100 digits, only a very small advantage. Difficulty Adjustability of Proof-of-Work One of Bitcoin’s innovations is the introduction of adjustable difficulty. This allows cryptocurrency to achieve controlled minting and relatively constant transaction  processing capacity. The advent of GPU mining and later ASIC mining of SHA-256 hashcash proof-of-work did not impact its inflation model exactly due to this mechanism. Of course, hashcash’s linear difficulty model made it easy. For prime proof-of-work, it is not apparent how this could be achieved. Initially I thought about using prime size as an indicator of difficulty. However, a non-linear difficulty curve would negatively impact block chain security. Also, using prime size as difficulty indicator would interfere with efficiency of verification. Eventually I discovered that the remainder of Fermat test could be used to construct a relatively linear continuous difficulty curve for a given prime chain length. This allows primecoin to largely keep the security property of bitcoin. Let k be the prime chain length. The prime chain is p0, p1, …, pk-1. Let r be the Fermat test remainder of the next number in chain pk. Now pk/r is used to measure the difficulty of the chain. Even though the distribution of r/pk is not strictly uniform, but experiments have shown that the difficulty adjustment behavior is reasonably good in practice. The prime chain length is then computed with a fractional length part: d = k + (pk-r)/pk Note if pk passes probable primality tests then it should be considered as a chain of higher integral length. A continuous length target adjustment is employed with similar features to the difficulty adjustment in ppcoin [King 2012]. Length target is stepped up or down through integral boundaries during length target adjustment, at fixed step-up/step-down threshold of 255/256 <-> 1. Main Chain Protocol In bitcoin, main chain protocol ensures that block chain consensus can be reached as long as more than half of the network mining power reaches consensus. Conversely, an attacker needs more than 50% of total network mining power to control block chain. This security property depends on the linear difficulty model of hashcash. In primecoin, it is slightly weakened as the difficulty model is not strictly linear, so an attacker may only need somewhat less than 50% of total network mining power through manipulation of difficulty. At integral length boundaries, a constant ratio is introduced to approximate the ratio of difficulties between prime chains with length difference of 1. The level of block chain security is dependent on the accuracy of this estimate. As the state of art of prime mining progresses in primecoin network, this ratio should be adjusted as needed to ensure better security. Minting Model Primecoin is designed as a pure proof-of-work cryptocurrency, to complement the proofof-stake design of ppcoin. Primecoin’s proof-of-work mint rate is determined by difficulty. This approach was first experimented in ppcoin. The scarcity of the currency is  not ensured by a fixed cap as in bitcoin, but regulated by Moore’s Law via mining hardware advances and by algorithmic improvements. This design is a more natural simulation of gold’s scarcity. Moreover, pure proof-of-work cryptocurrency depends on the mining market for its security. Network mining income, the sum of all miners’ income, is a direct measurement of the level of block chain security across competing pure proof-of-work cryptocurrency networks. A fixed cap scarcity model relies heavily on transaction fees to sustain network security. However a higher transaction fee reduces the competitiveness of a cryptocurrency as payment-processing network. Since last year, bitcoin’s share of network mining income has shrunk much faster than its capital market share. Basically, for a pure proof-of-work design, it’s not realistic to expect all three goals to sustain: high network security, low inflation and low transaction fee. This topic has been explored in ppcoin paper, however it would become more evident as the competition intensifies in cryptocurrency market and bitcoin’s inflation rate drops further. As Moore’s Law approaches its limit, primecoin inflation rate would taper off and gives a slower drop toward zero. There is still good scarcity property similar to gold while network security is maintained without the need to raise transaction fee. The inflation in primecoin is designed to drop slower than ppcoin’s proof-of-work minting, to compensate for the need of sustained energy consumption of pure proof-of-work cryptocurrencies. Conclusion Primecoin is the first cryptocurrency on the market with non-hashcash proof-of-work, generating additional potential scientific value from the mining work. This research is meant to pave the way for other proof-of-work types with diverse scientific computing values to emerge. Around the time of this paper, several new cryptocurrencies have been released adopting other hash functions or composition of hash functions for hashcash proof-of-work. It appears there are market forces at play such that diversification of proof-of-work types is inevitable. It could prove difficult for any single type of proof-of-work to maintain dominance in the long term. I would expect proof-of-work in cryptocurrency to gradually transition toward energy-multiuse, that is, providing both security and scientific computing values. Acknowledgement I would like to thank Satoshi Nakamoto and Bitcoin developers whose brilliant pioneering work in Bitcoin made this project possible.  References Back A. (2002): Hashcash – A denial of service counter-measure. (http://www.hashcash.org/papers/hashcash.pdf) Caldwell C. (2002): Finding primes & proving primality. (http://primes.utm.edu/prove) Goldston D. A. (2009): Are there infinitely many twin primes? (http://www.math.sjsu.edu/~goldston/twinprimes.pdf) King S., Nadal S. (2012): PPCoin: peer-to-peer crypto-currency with proof-of-stake. (http://ppcoin.org/static/ppcoin-paper.pdf) Lifchitz H. (1998): Generalization of Euler-Lagrange theorem. (http://www.primenumbers.net/Henri/us/NouvTh1us.htm) Nakamoto S. (2008): Bitcoin: A peer-to-peer electronic cash system. (http://www.bitcoin.org/bitcoin.pdf) Pomerance C. (1981): On the distribution of pseudoprimes. Mathematics of Computation Volume 37 Number 156 OCT 1981  Page 1 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Table of Content 1.  Our Vision: ”Cryptocurrencies for Everyone!” ..................................................................... 4  2.  Our Story .............................................................................................................................. 9  3.  4.  2.1.  This is savedroid ........................................................................................................... 9  2.2.  Crypto Services ........................................................................................................... 13  2.3.  Partner Ecosystem ...................................................................................................... 16  2.4.  Team ........................................................................................................................... 17  2.5.  Media coverage........................................................................................................... 28  2.6.  Awards ........................................................................................................................ 28  The savedroid Token - SVD ................................................................................................ 29 3.1.  Value and Strategy ...................................................................................................... 29  3.2.  Token Supply and Specification ................................................................................. 32  3.3.  Benefits of SVD Usage for Users and within the Ecosystem ..................................... 33  3.4.  SVD Price Variations ................................................................................................... 36  The Initial Token Sale ......................................................................................................... 38 4.1.  Token Issuance ........................................................................................................... 38  4.2.  savedroid ITS Dates .................................................................................................... 39  4.3.  SVD Distribution and Allocation Scheme ................................................................... 41  4.4.  How to Buy SVDs......................................................................................................... 44  4.5.  Bounty Programs ........................................................................................................ 44  4.6.  Transparency and Trust .............................................................................................. 45  5.  Status quo & Roadmap ...................................................................................................... 47  6.  Financial Model .................................................................................................................. 53  7.  Acknowledgements ............................................................................................................ 55  Legal Note ................................................................................................................................. 56  Page 2 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Page 3 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  1. Our Vision: ”Cryptocurrencies for Everyone!” savedroid, the award-winning German FinTech specialized in artificial intelligence (“AI”) saving technology, has geared up to democratize cryptocurrencies: savedroid will create a unique AI fueled ecosystem of crypto saving and investing for the masses. Users will profit from easy access to AI-based crypto saving plans in Bitcoin, Ethereum, Bitcoin Cash, Ripple, Litecoin, IOTA, Dash, Stellar, etc. and superior crypto investment opportunities, such as portfolios, derivatives, and ICOs, without any technological adoption barriers. savedroid eliminates today’s complex and painful processes by creating a smart User Experience (“UX”), which conveniently auto-converts savings to cryptocurrencies, securely stores them, and makes them easily accessible, tradeable, and spendable. Hence, savedroid is the first German ICO1 driving crypto inclusion. savedroid will leverage its existing core assets of state-of-the-art AI technology and great UX based on maximum simplification and emotional gamification to deliver this exceptional value proposition. In fact, savedroid is the very first ICO of a German stock corporation that follows German legislation and regulation to provide participants with the highest level of security. So, join us in making the crypto world accessible for the masses and finally democratize financial services: Give power to the people!  savedroid is committed to a clear roadmap leveraging its existing core assets: TODAY  AI platform for saving and spending in fiat: Self-learning AI algorithm analyzing and optimizing users' personal finances to fulfil their wishes and maximize their lifestyle with 200K+ app downloads, 10M+ saving transactions, 4.2 app rating and 97% recommendation rate based on great UX, renowned B2B-Partners such as Deutsche Börse, Wirecard, etc., and an  Initial Coin Offering - although savedroid will offer a token and not a coin, we use ICO as a more common synonym in our communication. In this Whitepaper we will usually use the more correct technical expression Initial Token Sale (“ITS”). 1  Page 4 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  experienced diverse team of AI, UX, Tech, Finance, and Marketing experts. 2018  Self-learning AI algorithm enabling convenient crypto saving in Bitcoin, Ethereum, Bitcoin Cash, Ripple, Litecoin, IOTA, Dash, Stellar, etc., providing easy access without technological barriers.  2019  Self-learning AI algorithm enabling smart saving and spending in and across cryptocurrencies, providing hassle-free crypto trading and payments.  2020  Self-learning AI algorithm enabling smart investments in cryptocurrency portfolios, futures, and ICOs, providing cost-efficient and diversified returns.  Following this roadmap savedroid brings crypto saving and investing to the masses. Thereby, savedroid consistently extends its value proposition of enabling users to maximize their lifestyle and fulfil their wishes by saving more and spending less without even thinking about it. Through expanding the savedroid ecosystem to cryptos, savedroid finally provides average mass market users – like our existing user base – with easy access to cryptocurrencies and enables them to participate in the huge crypto opportunity to fulfil their wishes even faster.  Cryptocurrencies offer many advantages: They are fully decentralized and, hence, no bank can charge high fees and no government can take them away, they are fast and offer immediate settlement, and they are also very safe, just to name a few.2 There is only one problem: complex technological adoption barriers for average mass market users. Therefore, the world needs a crypto simplifier!  2  https://decentralize.today/5-benefits-of-cryptocurrency-a-new-economy-for-the-future-925747434103  Page 5 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Currently, the crypto world is like a deep jungle for many who have not grown up with Bitcoin, wallets, tokens, etc. The incredible hype around Bitcoin has raised many people’s attention. But, still, the adoption barriers for mass market users are extremely high. Just think about the cumbersome selection of a crypto exchange, the registration, selecting and maintaining a wallet, and storing and handling private keys. Honestly, the user experience along the entire user journey completely sucks – and, even worse, it keeps many from getting into cryptocurrency investments. No doubt, there are great crypto companies with cool products out on the market. But we at savedroid firmly believe that dealing with cryptocurrencies has to become much easier in order to reach the next growth step and make cryptos available to the masses. That’s why we are upgrading our mobile app to make cryptocurrency saving and investing just as simple as taking a photo. Our mission is to eliminate existing adoption barriers and make cryptocurrencies accessible for everyone. savedroid will offer smart and convenient crypto saving and investing. Absolutely no need for separate crypto exchanges, personal wallets, and private key handling. We aim to reduce the complexity of crypto savings and investments to a level that the mass market already knows and accepts when dealing with personal finances. This is simplification to the max. We call it the democratization of cryptocurrencies.  savedroid is the right player to make it happen: Founded in Q4 2015 in Frankfurt/Germany, savedroid has built a proven track record of its capabilities to simplify traditional financial savings processes and drive mass market adoption. savedroid has successfully launched Europe’s first AI-based savings app, which has generated more than 200K+ downloads and more than 10M saving transactions since its launch in October 2016. savedroid creates significant economic value for mass market users through offering automated savings, i.e., a self-learning AI algorithm analyzing users’ checking accounts to identify the savings potential and situational relevant spend optimization, e.g., a self-learning AI algorithm identifying contract optimization potential for utilities, mobile, banking, insurances etc.  Page 6 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  And this is savedroid’s “secret” sauce: Of course, people know that it would make sense to put money aside – in theory. Great wishes do need financing. But in real life, saving money is boring and difficult and it has another major disadvantage: putting money aside for a future goal prevents people from buying a lot of little goodies right on the spot. No good. So, in the end, saving money doesn’t feel like a cool thing to do, especially not for younger people. Then there is savedroid. We are changing this by connecting the saving transaction with users’ individual lifestyle, using our AI algorithm. Our approach has three important parts: Firstly, we make it easy and allow for micro savings. Secondly, we automate it, so that people don’t have to think about it each time they put money aside. Thirdly, we connect savings to unique events users personally like or dislike so that saving becomes emotional and cool. savedroid’s award-winning AI algorithm is based on self-learning “if-this-thenthat” rules, called smooves. For the start, users define personal saving goals called “wishes” (e.g., their next summer vacation, a new smartphone or the latest sneakers), things they want to purchase but can’t afford right now. Then users define personalized events that trigger saving transactions. To give an example, users can set the smoove: Save €5.00 every time I go running for at least 3 kilometers. Since the savedroid app is connected to the user’s fitness tracker, it notices every time you have been for a run. And since the savedroid app is connected to your bank account as well, it transfers the money to your savedroid savings account – just like that, fully automated and without the users needing to do anything. Over time, savedroid’s AI learns more about the individual user preferences and suggests personalized smooves to further optimize users’ saving and spending habits and enable them to fulfil their wishes even faster. In a nutshell, savedroid’s AI makes saving money easy, automated, and cool. And that is what we will now transfer to cryptocurrencies as well. savedroid will transfer the DNA of simplifying crypto saving and investing to provide average mass market users with easy access. savedroid will thereby support the growth of the crypto community and do its part to make cryptocurrencies the next big thing in our economy. Page 7 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Please also have a look at our “What’s savedroid’s vision?” video:  https://youtu.be/YRDK5aqiP1U  Page 8 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  2. Our Story 2.1. This is savedroid Within only 2 years, savedroid has already left its mark in the German FinTech space. We have built an award-winning, cutting-edge AI savings ecosystem that is available for free on Android and iOS in Germany. The app is targeting a rather younger user group - 90% between 18 and 44 years - and has registered more than 200K downloads and multiple millions of saving transactions have been processed so far. We have created enormous economic value for our mass market users. And high customer satisfaction – just have a look at our app store ratings:  It is savedroid’s goal to help people with putting money aside in a dedicated savings account, using personalized AI based savings rules. These rules — also known as “smooves”, a contraction of “smart saving move” — allow users to turn their everyday habits, such as exercising, checking their smartphone, using social media apps or shopping, into pure savings. savedroid has solved the challenge for common people to save money - it provides smart and fully-automated micro-savings and even spend optimization – using a self-learning algorithm with a convenient UX understandable for everybody. Thereby, we successfully democratized everyday savings. savedroid was selected to be a member of the Deutsche Börse Fintech Hub and Deutsche Börse Venture Network – a program run by one of the world’s leading stock exchanges to prepare selected growth companies for an IPO.  Page 9 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Figure 1: Overview of savedroid  2.1.1. Company History savedroid was founded as a stock corporation (Aktiengesellschaft) in 2015 as part of the Unibator program of the Goethe University Frankfurt/Germany – the financial center of the Euro zone. Since then savedroid has raised more than €3M in equity from renowned investors such as the Investment and Economic Development Bank of Rheinland-Pfalz (ISB), 360T Group Managing Partner Alfred Schorno, Infosys Germany Founder & former CEO Debjit D. Chaudhuri and Traxpay Founder Dr. Michael Rundshagen etc. Just a few months after its foundation savedroid already closed a partnership deal with Wirecard Bank and delivered the first mobile app with a fully deposit secured savings account. Not much later, savedroid launched a credit card so that saved money can be spent easily to purchase the desired wishes. Page 10 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  In 2017, savedroid closed a growth funding round that helped to boost its customer base to 200K downloads and more than 10M processed savings transactions. savedroid now employs a team of 20 professionals.  Figure 2: Age distribution of savedroid’s user base  2.1.2. Core Products AI-based Saving Unlike most FinTech companies, savedroid targets the average consumer who does not want to spend his time doing financial research. savedroid does not address a small elite group of hardcore financial experts but the vast majority of mobile users (yes, it is an immense market potential). savedroid’s users are looking for an easy-to-use, lifestyle-oriented and even fun way of micro-saving to fulfill their wishes, like their next vacation or new notebook. And who does not need to save some money for bigger ticket purchases? Page 11 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  But how does it work? Based on our AI technology, so called “smooves” turn the user’s everyday activities into automated savings, e.g.: You go for a run – savedroid saves €5.00 for you, you hit the snooze button – we save €0.50 for you, you buy at Amazon – we save 5% for you … and many more. Via our partner bank Wirecard, we offer every user a free savings account. With every triggered smoove, a small sum is transferred to the savings account, so that the saved amount grows day by day. And the system is working: multiple millions of savings transactions have been triggered through the savedroid app and multiple millions of euros have been set aside to achieve our users’ individual wishes and maximize their lifestyle. Optimization of Spend savedroid is not only about “saving money” in the meaning of “putting money aside” but also in the meaning of spending less for everyday costs. Since savedroid’s algorithms have insight into what kind of transfers the user is making from his bank account, it can identify what contracts he is paying for. Using this information, savedroid’s self-learning AI algorithm directly suggest tailor-made offers which are cheaper or provide better value for money. Is there a less expensive mobile phone contract available than the one that the user currently holds, meeting the same or even better conditions? A better electricity or gas provider? A cheaper credit card? If so, the user will receive a notification together with an invitation to switch to the new provider, all in-app. This way, savedroid is not only a micro-savings simplifier but also a personal finance optimizer. And it is all based on the latest artificial intelligence technology!  Page 12 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Please also have a look at our “Who is savedroid?” video:  https://youtu.be/Ez1AaZhGtn4  2.2. Crypto Services We will transfer savedroid’s successful and value-adding products into the cryptocurrency world to enable smart and convenient crypto saving and investing. In order to eliminate the adoption barriers and make cryptocurrencies accessible for everyone, savedroid will provide all the tools necessary for its users: tools to purchase cryptocurrencies, to store cryptocurrencies, to spend cryptocurrencies, and to invest in cryptocurrencies. Of course, the popular smooves will also be available so that users can not only save automatically in fiat but directly in cryptocurrencies without any further hassle! The final design of the following crypto tools is at savedroid’s sole discretion and may be adjusted or not offered at all depending, among other factors, on new technical trends, user demand, availability of business partners, and changes in the regulatory environment. Page 13 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  2.2.1. Convenient crypto saving savedroid will extend its proven self-learning AI saving algorithm from fiat to cryptocurrencies. App users will not have to worry about selecting and registering at a crypto exchange, selecting and maintaining a compatible wallet, and handling private keys. After registering in the savedroid app, users can start saving in cryptocurrencies (Bitcoin, Ethereum, and many other Altcoins) right away. They will automatically receive a virtual, fully secured wallet and all fiat funds that the user wants to covert to cryptocurrencies will be converted magically in the background to the selected cryptocurrency. The crypto saving targets all current savedroid users and young to middle-age mobile app users who would like to start with cryptocurrencies and value a great user experience (UX) over a large set of customization options or technical details. So savedroid will eliminate all currently existing technological adoption barriers and will finally provide typical mass market users – like our existing user base – with easy access to cryptocurrencies.  2.2.2. Smart saving and spending in and across cryptocurrencies savedroid’s self-learning AI algorithm will enable users to smartly switch their savings between cryptocurrencies and to also conveniently spend their crypto savings to fulfill their desired wishes at their preferred merchants. Users can, therefore, trade cryptocurrencies at their fingertips without the need to worry about cumbersome crypto exchange orders or outrageous bid-ask spreads. The goal is to build an ecosystem where users can swap cryptocurrencies within the savedroid community at the lowest cost and best UX available. Also, to make it easier to spend cryptocurrencies in shops that do not yet offer payment in cryptocurrencies, savedroid will offer its users a credit card, the “cryptocurrency credit card”, from a major card scheme, to purchase their wishes. This will significantly extend the functionalities of savedroid’s existing fiat credit card. Whenever the user makes a purchase with the cryptocurrency credit card, his cryptocurrencies will be auto-converted to fiat and transferred to the merchant.  Page 14 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  The easy crypto trading as well as the cryptocurrency credit card can also be connected to users’ external wallets and is thus open for a much broader audience. Hence, both the crypto trading and the cryptocurrency credit card do not only target the savedroid crypto saving users but also more experienced cryptocurrency users.  2.2.3. Smart investments in cryptocurrency portfolios, derivatives, and ICOs To provide cost-efficient and diversified return opportunities, savedroid will enable users to easily invest their savings in cryptos. Therefore, savedroid will upgrade its AI and offer a self-learning algorithm to invest in cryptocurrency portfolios, derivatives, and ICOs directly in the savedroid app. Hence, users can diversify their portfolio in multiple cryptocurrencies, crypto derivatives, and ICO coins and tokens and profit from the best risk-return profiles available on the market. This will complement savedroid’s exceptional crypto value proposition with state-of-the-art crypto investing and cater for the needs of even the most advanced crypto users.  With this three-staged roadmap, we will consistently leverage our existing core assets of latest AI technology and great UX to build an AI fueled crypto ecosystem to massively simplify the usage of cryptos from everyday micro savings over trading and payments to return optimized investing. Below the line, the savedroid offering will drive crypto adoption by average mass market users and, thus, help grow the crypto community even faster and bigger.  Page 15 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  2.3. Partner Ecosystem savedroid has built a very strong ecosystem of renowned partners3, many of them leaders in their respective field of operations. With these partners, savedroid can not only offer the best services to its users but can also benefit from the reach, trust and reliance of its strong partners. We choose our partners based on capability of innovation, implementation speed, reputation and user focus. Let’s start with Deutsche Börse. As operator of the trading platform Xetra and the Frankfurt Stock Exchange, Deutsche Börse calculates the DAX share index and is listed with their shares on the DAX. When the Deutsche Börse opened its Fintech Hub in Frankfurt in 2016, savedroid was one of the first tenants. Together with four other startups, savedroid uses the infrastructure of the incubator. This includes access to the network of Deutsche Börse AG. savedroid is thereby part of the Deutsche Börse Venture Network with over 400 members from all over Europe. The Deutsche Börse brings together young growth companies and investors on this platform. Since its founding in 2015, approximately 1.4 billion US dollars of investment were collected via the venture network. The smart saving app by savedroid is implemented with the support of the Wirecard Bank. Wirecard manages the technical and banking law requirements for the savedroid savings account, into which the users of the app, with the help of the so-called “smooves”, transfer money. Wirecard, based near Munich, is a publicly-traded international technology and financial services company with approximately 4,000 employees, over one billion euros turnover and a market capitalization of more than 10 billion euros. Wirecard is one of the top five stocks on the German technology stock index TecDAX. Another savedroid partner in the top five of the TecDAX is the telecommunications company Drillisch, located near Frankfurt. Drillisch is one of the largest high-flyers on the German stock market and its share price has increased by a factor of 80 since 2009.  https://medium.com/@ico_8796/ico-candidate-savedroid-part-of-a-strong-network-with-well-knownnames-b26f0d941f6c 3  Page 16 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  The company is one of the most aggressively priced discount cell phone providers in Germany and is active with the brand yourfone. For savedroid, Drillisch comes into play when recommending more affordable cell phone services to users of the app. Our second partner for the optimization of cell phone contracts is Freenet, a company with nearly 5,000 employees and more than three billion euros turnover. When it comes to the optimization of electricity contracts, savedroid cooperates with Check24. Check24 is Germany’s largest Internet comparison portal with a turnover of more than EUR 500 million. savedroid will release an all new feature in the next weeks that allows to convert savings in Amazon gift cards to get discounts on Amazon and thus realize wishes even earlier.  2.4. Team savedroid's team combines long-time industry experience in AI, UX, Tech, Finance, and Marketing with the right start-up mindset and user centricity. With this skill set, savedroid can deliver financial innovations faster and in a particularly user-friendly way which is unmatched so far.  Page 17 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  2.4.1. Management Board Dr. Yassin Hankir – Founder & CEO Yassin is a FinTech entrepreneur and enthusiast. He initiated savedroid, is co-host of the FinTech Meetup Frankfurt and used to be a co-founder of the robo-advisor vaamo. Before becoming an entrepreneur, he worked as Engagement Manager at McKinsey & Company, Inc. for four years. During his consulting job, his focus was on retail and private banking, especially growth strategies, product development and implementation, as well as marketing and distribution. Yassin completed his master in Economics and his Ph.D. in Finance at the Goethe-University Frankfurt/Germany.  Marco Trautmann – Founder & COO Marco is a strategy and banking IT professional. Before co-founding savedroid, Marco has already developed and launched digital business models and products at McKinsey & Company Inc. as a management consultant. Through his time as project manager at Accenture, Marco proved deep expertise in implementing technology projects. Marco graduated from EBS University, Germany as M.Sc. in Business Administration and from University of Pittsburgh, USA as M.Sc. in Management of Information Systems. As a student, he already gained his first experience as an entrepreneur by founding an IT Consulting partnership specialized in real estate valuations.  Tobias Zander – Founder & CTO Tobias is an entrepreneur and CTO. Previously, he was well regarded as a freelancer and the CTO of Sitewards, an e-commerce specialist in Frankfurt/Germany. He has built up development teams that thrive at the cutting edge of web development. With passion to inspire people, he takes part in and speaks at conferences worldwide. He also publishes articles in a wide range of magazines, several blogs and has written books about web security.  Page 18 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Please also have a look at our “Meet the savedroid Founders” video:  https://youtu.be/-CbY3iQR0YU  Michael “Mike” Fehse – Chief Data Officer Mike held various IT executive positions in life science, telco, banking and software during his 35+ years career. He led Arago Automation as CEO and Accretive Tech as CTO for more than four years. Before, Mike was Chief Scientist and VP in Telco, Pharma & Banking at T-Systems, Fresenius and others. Furthermore, he served as evaluator and audit reviewer of EU framework programs for the European Commission. He is always curious, has a passion for AI and will join savedroid in January as our Chief Data Officer. In this position, he will build out the AI department to enhance the analytics and machine learning capability even further.  Jan Gabriel Pleser – Chief Marketing Officer Jan is CMO of savedroid. After completing his studies in economics, he was drawn to the advertising industry, where has been working for more than 12 years for several wellregarded agencies, such as Publicis, and as a freelancer. As a Senior Brand Consultant, he most recently advised major brands, such as Deutsche Bank, Procter & Gamble and Nestlé, on brand and digital strategy.  Page 19 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  2.4.2. Team of Experts IT-Team Matthew Kirschnick - Senior Product Owner Matt is Senior Product Owner at savedroid and works with the team to set feature priorities and translate business requirements into a language developers can understand! Matt studied computer science and political science in Canada and worked for more than 15 years in game development at Electronic Arts.  Sebastian Hoffmann – Smart Contract Developer Sebastian is an advisor with several years of experience in strategy consulting, dealing with challenges and impacts of digitization at CapGemini. Besides this, he is an entrepreneur in the AI and crypto space. Sebastian holds a M.Sc. degree in Information Science from University of Amsterdam, having studied in Germany, China and the Netherlands.  Michael Ryvlin – Senior Backend Developer Michael is our backend developer from the beginning and built up and supports the entire backend of the app and the server and security structure. Before working at savedroid, he spent more than 10 years in the e-commerce environment. Michael, who is fluent, in Russian is our go-to-person for any business with Eastern Europe and Russia.  Patrick Täufer – Mobile Developer Patrick is an iOS and Android developer at savedroid. After completing his studies as a computer scientist and his Media Informatics degree, Patrick has spent almost 5 years in an agency and doing consulting. In addition to his job at savedroid, he travels a lot and is a passionate serial fan.  Patrick Gotta – UX Designer After completing his apprenticeship as IT specialist for application development at Deutsche Telekom, he studied Media System Design at Darmstadt University of Applied Sciences. Already during and especially after his studies, he has gained more than 9 years of experience in planning, conception and quality assurance of complex media and software projects in several projects and design agencies, thus training his interdisciplinary way of working and thinking. For almost two decades, the Web has been his second home and like his own, he makes it a little more pleasant every day.  Page 20 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Lennart Czienskowski – UX Designer Lennart is a passionate user experience designer and technology fan with an eye for the essentials. He is an expert in making a tough topic a piece of cake. He studied Interactive Media Design at Darmstadt University of Applied Sciences and has worked for the last seven years in various agencies before and during his studies at Deutsche Telekom and at the Max Planck Institute for Educational Research in Berlin.  Michaela “Ela” Wenner – Art Director As a passionate designer, almost non-stop everything revolves around the visual. However, anyone who believes that Ela only pays attention to superficial things is deceptively wrong. A good portion of facts with which you can knit a grandiose concept is the A and O of every design start. While working for the last seven years in large as well as in various small agencies and design offices, she gathered her expertise in a wide range of design topics, such as corporate identity, editorial design, analogue photography, re-drafting, conception, project management and analytical thinking.  Alla Brodski-Guerniero – Data Scientist Alla takes care of our data analysis and forecasts - for example she develops our algorithms that provide users with tips for the best smooves to fulfill their wishes. In addition to her work at savedroid, Alla also works on her Ph.D. in the field of neuroscience. For more than five years, she has explored what happens in our brain when information from memory affects our perception.  Gautam Kumar Pramanik – Data Scientist Gautam is currently supplementing the data science team. He contributes in data extraction, sorting, and analysis for providing tips to the savedroid app users for the best smooves. Apart from his work at savedroid, he is also involved in scientific research in the field of Multiple Sclerosis (MS). He is working on understanding network behavior of the brain at the early phase of MS. In his free time, he reads novel, hikes and travels to different places.  Page 21 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Business-Team Lucien Tavano – Vice President Internationalization Lucien – actually living in Paris - is responsible for our internationalization strategy. During the ITS, he coordinates our communication strategy and is one of our international community managers. Prior to joining savedroid, Lucien founded the fintech startup OhMyGeorg!, building forex trading apps for the B2C market. Before that, he worked for 5 years at Airbus as a business and mechanical engineering graduate, doing business development in Asia, flying more than 100+ times per year!  Alex Hofmann – Head of Business Development & HR Alex is Head of Business Development & HR with a degree in psychology, for which she has researched the reward systems of the human brain. Her specialty is to question things, to motivate others, and to generate strategies and ideas. With her passion for inspiring others, Alex gained new talents in recruiting and employer branding for Ferrero and helped shape the internal communication of the Nutella empire before joining savedroid. Now she brings new users for savedroid, ambitious team members - all savedroid enthusiasts - on board!  Lydia Witzmann - Digital Marketing Manager Lydia has been our Digital Marketing Manager since January 2017 and she is primarily responsible for social media management. Before savedroid, Lydia gained experience in project management at BMW, Jung von Matt, Tchibo and Gruner + Jahr (stern.de) and in ecommerce at Topshop / Topman in London in the last five years.  Sina Reubelt– Blockchain Partner Management Sina is working as Blockchain Partner Manager at savedroid. As such, she is aiming to establish a valuable network of partners, making sure they are provided with the necessary information. As co-organizer and vice-president of the FinTech conference in Liechtenstein, she has already been working in the FinTech area for a while. Earlier this year, she has finished her postgraduate studies in Information Systems with Majors in Data Science from the University of Liechtenstein.  Parna “Pari” Youssufzay – Customer Service Pari graduated with a degree in Business Administration with a focus on Marketing. At savedroid, she manages our customer service with heart and mind and is usually the “voice of savedroid”. Additionally, she supports our recruiting and the creation of social media content.  Page 22 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Stefan Telin – Customer Service Stefan supports us primarily in the areas of Customer Service, for which he is predestined by his previous work experience in customer service and his degree in International Finance.  Please also have a look at our “Meet the savedroid Team” video:  https://youtu.be/OyXLkc4mOHc  Page 23 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  2.4.3. Supervisory Board The savedroid Management Board is supported by our trusted serial entrepreneurs and Fintech evangelists in our Supervisory Board. Debjit “Debu” D. Chaudhuri (Chairman) Debu founded Infosys in Germany in 1999 and built up a large (>1,500 consultants), profitable and sustainable business with leading German (e.g., 15 Dax) clients. He advised and implemented business strategies with industry leaders in Spain, Italy, Switzerland and the Netherlands and founded the business in Spain and Italy. He is an entrepreneur and an investor (Family Office) since 2009 in sustainability, digital and machine learning related businesses.  Robert Hable Robert if founder and managing director at 2iQ Research, an enterprise specialized in Directors Dealing/Insider Transactions Data Analysis. 2iQ is developing alpha generating tools, quantitative long and short models based on an equity market neutral, strict scientific approach. Robert is engaged in the Cryptocurrency community right from the beginning and is a real expert.  Jochen Siegert Jochen is an experienced Chief Operations Officer at Traxpay, with a demonstrated history of working in the banking and payment industry, e.g., at PayPal and MasterCard, among others. He is a strong business development professional, skilled in Cryptocurrencies, Single Euro Payments Area (SEPA), Monetization, Banking, Electronic Payments, and E-commerce. Jochen is an internationally recognized FinTech and digitalization influencer, keynote speaker, blogger and podcaster.  Page 24 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  2.4.4. Advisors Prof. Danko Nikolic – AI-Kindergarten Brain and mind scientist and AI practitioner and visionary. He's foremost interested in i) closing the mind-body explanatory gap, and ii) using that knowledge to improve machine learning and AI. His work on brain research at Max Planck led him to develop the theory of practopoiesis. From there he proposed the concept of AI-Kindergarten -- a method for creation of biological-like artificial intelligence. Also, a few years back he introduced the concept of ideasthesia. Most recently, he's active in applied machine learning and AI.  Meinhard Benn – Blockchain Entrepreneur Meinhard founded SatoshiPay, a blockchain-based nanopayments provider aiming to fundamentally change the way web content is monetized. As open source developer, social entrepreneur and avid shoestring traveler with 50 countries under his belt, Meinhard experienced the need for an open, global and decentralized payment technology first-hand. He already experimented with different alternative payment systems for years when Bitcoin and its ground-breaking blockchain technology crossed his path in 2011. As developer, entrepreneur and speaker he pushes the blockchain ecosystem forward ever since.  Kęstutis Gardžiulis – Co-Founder & CIO at Etronika Kęstutis Gardžiulis is a Co-Founder and Chief Innovation Officer at ETRONIKA. He has over 20 years’ of experience in FinTech, digital banking, smart retail, on-line fraud detection and digital identity. His teams got multiple international awards for digital banking solutions and they were among the first in the world to implement the commercial mobile electronic signature solution which became the backbone of the national wireless PKI infrastructure in Lithuania. Kęstutis is also known for experimenting with natural user interface-based online banking prototypes. He acted as the CEO of ETRONIKA for many years, he also served as the vice-president of Lithuanian ICT association and he has previous experience in banking sector and voice-recognition for forensic examination. Currently, Kęstutis is an active enthusiast and contributor to FinTech and Blockchain communities, speaker in various international forums on the future of finance and the challenges of the industry 4.0. He holds a degree in Physics (Quantum Electronics) and Visuals Arts, and he has a keen interest in science, technology, AI, space, TED conferences and semantic web.  Page 25 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Alfred Schorno – Managing Partner 360T Alfred Schorno is managing partner of 360T since November 2003 and member of the Group Executive Board. Starting his investment banking career in 1978, he spent a total of 18 years with UBS trading and selling foreign exchange products, derivatives, money market instruments and bonds. During his time with UBS, Alfred was located in Zurich, Geneva, New York and latterly spent several years as treasurer of UBS Luxembourg. He then moved to Commerzbank / Frankfurt and spent five years leading the global sales and trading team as Global Head for FX. Prior to joining 360T, Alfred was COO at Commerzbank’s centralized portfolio management and research unit. He also served as an executive committee member of EBS Broking Services Ltd./ London for several years.  Oliver Naegele – Founding Member of Germany’s Association for Blockchain For more than 15 years, Oliver has been an IT specialist and consultant for security, enterprise java, intranet, extranet, infrastructure, databases and virtualization. In this broad fields, Oliver has a acquired a deep understanding of digital identity management and distributed authority. Additionally, he is the initiator of the HELIX LABs in Frankfurt am Main and of the German wide network FinTech Headquarter. He is often asked to speak for conferences, events and other gatherings of the startup, financial and IT communities. Oliver also engages to spread the word about blockchain and our digital future in general. Therefore, he is one of the founding members of Germany’s association for blockchain. This chamber is supposed to represent the interests of the German blockchain community to politicians.  Henri de Jong – Head of Business Development Quantoz Henri is responsible for business development at the blockchain company Quantoz. He has a track record of more than 25 years of bringing new technologies to the market, like the first route planning systems, digital road maps and electronic publications. Early 2014, Henri was invited by his former colleagues to join Quantoz. Blockchain Technology brings together Henri’s interest in self organization and software algorithms. His focus now is on developing Quasar, the Quantoz Digital Cash solution. Quasar delivers the infrastructure for instant micro payments between enterprises (unbanked), consumers and the Internet of Things (IoT), compliant with regulation.  Kilian Thalhammer – Managing Partner “Payment and Banking” Kilian has over 15 years’ experience in Payment, FinTech, eCommerce, and Loyalty, which makes him a real expert when it comes to serious product, strategy, and business development issues. After being Director Solutions for the Swiss Post, he joined RatePay (Otto Group) as CPO and was global CPO of PAYMILL (Rocket Internet) until 2014. Kilian works as a Consultant and Business Angel (among others, Fincompare, Forexfix, Lodgify, Loyaltyprime) in the Fintech-Field. He is also Co-Founder of the well-known German FinTech blog “Payment and Banking”.  Page 26 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Dr. Michael Rundshagen – Co-Founder Traxpay Michael is VP at Cognizant where he leads the consulting business in Germany, Austria and Switzerland. He has more than 25 years’ experience in management and strategy consulting and in leading numerous transformation programs for enterprises in various sectors. Michael was lead of strategy consulting ASG at Accenture. As Co-Founder of Traxpay and Business Angel, he is engaged in multiple start-ups with focus on AI, FinTech and Digitalization.  Dr. Matthias Hirtschulz – Senior Manager d-fine Matthias Hirtschulz is Senior Manager at d-fine. He has more than seven years’ experience in delivering innovations in the financial industry. As founding member of d-fine Next, Matthias drives blockchain-related projects for large and small clients. Matthias is also a mentor of the Accelerator Frankfurt.  Roland Klaus – Journalist for Financial Markets Roland is a freelance journalist with extensive knowledge in TV reporting about financial markets and the corporate landscape in Germany. Having worked as on-air reporter from the Frankfurt stock market for CNBC Europe, N24, and n-tv, he's ready to go for live coverage, both in German and English. He has a significant track record in moderating events and hosting discussion panels for the likes of Deutsche Bank, Dekabank, NordLB and other major companies. Besides that, he offers executive coaching for all those that want to prepare for an interview appearance, especially in radio and television.  Michael Hübl – Entrepreneur & Co-Founder flinc Michael Hübl is one of the most ambitious startup personalities in Germany. He founded his first company at the age of 17. He was one of the top 100 internet personalities in Germany in 2012. His current startup “flinc” is the world’s first dynamic ridesharing service. Users can share rides everywhere and in real-time, as if they were looking at the schedule at the bus stop. His vision is a new mobility system that is utilized by the community to share their rides by using existing resources. flinc was nominated as best startup for the T3N Web Awards and Techcrunch’s “The Europas” and was recently acquired by the leading German car manufacturer Daimler.  Page 27 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Anders Indset – Business Philosopher Anders Indset is one of Europe’s leading Business Philosophers offering a new perspective on the “art of thinking”. By bridging the philosophy of the past with the technology and science of tomorrow, he shows how leaders can cope with the 21st century. His 10 postulates of change provide a framework to cope with our fast-paced world. He is a Norwegian-born occasional tech-investor based out of Frankfurt, Germany and a visiting guest lecturer at leading international business schools, founding partner of Frankfurt International Alliance (FIA) and an advisory board member of German Tech Entrepreneurship Center (GTEC) as well as a trusted advisor for Global CEOs and leading politicians.  2.5. Media coverage savedroid has been featured in many trusted magazines, newspapers, blogs and even on radio and TV:  2.6. Awards savedroid and the savedroid solution to automatically save money and optimize personal spend based on self-learning AI technology have been recognized by various prestigious awards, amongst them:  Page 28 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  3. The savedroid Token - SVD We create an ERC20 utility token with a built-in deflation mechanism as suggested by Vitalik Buterin4 to support the future value of the savedroid token. The savedroid token – with the short name “SVD” – can be used to purchase the crypto services within the savedroid ecosystem. Users of our automated crypto saving and investing will use the savedroid token to pay the fees of these crypto services. Once, a savedroid token has been used for a purchase within the savedroid ecosystem we will burn a certain percentage of these tokens to support the future value of the tokens. You can also sell the SVD privately, in supported token exchanges, or hodl [sic] it. Moreover, using the SVDs within the savedroid ecosystem will provide additional user and network benefits to further stimulate savedroid’s growth, as described in more detail below.  3.1. Value and Strategy The intrinsic value of SVDs derives from its functionality within the fast growing savedroid ecosystem of AI based crypto saving and investing. Therefore, the savedroid token offers a highly attractive opportunity and may increase in value based on the financial model as outlined in more detail in chapter 6 below. savedroid, for the first time ever, provides average mass market users with easy access to cryptocurrencies without technological adoption barriers. Within savedroid’s unique AI fueled ecosystem, users will profit from convenient AI-based crypto saving plans in Bitcoin, Ethereum, Bitcoin Cash, Ripple, Litecoin, IOTA, Dash, Stellar, etc. and superior crypto investment opportunities, such as portfolios, derivatives, and ICOs. savedroid will create a smart UX which conveniently auto-converts savings to cryptocurrencies, securely stores them, and makes them easily accessible, tradeable, and spendable. We will offer all of these crypto services for a reasonable fee (we might adjust the fees over time to address latest market developments). Our users will be required to purchase all  4  http://vitalik.ca/general/2017/10/17/moe.html  Page 29 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  consumed crypto services within the savedroid ecosystem using SVD. Hence, more users of the savedroid ecosystem will translate into a higher demand for savedroid tokens. When a user takes his savedroid tokens to pay for a crypto service, we collect his savedroid tokens and automatically burn a percentage of these tokens. Thus, the number of available savedroid tokens will decrease over time. Below the line, this will support the future value of the savedroid token.  In other words, a percentage of the savedroid crypto service fees will automatically be removed from the pool of available SVDs by every single purchase. Thus, the more the savedroid ecosystem is used, the further the supply of SVD will decrease. So, eventually, our user growth will drive an increasing demand for savedroid tokens on the one hand and, at the same time, a decreasing supply of savedroid tokens on the other hand. Hence, the value of the savedroid token depends, among other factors, on our future user growth. A token holder that decides to hodl SVDs, though it may be subjected to exchange market fluctuations, might enjoy a protective layer that links the value of SVD to savedroid’s sustainable crypto ecosystem. This so called intrinsic deflationary mechanism makes SVD a valuable utility token. It should be stated however, that the value of the SVD will be driven by market supply and demand and will not be derived directly from savedroid’s activities. Please note that holding SVD does not trigger the right of redemption cash or cash equivalents or the payment of a fixed or determinable income by savedroid. Token holders must not reasonably expect to generate profits through the sale of SVD to other persons. The savedroid token will grant legal rights of access to crypto services only within the savedroid ecosystem. Any other forms of financial incentives token holders may or may not receive by holding a SVD are outside of savedroid’s control and will only be derived through their own efforts. As the savedroid token is not paying any interest or dividends it also qualifies as a Sharia-compliant Islamic investment opportunity. Page 30 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  The flow of SVDs is illustrated below.  Figure 3: SVD flows  An ITS participant can buy savedroid token during our ITS. He can then either use his SVD to become one of the first savedroid crypto saving and investing users and pay for the crypto service fees or hold on to his savedroid tokens and maybe privately sell them at a token exchange. A savedroid user, who has not participated in our ITS will have two options after our ITS: Either he directly buys SVD at a token exchange or, conveniently, directly within the savedroid mobile apps. The savedroid token can be stored in a wallet and used to pay for the consumed crypto services within the savedroid ecosystem. So, in the end, the price of the savedroid token will always be determined by the actual market supply and demand at the token exchanges. We will adjust our service fees denominated in SVD within the savedroid ecosystem according to the free market price of the savedroid Page 31 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  token. This will ensure that our service fee will stay attractive for all savedroid users over time, so, for example, a higher SVD price will eventually result in a lower service fee as quoted in savedroid token.  Please also have a look at our “The savedroid token” video:  https://youtu.be/CN4UZ1zIr2A  3.2. Token Supply and Specification A total of 10 billion SVDs will be issued on the Ethereum blockchain using a smart contract. SVD will comply with the ERC205 standard and will be freely transferable on the Ethereum blockchain. Ethereum was the natural choice for this project due to its broad adoption and industry-standard for issuing custom digital assets. The compatibility with the ERC20 standard leverages upon the existing infrastructure advantages of the Ethereum ecosystem, namely in terms of development tools, wallets, exchanges, and human resource expertise.  5  https://theethereum.wiki/w/index.php/ERC20_Token_Standard  Page 32 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  savedroid will build a sustainable ecosystem for the crypto community that grows fast with the community and their network effects. Hence, a contribution to SVD is a contribution to the future of the savedroid ecosystem. Our deflationary mechanism creates intrinsic value and a relationship between our ecosystem growth and SVD value. The purpose is to mitigate the effects of earlier ICOs, in which the tokens had a large demand in the first months but which then flattened over time. Whenever SVD is used for savedroid ecosystem purchases and a percentage of it is burned, the remainder will be allocated to the discretionary Monetary Reserve Pool (“MRP”) – held and managed by savedroid. All SVD held by savedroid are allocated to the MRP. savedroid can sell SVD of the MRP on token exchanges or directly to app users within the savedroid ecosystem. The MRP can be used to a) ensure sufficient SVD liquidity and b) prevent an overheated market by selling SVD. The proceeds of such SVD sales will be used for the development and extension of the savedroid ecosystem. Recirculating SVD at market price helps to raise their value while at the same time engaging the whole community. As the savedroid token grows in popularity, more and more users pay for savedroid services and, thus, higher proceeds will allow the continuous development and release of new crypto saving and investing features within the savedroid ecosystem. This will further increase the appeal of the savedroid ecosystem for the crypto community, which will in turn again lead to a further increase in the value of SVD.  3.3. Benefits of SVD Usage for Users and within the Ecosystem SVD will be used by the savedroid ecosystem users to purchase the savedroid’s AI fueled crypto saving and investing services as outlined below. Leveraging technical support processes, savedroid will enable all users, especially those who have not originally participated in the ITS, to conveniently buy SVD on the market through the savedroid ecosystem. Therefore, users will grant savedroid powers of attorney as part of the crypto service purchase to buy SVD at market price from third party exchanges or from the MRP and savedroid will forward these directly into the Page 33 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  crypto service payment channels, i.e. they are used automatically to pay for the crypto services. Since the exchange rate of SVD as measured in fiat and cryptocurrencies may fluctuate over time and thus the value of the savedroid tokens varies, it would be unattractive for new users to keep the price for the crypto services as denominated in SVD flat, because then new savedroid users might have to pay a much higher price for their consumed crypto services than early adopters who have joined the savedroid ecosystem right from the beginning. Therefore, savedroid will adjust the price of crypto services as denominated in SVD to keep the purchasing power of the savedroid token within the savedroid ecosystem approximately constant over time. This means, that, for example, if you will have to pay 100 SVD for a respective crypto service within the savedroid ecosystem after the ITS closing, you might have to pay only 50 SVD a few months later if the value of the savedroid token had doubled. The offered crypto services include: Convenient crypto saving All offered crypto saving services, including savedroid’s self-learning AI saving algorithm, auto-conversion from fiat to crypto and vice versa, and secure wallet storage, can be purchased with SVD. The crypto saving will be available for a monthly base fee plus a percentage transaction fee for every conversion of fiat to cryptocurrencies and from cryptocurrencies to fiat. Smart Saving and Spending in and across Cryptocurrencies These features include AI based switching of savings between cryptocurrencies and conveniently spending crypto savings to fulfill wishes at the preferred merchants. savedroid will charge a percentage transaction fee for smart crypto trading. The savedroid cryptocurrency credit card will be available for a monthly fixed fee. Smart investments in cryptocurrency portfolios, derivatives, and ICOs All AI based crypto investing services, such as smart investments in risk-return optimized cryptocurrency portfolios, derivatives, and ICOs, will also be charged in SVD.  Page 34 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  For these crypto investing services, a monthly base fee and a transaction based percentage fee payable in SVD apply. Exclusive Beta-Program and App-Features for ITS Participants Moreover, ITS participants will receive exclusive access to further privileges and premium features within the savedroid ecosystem free of charge: All savedroid token holders who have bought their SVD during our ITS may participate in savedroid’s beta program and benefit from new features first. Beta program participants will also have much stronger influence on the future development of the savedroid ecosystem by providing early feedback and having exclusive feedback channels directly to our development team. In addition, savedroid will also release premium app features exclusively to SVD owners, such as “ITS community only smooves” (automatic saving rules), limited crypto investment opportunities, fee discounts, and many more.  Page 35 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  The flow of fiat and cryptocurrencies as well as savedroid’s fees are illustrated in the figure below:  Figure 4: Fee Structure  As stated above, these crypto services and fee models are subject to change at the sole discretion of savedroid.  3.4. SVD Price Variations It is important to understand that the price of SVD will vary over time, based on different factors, of which the most prominent ones are listed below: •  change in demand for savedroid’s crypto services,  •  change in competitive landscape,  Page 36 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  •  speculative SVD demand,  •  supply by token holders willing to sell SVD,  •  EUR/ETH exchange rate, and  •  the cost of gas within the Ethereum network.  Please also have a look at our “savedroid Smart Contract” video:  https://youtu.be/5rNl4m8GBpc  Page 37 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  4. The Initial Token Sale 4.1. Token Issuance savedroid is up and running fast for more than 2 years, has a successful product live with a strong track record of the latest AI technology and state-of-the-art UX, a massively growing and highly engaged user base, and relevant revenue streams. Now integrating cryptocurrencies is a logical and very important leap forward to consistently expand the savedroid ecosystem. Further extending our AI technology, integrating B2B partners, growing our talent pool with the right developers, UX designers, marketing professionals, setting up the legal framework, and, of course, driving significant user growth through targeted performance marketing requires substantial additional investments. By launching an Initial Token Sale (“ITS”, often also referred to as ICO – although a token sale is not the offering of coins), we would like to give the crypto community the amazing opportunity to participate in savedroid’s success story, instead of limiting access to a small number of traditional venture capital funds. Besides, an ITS is quick, transparent, and efficient and, thus, empowers savedroid to expand its ecosystem and launch the AI fueled crypto saving and investing features much faster. Issuing SVD also allows savedroid to be backed by participants from all over the world, who will have access to SVD right after its creation and can help to spread the word of savedroid’s new crypto services to facilitate user growth and create additional value for the savedroid token. After the ITS, savedroid will aim to list SVD as soon as possible on relevant token exchanges. We will communicate the exact starting dates of the SVD trading once we have received final confirmation from the token exchanges. Participants can be part of this immense opportunity to allow everyone world-wide6 to have their micro-savings automatically converted in cryptocurrencies and profit from the massive future potential of cryptos.  6  except some countries we may have to exclude due to regulative constraints  Page 38 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  4.2. savedroid ITS Dates The savedroid ITS will be structured in two parts, starting with the Pre-Sale and followed by the Main Sale. The sale price for savedroid token during the ITS is fixed at 1 EUR = 100 SVD, whereas special Pre-Sale discounts will apply as outlined below. Hence, the price of SVD will fluctuate as measured in Bitcoin and Ethereum during the time of our ITS. Starting from the 12th until the 26th of January 2018, we will offer 5% of all SVD (i.e. 5 million SVD) for purchase in a public Pre-Sale with a bonus of 30%. This means that each Pre-Sale participant will receive an extra 30% of the number of tokens he purchases on top for free, so if one buys 1,000 SVD he will receive a total of 1,300 SVD for the same purchase price. From the 9th of February until the 9th of March 2018, we will offer an additional 55% of all SVDs for purchase by the public during our Main Sale (i.e., 5.5 billion SVD). In total we expect to sell a lower double-digit million amount of SVD. The smart contract of the savedroid token will be published on GitHub in January. We will release further details on where to find the smart contract on our Website (ico.savedroid.com). The SVD distribution ends officially on the 9th of March 2018. At this point in time, the sale agreement on the SVD between the ITS participant and savedroid will become legally effective and binding. The token distribution starts latest after the end of the Main Sale. At this point in time, all unsold SVD (out of the 6 billion SVD for sale) will be burned (or not minted in the first place) and after this point in time, the number of SVD will not increase from this effective number of SVD. There will be no further tokens created after the Hard Cap is reached and the total amount of 10 billion SVD will not be exceeded in any case. We will not create any new number of SVDs after the savedroid ITS. All tokens for sale (see SVD Distribution below) that are not sold by the end of the distribution will be burned.  Page 39 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  All reserved tokens (see SVD Distribution below) will be burned proportionally, so that the share of reserved versus token for sale, i.e. the free float of SVD, is fixed. Thus, the maximum number of SVD is determined. In the event that the Min Cap of 50 million sold SVDs is not reached before the end of the Main Sale, all payments will be returned, less charges depending on the payment method (credit card fee or blockchain gas). Each SVD token will be sold for 1 EUR = 100 SVD. There is a minimum purchase amount of 1,000 SVDs per purchase. We will accept a maximum (fixed limit) of 10 million SVDs per ITS participant for purchase in the savedroid ITS. The commencement of trading is planned a few days after the end of the ITS depending on the listing speed of token exchanges. SVD will be transferable on the first day of trading. At the beginning, the tokens will not have any features. The use as a means of payment for crypto services on the savedroid ecosystem will only be possible after the launch of such features according to the roadmap below. The dates of the roadmap depend on external factors such a partner set-up or regulation and can thus not be guaranteed. Metric  Parameter  Start of Pre-Sale Start of Main Sale Token Price Tokens Issued Tokens for Sale Min Cap Min number per subscription Max number per subscription  12 January 2018 09 February 2018 1 EUR = 100 SVD 10,000,000,000 6,000,000,000 50,000,000 1,000 10,000,000  Please note: ITS participants will not see their amounts as ether summaries, as our ITS uses fiat, Bitcoin, and ether via separate subscription systems.  Page 40 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  4.3. SVD Distribution and Allocation Scheme  4.3.1. SVD Distribution 60% of the maximum number of SVD will be offered for sale during the Pre-Sale and the Main Sale. The remaining SVDs are reserved and will be distributed as follows: 10% will be reserved for community initiatives, business development, and expansion. 5% will be allocated for our bounty program for ITS participants. A further 10% will be retained for distribution among advisors, community managers, smart contract developers, and legal. The remaining 15% of SVDs will be distributed to savedroid’s early equity investors and the savedroid team as an incentive to ensure long-term alignment of interests and commitment to the savedroid ecosystem and, hence, the savedroid token and their future value. All reserved tokens (except bounty tokens) will be locked and vest 1/20 per quarter, i.e., linear vesting over a total period of 5 years. This means that, each quarter, the owners of reserved tokens can only sell up to an additional 1/20 of their effectively allocated SVD. All lock-ups will be controlled by a dedicated smart contract (“Vesting Contract”) that will be publicly available.  Page 41 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Figure 5: SVD Distribution  Page 42 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  4.3.2. Proceeds Allocation The funds raised will be used as follows:  Figure 6: Allocation  Page 43 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  4.4. How to Buy SVDs The savedroid token can be bought with the cryptocurrencies of Bitcoin and Ethereum as well as with fiat via bank credit transfer, SOFORT Überweisung, and credit card. The exact procedure of how to buy SVDs is available on our Website (ico.savedroid.com) and in our social media channels. savedroid may adjust the procedure and payment channels during the sale if required, so please always check the latest updates on our Website.  JOIN THE REVOLUTION – NOW!  4.5. Bounty Programs savedroid will offer different promotion programs for the ITS, the first programs are described below.  4.5.1. Referral Program Our Referral Program will grant every token buyer that invites a friend 5% free savedroid tokens of the amount the invited friend purchases. For example, if your invited friend buys 100 savedroid tokens, you will receive 5 savedroid tokens for free. The free referral savedroid tokens will be assigned latest one week after the Closing of the Main Sale of the ITS. You can find more details on our Bounty Program Page on Bitcointalk (https://bitcointalk.org/index.php?topic=2700275).  4.5.2. Social Media Bounty Program Our Social Media Bounty Program will reward you for spreading the word about the savedroid ITS across various channels such as Bitcointalk, Facebook, Twitter, or Page 44 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Youtube. What you need to do to get the reward tokens you can find on on our Bounty Program Page on Bitcointalk (https://bitcointalk.org/index.php?topic=2700275).  4.5.3. Bug Bounty Program Although our smart contracts are already audited as described in chapter 4.6.2, we still set up a Bug Bounty Program to encourage the Blockchain developer community to review our smart contracts. The smart contracts will be published on GitHub in January. We will release further details on where to find the smart contract on our Website (ico.savedroid.com).  4.6. Transparency and Trust  4.6.1. Funds Escrow All payments in cryptocurrencies received for SVDs in connection with the savedroid ITS will be held in escrow by attorney-at-law Axel Hellinger admitted to the German Bar. He is a well-recognized specialist for cryptocurrency and ICO law in Germany and owns an established escrow and tax advising firm (https://hellinger.eu, for his speech on cryptocurrency  law  at  FrOSCon  watch:  Axel Hellinger  https://www.youtube.com/watch?v=50CJIn5wprs).  Mr. Hellinger will hold the ITS funds until the Min Cap is reached and the respective savedroid tokens are minted. If the Min Cap is not reached until the end of the Main Sale, he will ensure all funds will be returned. Funds will not be released to savedroid before the validation period explained below expires and savedroid proves that the respective savedroid tokens are minted and assigned to the buyers throughout the whole Pre-Sale and Main Sale of our ITS. The validation period lasts three days and gives participant the chance to file complaints if they have not received the savedroid tokens in their provided wallet before Mr. Page 45 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Hellinger releases the funds to savedroid. The validation period starts when savedroid notifies the participant via email that his savedroid token have been minted. During the validation period, participants can file claims of missing savedroid tokens to Mr. Hellinger via the contact details on his website (https://hellinger.eu). All payments in fiat will be processed and managed by Bank Frick, a renowned family-run private bank from Lichtenstein which is among the few banks specialized in providing services for ICOs in Europe. Bank Frick will also manage returns, e.g. if the Min Cap is not reached.  4.6.2. Security and Data Protection The savedroid SVD wallet is secured by state-of-the-art security measures following highest industry standards. Every year, the security and data protection measures of savedroid are audited by the TÜV Saarland Group, an internationally well renowned German technical inspection authority. savedroid runs numerous tests in accordance with best-practices and contract source code verification. In addition to that, savedroid conducts a smart contract audit with the help of external partners. This audit will cover the technical aspects of all token sale contracts to ensure that programmed algorithms work as expected. Furthermore, potential vulnerabilities will be identified beforehand to mitigate risks, so funds cannot be easily attacked by third parties. The audit does not guarantee that the code is bugfree, but highlights potential weaknesses and areas of improvement. In addition, we will set up a special Bug Bounty Program as described in chapter 4.5 that is going to encourage the Blockchain developer community to report all bugs and any security issues that may be present. Due to a strong desire to put forth the best systems possible, we are continuously considering to add even more members to our team to cover all relevant areas, like crypto security experts.  Page 46 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  5. Status quo & Roadmap Since its launch, savedroid has built a fast-growing AI platform for saving and spending based on self-learning AI algorithms analyzing and optimizing users' personal finances to fulfil their wishes and maximize their lifestyle. savedroid has delivered strong momentum with 200K+ app downloads, 10M+ saving transactions, high user satisfaction and engagement, a 97% user recommendation rate based on great UX, convinced many blue chip B2B partners as well as trusted media and won prestigious awards, and, furthermore, formed an experienced and diverse team of AI, UX, Tech, Finance, and Marketing experts. Thus, savedroid has built a proven track record of its capabilities to simplify complex savings processes and is now all set and ready to simplify crypto. savedroid is fully committed to continue its path to success and pursue a clear roadmap leveraging its existing core assets of state-of-the-art AI technology and great UX to bring cryptocurrency saving and investing to the masses.  Page 47 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Page 48 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  In even more detail, savedroid has elaborated a quarterly roadmap until 2020 to consistently expand the savedroid ecosystem of AI fueled crypto saving and investing. Our expansion will follow a clear-cut step by step logic in terms of geography from Europe to the world as well as in terms of functionality from convenient crypto saving via smart saving and spending in and across cryptocurrencies to smart investments in cryptocurrency portfolios, derivatives, and ICOs. This will enable savedroid to deliver this one-of-a-kind crypto value proposition.  Page 49 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Page 50 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Page 51 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Page 52 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  6. Financial Model savedroid’s crypto saving and investing business case may offer significant growth opportunities based on maximum simplification, to provide average mass market users with easy access to cryptocurrencies and to drive fast user adoption. Based on these prosperous market opportunities, our financial experts have prepared an Excel spreadsheet for you, which you can use to easily simulate various SVD value scenarios. The Excel sheet can be downloaded on the Website using the following link: https://ico.savedroid.com/savedroid%20tokenmodel.xlsx. Please consider the risks stated below when working with the financial model and interpreting the results. The model is based on a micro-economic model that simulates possible price developments of SVD on third party token exchanges. The model includes parameters for possible future prices and revenues of the savedroid crypto services, assumptions for ITS sales, sell pressure, and savedroid token burn percentage. For more information about the savedroid token economics, please refer to chapter 3 above, where the supply and demand model behind the forecasted SVD price development is explained in detail. In the charts below, we simulated a possible outcome to illustrate the functioning of the model.  SVD Price Scenario €10,00  €9,08 €8,14 €7,20  Value of 100 SVD  €8,00 €6,27 €5,48  €6,00  €4,82  €4,00 €2,00  €1,64 €1,92  €2,08  €2,79 €2,19 €2,27 €2,49  €3,05 €3,27  €3,54  €4,17  €0,95  €0,00 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 2018 2018 2019 2019 2019 2019 2020 2020 2020 2020 2021 2021 2021 2021 2022 2022 2022 2022  Figure 7: Example SVD price developments  Page 53 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Token Demand vs. Sale 14.000.000.000 12.000.000.000 10.000.000.000 8.000.000.000 6.000.000.000 4.000.000.000 2.000.000.000 0 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 2018 2018 2019 2019 2019 2019 2020 2020 2020 2020 2021 2021 2021 2021 2022 2022 2022 2022 Tokens for Sale  Token Demand  Figure 8: Example Token Demand vs. Sale  Burned Tokens 6.400.000.000  140.000.000  6.200.000.000  120.000.000  6.000.000.000  100.000.000  5.800.000.000  80.000.000  5.600.000.000  60.000.000  5.400.000.000 5.200.000.000  40.000.000  5.000.000.000  20.000.000  4.800.000.000  0 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 2018 2018 2019 2019 2019 2019 2020 2020 2020 2020 2021 2021 2021 2021 2022 2022 2022 2022 Total tokens floating  Tokens burnt  Figure 9: Example Token floating vs. burning  Page 54 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  7. Acknowledgements We would like to thank everyone who helped us with their advice, network, feedback, and so much more, to shape the savedroid Initial Token Sale over all this time. We also wish to thank the entire blockchain community for their open mind and their support for bringing cryptocurrencies to the average user and, thus, grow cryptocurrencies way beyond today’s reach.  Page 55 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Legal Note PLEASE READ THE FOLLOWING SECTIONS AS WELL AS THE “TOKEN SALE TERMS & CONDITIONS” (SALE T&C) AND THE “TOKEN REDEMPTION TERMS & CONDITIONS” (REDEMPTION T&C) CAREFULLY. Any agreement as between savedroid and you as a Participant, and in relation to any sale and purchase, of SVD (as referred to in this Whitepaper) is to be governed by only a separate document, the Sale T&C, setting out the applicable terms & conditions. Any agreement as between savedroid and you as a Participant, and in relation to any redemption of SVD (as referred to in this Whitepaper) is to be governed by only a separate document, the Redemption T&C, setting out the applicable terms & conditions. The Redemption T&C are subject to constantly update. In the event of any inconsistencies between any of these T&Cs and this Whitepaper, the former shall prevail.  No part of this Whitepaper is to be reproduced, distributed or disseminated without including this section and the following sections entitled “Legal Nature of SVD and of this Whitepaper”, “Disclaimer of Liability”, “No Representations and Warranties”, “Cautionary Note On Forward-Looking Statements”, “Market and Industry Information and No Consent of Other Persons”, “Terms Used”, “No Advice”, “No Further Information or Update”, “Restrictions On Distribution and Dissemination”, “No Offer of Securities or Registration” and “Risks and Uncertainties”.  LEGAL NATURE OF SVD AND OF THIS WHITEPAPER SVD are not intended to constitute securities or any other form of capital investment product in any jurisdiction. They do not grant any rights in any company, dividends, payment of any interest, profit participation or any other remuneration for the provision of capital. They only represent the claim for performance of the Participant (and its successor) against savedroid in relation to goods and services offered from time to time by savedroid against SVD. Those services are subject to change in the sole discretion of Page 56 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  savedroid and roughly described in this Whitepaper. The purchase of SVD represents the prepayment (advance) of such services. savedroid will deploy all proceeds of sale of the SVD to provide the goods and services offered against SVD when requested by the Participant. SVD cannot be cashed in at savedroid and savedroid is not obliged to redeem any SVD against cash. This Whitepaper does not constitute a prospectus or offer document of any sort. It is not intended to constitute an offer for sale of, nor an invitation for an offer to purchase or subscribe for, SVD in general and securities or any other form of capital investment product in particular. This Whitepaper is also not intended to constitute a solicitation for investment in securities or any other form of capital investment product in any jurisdiction. This Whitepaper, any part thereof and any copy thereof must not be taken or transmitted to any country where distribution or dissemination of this Whitepaper is prohibited or restricted.  DISCLAIMER OF LIABILITY To the maximum extent permitted by the applicable laws, regulations and rules, savedroid shall not be liable for any indirect, special, incidental, consequential or other losses of any kind, in tort, contract or otherwise (including but not limited to loss of revenue, income or profits, and loss of use or data), arising out of or in connection with any acceptance of or reliance on this Whitepaper or any part thereof.  NO REPRESENTATIONS AND WARRANTIES savedroid does not make or purport to make, and hereby disclaims, any representation, warranty or undertaking in any form whatsoever to any entity or person, including any representation, warranty or undertaking in relation to the truth, accuracy and completeness of any of the information set out in this Whitepaper. The only binding documents are the Sale T&C and the Redemption T&C. Page 57 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  CAUTIONARY NOTE ON FORWARD-LOOKING STATEMENTS The statements contained in this Whitepaper may include statements of future expectations and other forward-looking statements that are based on management's current views and assumptions and involve known and unknown risks and uncertainties that could cause actual results, performance or events to differ materially from those expressed or implied in such statements. Some of these statements can be identified by forward-looking terms such as “aim”, “target”, “anticipate”, “believe”, “could”, “estimate”, “expect”, “if”, “intend”, “may”, “plan”, “possible”, “probable”, “project”, “should”, “would”, “will” or other similar terms. However, these terms are not the exclusive means of identifying forward-looking statements. All statements regarding savedroid’s financial position, business strategies, plans and prospects and the future prospects of the industry which savedroid is in are forward-looking statements. These forward-looking statements, including but not limited to statements as to savedroid’s revenue and profitability, prospects, future plans, other expected industry trends and other matters discussed in this Whitepaper regarding savedroid are matters that are not historic facts, but only predictions. These forward-looking statements involve known and unknown risks, uncertainties and other factors that may cause the actual future results, performance or achievements of savedroid to be materially different from any future results, performance or achievements expected, expressed or implied by such forward-looking statements. These factors include, amongst others: a. changes in political, social, economic and stock or cryptocurrency market conditions, and the regulatory environment in the countries in which savedroid conducts its businesses and operations; b. the risk that savedroid may be unable or execute or implement their respective business strategies and future plans; c. changes in interest rates and exchange rates of fiat currencies and cryptocurrencies; d. changes in the anticipated growth strategies and expected internal growth of savedroid; Page 58 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  e. changes in the availability and fees payable to savedroid in connection with its businesses and operations; f. changes in the availability and salaries of employees who are required by savedroid to operate its businesses and operations; g. changes in preferences of customers of savedroid; h. changes in competitive conditions under which savedroid operate, and the ability of savedroid to compete under such conditions; i. changes in the future capital needs of savedroid and the availability of financing and capital to fund such needs; j. war or acts of international or domestic terrorism; k. occurrences of catastrophic events, natural disasters and acts of God that affect the businesses and/or operations of savedroid; l. other factors beyond the control of savedroid; and m. any risk and uncertainties associated with savedroid and their businesses and operations, the SVD and the ITS (each as referred to in the Whitepaper). All forward-looking statements made by or attributable to savedroid or persons acting on behalf of savedroid are expressly qualified in their entirety by such factors. Given that risks and uncertainties that may cause the actual future results, performance or achievements of savedroid to be materially different from that expected, expressed or implied by the forward-looking statements in this Whitepaper, undue reliance must not be placed on these statements. These forward-looking statements are applicable only as of the date of this Whitepaper. Neither savedroid nor any other person represents, warrants and/or undertakes that the actual future results, performance or achievements of savedroid will be as discussed in those forward-looking statements. The actual results, performance or achievements of savedroid may differ materially from those anticipated in these forwardlooking statements.  Page 59 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Nothing contained in this Whitepaper is or may be relied upon as a promise, representation or undertaking as to the future performance or policies of savedroid. Further, savedroid disclaims any responsibility to update any of those forwardlooking statements or publicly announce any revisions to those forward-looking statements to reflect future developments, events or circumstances, even if new information becomes available or other events occur in the future. The same applies to statements made in press releases or in any place accessible by the public and oral statements that may be made by savedroid or its directors, executive officers or employees acting on behalf of savedroid.  MARKET AND INDUSTRY INFORMATION AND NO CONSENT OF OTHER PERSONS This Whitepaper includes market and industry information and forecasts that have been obtained from internal surveys, reports and studies, where appropriate, as well as market research, publicly available information and industry publications. Such surveys, reports, studies, market research, publicly available information and publications generally state that the information that they contain has been obtained from sources believed to be reliable, but there can be no assurance as to the accuracy or completeness of such included information. Save for savedroid and its directors, executive officers and employees, no person has provided his or her consent to the inclusion of his or her name and/or other information attributed or perceived to be attributed to such person in connection therewith in this Whitepaper and no representation, warranty or undertaking is or purported to be provided as to the accuracy or completeness of such information by such person and such persons shall not be obliged to provide any updates on the same. While savedroid has taken reasonable actions to ensure that the information is extracted accurately and in its proper context, savedroid has not conducted any independent review of the information extracted from third party sources, verified the accuracy or completeness of such information or ascertained the underlying economic assumptions relied upon therein. Consequently, neither savedroid nor its directors, Page 60 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  executive officers and employees acting on their behalf makes any representation or warranty as to the accuracy or completeness of such information and shall not be obliged to provide any updates on the same.  TERMS USED To facilitate a better understanding of the SVD being offered for purchase by savedroid, and the businesses and operations of savedroid, certain technical terms and abbreviations, as well as, in certain instances, their descriptions, have been used in this Whitepaper. These descriptions and assigned meanings should not be treated as being definitive of their meanings and may not correspond to standard industry meanings or usage. Words importing the singular shall, where applicable, include the plural and vice versa and words importing the masculine gender shall, where applicable, include the feminine and neuter genders and vice versa. References to persons shall include corporations.  NO ADVICE This Whitepaper does not constitute or form part of any opinion on any advice to sell, or any solicitation of any offer by savedroid to purchase any SVD nor shall it or any part of it nor the fact of its presentation form the basis of, or be relied upon in connection with, any contract or investment decision. No information in this Whitepaper should be considered to be business, legal, financial or tax advice regarding savedroid, the SVD and the ITS (each as referred to in the Whitepaper). Each potential Participant should consult its own legal, financial, tax or other professional adviser regarding savedroid and its businesses and operations, the SVD and the ITS (each as referred to in the Whitepaper). You should be aware that you may be required to bear the financial risk of any purchase of SVD for an indefinite period of time.  Page 61 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  NO FURTHER INFORMATION OR UPDATE No person has been or is authorised to give any information or representation not contained in this Whitepaper in connection with savedroid and its businesses and operations, the SVD and the ITS (each as referred to in the Whitepaper) and, if given, such information or representation must not be relied upon as having been authorized by or on behalf of savedroid. The ITS (as referred to in the Whitepaper) shall not, under any circumstances, constitute a continuing representation or create any suggestion or implication that there has been no change, or development reasonably likely to involve a material change in the affairs, conditions and prospects of savedroid or in any statement of fact or information contained in this Whitepaper since the date hereof.  RESTRICTIONS ON DISTRIBUTION AND DISSEMINATION The distribution or dissemination of this Whitepaper or any part thereof may be prohibited or restricted by the laws, regulatory requirements and rules of any jurisdiction. In the case where any restriction applies, everybody has to inform oneself about, and to observe, any restrictions which are applicable to the possession of this Whitepaper or such part thereof (as the case may be) at one’s own expense and without liability to savedroid. Persons to whom a copy of this Whitepaper has been distributed or disseminated, provided access to or who otherwise have the Whitepaper in their possession shall not circulate it to any other persons, reproduce or otherwise distribute this Whitepaper or any information contained herein for any purpose whatsoever nor permit or cause the same to occur.  NO OFFER OF SECURITIES OR REGISTRATION This Whitepaper does not constitute a prospectus or any other form of capital investment product or offer document of any sort and is not intended to constitute an offer of securities or any other form of capital investment product or a solicitation for investment in securities or any other form of capital investment product in any Page 62 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  jurisdiction. No person is bound to enter into any contract or binding legal commitment and no cryptocurrency or other form of payment is to be accepted on the basis of this Whitepaper. Any agreement in relation to any sale and purchase of SVD (as referred to in this Whitepaper) is to be governed by only the Sale T&C of such agreement and no other document. In the event of any inconsistencies between the Sale T&C and this Whitepaper, the former shall prevail. Persons are not eligible and are not to purchase any SVD in the ITS (as referred to in this Whitepaper) if they are citizens, residents (tax or otherwise) or green card holders of the United States of America, People’s Republic of China or a citizen or resident of the Republic of Singapore, Socialist Republic of Vietnam or resident of a country where American embargoes and sanctions are in force, namely Iran, North Korea, Syria, Sudan, or Cuba or any other geographic area in which the purchase of SVD is prohibited by applicable law, decree, regulation, treaty, or administrative act. This Whitepaper has not been filed with or approved by any regulatory authority. No regulatory authority has examined or approved of any of the information set out in this Whitepaper, nor was such examination or approval sought. No such action has been or will be taken under the laws, regulatory requirements or rules of any jurisdiction. The publication, distribution or dissemination of this Whitepaper does not imply that the applicable laws, regulatory requirements or rules have been complied with.  RISKS AND UNCERTAINTIES The purchase of SVD is associated with significant risks and may lead to the loss of the contributed amount. There is no income, earning or return related to SVD. The tradability of SVD is unclear and might be very limited. savedroid and its businesses and operations, the SVD and the ITS (each as referred to in this Whitepaper) are subject to risks and uncertainties. In particular, economic and political/regulatory risks may influence the SVD and the further business of savedroid and the usage of SVD (up to the non-tradability and worthlessness).  Page 63 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Prospective Participants of SVD (as referred to in this Whitepaper) should carefully consider and evaluate all risks and uncertainties associated with savedroid and its businesses and operations, the SVD and the ITS (each as referred to in the Whitepaper), all information set out in this Whitepaper and the T&Cs prior to any purchase of SVD. If any of such risks and uncertainties develops into actual events, the business, financial condition, results of operations and prospects of savedroid could be materially and adversely affected. In such cases, Participants may lose all or part of the value of the SVD.  IF YOU ARE IN ANY DOUBT AS TO THE ACTION YOU SHOULD TAKE, YOU SHOULD CONSULT YOUR LEGAL, FINANCIAL, TAX OR OTHER PROFESSIONAL ADVISOR(S).  Page 64 of 64 savedroid AG  ICO Whitepaper v1.4  26.01.2018  Tether: Fiat currencies on the Bitcoin blockchain  Abstract​ . A digital token backed by fiat currency provides individuals and organizations with a robust and decentralized method of exchanging value while using a familiar accounting unit. The innovation of blockchains is an auditable and cryptographically secured global ledger. Asset­backed token issuers and other market participants can take advantage of blockchain technology, along with embedded consensus systems, to transact in familiar, less volatile currencies and assets. In order to maintain accountability and to ensure stability in exchange price, we propose a method to maintain a one­to­one reserve ratio between a cryptocurrency token, called tethers, and its associated real­world asset, fiat currency. This method uses the Bitcoin blockchain, Proof of Reserves, and other audit methods to prove that issued tokens are fully backed and reserved at all times.  1  Table of Contents Table of Contents Introduction Technology Stack and Processes Tether Technology Stack Flow of Funds Process Proof of Reserves Process Implementation Weaknesses Main Applications For Exchanges For Individuals For Merchants Future Innovations Multi­sig and Smart Contracts Proof of Solvency Innovations Conclusion Appendix Audit Flaws: Exchanges and Wallets Limitations of Existing Fiat­pegging Systems Market Risk Examples Legal and Compliance Glossary of Terms References  2  Introduction There exists a vast array of assets in the world which people freely choose as a store­of­value, a transactional medium, or an investment. We believe the Bitcoin blockchain is a better technology for transacting, storing, and accounting for these assets. Most estimates measure global wealth around 250 trillion dollars [1] with much of that being held by banks or similar financial institutions. The migration of these assets onto the Bitcoin blockchain represents a proportionally large opportunity.  Bitcoin was created as “an electronic payment system based on cryptographic proof instead of trust, allowing any two willing parties to transact directly with each other without the need for a trusted third party.”[2]. Bitcoin created a new class of digital currency, a decentralized digital currency or cryptocurrency1 .  Some of the primary advantages of cryptocurrencies are: low transaction costs, international borderless transferability and convertibility, trustless ownership and exchange, pseudo­anonymity, real­time transparency, and immunity from legacy banking system problems [3]. Common explanations for the current limited mainstream use of cryptocurrencies include: volatile price swings, inadequate mass­market understanding of the technology, and insufficient ease­of­use for non­technical users.  The idea for asset­pegged cryptocurrencies was initially popularized2 in the Bitcoin community by the Mastercoin white paper authored by J.R. Willett in January 2012[4]. Today, we’re starting to see these ideas built with the likes of BitAssets, Ripple, Omni, Nxt, NuShares/Bits, and others. One should note that all Bitcoin exchanges and wallets (like Coinbase, Bitfinex, and Coinapult) which allow you to hold value as a fiat currency already provide a ​ similar​ service in that users can avoid the volatility (or other traits) of a particular cryptocurrency by selling them for fiat currency, gold, or another asset. Further, almost all types of existing financial institutions, payment providers, etc, which allow you to hold fiat value (or other assets) subsequently provide a similar service. In this white paper we focus on applications wherein the fiat value is stored and transmitted with software that is open­source, cryptographically secure, and uses distributed ledger technology, i.e. a true cryptocurrency.  While the goal of any successful cryptocurrency is to completely eliminate the requirement of trust, each of the aforementioned implementations either rely on a trusted third party or have other technical, market­based, or process­based drawbacks and limitations 3.  1 2 3  For definitions throughout, see ​ Glossary of Terms But has been discussed since Dr. Szabo’s proposed BitGold [5] Summarized in the Appendix, here: ​ Limitations of Existing Fiat­pegging Systems  3  In our solution, fiat­pegged cryptocurrencies are called “tethers”. All tethers will initially4 be issued on the Bitcoin blockchain via the Omni Layer protocol and so they exist as a cryptocurrency token. Each tether unit issued into circulation is backed in a one­to­one ratio (i.e. one Tether USDT is one US dollar) by the corresponding fiat currency unit held in deposit by Hong Kong based Tether Limited. Tethers may be redeemable/exchangeable for the underlying fiat currency pursuant to Tether Limited’s terms of service or, if the holder prefers, the equivalent spot value in Bitcoin. Once a tether has been issued, it can be transferred, stored, spent, etc just like bitcoins or any other cryptocurrency. The fiat currency on reserve has gained the properties of a cryptocurrency and its price is permanently ​ tethered​ to the price of the fiat currency. Our implementation has the following advantages over other fiat­pegged cryptocurrencies:    Tethers exist on the Bitcoin blockchain rather than a less developed/tested “altcoin” blockchain nor within closed­source software running on centralized, private databases.    Tethers can be used just like bitcoins, i.e. in a p2p, pseudo­anonymous, decentralized, cryptographically secure environment.    Tethers can be integrated with merchants, exchanges, and wallets just as easily as Bitcoin or any other cryptocurrencies can be integrated.    Tethers inherit the properties of the Omni Layer protocol which include: a decentralized exchange; browser­based, open­source, wallet encryption; Bitcoin­based transparency, accountability, multi­party security and reporting functions.    Tether Limited employs a simple but effective approach for conducting Proof of Reserves which significantly reduces our counterparty risk as the custodian of the reserve assets.    Tether issuance or redemption will not face any pricing or liquidity constraints. Users can buy or sell as many tethers as they want, quickly, and with very low fees.    Tethers will not face any market risks5 such as Black Swan events, liquidity crunches, etc as reserves are maintained in a one­to­one ratio rather than relying on market forces.    Tether’s one­to­one backing implementation is easier for non­technical users to understand as opposed to collateralization techniques or derivative strategies.  At any given time the balance of fiat currency held in our reserves will be equal to (or greater than) the number of tethers in circulation. This simple configuration most easily supports a reliable Proof of Reserves process; a process which is fundamental to maintaining the price­parity between tethers in circulation and the underlying fiat currency held in reserves. In this paper we provide evidence6 that shows exchange and  4 5 6  ​ore Bitcoin 2.0 protocols will come soon, like Ripple, Nxt, etc M ​ee Appendix, section: ​ S Market Risk Examples See section: ​ ​ Proof of Solvency Process 4  wallet audits (in their current state) are very unreliable (i.e. flaws in Proof of Solvency[6] methods) and instead propose that exchanges and wallets ​ outsource​ the custody of user funds to us via tethers. Users can purchase tethers from Tether.to (our web­wallet) or from supported exchanges such as Bitfinex who support tethers as a deposit and withdrawal method. Users can also transact and store tethers with any Omni Layer enabled wallet like Ambisafe, Holy Transaction or Omni Wallet. Other exchanges, wallets, and merchants are encouraged to reach out to us about integrating tether as a surrogate for traditional fiat payment methods.  We recognize that our implementation isn’t perfectly decentralized7 since Tether Limited must act as a centralized custodian of reserve assets (albeit tethers in circulation exist as a decentralized digital currency). However, we believe this implementation sets the foundation for building future innovations that will eliminate these weaknesses, create a robust platform for new products and services, and support the growth and utility of the Bitcoin blockchain over the long run. Some of these innovations include:    Mobile payment facilitation between users and other parties, including other users and merchants    Instant or near­instant fiat value transfer between decentralized parties (such as multiple exchanges)    Introduction to the use of smart contracts and multi­signature capabilities to further improve the general security process, Proof of Reserves, and enable new features.  Technology Stack and Processes Each tether issued into circulation will be backed in a one­to­one ratio with the equivalent amount of corresponding fiat currency held in reserves by Hong Kong based Tether Limited. As the custodian of the backing asset we are acting as a trusted third party responsible for that asset. This risk is mitigated by a simple implementation that collectively reduces the complexity of conducting both fiat and crypto audits while increasing the security, provability, and transparency of these audits.  Tether Technology Stack The stack has 3 layers, and numerous features, best understood via a diagram  7  See section: ​ Implementation Weaknesses  5  Here is a review of each layer.  1) The first layer is the Bitcoin blockchain. The Tether transactional ledger is embedded in the Bitcoin blockchain as meta­data via the embedded consensus system, Omni.  2) The second layer is the Omni Layer protocol. Omni is a foundational technology that can: a) Grant (create) and revoke (destroy) digital tokens represented as meta­data embedded in the Bitcoin blockchain; in this case, fiat­pegged digital tokens, tethers. b) Track and report the circulation of tethers via Omnichest.info (Omni asset ID #31, for example, represents TetherUSD) and Omnicore API. c)  Enable users to transact and store tethers and other assets/tokens in a: i)  p2p, pseudo­anonymous, cryptographically secure environment.  ii)  open­source, browser­based, encrypted web­wallet: Omni Wallet.  iii)  multi­signature and offline cold storage­supporting system  3) The third layer is Tether Limited, our business entity primarily responsible for: a) Accepting fiat deposits and issuing the corresponding tethers b) Sending fiat withdrawals and revoking the corresponding tethers c)  Custody of the fiat reserves that back all tethers in circulation 6  d) Publicly reporting Proof of Reserves and other audit results e) Initiating and managing integrations with existing Bitcoin/blockchain wallets, exchanges, and merchants f)  Operating Tether.to, a web­wallet which allows users to send, receive, store, and convert tethers conveniently.  Flow of Funds Process There are five steps in the lifecycle of a tether, best understood via a diagram.  Step 1​ ­ User deposits fiat currency into Tether Limited's bank account. Step 2​ ­ Tether Limited generates and credits the user's tether account. Tethers enter circulation. Amount of fiat currency deposited by user = amount of tethers issued to user (i.e. 10k USD deposited = 10k tetherUSD issued).  7  Step 3​ ­ Users transact with tethers8 . The user can transfer, exchange, and store tethers via a p2p open­source, pseudo­anonymous, Bitcoin­based platform.  Step 4​ ­ The user deposits tethers with Tether Limited for redemption into fiat currency. Step 5​ ­ Tether Limited destroys the tethers and sends fiat currency to the user’s bank account.  Users can obtain tethers outside of the aforementioned process via an exchange or another individual. Once a tether enters circulation it can be traded freely between any business or individual. For example, users can purchase tethers from Bitfinex, with more exchanges to follow soon.  The main concept to be conveyed by the Flow of Funds diagram is that Tether Limited is the only party who can issue tethers into circulation (create them) or take them out of circulation (destroy them). This is the main process by which the system solvency is maintained.  Proof of Reserves Process Proof of Solvency, Proof of Reserves, Real­Time Transparency, and other similar phrases have been growing and resonating across the cryptocurrency industry.  Exchange and wallets audits, in their current form, are very unreliable. Insolvency has occurred numerous times in the Bitcoin ecosystem, either via hacks, mismanagement, or outright fraud. Users must be diligent with their exchange selection and vigilant in their use of exchanges. Even then, a savvy user will not be able to fully eliminate the risks. Further, there are exchange users like traders and businesses who must keep non­trivial fiat balances in exchanges at all times. In financial language, this is known as the “counterparty risk” of storing value with a third party.  We believe it’s safe to conclude that exchange and wallet audits in their current form are not very reliable. These processes do not guarantee users that a custodian or exchange is solvent. Although there have been great contributions to improving the exchange audit processes, like the Merkle tree approach[6], major flaws 9  still remain.  Tether’s Proof of Reserves configuration is novel because it simplifies the process of proving that the total number of tethers in circulation (liabilities) are always fully backed by an equal amount of fiat currency held  8  9  See benefits of using tethers in the section: M ​ain Applications See section: ​ ​ Audit Flaws: Exchanges and Wallets  8  in reserve (assets). In our configuration, each tetherUSD in circulation represents one US dollar held in our reserves (i.e. a one­to­one ratio) which means the system is fully reserved when the sum of all tethers in existence (at any point in time) is exactly equal to the balance of USD held in our reserve. Since tethers live on the Bitcoin blockchain, the provability and accounting of tethers at any given point in time is trivial. Conversely, the corresponding total amount of USD held in our reserves is proved by publishing the bank balance and undergoing periodic audits by professionals. Find this implementation further detailed below:    Tether Limited issues all tethers via the Omni Layer protocol. Omni operates on top of the Bitcoin blockchain and therefore all issued, redeemed, and existing tethers, including transactional history, are publicly auditable via the tools provided at Omnichest.info. ○  The Omnichest.info asset ID for tetherUSD is #31. ■    Here is a link: ​ http://omnichest.info/lookupsp.aspx?sp=31  ○  Let the total number of tethers issued under this asset ID be denoted as TUSDissue  ○  Let the total number of tethers redeemed under this asset ID be denoted as TUSDredeem  ○  Let the total number of tethers in circulation at any time be denoted as TUSD ■  TUSD = TUSDissue ­ TUSDredeem  ■  TUSD = “Total Property Tokens” @ ​ http://omnichest.info/lookupsp.aspx?sp=31  Tether Limited has a bank account which will receive and send fiat currency to users who purchase/redeem tethers directly with us. ○  Let the total amount deposited into this account be denoted as DUSDdepo  ○  Let the total amount withdrawn from this account be denoted as DUSDwithd  ○  Let the dollar balance of this bank account be denoted as DUSD ■    DUSD = DUSDdepo ­ DUSDwithd  Each tether issued will be backed by the equivalent amount of currency unit (one tetherUSD equals one dollar). By combining the above crypto and fiat accounting processes, we conclude the “Solvency Equation” for the Tether System. ○  The Solvency Equation is simply TUSD = DUSD.  ○  Every tether issued or redeemed, as publicly recorded by the Bitcoin blockchain will correspond to a deposit or withdrawal of funds from the bank account.  ○  The provability of TUSD relies on the Bitcoin blockchain as discussed previously.  ○  The provability of DUSD will rely on several processes: ■  We publish the bank account balance on our website’s Transparency page.  ■  Professional auditors will regularly verify, sign, and publish our underlying bank balance and financial transfer statement.  9  Users will be able to view this information from our Transparency Page, which will look like:  For clarity, we’d like to acknowledge that the Tether System10 is different than the Tether.to web­wallet in terms of Proof of Reserves. In this paper, we mostly focus on Proof of Reserves for the Tether System; i.e. all tethers in circulation at any point in time. The Tether.to wallet is a consumer facing web­wallet operating on closed­source code and centralized servers. Conducting a Proof of Reserves for this wallet is fundamentally different than what we’ve outlined for the Tether System.  We’re planning the deployment of a PoR­based transparency solution for the Tether.to wallet. We believe it will be the most advanced PoR system in existence today. It overcomes almost all of the challenges outlined in the appendix11 on this topic. Mind you, users can always secure tethers through managing the private keys themselves or through Omni Wallet.  Implementation Weaknesses We understand that our implementation doesn’t immediately create a fully trustless cryptocurrency system. Mainly because users must trust Tether Limited and our corresponding legacy banking institution to be the custodian of the reserve assets. However, almost all exchanges and wallets (assuming they hold USD/fiats) are subject to the same weaknesses. Users of these services are already subject to these risks. Here is a summary of the weaknesses in our approach:  10  11    We could go bankrupt    Our bank could go insolvent    Our bank could freeze or confiscate the funds    We could abscond with the reserve funds  See ​ Glossary of Terms See ​ ​ Audit Flaws: Exchanges and Wallets  10    Re­centralized of risk to a single point of failure  Observe that almost all digital currency exchanges and wallets (assuming they hold USD/fiat) already face many of these challenges. Therefore, users of these services are already subject to these risks. Below we describe how each of these concerns are being addressed.  We could go bankrupt​ ­ In this case, the business entity Tether Limited would go bankrupt but client funds would be safe, and subsequently, all tethers will remain redeemable. Most security breaches on Bitcoin businesses have targeted cryptocurrencies rather than bank accounts. Since all tethers exist on the Bitcoin blockchain they can be stored by individuals directly through securing their own private keys.  Our bank could go insolvent​ ­ This is a risk faced by all users of the legacy financial system and by all exchange operators. Tether Limited currently has accounts with Cathay United Bank and Hwatai Bank in Taiwan, both of whom are aware and confident that Tether’s business model is acceptable. Additional banking partners are being established in other jurisdictions to further mitigate this concern.  Our bank could freeze or confiscate the funds​ ­ Our banks are aware of the nature of Bitcoin and are accepting of Bitcoin businesses. They also provide banking services to some of the largest Bitcoin exchanges globally. The KYC/AML processes we follow are also used by the other digital currency exchanges they currently bank. They have assured us we are in full compliance12 .  We could abscond with the reserve assets​ ­ The corporate charter is public13 as well as the business owners names, locations, and reputations. Ownership of the account is legally bound to the corporate charter. Any transfers in or out of the bank account will have the associated traces and are bound by rigid internal policies.  Re­centralization of risk to a single point of failure​ ­ We have some ideas on how to overcome this and we’ll be sharing them in upcoming blog and product updates. There are many ways to tackle this problem. For now, this initial implementation gets us on the right track to realize these innovations in following versions. By leveraging the platforms we have chosen, we have reduced the centralization risk to one singular responsibility: the creation and redemption of tokens. All other aspects of the system are decentralized.  12 13  ​ee section on ​ S Legal and Compliance​ for more information ​ame as footnote #10 S 11  Main Applications In this section we’ll summarize and discuss the main applications of tethers across the Bitcoin/blockchain ecosystem and for other consumers globally. We break up the beneficiaries into three user groups: Exchanges, Individuals, and Merchants.  The main benefits, applicable to all groups:   Properties of Bitcoin bestowed upon other asset classes    Less volatile, familiar unit of account    World’s assets migrate to the Bitcoin blockchain  For Exchanges Exchange operators understand that accepting fiat deposits and withdrawals using legacy financial systems can be complicated, risky, slow, and expensive. Some of these issues include:    Identifying the right payment providers for your exchange ○  irreversible transactions, fraud protection, lowest fees, etc    Integrating the platform with banks who have no APIs    Liaising with these banks to coordinate compliance, security, and to build trust    Prohibitive costs for small value transfers    3­7 days for international wire transfers to clear    Poor and unfavorable currency conversion fees  By offering tethers, an exchange can relieve themselves of the above complications and gain additional benefits, such as:    Accept crypto­fiats as deposit/withdrawal/storage method rather than using a legacy bank or payment provider ○  Allows users to move fiat in and out of exchange more freely, quickly, cheaply    Outsource fiat custodial risk to Tether Limited ­ just manage cryptos    Easily add other tethered fiat currencies as trading pairs to the platform    Secure customer assets purely through accepted crypto­processes ○  Multi­signature security, cold and hot wallets, HD wallets, etc 12  ○   Conduct audits easier and more securely in a purely crypto environment  Anything one can do with Bitcoin as an exchange can be done with tethers  Exchange users know how risky it can be to hold fiat currencies on an exchange. With the growing number of insolvency events it can be quite dangerous. As mentioned previously, we believe that using tethers exposes exchange users to less counterparty risk than continually holding fiat on exchanges. Additionally, there are other benefits to holding tethers, explained in the next section.  For Individuals There are many types of individual Bitcoin users in the world today. From traders looking to earn profits daily; to long term investors looking to store their Bitcoins securely; to tech­savvy shoppers looking to avoid credit card fees or maintain their privacy; to philosophical users looking to change the world; to those looking to remit payments globally more effectively; to those in third world countries looking for access to financial services for the first time; to developers looking to create new technologies; to all those who have found many uses for Bitcoin. For each of these individuals, we believe tethers are useful in similar ways, like:    Transact in USD/fiat value, pseudo­anonymously, without any middlemen/intermediaries    Cold store USD/fiat value by securing one’s own private keys    Avoid the risk of storing fiat on exchanges ­ move crypto­fiat in and out of exchanges easily    Avoid having to open a fiat bank account to store fiat value    Easily enhance applications that work with bitcoin to also support tether    Anything one can do with Bitcoin as an individual one can also do with tether  For Merchants Merchants want to focus on their business, not on payments. The lack of global, inexpensive, ubiquitous payment solutions continue to plague merchants around the world both large and small. Merchants deserve more. Here are some of the ways tether can help them:    Price goods in USD/fiat value rather than Bitcoin (no moving conversion rates/purchase windows)    Avoid conversion from Bitcoin to USD/fiat and associated fees and processes    Prevent chargebacks, reduce fees, and gain greater privacy    Provide novel services because of fiat­crypto features ○    Microtipping, gift cards, more  Anything one can do with Bitcoin as a merchant one can also do with tether  13  Future Innovations Multi­sig and Smart Contracts Proof of Solvency Innovations  Conclusion Tether constitutes the first Bitcoin­based fiat­pegged cryptocurrencies in existence today. Tether is based on the Bitcoin blockchain, the most secure and well­tested blockchain and public ledger in existence. Tethers are fully reserved in a one­to­one ratio, completely independent of market forces, pricing, or liquidity constraints. Tether has a simple and reliable Proof of Reserves implementation and undergoes regular professional audits. Our underlying banking relationships, compliance, and legal structure provide a secure foundation for us to be the custodian of reserve assets and issuer of tethers. Our team is composed of experienced and respected entrepreneurs from the Bitcoin ecosystem and beyond.  We are focused on arranging integrations with existing businesses in the cryptocurrency space. Business like exchanges, wallets, merchants, and others. We’re already integrated with Bitfinex, HolyTransaction, Omni Wallet, Poloniex, C­CEX, and more to come. Please reach out to us to find out more.  Appendix Audit Flaws: Exchanges and Wallets Here is a summary of the current flaws found in technology­based14 exchange and wallet audits.  In the Merkle tree[6] approach users must manually report that their balances (user’s leaf) have been correctly incorporated in the liability declaration of the exchange (the Merkle hash of the exchange’s database of user balances). This proposed solution works if enough users verify that their account was included in the tree, and in a case where their account is not included this instance would be reported. One potential risk is that an exchange database owner could produce a hash that is not the true representation of  14  As opposed to hiring a professional auditor  14  the database at all; it hashes an incomplete database which would reduce its apparent liabilities to customers, making them appear solvent to a verifying party. Here are some scenarios where a fraudulent exchange would exclude accounts and :  ○  “Bitdust” Accounts: Inactive or low activity accounts would lower the chance that an uninterested user would check or report inconsistencies. In some cases these long­tail accounts could represent a significant percentage of the exchange’s liabilities.  ○  “Colluding Whales” Attack: There is evidence that large Bitcoin traders are operating on various exchanges and moving markets significantly. Such traders need to have capital reserves at the largest exchanges to quickly execute orders. Often, traders choose exchanges that they “trust”. In this way they can be assured that should a hack or liquidity issue arise, they have priority to get their money out. In this case, the exchange and trader could collude to remove the whales account balance from the database before it’s hashed.  ○  Key Rental Attack: To pass the audit, a malicious exchange could rent the private keys to bitcoins they do not own. This would make them appear solvent by increasing their assets without any acknowledgment that those funds were loaned to them. Likewise, they could “borrow” fiat currency to do the same.  ○  There are more attacks not discussed here.  Reaching Statistical Significance (reporting completeness): Even outside of these three attack vectors, a database that has been manipulated may never be detected if a sufficient number of users are not validating balances. The probability of getting 100% of the users to verify balances is likely zero, even with proper incentivization structure for users to verify their balances. Therefore, auditors would need statistical tools to make statements about the validity of an exchange’s database based on sampling frequency, size, and other properties.  Currently users have no way to receive compensation by legal means in case something goes wrong with the exchange. For example, when Mt.Gox closed operations, many users might not have independently recorded their account balances (prints screens, signed messages to themselves, etc) in a way that could conclusively prove to law enforcement that this exchange’s I.O.U’s actually existed. Such users are at the mercy of the exchange to somehow publish a record of that hash tree or original database.  The proposed structure in which these audits would be performed still contains some subtle but important flaws. In particular, the data reporting (hash tree) on the institution’s website gives no guarantee at all to 15  users, as a malicious exchange could publish different states/balances to different groups of users, or retroactively change the state. Thus it is fundamental to publish this data through a secure broadcast channel, e.g. the Bitcoin blockchain.  Privacy is a barrier to entry for the adoption of an automated/open auditing system. While some progress has been made towards better privacy there is no perfect solution yet. Further, to build up an accurate user verified liability space, these users will have to report account balances with the exchange and Bitcoin addresses. Some users likely would not report this information regardless of the incentive, therefore providing cryptographically secure privacy whilst obtaining the reporting goal is paramount.  Time Series: the Merkle tree hash is a single snapshot of the database at a single point in time. Not having a somewhat continuous time series of the database opens significant attack vectors. Additionally, a time series of user reported information would also be required for piecing together the history of any reported incidents of fraud.  Trusted Third Parties: All of the current exchange audits have relied on some “reputable” trusted third party to make some type of verification. In the Coinbase audit [7], that was Andreas Antonopoulos, in the Kraken audit [8], that was Stefan Thomas. If we absolutely must rely on a trusted third party then some audit standards and procedures should ensure this weaknesses is fortified.  Limitations of Existing Fiat­pegging Systems Here’s a list of some of the common drawbacks and limitations of existing fiat­pegging systems.    The systems are based on closed­source software, running on private, centralized databases, fundamentally no different than Paypal or any other existing mass­market retail/institutional asset trading/transfer/storage system.    Decentralized systems that rely on altcoin blockchains which haven’t been stress­tested, developed, or reviewed as closely as other blockchains, like Bitcoin.    Pegging processes that rely on hedging derivative meta­assets, efficient market theory, or collateralization of the underlying asset, wherein liquidity, transferability, security, and other issues can exist.  16    Lack of transparency and audits for the custodian, either crypto, fiat, or relating to their own internal ledgers (same as closed source and centralised databases).    Reliance on legacy banking systems and trusted third parties (bank account owners) as a transfer and settlement mechanism for reserve assets.  Market Risk Examples In the collateralization method, market risk exists because the price of the asset being used as collateral can move in an adverse direction to the price of the asset it’s backing/pegging. This would cause the total value of the collateral to become less than the total value of the issued asset and make the system insolvent. This risk is mitigated by the custodian closing the position before this happens; that is, when the collateral price equals the pegged asset price then the collateral is liquidated (sold on the open market) and the position is closed. A great approach, with merit, and used in many liquid markets across the traditional banking and financial markets. However, as we saw from the global financial crisis, situations can arise in which the acceleration of such events causes a “liquidity crunch” and thus the collateral is unable to be liquidated fast enough to meet trading obligations, subsequently creating losses. With the cryptocurrency markets being so small and volatile, this type of event is much more likely. Additionally, the overall approach suffers from other liquidity and pricing constraints since there must be a sufficient supply of users posting collateral for the creation of the pegged­assets to exist in the first place.  In the derivatives approach, the price of the asset is pegged through entering one of several derivatives strategies, such as: swap strategies, covered and naked options strategies, various futures and forwards strategies. Each strategy has their own strengths and weaknesses, the discussion of which we won’t engage in here. To summarize, each of these pegging processes themselves have similar “market risk” characteristics as the aforementioned collateralization method. It should be noted that the two methods are not mutually exclusive and often paired in a specific trading, hedging, or risk management function at legacy system financial institutions.  Finally, understand that we believe some combination of the above approaches may become a secure, reliable, and generally risk­free process for backing/pegging assets; however, at this point in time, this is not a direction we feel is feasible to take to ensure liquidity and price stability. Further, we believe that a reserve­based approach will always be in existence and complement these other approaches as the entire industry grows. As advances in technology continue, we will evaluate and incorporate any benefits available while maintaining the guarantee of 100% redeemability.  17  Legal and Compliance Tether Limited (“Tether”) is a limited company incorporated pursuant to the Hong Kong Companies Ordinance. It is wholly owned by Tether Holdings Limited, a BVI business company incorporated pursuant to the BVI Business Companies Act, 2004.  Tether is registered as a Money Services Business with the Financial Crimes Enforcement Network of the U.S. Department of the Treasury (MSB Registration Number 31000058542968). Tether is establishing a relationship with a U.S. financial institution for purposes of better servicing Tether users in the United States.  Tether is concluding a principal–agency agreement with RenRenBee Limited (“RenRenBee”). RenRenBee is licensed as a Money Services Operator by the Hong Kong Customs and Excise Department (Licence No. 13­09­01265). Pursuant to the agreement, RenRenBee will provide anti­money laundering compliance work and customer due diligence procedures as agent for Tether as principal.  Through these and other measures, Tether is undertaking customer due diligence, record­keeping, and reporting procedures consistent with U.S. law and with the Hong Kong Anti­Money Laundering and Counter­Terrorist Financing (Financial Institutions) Ordinance.  Tether Limited currently has accounts with Cathay Bank and Hwatai Bank in Taiwan, both of whom are aware and confident that Tether’s business model is acceptable.  These banks are satisfied with our processes and also satisfied that our business operates in accordance with Taiwan off­shore banking regulations, as all of the banks had been requested to check this with their own legal, compliance and head­office before opening accounts (also at our own request). It was our goal from the beginning to have a compliant operation and to provide the maximum level of comfort to our banking partners here. In addition these banks have and are working with other Bitcoin based businesses.  18  Glossary of Terms Digital currency:​ As defined by ​ http://en.wikipedia.org/wiki/Digital_currency Cryptocurrency or decentralized digital currency:​ any type of cryptocurrency that is open­source, cryptographically secure, and uses a distributed ledger. See: ​ http://en.wikipedia.org/wiki/Cryptocurrency  Real­world currency, or fiat currency, or national/sovereign currency:​ all types of currency that are not cryptocurrencies as defined above.  Cryptocurrency system:​ A collection of software and processes primarily created to enable the existence of a cryptocurrency.  Legacy financial system:​ any financial system that is not a cryptocurrency system. Utility­backed digital tokens, a.k.a Dapps:​ A decentralized digital token whose value is derived from the usefulness of its application rather than just being a value transfer system.  Asset­backed/pegged cryptocurrency:​ Any cryptocurrency whose price is pegged to a real­world asset, i.e. its not a “utility­backed” cryptocurrency.  Tether(s):​ a single unit (or multiple units) of fiat­pegged cryptocurrency issued by Tether Limited TetherUSD or tUSD:​ a single unit of crypto­USD issued by Tether Limited TUSD: ​ collective amount of tUSD in circulation at any point in time. Tether System:​ collectively refers to all process and technologies that enable tethers to exist Proof of Reserves:​ The process by which the issuer of any asset­backed decentralized digital token, cryptographically/mathematically proves that all tokens that have been issued are fully reserved and backed by the underlying asset.  19  References [1] https://www.thefinancialist.com/wp­content/uploads/2012/10/2012­GlobalWealthReport­.pdf [2] https://bitcoin.org/bitcoin.pdf [3]http://www.deloitte.com/assets/Dcom­UnitedStates/Local%20Assets/Documents/FSI/us_fsi_BitcointheNe wGoldRush_031814.pdf [4] https://github.com/mastercoin­MSC/spec [5] http://unenumerated.blogspot.com/2005/12/bit­gold.html [6] https://iwilcox.me.uk/2014/proving­bitcoin­reserves [7] http://antonopoulos.com/2014/02/25/coinbase­review/ [8] http://www.coindesk.com/krakens­audit­proves­holds­100­bitcoins­reserve/  20  